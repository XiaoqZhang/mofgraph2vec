2023-02-07 18:19:34.712 | DEBUG    | mofgraph2vec.trainer.sweep:sweep:19 - No sweep id provided, creating new sweep
wandb: WARNING Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.
wandb: WARNING To avoid this, please fix the sweep config schema violations below:
wandb: WARNING   Violation 1. model.sklearn.reg_alpha uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.
wandb: WARNING   Violation 2. model.sklearn.reg_lambda uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.
wandb: WARNING   Violation 3. model.gensim.alpha uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.
wandb: WARNING   Violation 4. model.sklearn.learning_rate uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.
Create sweep with ID: o7w8sqx8
Sweep URL: https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
[2023-02-07 18:19:37,247][wandb.agents.pyagent][INFO] - Starting sweep agent: entity=None, project=None, count=100
wandb: Agent Starting Run: i8x0ge96 with config:
wandb: 	data.data.wl_step: 5
wandb: 	data.nn.batch_size: 970
wandb: 	model.gensim.alpha: 0.1625417797694989
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.6771518936516385
wandb: 	model.gensim.vector_size: 434
wandb: 	model.gensim.window: 15
wandb: 	model.sklearn.learning_rate: 0.001050493736505865
wandb: 	model.sklearn.max_depth: 27
wandb: 	model.sklearn.min_child_weight: 0.038812863048478585
wandb: 	model.sklearn.n_estimators: 404
wandb: 	model.sklearn.num_leaves: 56
wandb: 	model.sklearn.reg_alpha: 0.42096155351713976
wandb: 	model.sklearn.reg_lambda: 0.25521384898096194
wandb: 	model.sklearn.subsample: 0.3014353953493428
wandb: Currently logged in as: xiaoqiz. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_181939-i8x0ge96
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-sweep-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/i8x0ge96
2023-02-07 18:19:48.483 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 5 for sweep.
2023-02-07 18:19:48.483 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 970 for sweep.
2023-02-07 18:19:48.484 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.1625417797694989 for sweep.
2023-02-07 18:19:48.484 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 18:19:48.484 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 18:19:48.484 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6771518936516385 for sweep.
2023-02-07 18:19:48.485 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 434 for sweep.
2023-02-07 18:19:48.485 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 15 for sweep.
2023-02-07 18:19:48.485 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.001050493736505865 for sweep.
2023-02-07 18:19:48.485 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 27 for sweep.
2023-02-07 18:19:48.486 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.038812863048478585 for sweep.
2023-02-07 18:19:48.487 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 404 for sweep.
2023-02-07 18:19:48.487 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 56 for sweep.
2023-02-07 18:19:48.487 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.42096155351713976 for sweep.
2023-02-07 18:19:48.488 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.25521384898096194 for sweep.
2023-02-07 18:19:48.488 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3014353953493428 for sweep.
2023-02-07 18:19:48.488 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:19:48.498 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 5}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_181939-i8x0ge96/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 970, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 434, 'window': 15, 'min_count': 4, 'dm': 0, 'sample': 0.6771518936516385, 'workers': 4, 'alpha': 0.1625417797694989, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 404, 'max_depth': 27, 'num_leaves': 56, 'reg_alpha': 0.42096155351713976, 'reg_lambda': 0.25521384898096194, 'subsample': 0.3014353953493428, 'min_child_weight': 0.038812863048478585, 'n_jobs': 4, 'learning_rate': 0.001050493736505865}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 15/3257 [00:00<00:22, 145.12it/s]  1%|          | 34/3257 [00:00<00:18, 170.45it/s]  2%|‚ñè         | 53/3257 [00:00<00:18, 174.37it/s]  2%|‚ñè         | 73/3257 [00:00<00:17, 183.03it/s]  3%|‚ñé         | 93/3257 [00:00<00:16, 187.48it/s]  3%|‚ñé         | 112/3257 [00:00<00:18, 171.01it/s]  4%|‚ñç         | 130/3257 [00:00<00:26, 116.12it/s]  5%|‚ñç         | 152/3257 [00:01<00:22, 138.31it/s]  5%|‚ñå         | 169/3257 [00:01<00:21, 145.36it/s]  6%|‚ñå         | 187/3257 [00:01<00:20, 153.27it/s]  6%|‚ñã         | 206/3257 [00:01<00:18, 162.29it/s]  7%|‚ñã         | 231/3257 [00:01<00:16, 185.92it/s]  8%|‚ñä         | 252/3257 [00:01<00:15, 191.21it/s]  8%|‚ñä         | 272/3257 [00:01<00:15, 193.49it/s]  9%|‚ñâ         | 297/3257 [00:01<00:14, 207.39it/s] 10%|‚ñâ         | 319/3257 [00:01<00:14, 205.35it/s] 10%|‚ñà         | 340/3257 [00:01<00:14, 200.13it/s] 11%|‚ñà         | 362/3257 [00:02<00:14, 203.46it/s] 12%|‚ñà‚ñè        | 383/3257 [00:02<00:14, 196.50it/s] 12%|‚ñà‚ñè        | 403/3257 [00:02<00:14, 195.10it/s] 13%|‚ñà‚ñé        | 423/3257 [00:02<00:14, 196.15it/s] 14%|‚ñà‚ñé        | 443/3257 [00:02<00:15, 176.99it/s] 14%|‚ñà‚ñç        | 465/3257 [00:02<00:14, 188.59it/s] 15%|‚ñà‚ñç        | 485/3257 [00:02<00:14, 187.67it/s] 16%|‚ñà‚ñå        | 509/3257 [00:02<00:13, 200.84it/s] 16%|‚ñà‚ñã        | 530/3257 [00:02<00:13, 197.98it/s] 17%|‚ñà‚ñã        | 550/3257 [00:03<00:13, 198.54it/s] 18%|‚ñà‚ñä        | 570/3257 [00:03<00:14, 190.28it/s] 18%|‚ñà‚ñä        | 590/3257 [00:03<00:14, 184.48it/s] 19%|‚ñà‚ñâ        | 613/3257 [00:03<00:13, 195.64it/s] 19%|‚ñà‚ñâ        | 633/3257 [00:03<00:13, 196.72it/s] 20%|‚ñà‚ñà        | 653/3257 [00:03<00:13, 186.12it/s] 21%|‚ñà‚ñà        | 672/3257 [00:03<00:14, 180.98it/s] 21%|‚ñà‚ñà        | 691/3257 [00:03<00:14, 172.78it/s] 22%|‚ñà‚ñà‚ñè       | 711/3257 [00:03<00:14, 178.79it/s] 22%|‚ñà‚ñà‚ñè       | 730/3257 [00:04<00:14, 173.43it/s] 23%|‚ñà‚ñà‚ñé       | 748/3257 [00:04<00:14, 170.00it/s] 24%|‚ñà‚ñà‚ñé       | 770/3257 [00:04<00:13, 178.58it/s] 24%|‚ñà‚ñà‚ñç       | 788/3257 [00:04<00:14, 174.58it/s] 25%|‚ñà‚ñà‚ñç       | 808/3257 [00:04<00:13, 181.52it/s] 25%|‚ñà‚ñà‚ñå       | 827/3257 [00:04<00:13, 180.36it/s] 26%|‚ñà‚ñà‚ñå       | 846/3257 [00:04<00:13, 172.43it/s] 27%|‚ñà‚ñà‚ñã       | 866/3257 [00:04<00:13, 179.04it/s] 27%|‚ñà‚ñà‚ñã       | 885/3257 [00:04<00:13, 176.81it/s] 28%|‚ñà‚ñà‚ñä       | 908/3257 [00:05<00:12, 190.54it/s] 28%|‚ñà‚ñà‚ñä       | 928/3257 [00:05<00:12, 191.66it/s] 29%|‚ñà‚ñà‚ñâ       | 948/3257 [00:05<00:12, 180.05it/s] 30%|‚ñà‚ñà‚ñâ       | 967/3257 [00:05<00:12, 180.63it/s] 30%|‚ñà‚ñà‚ñà       | 986/3257 [00:05<00:12, 175.46it/s] 31%|‚ñà‚ñà‚ñà       | 1005/3257 [00:05<00:12, 177.29it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1023/3257 [00:05<00:12, 176.97it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1041/3257 [00:05<00:13, 163.46it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1059/3257 [00:05<00:13, 165.80it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:06<00:13, 166.47it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1098/3257 [00:06<00:12, 167.24it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1115/3257 [00:06<00:12, 165.24it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1132/3257 [00:06<00:13, 157.72it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1148/3257 [00:06<00:13, 157.82it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1168/3257 [00:06<00:12, 168.80it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1185/3257 [00:06<00:13, 156.38it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1201/3257 [00:06<00:13, 150.88it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1218/3257 [00:06<00:13, 155.05it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1241/3257 [00:07<00:11, 172.70it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1259/3257 [00:07<00:11, 170.66it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1277/3257 [00:07<00:12, 164.22it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1294/3257 [00:07<00:12, 159.35it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1313/3257 [00:07<00:11, 164.62it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1331/3257 [00:07<00:11, 166.92it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:07<00:11, 167.15it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1367/3257 [00:07<00:11, 166.89it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:08<00:21, 87.44it/s]  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1403/3257 [00:08<00:17, 104.86it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1424/3257 [00:08<00:14, 125.62it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1443/3257 [00:08<00:13, 138.90it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1465/3257 [00:08<00:11, 158.05it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1484/3257 [00:08<00:10, 162.16it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1506/3257 [00:08<00:09, 176.67it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1526/3257 [00:08<00:10, 169.27it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1545/3257 [00:09<00:10, 165.18it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1565/3257 [00:09<00:09, 169.92it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1583/3257 [00:09<00:09, 168.69it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1603/3257 [00:09<00:09, 175.92it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1621/3257 [00:09<00:09, 175.60it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1639/3257 [00:09<00:09, 175.11it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1657/3257 [00:09<00:09, 169.94it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1675/3257 [00:09<00:09, 164.56it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1692/3257 [00:09<00:09, 163.11it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1711/3257 [00:10<00:09, 170.49it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1729/3257 [00:10<00:09, 162.14it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1746/3257 [00:10<00:09, 163.78it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1765/3257 [00:10<00:08, 171.13it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1783/3257 [00:10<00:08, 167.28it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1800/3257 [00:10<00:08, 166.04it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1817/3257 [00:10<00:08, 164.46it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1834/3257 [00:10<00:08, 163.09it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1852/3257 [00:10<00:08, 167.20it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1873/3257 [00:11<00:07, 177.95it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1891/3257 [00:11<00:07, 172.82it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1911/3257 [00:11<00:07, 179.10it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1929/3257 [00:11<00:07, 170.67it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1954/3257 [00:11<00:06, 191.42it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1974/3257 [00:11<00:06, 192.70it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1994/3257 [00:11<00:06, 188.61it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2013/3257 [00:11<00:06, 186.26it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2032/3257 [00:11<00:06, 185.26it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2051/3257 [00:12<00:07, 158.77it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2068/3257 [00:12<00:07, 153.82it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2087/3257 [00:12<00:07, 161.54it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2104/3257 [00:12<00:07, 162.70it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2121/3257 [00:12<00:07, 159.75it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2140/3257 [00:12<00:06, 165.56it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2157/3257 [00:12<00:06, 165.29it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2177/3257 [00:12<00:06, 173.40it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2195/3257 [00:12<00:06, 173.58it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2213/3257 [00:13<00:06, 164.32it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2233/3257 [00:13<00:05, 171.61it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2251/3257 [00:13<00:05, 171.35it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2269/3257 [00:13<00:05, 172.13it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2287/3257 [00:13<00:05, 168.92it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2304/3257 [00:13<00:05, 162.76it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2325/3257 [00:13<00:05, 174.35it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2347/3257 [00:13<00:04, 185.78it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2367/3257 [00:13<00:04, 189.05it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2388/3257 [00:13<00:04, 193.87it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2408/3257 [00:14<00:04, 187.87it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2428/3257 [00:14<00:04, 189.78it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2448/3257 [00:14<00:04, 177.41it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2472/3257 [00:14<00:04, 193.66it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2492/3257 [00:14<00:03, 192.18it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2514/3257 [00:14<00:03, 197.21it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2535/3257 [00:14<00:03, 200.54it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2556/3257 [00:14<00:03, 188.67it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2576/3257 [00:14<00:03, 180.76it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2595/3257 [00:15<00:03, 179.03it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2620/3257 [00:15<00:03, 196.57it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:15<00:03, 193.59it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2660/3257 [00:15<00:03, 166.54it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2678/3257 [00:15<00:03, 163.92it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2695/3257 [00:15<00:03, 159.86it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2712/3257 [00:16<00:06, 80.13it/s]  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2729/3257 [00:16<00:05, 94.01it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2747/3257 [00:16<00:04, 109.53it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2763/3257 [00:16<00:04, 119.43it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2779/3257 [00:16<00:03, 127.51it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2800/3257 [00:16<00:03, 146.25it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2817/3257 [00:16<00:02, 150.66it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2834/3257 [00:16<00:02, 155.10it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2855/3257 [00:16<00:02, 169.73it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:17<00:01, 192.09it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2901/3257 [00:17<00:02, 176.08it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2922/3257 [00:17<00:01, 185.12it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2942/3257 [00:17<00:01, 186.06it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2962/3257 [00:17<00:01, 175.63it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2980/3257 [00:17<00:01, 172.93it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3002/3257 [00:17<00:01, 182.56it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3021/3257 [00:17<00:01, 179.80it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3043/3257 [00:17<00:01, 190.50it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3066/3257 [00:18<00:00, 201.52it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3087/3257 [00:18<00:00, 198.37it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3111/3257 [00:18<00:00, 208.61it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3132/3257 [00:18<00:00, 207.67it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3153/3257 [00:18<00:00, 197.50it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3173/3257 [00:18<00:00, 193.37it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3193/3257 [00:18<00:00, 192.67it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3213/3257 [00:18<00:00, 185.32it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3236/3257 [00:18<00:00, 197.45it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 199.71it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 170.90it/s]
2023-02-07 18:20:09.441 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:20:09,468][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d434,n5,mc4,s0.677152,t4>', 'datetime': '2023-02-07T18:20:09.443492', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:20:09,469][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:20:09,469][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:20:09,864][gensim.models.doc2vec][INFO] - collected 21699 word types and 3257 unique tags from a corpus of 3257 examples and 4367244 words
[2023-02-07 18:20:09,865][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:20:09,907][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 14837 unique words (68.38% of original 21699, drops 6862)', 'datetime': '2023-02-07T18:20:09.906952', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:20:09,907][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 4356054 word corpus (99.74% of original 4367244, drops 11190)', 'datetime': '2023-02-07T18:20:09.907355', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:20:09,957][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 21699 items
[2023-02-07 18:20:09,958][gensim.models.word2vec][INFO] - sample=0.677152 downsamples 0 most-common words
[2023-02-07 18:20:09,958][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4356054 word corpus (100.0%% of prior 4356054)', 'datetime': '2023-02-07T18:20:09.958624', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:20:10,044][gensim.models.word2vec][INFO] - estimated required memory for 14837 words and 434 dimensions: 65238116 bytes
[2023-02-07 18:20:10,045][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:20:10,079][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 14837 vocabulary and 434 features, using sg=1 hs=0 sample=0.6771518936516385 negative=5 window=15 shrink_windows=True', 'datetime': '2023-02-07T18:20:10.079396', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:20:11,089][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 37.55% examples, 1673321 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:20:12,091][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 88.70% examples, 1932173 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:20:12,311][gensim.models.word2vec][INFO] - EPOCH 0: training on 4367244 raw words (4357583 effective words) took 2.2s, 1955250 effective words/s
[2023-02-07 18:20:13,319][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 52.63% examples, 2327015 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:20:14,188][gensim.models.word2vec][INFO] - EPOCH 1: training on 4367244 raw words (4357583 effective words) took 1.9s, 2324110 effective words/s
[2023-02-07 18:20:15,191][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 48.17% examples, 2142495 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:20:16,194][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 99.02% examples, 2155715 words/s, in_qsize 5, out_qsize 0
[2023-02-07 18:20:16,206][gensim.models.word2vec][INFO] - EPOCH 2: training on 4367244 raw words (4357583 effective words) took 2.0s, 2161480 effective words/s
[2023-02-07 18:20:17,209][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 39.79% examples, 1785711 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:20:18,211][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 91.40% examples, 2004885 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:20:18,362][gensim.models.word2vec][INFO] - EPOCH 3: training on 4367244 raw words (4357583 effective words) took 2.2s, 2022923 effective words/s
[2023-02-07 18:20:19,369][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 50.17% examples, 2223010 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:20:20,314][gensim.models.word2vec][INFO] - EPOCH 4: training on 4367244 raw words (4357583 effective words) took 1.9s, 2235571 effective words/s
[2023-02-07 18:20:21,318][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 51.43% examples, 2291255 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:20:22,239][gensim.models.word2vec][INFO] - EPOCH 5: training on 4367244 raw words (4357583 effective words) took 1.9s, 2267926 effective words/s
[2023-02-07 18:20:23,243][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 49.40% examples, 2192165 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:20:24,181][gensim.models.word2vec][INFO] - EPOCH 6: training on 4367244 raw words (4357583 effective words) took 1.9s, 2246213 effective words/s
[2023-02-07 18:20:25,193][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 50.88% examples, 2244231 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:20:26,125][gensim.models.word2vec][INFO] - EPOCH 7: training on 4367244 raw words (4357583 effective words) took 1.9s, 2243933 effective words/s
[2023-02-07 18:20:27,130][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 50.29% examples, 2224391 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:20:28,066][gensim.models.word2vec][INFO] - EPOCH 8: training on 4367244 raw words (4357583 effective words) took 1.9s, 2247617 effective words/s
[2023-02-07 18:20:29,073][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 40.93% examples, 1828554 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:20:30,078][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 94.20% examples, 2050705 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:20:30,180][gensim.models.word2vec][INFO] - EPOCH 9: training on 4367244 raw words (4357583 effective words) took 2.1s, 2065072 effective words/s
[2023-02-07 18:20:31,183][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 50.54% examples, 2244820 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:20:32,108][gensim.models.word2vec][INFO] - EPOCH 10: training on 4367244 raw words (4357583 effective words) took 1.9s, 2262874 effective words/s
[2023-02-07 18:20:33,110][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 51.27% examples, 2284455 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:20:34,017][gensim.models.word2vec][INFO] - EPOCH 11: training on 4367244 raw words (4357583 effective words) took 1.9s, 2285242 effective words/s
[2023-02-07 18:20:35,019][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 40.93% examples, 1833298 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:20:36,027][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 93.00% examples, 2030806 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:20:36,151][gensim.models.word2vec][INFO] - EPOCH 12: training on 4367244 raw words (4357583 effective words) took 2.1s, 2044059 effective words/s
[2023-02-07 18:20:37,159][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 50.17% examples, 2218654 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:20:38,111][gensim.models.word2vec][INFO] - EPOCH 13: training on 4367244 raw words (4357583 effective words) took 2.0s, 2225322 effective words/s
[2023-02-07 18:20:39,114][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 50.35% examples, 2228281 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:20:40,084][gensim.models.word2vec][INFO] - EPOCH 14: training on 4367244 raw words (4357583 effective words) took 2.0s, 2210781 effective words/s
[2023-02-07 18:20:40,085][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 65508660 raw words (65363745 effective words) took 30.0s, 2178384 effective words/s', 'datetime': '2023-02-07T18:20:40.085465', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:20:40.085 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:20:43,112][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_181939-i8x0ge96/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:20:43.112736', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:20:43,113][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:20:43,212][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_181939-i8x0ge96/files/../tmp/embedding_model.pt
2023-02-07 18:20:43.212 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:20:45.666 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:20:46.542 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:20:49.521 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2634849270909143, 'test_mae': 1.1195213909623745, 'test_r2': -2.459530720938428}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.93
wandb: percentage 0.31624
wandb:   test_mae 1.11952
wandb:   test_mse 2.26348
wandb:    test_r2 -2.45953
wandb: 
wandb: üöÄ View run fast-sweep-1 at: https://wandb.ai/xiaoqiz/mof2vec/runs/i8x0ge96
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_181939-i8x0ge96/logs
wandb: Agent Starting Run: 7t8kiqd6 with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 270
wandb: 	model.gensim.alpha: 0.12188773643850714
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.6628777052999235
wandb: 	model.gensim.vector_size: 341
wandb: 	model.gensim.window: 18
wandb: 	model.sklearn.learning_rate: 0.03894902414198146
wandb: 	model.sklearn.max_depth: 57
wandb: 	model.sklearn.min_child_weight: 0.018433665416503858
wandb: 	model.sklearn.n_estimators: 4742
wandb: 	model.sklearn.num_leaves: 270
wandb: 	model.sklearn.reg_alpha: 0.6459871177957479
wandb: 	model.sklearn.reg_lambda: 0.003700482670400724
wandb: 	model.sklearn.subsample: 0.8223219344390709
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182100-7t8kiqd6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/7t8kiqd6
2023-02-07 18:21:09.889 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 18:21:09.889 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 270 for sweep.
2023-02-07 18:21:09.890 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.12188773643850714 for sweep.
2023-02-07 18:21:09.890 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 18:21:09.890 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 18:21:09.890 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6628777052999235 for sweep.
2023-02-07 18:21:09.891 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 341 for sweep.
2023-02-07 18:21:09.891 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 18 for sweep.
2023-02-07 18:21:09.891 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.03894902414198146 for sweep.
2023-02-07 18:21:09.891 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 57 for sweep.
2023-02-07 18:21:09.891 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.018433665416503858 for sweep.
2023-02-07 18:21:09.892 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4742 for sweep.
2023-02-07 18:21:09.892 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 270 for sweep.
2023-02-07 18:21:09.892 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.6459871177957479 for sweep.
2023-02-07 18:21:09.892 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.003700482670400724 for sweep.
2023-02-07 18:21:09.893 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8223219344390709 for sweep.
2023-02-07 18:21:09.893 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:21:09.902 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182100-7t8kiqd6/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 270, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 341, 'window': 18, 'min_count': 1, 'dm': 0, 'sample': 0.6628777052999235, 'workers': 4, 'alpha': 0.12188773643850714, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4742, 'max_depth': 57, 'num_leaves': 270, 'reg_alpha': 0.6459871177957479, 'reg_lambda': 0.003700482670400724, 'subsample': 0.8223219344390709, 'min_child_weight': 0.018433665416503858, 'n_jobs': 4, 'learning_rate': 0.03894902414198146}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 21/3257 [00:00<00:15, 209.89it/s]  1%|‚ñè         | 42/3257 [00:00<00:15, 208.27it/s]  2%|‚ñè         | 67/3257 [00:00<00:14, 213.64it/s]  3%|‚ñé         | 91/3257 [00:00<00:14, 222.91it/s]  4%|‚ñé         | 114/3257 [00:00<00:14, 211.01it/s]  4%|‚ñç         | 136/3257 [00:00<00:15, 200.36it/s]  5%|‚ñç         | 157/3257 [00:00<00:15, 199.66it/s]  5%|‚ñå         | 178/3257 [00:00<00:15, 197.22it/s]  6%|‚ñå         | 201/3257 [00:00<00:15, 200.35it/s]  7%|‚ñã         | 229/3257 [00:01<00:13, 222.44it/s]  8%|‚ñä         | 252/3257 [00:01<00:13, 221.12it/s]  8%|‚ñä         | 275/3257 [00:01<00:13, 221.61it/s]  9%|‚ñâ         | 300/3257 [00:01<00:12, 228.13it/s] 10%|‚ñà         | 326/3257 [00:01<00:12, 237.34it/s] 11%|‚ñà         | 350/3257 [00:01<00:12, 226.41it/s] 12%|‚ñà‚ñè        | 375/3257 [00:01<00:12, 225.21it/s] 12%|‚ñà‚ñè        | 398/3257 [00:01<00:13, 218.00it/s] 13%|‚ñà‚ñé        | 421/3257 [00:01<00:12, 220.31it/s] 14%|‚ñà‚ñé        | 444/3257 [00:02<00:13, 201.79it/s] 14%|‚ñà‚ñç        | 468/3257 [00:02<00:13, 211.01it/s] 15%|‚ñà‚ñå        | 490/3257 [00:02<00:13, 211.30it/s] 16%|‚ñà‚ñå        | 515/3257 [00:02<00:12, 220.66it/s] 17%|‚ñà‚ñã        | 538/3257 [00:02<00:12, 217.45it/s] 17%|‚ñà‚ñã        | 560/3257 [00:02<00:13, 206.28it/s] 18%|‚ñà‚ñä        | 581/3257 [00:02<00:21, 126.76it/s] 19%|‚ñà‚ñä        | 605/3257 [00:03<00:17, 148.35it/s] 19%|‚ñà‚ñâ        | 627/3257 [00:03<00:16, 163.68it/s] 20%|‚ñà‚ñâ        | 649/3257 [00:03<00:14, 174.85it/s] 21%|‚ñà‚ñà        | 670/3257 [00:03<00:14, 182.16it/s] 21%|‚ñà‚ñà        | 691/3257 [00:03<00:13, 185.72it/s] 22%|‚ñà‚ñà‚ñè       | 715/3257 [00:03<00:12, 200.08it/s] 23%|‚ñà‚ñà‚ñé       | 737/3257 [00:03<00:13, 191.21it/s] 23%|‚ñà‚ñà‚ñé       | 758/3257 [00:03<00:12, 194.16it/s] 24%|‚ñà‚ñà‚ñç       | 779/3257 [00:03<00:12, 196.28it/s] 25%|‚ñà‚ñà‚ñç       | 803/3257 [00:04<00:11, 208.25it/s] 25%|‚ñà‚ñà‚ñå       | 825/3257 [00:04<00:12, 195.79it/s] 26%|‚ñà‚ñà‚ñå       | 845/3257 [00:04<00:12, 192.84it/s] 27%|‚ñà‚ñà‚ñã       | 866/3257 [00:04<00:12, 196.14it/s] 27%|‚ñà‚ñà‚ñã       | 886/3257 [00:04<00:12, 192.33it/s] 28%|‚ñà‚ñà‚ñä       | 909/3257 [00:04<00:11, 202.27it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:04<00:11, 206.72it/s] 29%|‚ñà‚ñà‚ñâ       | 954/3257 [00:04<00:10, 213.30it/s] 30%|‚ñà‚ñà‚ñâ       | 977/3257 [00:04<00:10, 216.57it/s] 31%|‚ñà‚ñà‚ñà       | 999/3257 [00:04<00:10, 206.58it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1022/3257 [00:05<00:10, 210.43it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1044/3257 [00:05<00:11, 198.42it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:05<00:10, 203.40it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1088/3257 [00:05<00:10, 201.16it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1109/3257 [00:05<00:10, 199.69it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1130/3257 [00:05<00:11, 192.02it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1150/3257 [00:05<00:10, 192.03it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:05<00:10, 202.07it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1194/3257 [00:05<00:10, 189.29it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1214/3257 [00:06<00:10, 188.86it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1241/3257 [00:06<00:09, 207.58it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1262/3257 [00:06<00:09, 207.67it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1283/3257 [00:06<00:09, 197.58it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1303/3257 [00:06<00:09, 195.68it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1325/3257 [00:06<00:09, 202.27it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1348/3257 [00:06<00:09, 209.93it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:06<00:09, 203.25it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1391/3257 [00:06<00:09, 202.23it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1419/3257 [00:07<00:08, 223.19it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1442/3257 [00:07<00:08, 219.18it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1465/3257 [00:07<00:08, 221.89it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1488/3257 [00:07<00:08, 215.04it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:07<00:07, 222.66it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1536/3257 [00:07<00:08, 208.85it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1558/3257 [00:07<00:08, 206.90it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1579/3257 [00:07<00:08, 203.04it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1603/3257 [00:07<00:07, 212.56it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1625/3257 [00:08<00:07, 212.83it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1647/3257 [00:08<00:07, 205.80it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1668/3257 [00:08<00:07, 199.55it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1689/3257 [00:08<00:13, 112.79it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1713/3257 [00:08<00:11, 135.24it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:08<00:10, 142.54it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1752/3257 [00:08<00:09, 157.55it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1776/3257 [00:09<00:08, 176.70it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1800/3257 [00:09<00:07, 192.74it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1823/3257 [00:09<00:07, 199.65it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1845/3257 [00:09<00:07, 198.33it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:09<00:06, 203.65it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1891/3257 [00:09<00:06, 206.73it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:09<00:06, 213.43it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1941/3257 [00:09<00:05, 223.61it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1970/3257 [00:09<00:05, 242.06it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1995/3257 [00:10<00:05, 231.61it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2019/3257 [00:10<00:05, 224.07it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2042/3257 [00:10<00:05, 217.10it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2064/3257 [00:10<00:05, 208.56it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2086/3257 [00:10<00:05, 210.15it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2108/3257 [00:10<00:05, 210.51it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2130/3257 [00:10<00:05, 200.68it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2151/3257 [00:10<00:05, 201.77it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2176/3257 [00:10<00:05, 214.25it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2198/3257 [00:10<00:05, 211.08it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2220/3257 [00:11<00:05, 203.31it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2241/3257 [00:11<00:05, 196.52it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2262/3257 [00:11<00:04, 199.59it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2283/3257 [00:11<00:04, 199.82it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2304/3257 [00:11<00:04, 199.97it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2330/3257 [00:11<00:04, 215.27it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2356/3257 [00:11<00:03, 227.42it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2379/3257 [00:11<00:04, 219.20it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2403/3257 [00:11<00:03, 223.15it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2426/3257 [00:12<00:03, 212.11it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2448/3257 [00:12<00:04, 195.85it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2474/3257 [00:12<00:03, 211.49it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2496/3257 [00:12<00:03, 213.61it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2519/3257 [00:12<00:03, 217.25it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2541/3257 [00:12<00:03, 215.00it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2563/3257 [00:12<00:03, 208.34it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2584/3257 [00:12<00:03, 199.54it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2607/3257 [00:12<00:03, 207.31it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2632/3257 [00:13<00:02, 217.09it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2654/3257 [00:13<00:02, 202.12it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2675/3257 [00:13<00:02, 199.95it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2696/3257 [00:13<00:02, 196.06it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2716/3257 [00:13<00:02, 185.94it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2741/3257 [00:13<00:02, 202.61it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2762/3257 [00:13<00:02, 187.70it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2782/3257 [00:13<00:02, 186.50it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2803/3257 [00:13<00:02, 190.17it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2823/3257 [00:14<00:02, 190.76it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2843/3257 [00:14<00:02, 189.95it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2870/3257 [00:14<00:01, 211.17it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2892/3257 [00:14<00:01, 208.32it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2913/3257 [00:14<00:01, 208.20it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2934/3257 [00:14<00:01, 207.26it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2955/3257 [00:14<00:01, 202.20it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:14<00:01, 202.06it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2999/3257 [00:14<00:01, 209.50it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3020/3257 [00:15<00:01, 206.47it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3046/3257 [00:15<00:00, 219.56it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3072/3257 [00:15<00:00, 230.19it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3096/3257 [00:15<00:01, 118.54it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3121/3257 [00:15<00:00, 141.35it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3141/3257 [00:15<00:00, 151.10it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3161/3257 [00:15<00:00, 161.46it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3181/3257 [00:16<00:00, 165.36it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3205/3257 [00:16<00:00, 183.56it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3226/3257 [00:16<00:00, 187.03it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:16<00:00, 200.55it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 198.43it/s]
2023-02-07 18:21:26.844 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:21:26,845][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d341,n5,s0.662878,t4>', 'datetime': '2023-02-07T18:21:26.845861', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:21:26,846][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:21:26,846][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:21:27,162][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 18:21:27,163][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:21:27,195][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 13061 unique words (100.00% of original 13061, drops 0)', 'datetime': '2023-02-07T18:21:27.195449', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:21:27,196][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 3639370 word corpus (100.00% of original 3639370, drops 0)', 'datetime': '2023-02-07T18:21:27.195998', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:21:27,239][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 18:21:27,240][gensim.models.word2vec][INFO] - sample=0.662878 downsamples 0 most-common words
[2023-02-07 18:21:27,240][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3639370 word corpus (100.0%% of prior 3639370)', 'datetime': '2023-02-07T18:21:27.240748', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:21:27,315][gensim.models.word2vec][INFO] - estimated required memory for 13061 words and 341 dimensions: 47254856 bytes
[2023-02-07 18:21:27,316][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:21:27,342][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 13061 vocabulary and 341 features, using sg=1 hs=0 sample=0.6628777052999235 negative=5 window=18 shrink_windows=True', 'datetime': '2023-02-07T18:21:27.342187', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:21:28,349][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 63.68% examples, 2349841 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:21:28,892][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3642627 effective words) took 1.5s, 2353755 effective words/s
[2023-02-07 18:21:29,894][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 67.12% examples, 2498726 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:21:30,341][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3642627 effective words) took 1.4s, 2517775 effective words/s
[2023-02-07 18:21:31,345][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 53.42% examples, 1998638 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:21:31,974][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3642627 effective words) took 1.6s, 2234306 effective words/s
[2023-02-07 18:21:32,977][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 73.60% examples, 2717516 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:21:33,323][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3642627 effective words) took 1.3s, 2705286 effective words/s
[2023-02-07 18:21:34,328][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 55.05% examples, 2052904 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:21:35,114][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3642627 effective words) took 1.8s, 2037890 effective words/s
[2023-02-07 18:21:36,120][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 71.05% examples, 2636427 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:21:36,494][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3642627 effective words) took 1.4s, 2643069 effective words/s
[2023-02-07 18:21:37,500][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 73.20% examples, 2698491 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:21:37,846][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3642627 effective words) took 1.3s, 2699109 effective words/s
[2023-02-07 18:21:38,853][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 56.19% examples, 2092793 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:21:39,428][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3642627 effective words) took 1.6s, 2305355 effective words/s
[2023-02-07 18:21:40,432][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 74.61% examples, 2753615 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:21:40,753][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3642627 effective words) took 1.3s, 2753727 effective words/s
[2023-02-07 18:21:41,756][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 73.81% examples, 2723167 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:21:42,105][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3642627 effective words) took 1.3s, 2699564 effective words/s
[2023-02-07 18:21:43,112][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 57.78% examples, 2145610 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:21:43,678][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3642627 effective words) took 1.6s, 2320717 effective words/s
[2023-02-07 18:21:44,681][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 74.06% examples, 2738693 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:21:45,002][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3642627 effective words) took 1.3s, 2757337 effective words/s
[2023-02-07 18:21:46,010][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 76.24% examples, 2804270 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:21:46,307][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3642627 effective words) took 1.3s, 2797655 effective words/s
[2023-02-07 18:21:47,312][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 72.64% examples, 2681042 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:21:47,677][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3642627 effective words) took 1.4s, 2664229 effective words/s
[2023-02-07 18:21:48,681][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 53.21% examples, 1986747 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:21:49,539][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3642627 effective words) took 1.9s, 1958779 effective words/s
[2023-02-07 18:21:49,539][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54639405 effective words) took 22.2s, 2461548 effective words/s', 'datetime': '2023-02-07T18:21:49.539867', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:21:49.540 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:21:51,586][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182100-7t8kiqd6/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:21:51.585928', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:21:51,587][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:21:51,677][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182100-7t8kiqd6/files/../tmp/embedding_model.pt
2023-02-07 18:21:51.678 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:21:53.823 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:21:54.617 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:21:57.146 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.3899557231791326, 'test_mae': 1.1411248195553598, 'test_r2': -3.001986754509031}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.91
wandb: percentage 0.0
wandb:   test_mae 1.14112
wandb:   test_mse 2.38996
wandb:    test_r2 -3.00199
wandb: 
wandb: üöÄ View run cosmic-sweep-2 at: https://wandb.ai/xiaoqiz/mof2vec/runs/7t8kiqd6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_182100-7t8kiqd6/logs
wandb: Agent Starting Run: a80hmvck with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 747
wandb: 	model.gensim.alpha: 0.37240889070793703
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.6133012482338334
wandb: 	model.gensim.vector_size: 380
wandb: 	model.gensim.window: 18
wandb: 	model.sklearn.learning_rate: 0.9235556507122312
wandb: 	model.sklearn.max_depth: 48
wandb: 	model.sklearn.min_child_weight: 0.02287074148118717
wandb: 	model.sklearn.n_estimators: 4490
wandb: 	model.sklearn.num_leaves: 125
wandb: 	model.sklearn.reg_alpha: 0.0027897412708716785
wandb: 	model.sklearn.reg_lambda: 0.44956976158885736
wandb: 	model.sklearn.subsample: 0.2825759400690201
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182208-a80hmvck
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/a80hmvck
2023-02-07 18:22:17.320 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 18:22:17.321 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 747 for sweep.
2023-02-07 18:22:17.321 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.37240889070793703 for sweep.
2023-02-07 18:22:17.322 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 18:22:17.322 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 18:22:17.322 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6133012482338334 for sweep.
2023-02-07 18:22:17.322 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 380 for sweep.
2023-02-07 18:22:17.323 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 18 for sweep.
2023-02-07 18:22:17.323 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.9235556507122312 for sweep.
2023-02-07 18:22:17.323 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 48 for sweep.
2023-02-07 18:22:17.323 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.02287074148118717 for sweep.
2023-02-07 18:22:17.324 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4490 for sweep.
2023-02-07 18:22:17.324 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 125 for sweep.
2023-02-07 18:22:17.324 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.0027897412708716785 for sweep.
2023-02-07 18:22:17.324 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.44956976158885736 for sweep.
2023-02-07 18:22:17.324 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2825759400690201 for sweep.
2023-02-07 18:22:17.325 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:22:17.331 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182208-a80hmvck/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 747, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 380, 'window': 18, 'min_count': 2, 'dm': 1, 'sample': 0.6133012482338334, 'workers': 4, 'alpha': 0.37240889070793703, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4490, 'max_depth': 48, 'num_leaves': 125, 'reg_alpha': 0.0027897412708716785, 'reg_lambda': 0.44956976158885736, 'subsample': 0.2825759400690201, 'min_child_weight': 0.02287074148118717, 'n_jobs': 4, 'learning_rate': 0.9235556507122312}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 25/3257 [00:00<00:13, 243.73it/s]  2%|‚ñè         | 52/3257 [00:00<00:12, 255.55it/s]  2%|‚ñè         | 78/3257 [00:00<00:12, 253.57it/s]  3%|‚ñé         | 104/3257 [00:00<00:13, 236.06it/s]  4%|‚ñç         | 128/3257 [00:00<00:13, 228.34it/s]  5%|‚ñç         | 157/3257 [00:00<00:12, 246.12it/s]  6%|‚ñå         | 182/3257 [00:00<00:12, 240.84it/s]  6%|‚ñã         | 211/3257 [00:00<00:11, 253.85it/s]  7%|‚ñã         | 242/3257 [00:00<00:11, 269.52it/s]  8%|‚ñä         | 270/3257 [00:01<00:11, 264.34it/s]  9%|‚ñâ         | 301/3257 [00:01<00:10, 276.88it/s] 10%|‚ñà         | 331/3257 [00:01<00:10, 280.22it/s] 11%|‚ñà         | 360/3257 [00:01<00:10, 270.99it/s] 12%|‚ñà‚ñè        | 388/3257 [00:01<00:11, 246.69it/s] 13%|‚ñà‚ñé        | 416/3257 [00:01<00:11, 255.10it/s] 14%|‚ñà‚ñé        | 442/3257 [00:01<00:11, 236.69it/s] 14%|‚ñà‚ñç        | 470/3257 [00:01<00:11, 248.22it/s] 15%|‚ñà‚ñå        | 498/3257 [00:01<00:10, 254.68it/s] 16%|‚ñà‚ñå        | 526/3257 [00:02<00:10, 257.07it/s] 17%|‚ñà‚ñã        | 552/3257 [00:02<00:10, 255.56it/s] 18%|‚ñà‚ñä        | 578/3257 [00:02<00:11, 241.36it/s] 19%|‚ñà‚ñä        | 606/3257 [00:02<00:10, 251.97it/s] 20%|‚ñà‚ñâ        | 636/3257 [00:02<00:09, 262.32it/s] 20%|‚ñà‚ñà        | 663/3257 [00:02<00:10, 248.95it/s] 21%|‚ñà‚ñà        | 689/3257 [00:02<00:10, 240.04it/s] 22%|‚ñà‚ñà‚ñè       | 714/3257 [00:02<00:10, 240.45it/s] 23%|‚ñà‚ñà‚ñé       | 739/3257 [00:02<00:10, 229.52it/s] 24%|‚ñà‚ñà‚ñé       | 768/3257 [00:03<00:10, 243.81it/s] 24%|‚ñà‚ñà‚ñç       | 793/3257 [00:03<00:10, 242.66it/s] 25%|‚ñà‚ñà‚ñå       | 821/3257 [00:03<00:09, 249.77it/s] 26%|‚ñà‚ñà‚ñå       | 847/3257 [00:03<00:09, 241.93it/s] 27%|‚ñà‚ñà‚ñã       | 872/3257 [00:03<00:09, 242.29it/s] 28%|‚ñà‚ñà‚ñä       | 897/3257 [00:03<00:10, 223.98it/s] 28%|‚ñà‚ñà‚ñä       | 924/3257 [00:03<00:09, 234.79it/s] 29%|‚ñà‚ñà‚ñâ       | 948/3257 [00:03<00:09, 235.01it/s] 30%|‚ñà‚ñà‚ñâ       | 972/3257 [00:03<00:09, 235.48it/s] 31%|‚ñà‚ñà‚ñà       | 996/3257 [00:04<00:09, 235.50it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1021/3257 [00:04<00:09, 239.21it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1046/3257 [00:04<00:15, 138.49it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1073/3257 [00:04<00:13, 163.34it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1098/3257 [00:04<00:11, 181.21it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1124/3257 [00:04<00:10, 198.57it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1148/3257 [00:04<00:10, 206.91it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1175/3257 [00:05<00:09, 220.90it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1199/3257 [00:05<00:09, 217.46it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1223/3257 [00:05<00:09, 221.19it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:05<00:08, 229.20it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1274/3257 [00:05<00:08, 234.69it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1298/3257 [00:05<00:08, 222.66it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1325/3257 [00:05<00:08, 234.28it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:05<00:08, 237.95it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1375/3257 [00:05<00:08, 234.46it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1402/3257 [00:05<00:07, 240.82it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1431/3257 [00:06<00:07, 252.72it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1457/3257 [00:06<00:07, 254.09it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1484/3257 [00:06<00:06, 258.42it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:06<00:06, 263.37it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1540/3257 [00:06<00:07, 234.59it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1565/3257 [00:06<00:07, 229.44it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1589/3257 [00:06<00:07, 229.10it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:06<00:06, 247.15it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1645/3257 [00:06<00:06, 249.05it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1671/3257 [00:07<00:06, 238.78it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1696/3257 [00:07<00:06, 235.60it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1725/3257 [00:07<00:06, 249.84it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1751/3257 [00:07<00:06, 239.72it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1779/3257 [00:07<00:05, 247.58it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1807/3257 [00:07<00:05, 250.34it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1837/3257 [00:07<00:05, 262.87it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1865/3257 [00:07<00:05, 267.74it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1892/3257 [00:07<00:05, 265.86it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1920/3257 [00:08<00:04, 268.10it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1953/3257 [00:08<00:04, 285.68it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1982/3257 [00:08<00:04, 283.70it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2011/3257 [00:08<00:04, 283.85it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2040/3257 [00:08<00:04, 270.72it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2068/3257 [00:08<00:04, 258.68it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2096/3257 [00:08<00:04, 260.66it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2123/3257 [00:08<00:04, 252.44it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2150/3257 [00:08<00:04, 256.04it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:08<00:04, 266.87it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2208/3257 [00:09<00:03, 263.62it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:09<00:07, 141.79it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2260/3257 [00:09<00:06, 158.59it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2287/3257 [00:09<00:05, 180.57it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2314/3257 [00:09<00:04, 200.03it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2348/3257 [00:09<00:03, 232.68it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2376/3257 [00:10<00:03, 244.36it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:10<00:03, 253.14it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2433/3257 [00:10<00:03, 248.48it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2462/3257 [00:10<00:03, 257.97it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2493/3257 [00:10<00:02, 272.41it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2524/3257 [00:10<00:02, 281.93it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2553/3257 [00:10<00:02, 279.80it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2582/3257 [00:10<00:02, 266.85it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2611/3257 [00:10<00:02, 271.78it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2642/3257 [00:10<00:02, 278.82it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2671/3257 [00:11<00:02, 274.76it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2699/3257 [00:11<00:02, 261.44it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2726/3257 [00:11<00:02, 260.97it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:11<00:01, 271.69it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2784/3257 [00:11<00:01, 272.00it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2812/3257 [00:11<00:01, 273.90it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2840/3257 [00:11<00:01, 259.81it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2874/3257 [00:11<00:01, 281.48it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2903/3257 [00:11<00:01, 264.29it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2932/3257 [00:12<00:01, 270.27it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2960/3257 [00:12<00:01, 258.47it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2987/3257 [00:12<00:01, 252.64it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3018/3257 [00:12<00:00, 264.46it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:12<00:00, 273.64it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3078/3257 [00:12<00:00, 277.96it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3106/3257 [00:12<00:00, 267.84it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3133/3257 [00:12<00:00, 258.21it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3160/3257 [00:12<00:00, 259.17it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3187/3257 [00:13<00:00, 258.67it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3215/3257 [00:13<00:00, 260.98it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3248/3257 [00:13<00:00, 280.62it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 245.15it/s]
2023-02-07 18:22:30.936 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:22:30,937][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d380,n5,w18,mc2,s0.613301,t4>', 'datetime': '2023-02-07T18:22:30.937824', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:22:30,938][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:22:30,938][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:22:31,142][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 18:22:31,143][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:22:31,150][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 2542 unique words (90.17% of original 2819, drops 277)', 'datetime': '2023-02-07T18:22:31.150498', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:22:31,150][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 2183345 word corpus (99.99% of original 2183622, drops 277)', 'datetime': '2023-02-07T18:22:31.150964', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:22:31,159][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 18:22:31,160][gensim.models.word2vec][INFO] - sample=0.613301 downsamples 0 most-common words
[2023-02-07 18:22:31,160][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2183345 word corpus (100.0%% of prior 2183345)', 'datetime': '2023-02-07T18:22:31.160440', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:22:31,175][gensim.models.word2vec][INFO] - estimated required memory for 2542 words and 380 dimensions: 14600720 bytes
[2023-02-07 18:22:31,175][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:22:31,187][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2542 vocabulary and 380 features, using sg=0 hs=0 sample=0.6133012482338334 negative=5 window=18 shrink_windows=True', 'datetime': '2023-02-07T18:22:31.187085', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:22:32,215][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 18.51% examples, 385978 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:22:33,227][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 34.94% examples, 380428 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:22:34,243][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 51.55% examples, 377782 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:22:35,262][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 67.85% examples, 371868 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:22:36,278][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 87.23% examples, 378479 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:22:36,934][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2186602 effective words) took 5.7s, 380641 effective words/s
[2023-02-07 18:22:37,950][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 30.37% examples, 658286 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:22:38,983][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 57.88% examples, 632223 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:22:40,015][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 76.14% examples, 548708 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:22:41,015][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 93.21% examples, 502465 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:22:41,366][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2186602 effective words) took 4.4s, 493674 effective words/s
[2023-02-07 18:22:42,371][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 16.52% examples, 347704 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:22:43,384][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 40.84% examples, 455519 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:22:44,412][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 71.32% examples, 522982 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:22:45,267][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2186602 effective words) took 3.9s, 560871 effective words/s
[2023-02-07 18:22:46,272][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 27.39% examples, 599537 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:22:47,280][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 57.38% examples, 640075 words/s, in_qsize 8, out_qsize 1
[2023-02-07 18:22:48,281][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 88.12% examples, 645968 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:22:48,624][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2186602 effective words) took 3.4s, 651786 effective words/s
[2023-02-07 18:22:49,628][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 30.80% examples, 675533 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:22:50,628][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 61.28% examples, 680012 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:22:51,630][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 92.20% examples, 675640 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:22:51,849][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2186602 effective words) took 3.2s, 678560 effective words/s
[2023-02-07 18:22:52,853][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 27.82% examples, 609414 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:22:53,869][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 58.40% examples, 647569 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:22:54,874][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 89.25% examples, 649941 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:22:55,187][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2186602 effective words) took 3.3s, 655469 effective words/s
[2023-02-07 18:22:56,199][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 29.20% examples, 633159 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:22:57,202][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 58.80% examples, 652391 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:22:58,207][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 90.14% examples, 657250 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:22:58,475][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2186602 effective words) took 3.3s, 665331 effective words/s
[2023-02-07 18:22:59,499][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 23.43% examples, 497499 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:00,531][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 39.18% examples, 428300 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:01,538][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 54.93% examples, 401821 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:02,568][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 83.79% examples, 452485 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:03,032][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2186602 effective words) took 4.6s, 480161 effective words/s
[2023-02-07 18:23:04,045][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 27.82% examples, 605166 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:23:05,047][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 56.16% examples, 625994 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:23:06,052][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 87.87% examples, 641749 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:06,408][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2186602 effective words) took 3.4s, 648537 effective words/s
[2023-02-07 18:23:07,420][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 27.33% examples, 595504 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:08,436][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 55.63% examples, 616591 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:23:09,445][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 86.83% examples, 631337 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:09,827][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2186602 effective words) took 3.4s, 640079 effective words/s
[2023-02-07 18:23:10,835][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 27.82% examples, 608169 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:11,848][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 56.59% examples, 628175 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:12,868][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 87.87% examples, 637089 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:13,221][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2186602 effective words) took 3.4s, 644844 effective words/s
[2023-02-07 18:23:14,228][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 26.56% examples, 581155 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:15,232][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 43.23% examples, 484420 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:16,257][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 62.91% examples, 461393 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:17,272][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 80.78% examples, 440696 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:18,283][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 99.39% examples, 430290 words/s, in_qsize 1, out_qsize 1
[2023-02-07 18:23:18,295][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2186602 effective words) took 5.1s, 431206 effective words/s
[2023-02-07 18:23:19,301][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 28.68% examples, 627744 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:20,308][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 57.54% examples, 640171 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:21,308][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 87.47% examples, 642799 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:21,658][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2186602 effective words) took 3.4s, 650607 effective words/s
[2023-02-07 18:23:22,667][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 26.13% examples, 571026 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:23,669][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 55.63% examples, 621785 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:24,682][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 86.46% examples, 631107 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:23:25,079][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2186602 effective words) took 3.4s, 639790 effective words/s
[2023-02-07 18:23:26,085][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 17.26% examples, 366582 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:27,096][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 47.37% examples, 524897 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:28,122][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 78.60% examples, 571072 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:23:28,758][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2186602 effective words) took 3.7s, 594733 effective words/s
[2023-02-07 18:23:28,758][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 32754330 raw words (32799030 effective words) took 57.6s, 569711 effective words/s', 'datetime': '2023-02-07T18:23:28.758913', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:23:28.759 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:23:31,140][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182208-a80hmvck/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:23:31.140843', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:23:31,141][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:23:31,170][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182208-a80hmvck/files/../tmp/embedding_model.pt
2023-02-07 18:23:31.171 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:23:33.409 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:23:34.213 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:23:53.688 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.873602852101316, 'test_mae': 1.2732159374867411, 'test_r2': -5.90338548769563}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.0
wandb: percentage 0.09826
wandb:   test_mae 1.27322
wandb:   test_mse 2.8736
wandb:    test_r2 -5.90339
wandb: 
wandb: üöÄ View run summer-sweep-3 at: https://wandb.ai/xiaoqiz/mof2vec/runs/a80hmvck
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_182208-a80hmvck/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: bpyd4yno with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 699
wandb: 	model.gensim.alpha: 0.00286768753873201
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.9540100664111212
wandb: 	model.gensim.vector_size: 194
wandb: 	model.gensim.window: 4
wandb: 	model.sklearn.learning_rate: 0.01663702819458328
wandb: 	model.sklearn.max_depth: 58
wandb: 	model.sklearn.min_child_weight: 0.06582011581918548
wandb: 	model.sklearn.n_estimators: 413
wandb: 	model.sklearn.num_leaves: 187
wandb: 	model.sklearn.reg_alpha: 0.33454056953405475
wandb: 	model.sklearn.reg_lambda: 0.04604732947767812
wandb: 	model.sklearn.subsample: 0.48768162180774655
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182443-bpyd4yno
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/bpyd4yno
2023-02-07 18:24:52.813 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 18:24:52.814 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 699 for sweep.
2023-02-07 18:24:52.814 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.00286768753873201 for sweep.
2023-02-07 18:24:52.815 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 18:24:52.815 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 18:24:52.815 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.9540100664111212 for sweep.
2023-02-07 18:24:52.815 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 194 for sweep.
2023-02-07 18:24:52.816 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 4 for sweep.
2023-02-07 18:24:52.816 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.01663702819458328 for sweep.
2023-02-07 18:24:52.816 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 58 for sweep.
2023-02-07 18:24:52.816 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.06582011581918548 for sweep.
2023-02-07 18:24:52.817 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 413 for sweep.
2023-02-07 18:24:52.817 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 187 for sweep.
2023-02-07 18:24:52.817 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.33454056953405475 for sweep.
2023-02-07 18:24:52.817 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.04604732947767812 for sweep.
2023-02-07 18:24:52.817 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.48768162180774655 for sweep.
2023-02-07 18:24:52.818 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:24:52.828 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182443-bpyd4yno/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 699, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 194, 'window': 4, 'min_count': 6, 'dm': 1, 'sample': 0.9540100664111212, 'workers': 4, 'alpha': 0.00286768753873201, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 413, 'max_depth': 58, 'num_leaves': 187, 'reg_alpha': 0.33454056953405475, 'reg_lambda': 0.04604732947767812, 'subsample': 0.48768162180774655, 'min_child_weight': 0.06582011581918548, 'n_jobs': 4, 'learning_rate': 0.01663702819458328}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 14/3257 [00:00<00:23, 137.93it/s]  1%|          | 32/3257 [00:00<00:20, 157.29it/s]  1%|‚ñè         | 48/3257 [00:00<00:21, 152.01it/s]  2%|‚ñè         | 64/3257 [00:00<00:21, 151.92it/s]  2%|‚ñè         | 81/3257 [00:00<00:20, 155.70it/s]  3%|‚ñé         | 97/3257 [00:00<00:20, 150.70it/s]  3%|‚ñé         | 113/3257 [00:00<00:21, 146.11it/s]  4%|‚ñç         | 128/3257 [00:00<00:21, 146.47it/s]  4%|‚ñç         | 146/3257 [00:00<00:20, 154.68it/s]  5%|‚ñç         | 162/3257 [00:01<00:20, 150.72it/s]  5%|‚ñå         | 178/3257 [00:01<00:20, 148.42it/s]  6%|‚ñå         | 196/3257 [00:01<00:19, 156.53it/s]  7%|‚ñã         | 214/3257 [00:01<00:18, 162.09it/s]  7%|‚ñã         | 234/3257 [00:01<00:17, 172.89it/s]  8%|‚ñä         | 252/3257 [00:01<00:18, 166.91it/s]  8%|‚ñä         | 269/3257 [00:01<00:18, 159.10it/s]  9%|‚ñâ         | 291/3257 [00:01<00:16, 175.98it/s]  9%|‚ñâ         | 309/3257 [00:01<00:17, 168.55it/s] 10%|‚ñà         | 327/3257 [00:02<00:17, 168.06it/s] 11%|‚ñà         | 344/3257 [00:02<00:19, 152.29it/s] 11%|‚ñà         | 361/3257 [00:02<00:18, 155.32it/s] 12%|‚ñà‚ñè        | 377/3257 [00:02<00:19, 144.91it/s] 12%|‚ñà‚ñè        | 392/3257 [00:02<00:19, 145.85it/s] 13%|‚ñà‚ñé        | 409/3257 [00:02<00:18, 152.39it/s] 13%|‚ñà‚ñé        | 425/3257 [00:02<00:31, 89.27it/s]  13%|‚ñà‚ñé        | 438/3257 [00:03<00:29, 95.05it/s] 14%|‚ñà‚ñç        | 455/3257 [00:03<00:25, 110.41it/s] 14%|‚ñà‚ñç        | 471/3257 [00:03<00:23, 120.56it/s] 15%|‚ñà‚ñç        | 486/3257 [00:03<00:22, 123.83it/s] 15%|‚ñà‚ñå        | 504/3257 [00:03<00:20, 137.10it/s] 16%|‚ñà‚ñå        | 522/3257 [00:03<00:18, 145.30it/s] 17%|‚ñà‚ñã        | 538/3257 [00:03<00:18, 148.62it/s] 17%|‚ñà‚ñã        | 555/3257 [00:03<00:17, 153.50it/s] 18%|‚ñà‚ñä        | 571/3257 [00:03<00:20, 133.43it/s] 18%|‚ñà‚ñä        | 588/3257 [00:04<00:18, 141.88it/s] 19%|‚ñà‚ñä        | 604/3257 [00:04<00:18, 145.83it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:04<00:18, 146.49it/s] 20%|‚ñà‚ñâ        | 638/3257 [00:04<00:16, 155.37it/s] 20%|‚ñà‚ñà        | 654/3257 [00:04<00:17, 148.70it/s] 21%|‚ñà‚ñà        | 670/3257 [00:04<00:17, 146.42it/s] 21%|‚ñà‚ñà        | 685/3257 [00:04<00:18, 141.01it/s] 21%|‚ñà‚ñà‚ñè       | 700/3257 [00:04<00:18, 141.40it/s] 22%|‚ñà‚ñà‚ñè       | 718/3257 [00:04<00:16, 149.92it/s] 23%|‚ñà‚ñà‚ñé       | 734/3257 [00:05<00:17, 143.77it/s] 23%|‚ñà‚ñà‚ñé       | 749/3257 [00:05<00:18, 138.35it/s] 24%|‚ñà‚ñà‚ñé       | 768/3257 [00:05<00:16, 151.98it/s] 24%|‚ñà‚ñà‚ñç       | 784/3257 [00:05<00:17, 142.35it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:05<00:16, 152.27it/s] 25%|‚ñà‚ñà‚ñå       | 820/3257 [00:05<00:16, 150.73it/s] 26%|‚ñà‚ñà‚ñå       | 836/3257 [00:05<00:16, 145.08it/s] 26%|‚ñà‚ñà‚ñå       | 851/3257 [00:05<00:16, 142.97it/s] 27%|‚ñà‚ñà‚ñã       | 868/3257 [00:05<00:16, 149.09it/s] 27%|‚ñà‚ñà‚ñã       | 884/3257 [00:06<00:16, 144.45it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:06<00:14, 159.65it/s] 28%|‚ñà‚ñà‚ñä       | 922/3257 [00:06<00:14, 160.49it/s] 29%|‚ñà‚ñà‚ñâ       | 939/3257 [00:06<00:14, 155.68it/s] 29%|‚ñà‚ñà‚ñâ       | 957/3257 [00:06<00:14, 162.24it/s] 30%|‚ñà‚ñà‚ñâ       | 974/3257 [00:06<00:14, 161.04it/s] 30%|‚ñà‚ñà‚ñà       | 991/3257 [00:06<00:14, 155.71it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:06<00:14, 151.65it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1023/3257 [00:06<00:14, 151.53it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1039/3257 [00:07<00:16, 138.57it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1054/3257 [00:07<00:15, 141.04it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1072/3257 [00:07<00:14, 150.16it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1088/3257 [00:07<00:14, 148.68it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1104/3257 [00:07<00:14, 151.76it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1120/3257 [00:07<00:14, 149.30it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1136/3257 [00:07<00:14, 150.92it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1152/3257 [00:07<00:14, 143.09it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:07<00:14, 146.99it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1185/3257 [00:08<00:15, 136.49it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1199/3257 [00:08<00:15, 132.07it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1213/3257 [00:08<00:15, 130.82it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1233/3257 [00:08<00:13, 148.34it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:08<00:13, 146.93it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1266/3257 [00:08<00:13, 152.76it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1282/3257 [00:08<00:14, 132.73it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1296/3257 [00:08<00:14, 131.12it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1313/3257 [00:09<00:13, 139.48it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1331/3257 [00:09<00:13, 147.76it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1349/3257 [00:09<00:12, 156.04it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1365/3257 [00:09<00:12, 150.00it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1381/3257 [00:09<00:12, 147.81it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1398/3257 [00:09<00:12, 153.44it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1418/3257 [00:09<00:11, 165.43it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1435/3257 [00:09<00:11, 152.75it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1452/3257 [00:09<00:11, 154.50it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1470/3257 [00:10<00:11, 161.26it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1487/3257 [00:10<00:11, 155.12it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1507/3257 [00:10<00:10, 166.74it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1524/3257 [00:10<00:11, 153.91it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1540/3257 [00:10<00:11, 144.93it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1555/3257 [00:10<00:12, 141.08it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1573/3257 [00:10<00:11, 150.24it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1589/3257 [00:10<00:11, 147.84it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1606/3257 [00:10<00:10, 150.59it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1623/3257 [00:11<00:10, 154.03it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1639/3257 [00:11<00:10, 153.55it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1655/3257 [00:11<00:10, 150.74it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1671/3257 [00:11<00:10, 144.47it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1687/3257 [00:11<00:10, 148.01it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1704/3257 [00:11<00:10, 151.93it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1720/3257 [00:11<00:10, 153.27it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1736/3257 [00:12<00:19, 79.33it/s]  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1753/3257 [00:12<00:16, 93.94it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1769/3257 [00:12<00:13, 106.54it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1787/3257 [00:12<00:12, 121.34it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1803/3257 [00:12<00:11, 128.14it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1818/3257 [00:12<00:11, 129.83it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1833/3257 [00:12<00:10, 129.77it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1850/3257 [00:12<00:10, 138.54it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:12<00:09, 150.35it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1885/3257 [00:13<00:09, 147.97it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1901/3257 [00:13<00:09, 149.27it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1917/3257 [00:13<00:08, 150.44it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1935/3257 [00:13<00:08, 158.70it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1958/3257 [00:13<00:07, 176.94it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1976/3257 [00:13<00:07, 174.92it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1994/3257 [00:13<00:07, 170.07it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2012/3257 [00:13<00:07, 164.47it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2032/3257 [00:13<00:07, 172.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2050/3257 [00:14<00:07, 153.95it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2066/3257 [00:14<00:07, 149.97it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2083/3257 [00:14<00:07, 154.53it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2099/3257 [00:14<00:07, 150.96it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2117/3257 [00:14<00:07, 158.24it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2133/3257 [00:14<00:07, 145.83it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2148/3257 [00:14<00:07, 143.72it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2167/3257 [00:14<00:07, 155.67it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2183/3257 [00:14<00:07, 151.91it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2203/3257 [00:15<00:06, 164.05it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2220/3257 [00:15<00:06, 159.29it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2237/3257 [00:15<00:06, 157.89it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2253/3257 [00:15<00:06, 155.18it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2269/3257 [00:15<00:06, 153.96it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2285/3257 [00:15<00:06, 154.63it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2301/3257 [00:15<00:06, 150.67it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2321/3257 [00:15<00:05, 164.49it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2341/3257 [00:15<00:05, 173.53it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:16<00:05, 175.03it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2378/3257 [00:16<00:05, 175.30it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2398/3257 [00:16<00:04, 179.80it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2417/3257 [00:16<00:05, 167.21it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2434/3257 [00:16<00:05, 157.45it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2450/3257 [00:16<00:05, 152.66it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2473/3257 [00:16<00:04, 171.67it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2491/3257 [00:16<00:04, 170.11it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2511/3257 [00:16<00:04, 177.47it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2529/3257 [00:17<00:04, 177.45it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2547/3257 [00:17<00:04, 176.56it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2565/3257 [00:17<00:04, 161.41it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2582/3257 [00:17<00:04, 157.67it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2598/3257 [00:17<00:04, 157.14it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2621/3257 [00:17<00:03, 177.28it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:17<00:03, 176.03it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2658/3257 [00:17<00:03, 163.49it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2675/3257 [00:17<00:03, 165.22it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2693/3257 [00:18<00:03, 168.54it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2711/3257 [00:18<00:03, 143.81it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2730/3257 [00:18<00:03, 154.36it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2749/3257 [00:18<00:03, 163.41it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2766/3257 [00:18<00:03, 163.12it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2783/3257 [00:18<00:02, 161.44it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2803/3257 [00:18<00:02, 169.89it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2821/3257 [00:18<00:02, 163.12it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2838/3257 [00:18<00:02, 154.71it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2857/3257 [00:19<00:02, 163.32it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:19<00:02, 178.36it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2899/3257 [00:19<00:02, 167.66it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2916/3257 [00:19<00:02, 166.54it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2933/3257 [00:19<00:01, 166.30it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:19<00:01, 155.86it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2968/3257 [00:19<00:01, 161.38it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:19<00:01, 152.37it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3004/3257 [00:19<00:01, 160.45it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3021/3257 [00:20<00:01, 156.99it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3041/3257 [00:20<00:01, 166.61it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3063/3257 [00:20<00:01, 178.96it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3082/3257 [00:20<00:01, 167.72it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3100/3257 [00:20<00:00, 164.13it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3120/3257 [00:20<00:00, 172.46it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3138/3257 [00:20<00:00, 164.38it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3155/3257 [00:21<00:01, 69.04it/s]  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3172/3257 [00:21<00:01, 82.69it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3186/3257 [00:21<00:00, 91.03it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3202/3257 [00:21<00:00, 103.65it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3217/3257 [00:21<00:00, 108.55it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3237/3257 [00:21<00:00, 127.78it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3254/3257 [00:22<00:00, 137.19it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:22<00:00, 147.82it/s]
2023-02-07 18:25:15.663 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:25:15,664][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d194,n5,w4,mc6,s0.95401,t4>', 'datetime': '2023-02-07T18:25:15.664548', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:25:15,664][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:25:15,665][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:25:16,225][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 18:25:16,225][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:25:16,285][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 20659 unique words (48.38% of original 42701, drops 22042)', 'datetime': '2023-02-07T18:25:16.285480', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:25:16,285][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 5765964 word corpus (99.02% of original 5822992, drops 57028)', 'datetime': '2023-02-07T18:25:16.285905', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:25:16,356][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 18:25:16,357][gensim.models.word2vec][INFO] - sample=0.95401 downsamples 0 most-common words
[2023-02-07 18:25:16,358][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5765964 word corpus (100.0%% of prior 5765964)', 'datetime': '2023-02-07T18:25:16.357991', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:25:16,477][gensim.models.word2vec][INFO] - estimated required memory for 20659 words and 194 dimensions: 45571100 bytes
[2023-02-07 18:25:16,477][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:25:16,497][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 20659 vocabulary and 194 features, using sg=0 hs=0 sample=0.9540100664111212 negative=5 window=4 shrink_windows=True', 'datetime': '2023-02-07T18:25:16.497418', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:25:17,500][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 26.34% examples, 1515041 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:18,504][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 55.20% examples, 1616377 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:19,505][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 86.49% examples, 1668896 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:19,906][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5739976 effective words) took 3.4s, 1685111 effective words/s
[2023-02-07 18:25:20,916][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 30.24% examples, 1726420 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:25:21,917][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 60.06% examples, 1743832 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:22,918][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 91.34% examples, 1752711 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:25:23,173][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5739976 effective words) took 3.3s, 1757542 effective words/s
[2023-02-07 18:25:24,178][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 31.50% examples, 1812473 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:25,184][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 63.89% examples, 1856002 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:26,188][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 98.22% examples, 1871870 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:26,228][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5739976 effective words) took 3.1s, 1879607 effective words/s
[2023-02-07 18:25:27,236][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 32.51% examples, 1880086 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:28,236][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 72.21% examples, 2098550 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:28,925][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5739976 effective words) took 2.7s, 2129909 effective words/s
[2023-02-07 18:25:29,929][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 35.31% examples, 2056029 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:30,929][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 75.90% examples, 2205057 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:25:31,501][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5739976 effective words) took 2.6s, 2230351 effective words/s
[2023-02-07 18:25:32,505][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 31.13% examples, 1798313 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:25:33,506][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 63.22% examples, 1844748 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:34,513][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 97.08% examples, 1854321 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:34,603][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5739976 effective words) took 3.1s, 1852059 effective words/s
[2023-02-07 18:25:35,608][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 32.36% examples, 1876550 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:36,610][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 64.38% examples, 1877922 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:37,615][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 98.89% examples, 1889456 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:37,641][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5739976 effective words) took 3.0s, 1890859 effective words/s
[2023-02-07 18:25:38,644][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 31.90% examples, 1853293 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:39,646][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 64.38% examples, 1880232 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:40,649][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 99.14% examples, 1896365 words/s, in_qsize 5, out_qsize 0
[2023-02-07 18:25:40,667][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5739976 effective words) took 3.0s, 1898536 effective words/s
[2023-02-07 18:25:41,673][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 31.53% examples, 1821456 words/s, in_qsize 8, out_qsize 2
[2023-02-07 18:25:42,674][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 64.38% examples, 1877411 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:43,681][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 99.51% examples, 1895752 words/s, in_qsize 3, out_qsize 1
[2023-02-07 18:25:43,691][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5739976 effective words) took 3.0s, 1899254 effective words/s
[2023-02-07 18:25:44,694][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 32.88% examples, 1904316 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:45,695][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 73.38% examples, 2131354 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:46,338][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5739976 effective words) took 2.6s, 2169647 effective words/s
[2023-02-07 18:25:47,345][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 32.36% examples, 1870419 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:48,348][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 71.69% examples, 2087621 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:49,016][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5739976 effective words) took 2.7s, 2144547 effective words/s
[2023-02-07 18:25:50,035][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 32.51% examples, 1858925 words/s, in_qsize 8, out_qsize 2
[2023-02-07 18:25:51,036][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 66.04% examples, 1918800 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:51,984][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5739976 effective words) took 3.0s, 1935850 effective words/s
[2023-02-07 18:25:52,994][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 32.36% examples, 1865767 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:53,995][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 74.52% examples, 2157013 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:54,569][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5739976 effective words) took 2.6s, 2221847 effective words/s
[2023-02-07 18:25:55,571][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 35.49% examples, 2067075 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:56,576][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 71.45% examples, 2085824 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:57,386][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5739976 effective words) took 2.8s, 2038687 effective words/s
[2023-02-07 18:25:58,395][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 39.94% examples, 2343143 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:25:59,396][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 84.22% examples, 2429971 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:25:59,743][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5739976 effective words) took 2.4s, 2438085 effective words/s
[2023-02-07 18:25:59,743][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86099640 effective words) took 43.2s, 1990936 effective words/s', 'datetime': '2023-02-07T18:25:59.743727', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:25:59.744 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:26:03,558][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182443-bpyd4yno/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:26:03.558417', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:26:03,559][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:26:03,621][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182443-bpyd4yno/files/../tmp/embedding_model.pt
2023-02-07 18:26:03.621 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:26:05.200 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:26:05.796 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:26:07.157 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.4333150197423596, 'test_mae': 1.1490645897903258, 'test_r2': -2.3555602250260987}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.38
wandb: percentage 0.51619
wandb:   test_mae 1.14906
wandb:   test_mse 2.43332
wandb:    test_r2 -2.35556
wandb: 
wandb: üöÄ View run zesty-sweep-4 at: https://wandb.ai/xiaoqiz/mof2vec/runs/bpyd4yno
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_182443-bpyd4yno/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: e7nqwuin with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 829
wandb: 	model.gensim.alpha: 0.09552635741173368
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 8
wandb: 	model.gensim.sample: 0.7988363566492513
wandb: 	model.gensim.vector_size: 410
wandb: 	model.gensim.window: 19
wandb: 	model.sklearn.learning_rate: 0.0017724058389533926
wandb: 	model.sklearn.max_depth: 11
wandb: 	model.sklearn.min_child_weight: 0.04460132759918113
wandb: 	model.sklearn.n_estimators: 812
wandb: 	model.sklearn.num_leaves: 125
wandb: 	model.sklearn.reg_alpha: 0.09085941651733125
wandb: 	model.sklearn.reg_lambda: 0.11169790538777095
wandb: 	model.sklearn.subsample: 0.2286943773773655
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182626-e7nqwuin
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/e7nqwuin
2023-02-07 18:26:35.343 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 18:26:35.344 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 829 for sweep.
2023-02-07 18:26:35.344 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.09552635741173368 for sweep.
2023-02-07 18:26:35.344 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 18:26:35.344 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 8 for sweep.
2023-02-07 18:26:35.345 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7988363566492513 for sweep.
2023-02-07 18:26:35.345 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 410 for sweep.
2023-02-07 18:26:35.345 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 19 for sweep.
2023-02-07 18:26:35.345 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0017724058389533926 for sweep.
2023-02-07 18:26:35.346 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 11 for sweep.
2023-02-07 18:26:35.346 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.04460132759918113 for sweep.
2023-02-07 18:26:35.346 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 812 for sweep.
2023-02-07 18:26:35.346 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 125 for sweep.
2023-02-07 18:26:35.347 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.09085941651733125 for sweep.
2023-02-07 18:26:35.347 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.11169790538777095 for sweep.
2023-02-07 18:26:35.347 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2286943773773655 for sweep.
2023-02-07 18:26:35.347 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:26:35.354 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182626-e7nqwuin/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 829, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 410, 'window': 19, 'min_count': 8, 'dm': 1, 'sample': 0.7988363566492513, 'workers': 4, 'alpha': 0.09552635741173368, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 812, 'max_depth': 11, 'num_leaves': 125, 'reg_alpha': 0.09085941651733125, 'reg_lambda': 0.11169790538777095, 'subsample': 0.2286943773773655, 'min_child_weight': 0.04460132759918113, 'n_jobs': 4, 'learning_rate': 0.0017724058389533926}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:19, 165.02it/s]  1%|          | 34/3257 [00:00<00:19, 166.03it/s]  2%|‚ñè         | 53/3257 [00:00<00:19, 167.76it/s]  2%|‚ñè         | 70/3257 [00:00<00:19, 164.50it/s]  3%|‚ñé         | 89/3257 [00:00<00:18, 168.63it/s]  3%|‚ñé         | 106/3257 [00:00<00:20, 153.63it/s]  4%|‚ñç         | 124/3257 [00:00<00:19, 160.32it/s]  4%|‚ñç         | 145/3257 [00:00<00:17, 174.89it/s]  5%|‚ñå         | 163/3257 [00:00<00:18, 167.63it/s]  6%|‚ñå         | 182/3257 [00:01<00:17, 172.93it/s]  6%|‚ñå         | 201/3257 [00:01<00:17, 173.53it/s]  7%|‚ñã         | 225/3257 [00:01<00:15, 192.43it/s]  8%|‚ñä         | 245/3257 [00:01<00:15, 190.64it/s]  8%|‚ñä         | 265/3257 [00:01<00:16, 184.69it/s]  9%|‚ñâ         | 289/3257 [00:01<00:14, 199.33it/s] 10%|‚ñâ         | 310/3257 [00:01<00:15, 194.78it/s] 10%|‚ñà         | 330/3257 [00:01<00:14, 195.66it/s] 11%|‚ñà         | 350/3257 [00:01<00:15, 188.98it/s] 11%|‚ñà‚ñè        | 370/3257 [00:02<00:15, 191.86it/s] 12%|‚ñà‚ñè        | 390/3257 [00:02<00:16, 176.27it/s] 13%|‚ñà‚ñé        | 411/3257 [00:02<00:15, 182.74it/s] 13%|‚ñà‚ñé        | 430/3257 [00:02<00:17, 162.57it/s] 14%|‚ñà‚ñé        | 447/3257 [00:02<00:17, 164.07it/s] 14%|‚ñà‚ñç        | 468/3257 [00:02<00:15, 175.50it/s] 15%|‚ñà‚ñç        | 486/3257 [00:02<00:15, 175.62it/s] 16%|‚ñà‚ñå        | 507/3257 [00:02<00:14, 185.03it/s] 16%|‚ñà‚ñå        | 526/3257 [00:02<00:15, 180.84it/s] 17%|‚ñà‚ñã        | 546/3257 [00:03<00:14, 182.62it/s] 17%|‚ñà‚ñã        | 565/3257 [00:03<00:15, 171.11it/s] 18%|‚ñà‚ñä        | 583/3257 [00:03<00:16, 163.44it/s] 18%|‚ñà‚ñä        | 601/3257 [00:03<00:15, 167.76it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:16, 164.75it/s] 20%|‚ñà‚ñâ        | 641/3257 [00:03<00:15, 172.76it/s] 20%|‚ñà‚ñà        | 659/3257 [00:03<00:16, 161.38it/s] 21%|‚ñà‚ñà        | 679/3257 [00:03<00:15, 169.65it/s] 21%|‚ñà‚ñà‚ñè       | 697/3257 [00:03<00:15, 164.36it/s] 22%|‚ñà‚ñà‚ñè       | 716/3257 [00:04<00:14, 169.93it/s] 23%|‚ñà‚ñà‚ñé       | 734/3257 [00:04<00:15, 160.38it/s] 23%|‚ñà‚ñà‚ñé       | 751/3257 [00:04<00:16, 156.41it/s] 24%|‚ñà‚ñà‚ñé       | 770/3257 [00:04<00:15, 163.65it/s] 24%|‚ñà‚ñà‚ñç       | 787/3257 [00:04<00:15, 160.04it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:04<00:15, 161.41it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:04<00:15, 159.91it/s] 26%|‚ñà‚ñà‚ñå       | 840/3257 [00:04<00:15, 159.52it/s] 26%|‚ñà‚ñà‚ñã       | 856/3257 [00:04<00:15, 156.64it/s] 27%|‚ñà‚ñà‚ñã       | 873/3257 [00:05<00:14, 159.95it/s] 27%|‚ñà‚ñà‚ñã       | 890/3257 [00:05<00:14, 159.95it/s] 28%|‚ñà‚ñà‚ñä       | 909/3257 [00:05<00:14, 167.39it/s] 28%|‚ñà‚ñà‚ñä       | 928/3257 [00:05<00:13, 173.76it/s] 29%|‚ñà‚ñà‚ñâ       | 946/3257 [00:05<00:13, 169.96it/s] 30%|‚ñà‚ñà‚ñâ       | 965/3257 [00:05<00:13, 175.17it/s] 30%|‚ñà‚ñà‚ñà       | 983/3257 [00:05<00:13, 168.77it/s] 31%|‚ñà‚ñà‚ñà       | 1000/3257 [00:05<00:13, 162.57it/s] 31%|‚ñà‚ñà‚ñà       | 1017/3257 [00:05<00:13, 161.34it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1034/3257 [00:06<00:14, 152.43it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1050/3257 [00:06<00:14, 151.24it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1070/3257 [00:06<00:13, 162.87it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1087/3257 [00:06<00:13, 157.86it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1103/3257 [00:06<00:13, 158.37it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1119/3257 [00:06<00:13, 158.47it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1136/3257 [00:06<00:13, 161.73it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1153/3257 [00:06<00:13, 158.90it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1169/3257 [00:07<00:22, 94.87it/s]  36%|‚ñà‚ñà‚ñà‚ñã      | 1182/3257 [00:07<00:20, 100.01it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1196/3257 [00:07<00:19, 108.11it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1210/3257 [00:07<00:17, 114.05it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1230/3257 [00:07<00:14, 135.27it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1246/3257 [00:07<00:14, 141.16it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1264/3257 [00:07<00:13, 151.48it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1281/3257 [00:07<00:13, 144.32it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1297/3257 [00:08<00:13, 147.78it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1317/3257 [00:08<00:12, 160.86it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1334/3257 [00:08<00:11, 163.38it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1351/3257 [00:08<00:11, 158.90it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1368/3257 [00:08<00:11, 159.59it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1385/3257 [00:08<00:11, 157.28it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1403/3257 [00:08<00:11, 161.51it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:08<00:10, 168.26it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1440/3257 [00:08<00:11, 163.36it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1460/3257 [00:08<00:10, 172.20it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1478/3257 [00:09<00:10, 171.98it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1499/3257 [00:09<00:09, 181.74it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1518/3257 [00:09<00:09, 177.65it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1536/3257 [00:09<00:10, 159.96it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1553/3257 [00:09<00:10, 158.50it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1572/3257 [00:09<00:10, 164.83it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1589/3257 [00:09<00:10, 165.00it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1608/3257 [00:09<00:09, 170.91it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1628/3257 [00:09<00:09, 177.25it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1646/3257 [00:10<00:09, 168.18it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1663/3257 [00:10<00:10, 157.92it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1679/3257 [00:10<00:10, 155.37it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1695/3257 [00:10<00:10, 154.87it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1714/3257 [00:10<00:09, 162.60it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:10<00:09, 153.50it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1748/3257 [00:10<00:09, 157.08it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1766/3257 [00:10<00:09, 161.89it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1784/3257 [00:10<00:08, 165.17it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1801/3257 [00:11<00:08, 163.83it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1818/3257 [00:11<00:08, 160.51it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1835/3257 [00:11<00:09, 154.20it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1853/3257 [00:11<00:08, 160.79it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1873/3257 [00:11<00:08, 170.97it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1891/3257 [00:11<00:08, 165.83it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1909/3257 [00:11<00:07, 169.51it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1927/3257 [00:11<00:08, 165.85it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1952/3257 [00:11<00:06, 188.21it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1973/3257 [00:12<00:06, 193.77it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1993/3257 [00:12<00:06, 183.29it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2012/3257 [00:12<00:06, 179.49it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2032/3257 [00:12<00:06, 184.43it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2051/3257 [00:12<00:07, 167.12it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2069/3257 [00:12<00:07, 162.50it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2087/3257 [00:12<00:07, 166.93it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2105/3257 [00:12<00:06, 169.84it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2123/3257 [00:12<00:07, 161.67it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2141/3257 [00:13<00:06, 166.37it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2159/3257 [00:13<00:06, 168.59it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:13<00:06, 169.98it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2196/3257 [00:13<00:06, 171.88it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2214/3257 [00:13<00:06, 165.48it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2232/3257 [00:13<00:06, 169.52it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2250/3257 [00:13<00:06, 160.50it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2267/3257 [00:13<00:06, 158.73it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2286/3257 [00:13<00:05, 165.52it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2303/3257 [00:14<00:05, 165.65it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2326/3257 [00:14<00:05, 183.42it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2348/3257 [00:14<00:04, 192.05it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2368/3257 [00:14<00:04, 190.09it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2389/3257 [00:14<00:04, 194.61it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2409/3257 [00:14<00:04, 182.99it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2428/3257 [00:15<00:08, 92.35it/s]  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2444/3257 [00:15<00:07, 102.97it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2461/3257 [00:15<00:06, 115.20it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2481/3257 [00:15<00:05, 132.32it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2504/3257 [00:15<00:04, 154.39it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2524/3257 [00:15<00:04, 165.27it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2545/3257 [00:15<00:04, 175.55it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2565/3257 [00:15<00:04, 171.08it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2584/3257 [00:15<00:03, 170.03it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2604/3257 [00:15<00:03, 177.48it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2629/3257 [00:16<00:03, 196.07it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2650/3257 [00:16<00:03, 186.54it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:16<00:03, 188.31it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2690/3257 [00:16<00:02, 190.90it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2710/3257 [00:16<00:03, 167.76it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2731/3257 [00:16<00:02, 177.79it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2751/3257 [00:16<00:02, 183.31it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2770/3257 [00:16<00:02, 175.87it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2792/3257 [00:16<00:02, 185.80it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2811/3257 [00:17<00:02, 186.00it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2830/3257 [00:17<00:02, 173.15it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2851/3257 [00:17<00:02, 181.55it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2877/3257 [00:17<00:01, 202.29it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2898/3257 [00:17<00:01, 185.56it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2918/3257 [00:17<00:01, 188.37it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2938/3257 [00:17<00:01, 184.55it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2957/3257 [00:17<00:01, 170.53it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:18<00:01, 175.56it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2994/3257 [00:18<00:01, 174.94it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3016/3257 [00:18<00:01, 182.17it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3035/3257 [00:18<00:01, 183.75it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3057/3257 [00:18<00:01, 191.98it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:18<00:00, 198.62it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3099/3257 [00:18<00:00, 196.29it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3122/3257 [00:18<00:00, 204.61it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3143/3257 [00:18<00:00, 188.28it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3163/3257 [00:18<00:00, 184.22it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3182/3257 [00:19<00:00, 174.13it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3204/3257 [00:19<00:00, 183.93it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3223/3257 [00:19<00:00, 176.03it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:19<00:00, 188.23it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 166.97it/s]
2023-02-07 18:26:55.635 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:26:55,637][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d410,n5,w19,mc8,s0.798836,t4>', 'datetime': '2023-02-07T18:26:55.637274', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:26:55,637][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:26:55,638][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:26:56,116][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 18:26:56,117][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:26:56,160][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 retains 13798 unique words (43.39% of original 31803, drops 18005)', 'datetime': '2023-02-07T18:26:56.159950', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:26:56,160][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 leaves 5042429 word corpus (98.97% of original 5095118, drops 52689)', 'datetime': '2023-02-07T18:26:56.160400', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:26:56,207][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 18:26:56,208][gensim.models.word2vec][INFO] - sample=0.798836 downsamples 0 most-common words
[2023-02-07 18:26:56,208][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5042429 word corpus (100.0%% of prior 5042429)', 'datetime': '2023-02-07T18:26:56.208805', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:26:56,290][gensim.models.word2vec][INFO] - estimated required memory for 13798 words and 410 dimensions: 58149320 bytes
[2023-02-07 18:26:56,291][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:26:56,324][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 13798 vocabulary and 410 features, using sg=0 hs=0 sample=0.7988363566492513 negative=5 window=19 shrink_windows=True', 'datetime': '2023-02-07T18:26:56.324183', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:26:57,327][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 16.27% examples, 794039 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:26:58,336][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 32.55% examples, 826469 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:26:59,356][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 49.25% examples, 835033 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:27:00,369][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 65.77% examples, 836378 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:01,372][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 83.14% examples, 838437 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:02,329][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5034594 effective words) took 6.0s, 838755 effective words/s
[2023-02-07 18:27:03,334][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 12.90% examples, 616897 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:27:04,336][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 27.14% examples, 689089 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:05,348][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 42.55% examples, 733002 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:06,364][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 59.47% examples, 758420 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:27:07,378][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 77.06% examples, 777504 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:08,400][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 94.35% examples, 785440 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:08,696][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5034594 effective words) took 6.4s, 791054 effective words/s
[2023-02-07 18:27:09,719][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 13.17% examples, 619500 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:10,730][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 25.02% examples, 618730 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:27:11,741][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 37.21% examples, 632489 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:12,762][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 50.38% examples, 636617 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:13,767][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 63.59% examples, 642551 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:14,773][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 76.08% examples, 639490 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:27:15,775][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 89.87% examples, 642480 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:16,514][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5034594 effective words) took 7.8s, 644151 effective words/s
[2023-02-07 18:27:17,542][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 13.11% examples, 628248 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:27:18,555][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 25.76% examples, 637229 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:19,561][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 38.29% examples, 648878 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:27:20,566][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 50.54% examples, 640876 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:27:21,575][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 66.56% examples, 676811 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:22,577][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 83.85% examples, 703609 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:23,485][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5034594 effective words) took 7.0s, 722548 effective words/s
[2023-02-07 18:27:24,503][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 12.56% examples, 590758 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:25,516][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 24.62% examples, 609872 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:26,527][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 39.70% examples, 677193 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:27,540][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 56.25% examples, 716852 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:27:28,546][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 72.92% examples, 737107 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:29,549][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 90.33% examples, 754329 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:30,077][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5034594 effective words) took 6.6s, 763937 effective words/s
[2023-02-07 18:27:31,081][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 16.00% examples, 776662 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:32,090][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 31.84% examples, 808498 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:33,112][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 48.36% examples, 821698 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:27:34,118][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 65.09% examples, 827966 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:27:35,127][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 82.78% examples, 831677 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:36,110][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5034594 effective words) took 6.0s, 834880 effective words/s
[2023-02-07 18:27:37,113][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 16.70% examples, 812054 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:38,113][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 32.73% examples, 833389 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:39,114][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 48.82% examples, 836395 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:40,136][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 65.27% examples, 830686 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:41,137][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 82.47% examples, 832383 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:42,151][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 99.11% examples, 827219 words/s, in_qsize 5, out_qsize 0
[2023-02-07 18:27:42,192][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5034594 effective words) took 6.1s, 828103 effective words/s
[2023-02-07 18:27:43,196][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 12.93% examples, 627379 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:44,243][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 25.39% examples, 621379 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:45,249][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 37.21% examples, 630088 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:27:46,259][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 50.14% examples, 631984 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:47,261][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 62.60% examples, 631288 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:48,269][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 78.91% examples, 661680 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:49,278][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 96.28% examples, 684925 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:49,479][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5034594 effective words) took 7.3s, 691090 effective words/s
[2023-02-07 18:27:50,485][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 13.05% examples, 634115 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:51,513][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 25.18% examples, 622714 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:52,514][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 37.86% examples, 643786 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:53,527][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 50.17% examples, 637609 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:54,529][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 62.85% examples, 637696 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:27:55,530][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 75.96% examples, 640975 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:56,547][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 90.67% examples, 650879 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:27:57,055][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5034594 effective words) took 7.6s, 664849 effective words/s
[2023-02-07 18:27:58,082][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 12.93% examples, 613345 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:27:59,086][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 24.78% examples, 611594 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:00,093][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 36.66% examples, 619560 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:01,099][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 49.46% examples, 628444 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:02,099][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 62.45% examples, 632495 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:03,117][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 74.73% examples, 629655 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:04,137][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 88.79% examples, 634835 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:28:04,929][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5034594 effective words) took 7.9s, 639610 effective words/s
[2023-02-07 18:28:05,942][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 13.05% examples, 629259 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:06,948][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 24.96% examples, 623350 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:07,950][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 40.28% examples, 691638 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:08,961][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 56.65% examples, 725141 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:09,965][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 73.63% examples, 745951 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:10,991][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 90.85% examples, 760536 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:28:11,457][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5034594 effective words) took 6.5s, 771592 effective words/s
[2023-02-07 18:28:12,467][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 17.22% examples, 834061 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:13,470][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 33.34% examples, 840655 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:28:14,476][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 48.08% examples, 820264 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:28:15,478][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 60.73% examples, 774127 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:28:16,507][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 74.03% examples, 749354 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:17,516][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 91.28% examples, 763727 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:18,007][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5034594 effective words) took 6.5s, 768806 effective words/s
[2023-02-07 18:28:19,015][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 17.10% examples, 828498 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:28:20,025][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 33.13% examples, 835318 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:21,029][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 47.44% examples, 809197 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:22,036][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 60.27% examples, 765752 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:28:23,060][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 73.35% examples, 741890 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:28:24,062][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 87.10% examples, 731511 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:24,863][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5034594 effective words) took 6.9s, 734684 effective words/s
[2023-02-07 18:28:25,873][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 17.22% examples, 833374 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:28:26,879][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 33.34% examples, 839190 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:28:27,882][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 49.49% examples, 841682 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:28,885][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 66.10% examples, 845560 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:29,889][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 83.39% examples, 845774 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:30,819][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5034594 effective words) took 6.0s, 845547 effective words/s
[2023-02-07 18:28:31,853][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 12.16% examples, 565972 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:32,860][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 23.70% examples, 580813 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:33,866][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 36.11% examples, 607397 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:28:34,885][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 49.06% examples, 620231 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:35,896][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 62.27% examples, 626371 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:36,921][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 74.67% examples, 624651 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:37,921][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 88.85% examples, 634497 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:28:38,732][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5034594 effective words) took 7.9s, 636400 effective words/s
[2023-02-07 18:28:38,733][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (75518910 effective words) took 102.4s, 737425 effective words/s', 'datetime': '2023-02-07T18:28:38.733656', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:28:38.734 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:28:44,796][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182626-e7nqwuin/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:28:44.796483', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:28:44,797][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:28:44,887][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182626-e7nqwuin/files/../tmp/embedding_model.pt
2023-02-07 18:28:44.887 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:28:47.247 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:28:48.055 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:28:50.805 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.78057179494506, 'test_mae': 1.2831083174774915, 'test_r2': -4.246125443670438}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.037 MB of 0.038 MB uploaded (0.000 MB deduped)wandb: | 0.037 MB of 0.038 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.0
wandb: percentage 0.56614
wandb:   test_mae 1.28311
wandb:   test_mse 2.78057
wandb:    test_r2 -4.24613
wandb: 
wandb: üöÄ View run electric-sweep-5 at: https://wandb.ai/xiaoqiz/mof2vec/runs/e7nqwuin
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_182626-e7nqwuin/logs
wandb: Agent Starting Run: t3wk8wlw with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 851
wandb: 	model.gensim.alpha: 0.022857302680346716
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.2402509057906793
wandb: 	model.gensim.vector_size: 134
wandb: 	model.gensim.window: 16
wandb: 	model.sklearn.learning_rate: 0.046020500140146546
wandb: 	model.sklearn.max_depth: 75
wandb: 	model.sklearn.min_child_weight: 0.005644100669098814
wandb: 	model.sklearn.n_estimators: 1991
wandb: 	model.sklearn.num_leaves: 368
wandb: 	model.sklearn.reg_alpha: 0.07119565064287184
wandb: 	model.sklearn.reg_lambda: 0.004330033585046137
wandb: 	model.sklearn.subsample: 0.8558493466907928
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182900-t3wk8wlw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/t3wk8wlw
2023-02-07 18:29:08.779 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 18:29:08.780 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 851 for sweep.
2023-02-07 18:29:08.780 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.022857302680346716 for sweep.
2023-02-07 18:29:08.780 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 18:29:08.780 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 18:29:08.781 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2402509057906793 for sweep.
2023-02-07 18:29:08.781 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 134 for sweep.
2023-02-07 18:29:08.781 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 16 for sweep.
2023-02-07 18:29:08.781 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.046020500140146546 for sweep.
2023-02-07 18:29:08.782 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 75 for sweep.
2023-02-07 18:29:08.782 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.005644100669098814 for sweep.
2023-02-07 18:29:08.782 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1991 for sweep.
2023-02-07 18:29:08.782 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 368 for sweep.
2023-02-07 18:29:08.783 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.07119565064287184 for sweep.
2023-02-07 18:29:08.783 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.004330033585046137 for sweep.
2023-02-07 18:29:08.783 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8558493466907928 for sweep.
2023-02-07 18:29:08.783 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:29:08.789 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182900-t3wk8wlw/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 851, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 134, 'window': 16, 'min_count': 6, 'dm': 0, 'sample': 0.2402509057906793, 'workers': 4, 'alpha': 0.022857302680346716, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1991, 'max_depth': 75, 'num_leaves': 368, 'reg_alpha': 0.07119565064287184, 'reg_lambda': 0.004330033585046137, 'subsample': 0.8558493466907928, 'min_child_weight': 0.005644100669098814, 'n_jobs': 4, 'learning_rate': 0.046020500140146546}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 19/3257 [00:00<00:17, 187.81it/s]  1%|‚ñè         | 41/3257 [00:00<00:15, 204.43it/s]  2%|‚ñè         | 65/3257 [00:00<00:14, 218.20it/s]  3%|‚ñé         | 88/3257 [00:00<00:14, 220.62it/s]  3%|‚ñé         | 111/3257 [00:00<00:15, 198.96it/s]  4%|‚ñç         | 132/3257 [00:00<00:15, 200.33it/s]  5%|‚ñç         | 154/3257 [00:00<00:15, 206.10it/s]  5%|‚ñå         | 175/3257 [00:00<00:15, 202.31it/s]  6%|‚ñå         | 199/3257 [00:00<00:14, 213.31it/s]  7%|‚ñã         | 225/3257 [00:01<00:13, 226.50it/s]  8%|‚ñä         | 248/3257 [00:01<00:13, 227.22it/s]  8%|‚ñä         | 271/3257 [00:01<00:13, 215.99it/s]  9%|‚ñâ         | 299/3257 [00:01<00:12, 231.29it/s] 10%|‚ñâ         | 323/3257 [00:01<00:12, 232.71it/s] 11%|‚ñà         | 347/3257 [00:01<00:13, 219.85it/s] 11%|‚ñà‚ñè        | 371/3257 [00:01<00:12, 225.30it/s] 12%|‚ñà‚ñè        | 394/3257 [00:01<00:13, 207.37it/s] 13%|‚ñà‚ñé        | 418/3257 [00:01<00:13, 215.08it/s] 14%|‚ñà‚ñé        | 440/3257 [00:02<00:14, 192.15it/s] 14%|‚ñà‚ñç        | 465/3257 [00:02<00:13, 204.53it/s] 15%|‚ñà‚ñç        | 487/3257 [00:02<00:13, 205.61it/s] 16%|‚ñà‚ñå        | 512/3257 [00:02<00:12, 215.88it/s] 16%|‚ñà‚ñã        | 534/3257 [00:02<00:12, 213.29it/s] 17%|‚ñà‚ñã        | 556/3257 [00:02<00:13, 198.92it/s] 18%|‚ñà‚ñä        | 577/3257 [00:02<00:21, 125.26it/s] 18%|‚ñà‚ñä        | 602/3257 [00:03<00:17, 148.48it/s] 19%|‚ñà‚ñâ        | 621/3257 [00:03<00:16, 156.96it/s] 20%|‚ñà‚ñâ        | 644/3257 [00:03<00:15, 172.70it/s] 20%|‚ñà‚ñà        | 664/3257 [00:03<00:15, 170.75it/s] 21%|‚ñà‚ñà        | 685/3257 [00:03<00:14, 180.35it/s] 22%|‚ñà‚ñà‚ñè       | 707/3257 [00:03<00:13, 190.12it/s] 22%|‚ñà‚ñà‚ñè       | 728/3257 [00:03<00:13, 190.23it/s] 23%|‚ñà‚ñà‚ñé       | 749/3257 [00:03<00:13, 190.43it/s] 24%|‚ñà‚ñà‚ñé       | 770/3257 [00:03<00:12, 194.09it/s] 24%|‚ñà‚ñà‚ñç       | 790/3257 [00:04<00:12, 193.31it/s] 25%|‚ñà‚ñà‚ñç       | 812/3257 [00:04<00:12, 197.71it/s] 26%|‚ñà‚ñà‚ñå       | 832/3257 [00:04<00:12, 194.55it/s] 26%|‚ñà‚ñà‚ñå       | 852/3257 [00:04<00:13, 179.77it/s] 27%|‚ñà‚ñà‚ñã       | 871/3257 [00:04<00:13, 173.77it/s] 27%|‚ñà‚ñà‚ñã       | 890/3257 [00:04<00:13, 176.83it/s] 28%|‚ñà‚ñà‚ñä       | 913/3257 [00:04<00:12, 190.62it/s] 29%|‚ñà‚ñà‚ñä       | 933/3257 [00:04<00:12, 192.20it/s] 29%|‚ñà‚ñà‚ñâ       | 957/3257 [00:04<00:11, 205.63it/s] 30%|‚ñà‚ñà‚ñà       | 978/3257 [00:04<00:11, 201.01it/s] 31%|‚ñà‚ñà‚ñà       | 999/3257 [00:05<00:12, 178.06it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1020/3257 [00:05<00:12, 185.91it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1040/3257 [00:05<00:12, 182.94it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1060/3257 [00:05<00:11, 185.87it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:05<00:11, 189.33it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1100/3257 [00:05<00:11, 190.06it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1120/3257 [00:05<00:11, 189.32it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1140/3257 [00:05<00:11, 186.97it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1161/3257 [00:05<00:10, 193.20it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1181/3257 [00:06<00:11, 185.82it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1200/3257 [00:06<00:11, 179.53it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1219/3257 [00:06<00:12, 168.38it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:06<00:11, 181.62it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1262/3257 [00:06<00:10, 183.95it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1281/3257 [00:06<00:11, 178.63it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1301/3257 [00:06<00:10, 184.50it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1323/3257 [00:06<00:10, 192.91it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1344/3257 [00:06<00:09, 196.63it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1364/3257 [00:07<00:09, 189.98it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:07<00:10, 184.48it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1403/3257 [00:07<00:10, 183.43it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:07<00:10, 182.42it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1444/3257 [00:07<00:09, 189.93it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1470/3257 [00:07<00:08, 208.90it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1492/3257 [00:07<00:08, 211.27it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1515/3257 [00:07<00:08, 214.96it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1537/3257 [00:07<00:08, 202.36it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1558/3257 [00:08<00:08, 202.88it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1579/3257 [00:08<00:08, 204.55it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1603/3257 [00:08<00:07, 211.85it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1625/3257 [00:08<00:07, 206.85it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1646/3257 [00:08<00:08, 199.79it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1667/3257 [00:08<00:08, 191.35it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1687/3257 [00:08<00:08, 192.06it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1709/3257 [00:08<00:07, 198.85it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1729/3257 [00:09<00:13, 116.16it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1748/3257 [00:09<00:11, 130.04it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1770/3257 [00:09<00:09, 148.82it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1792/3257 [00:09<00:08, 164.96it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1812/3257 [00:09<00:08, 169.63it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1831/3257 [00:09<00:08, 173.74it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1851/3257 [00:09<00:07, 180.28it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1873/3257 [00:09<00:07, 189.31it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1893/3257 [00:10<00:07, 185.96it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:10<00:06, 194.76it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1941/3257 [00:10<00:06, 207.97it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1967/3257 [00:10<00:05, 222.71it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1990/3257 [00:10<00:05, 211.57it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2012/3257 [00:10<00:06, 202.20it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2034/3257 [00:10<00:05, 204.61it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2055/3257 [00:10<00:06, 189.65it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2076/3257 [00:10<00:06, 194.33it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2096/3257 [00:11<00:06, 190.41it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2117/3257 [00:11<00:05, 193.83it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2137/3257 [00:11<00:06, 184.93it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2156/3257 [00:11<00:05, 184.84it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:11<00:05, 193.88it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2201/3257 [00:11<00:05, 200.96it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2222/3257 [00:11<00:05, 196.92it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2242/3257 [00:11<00:05, 189.83it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2262/3257 [00:11<00:05, 191.14it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2282/3257 [00:11<00:05, 192.41it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2304/3257 [00:12<00:04, 198.54it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2330/3257 [00:12<00:04, 214.76it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2353/3257 [00:12<00:04, 217.70it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2375/3257 [00:12<00:04, 216.45it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2399/3257 [00:12<00:03, 222.04it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2422/3257 [00:12<00:03, 214.25it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2444/3257 [00:12<00:03, 204.56it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2469/3257 [00:12<00:03, 214.82it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2492/3257 [00:12<00:03, 218.02it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2517/3257 [00:13<00:03, 226.46it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2540/3257 [00:13<00:03, 224.06it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2563/3257 [00:13<00:03, 214.01it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2585/3257 [00:13<00:03, 203.07it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2610/3257 [00:13<00:03, 212.84it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2635/3257 [00:13<00:02, 223.08it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2658/3257 [00:13<00:02, 207.55it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2680/3257 [00:13<00:02, 202.74it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2701/3257 [00:13<00:03, 184.16it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2721/3257 [00:14<00:02, 187.98it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2748/3257 [00:14<00:02, 208.41it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2770/3257 [00:14<00:02, 201.18it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2794/3257 [00:14<00:02, 211.37it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2816/3257 [00:14<00:02, 202.85it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2837/3257 [00:14<00:02, 196.87it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2862/3257 [00:14<00:01, 209.89it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:14<00:01, 218.15it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2909/3257 [00:14<00:01, 211.28it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2931/3257 [00:15<00:01, 210.00it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2953/3257 [00:15<00:01, 204.35it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2975/3257 [00:15<00:01, 206.78it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2996/3257 [00:15<00:01, 204.19it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3020/3257 [00:15<00:01, 212.22it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3046/3257 [00:15<00:00, 224.16it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3073/3257 [00:15<00:00, 234.74it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3097/3257 [00:15<00:00, 223.39it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3121/3257 [00:15<00:00, 227.57it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3144/3257 [00:16<00:00, 217.54it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3166/3257 [00:16<00:00, 119.32it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3187/3257 [00:16<00:00, 135.42it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3211/3257 [00:16<00:00, 155.00it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3237/3257 [00:16<00:00, 177.43it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 193.77it/s]
2023-02-07 18:29:26.076 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:29:26,077][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d134,n5,mc6,s0.240251,t4>', 'datetime': '2023-02-07T18:29:26.077636', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:29:26,077][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:29:26,078][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:29:26,405][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 18:29:26,406][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:29:26,425][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 6701 unique words (51.31% of original 13061, drops 6360)', 'datetime': '2023-02-07T18:29:26.425242', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:29:26,425][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 3623361 word corpus (99.56% of original 3639370, drops 16009)', 'datetime': '2023-02-07T18:29:26.425619', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:29:26,448][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 18:29:26,448][gensim.models.word2vec][INFO] - sample=0.240251 downsamples 0 most-common words
[2023-02-07 18:29:26,448][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3623361 word corpus (100.0%% of prior 3623361)', 'datetime': '2023-02-07T18:29:26.448953', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:29:26,486][gensim.models.word2vec][INFO] - estimated required memory for 6701 words and 134 dimensions: 12931124 bytes
[2023-02-07 18:29:26,487][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:29:26,492][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 6701 vocabulary and 134 features, using sg=1 hs=0 sample=0.2402509057906793 negative=5 window=16 shrink_windows=True', 'datetime': '2023-02-07T18:29:26.492473', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:29:27,497][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 94.41% examples, 3424330 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:29:27,551][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3626618 effective words) took 1.1s, 3431041 effective words/s
[2023-02-07 18:29:28,554][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 74.49% examples, 2738189 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:29:28,890][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3626618 effective words) took 1.3s, 2714203 effective words/s
[2023-02-07 18:29:29,894][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 73.07% examples, 2687186 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:29:30,150][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3626618 effective words) took 1.3s, 2882163 effective words/s
[2023-02-07 18:29:31,154][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 73.81% examples, 2706768 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:29:31,489][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3626618 effective words) took 1.3s, 2713330 effective words/s
[2023-02-07 18:29:32,498][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 74.92% examples, 2735100 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:29:32,811][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3626618 effective words) took 1.3s, 2746764 effective words/s
[2023-02-07 18:29:33,813][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 73.72% examples, 2710776 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:29:34,140][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3626618 effective words) took 1.3s, 2730473 effective words/s
[2023-02-07 18:29:35,151][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 74.79% examples, 2736102 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:29:35,462][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3626618 effective words) took 1.3s, 2750710 effective words/s
[2023-02-07 18:29:36,470][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 74.30% examples, 2721018 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:29:36,798][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3626618 effective words) took 1.3s, 2719759 effective words/s
[2023-02-07 18:29:37,777][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3626618 effective words) took 1.0s, 3712784 effective words/s
[2023-02-07 18:29:38,782][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 75.68% examples, 2776964 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:29:39,082][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3626618 effective words) took 1.3s, 2782479 effective words/s
[2023-02-07 18:29:40,089][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 74.18% examples, 2719319 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:29:40,418][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3626618 effective words) took 1.3s, 2721318 effective words/s
[2023-02-07 18:29:41,425][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 73.26% examples, 2682355 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:29:41,772][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3626618 effective words) took 1.4s, 2682351 effective words/s
[2023-02-07 18:29:42,780][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 72.86% examples, 2671546 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:29:43,134][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3626618 effective words) took 1.4s, 2667792 effective words/s
[2023-02-07 18:29:44,137][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 98.62% examples, 3576951 words/s, in_qsize 5, out_qsize 0
[2023-02-07 18:29:44,152][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3626618 effective words) took 1.0s, 3570678 effective words/s
[2023-02-07 18:29:45,132][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3626618 effective words) took 1.0s, 3705139 effective words/s
[2023-02-07 18:29:45,133][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54399270 effective words) took 18.6s, 2918325 effective words/s', 'datetime': '2023-02-07T18:29:45.133530', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:29:45.133 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:29:46,560][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182900-t3wk8wlw/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:29:46.560767', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:29:46,562][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:29:46,592][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_182900-t3wk8wlw/files/../tmp/embedding_model.pt
2023-02-07 18:29:46.592 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:29:47.945 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:29:48.440 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:29:50.728 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.246364165209805, 'test_mae': 1.0801081650281907, 'test_r2': -2.2051764862621326}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.94
wandb: percentage 0.48695
wandb:   test_mae 1.08011
wandb:   test_mse 2.24636
wandb:    test_r2 -2.20518
wandb: 
wandb: üöÄ View run different-sweep-6 at: https://wandb.ai/xiaoqiz/mof2vec/runs/t3wk8wlw
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_182900-t3wk8wlw/logs
wandb: Agent Starting Run: awqdsul2 with config:
wandb: 	data.data.wl_step: 5
wandb: 	data.nn.batch_size: 347
wandb: 	model.gensim.alpha: 0.03385434792970045
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.7192962621598449
wandb: 	model.gensim.vector_size: 47
wandb: 	model.gensim.window: 4
wandb: 	model.sklearn.learning_rate: 0.0031832995350322343
wandb: 	model.sklearn.max_depth: 45
wandb: 	model.sklearn.min_child_weight: 0.023787441394491623
wandb: 	model.sklearn.n_estimators: 456
wandb: 	model.sklearn.num_leaves: 376
wandb: 	model.sklearn.reg_alpha: 0.11103841819870812
wandb: 	model.sklearn.reg_lambda: 0.0060461051393499434
wandb: 	model.sklearn.subsample: 0.4540846435096595
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183002-awqdsul2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/awqdsul2
2023-02-07 18:30:11.378 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 5 for sweep.
2023-02-07 18:30:11.379 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 347 for sweep.
2023-02-07 18:30:11.379 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.03385434792970045 for sweep.
2023-02-07 18:30:11.380 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 18:30:11.380 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 18:30:11.380 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7192962621598449 for sweep.
2023-02-07 18:30:11.380 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 47 for sweep.
2023-02-07 18:30:11.381 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 4 for sweep.
2023-02-07 18:30:11.381 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0031832995350322343 for sweep.
2023-02-07 18:30:11.381 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 45 for sweep.
2023-02-07 18:30:11.381 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.023787441394491623 for sweep.
2023-02-07 18:30:11.382 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 456 for sweep.
2023-02-07 18:30:11.382 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 376 for sweep.
2023-02-07 18:30:11.382 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.11103841819870812 for sweep.
2023-02-07 18:30:11.382 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.0060461051393499434 for sweep.
2023-02-07 18:30:11.382 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.4540846435096595 for sweep.
2023-02-07 18:30:11.383 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:30:11.387 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 5}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183002-awqdsul2/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 347, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 47, 'window': 4, 'min_count': 4, 'dm': 1, 'sample': 0.7192962621598449, 'workers': 4, 'alpha': 0.03385434792970045, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 456, 'max_depth': 45, 'num_leaves': 376, 'reg_alpha': 0.11103841819870812, 'reg_lambda': 0.0060461051393499434, 'subsample': 0.4540846435096595, 'min_child_weight': 0.023787441394491623, 'n_jobs': 4, 'learning_rate': 0.0031832995350322343}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:20, 161.71it/s]  1%|          | 34/3257 [00:00<00:20, 153.49it/s]  2%|‚ñè         | 53/3257 [00:00<00:19, 160.32it/s]  2%|‚ñè         | 74/3257 [00:00<00:18, 176.78it/s]  3%|‚ñé         | 93/3257 [00:00<00:17, 181.16it/s]  3%|‚ñé         | 112/3257 [00:00<00:18, 173.12it/s]  4%|‚ñç         | 131/3257 [00:00<00:17, 176.05it/s]  5%|‚ñç         | 151/3257 [00:00<00:17, 178.85it/s]  5%|‚ñå         | 169/3257 [00:00<00:17, 176.33it/s]  6%|‚ñå         | 188/3257 [00:01<00:17, 179.18it/s]  6%|‚ñã         | 207/3257 [00:01<00:16, 180.85it/s]  7%|‚ñã         | 231/3257 [00:01<00:15, 195.46it/s]  8%|‚ñä         | 252/3257 [00:01<00:15, 197.98it/s]  8%|‚ñä         | 272/3257 [00:01<00:15, 192.52it/s]  9%|‚ñâ         | 294/3257 [00:01<00:14, 198.38it/s] 10%|‚ñâ         | 314/3257 [00:01<00:15, 187.03it/s] 10%|‚ñà         | 334/3257 [00:01<00:15, 190.33it/s] 11%|‚ñà         | 354/3257 [00:01<00:15, 187.20it/s] 12%|‚ñà‚ñè        | 375/3257 [00:02<00:15, 185.96it/s] 12%|‚ñà‚ñè        | 394/3257 [00:02<00:15, 180.05it/s] 13%|‚ñà‚ñé        | 414/3257 [00:02<00:15, 185.52it/s] 13%|‚ñà‚ñé        | 433/3257 [00:02<00:17, 159.30it/s] 14%|‚ñà‚ñç        | 452/3257 [00:02<00:16, 165.28it/s] 14%|‚ñà‚ñç        | 472/3257 [00:02<00:15, 174.38it/s] 15%|‚ñà‚ñå        | 490/3257 [00:02<00:15, 175.04it/s] 16%|‚ñà‚ñå        | 512/3257 [00:02<00:14, 186.28it/s] 16%|‚ñà‚ñã        | 531/3257 [00:02<00:15, 180.82it/s] 17%|‚ñà‚ñã        | 550/3257 [00:03<00:14, 181.24it/s] 17%|‚ñà‚ñã        | 569/3257 [00:03<00:15, 174.55it/s] 18%|‚ñà‚ñä        | 587/3257 [00:03<00:15, 168.13it/s] 19%|‚ñà‚ñä        | 607/3257 [00:03<00:15, 175.24it/s] 19%|‚ñà‚ñâ        | 625/3257 [00:03<00:15, 174.76it/s] 20%|‚ñà‚ñâ        | 645/3257 [00:03<00:14, 178.19it/s] 20%|‚ñà‚ñà        | 663/3257 [00:03<00:15, 172.50it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:14, 171.97it/s] 22%|‚ñà‚ñà‚ñè       | 703/3257 [00:03<00:14, 178.39it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:04<00:14, 178.03it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:04<00:14, 170.32it/s] 23%|‚ñà‚ñà‚ñé       | 762/3257 [00:04<00:13, 181.87it/s] 24%|‚ñà‚ñà‚ñç       | 781/3257 [00:04<00:14, 175.33it/s] 25%|‚ñà‚ñà‚ñç       | 803/3257 [00:04<00:13, 186.24it/s] 25%|‚ñà‚ñà‚ñå       | 822/3257 [00:04<00:13, 178.37it/s] 26%|‚ñà‚ñà‚ñå       | 840/3257 [00:04<00:14, 167.97it/s] 26%|‚ñà‚ñà‚ñã       | 857/3257 [00:04<00:14, 163.71it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:04<00:14, 165.07it/s] 27%|‚ñà‚ñà‚ñã       | 893/3257 [00:05<00:14, 168.35it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:05<00:13, 174.06it/s] 29%|‚ñà‚ñà‚ñä       | 933/3257 [00:05<00:13, 177.59it/s] 29%|‚ñà‚ñà‚ñâ       | 954/3257 [00:05<00:12, 186.30it/s] 30%|‚ñà‚ñà‚ñâ       | 973/3257 [00:05<00:12, 186.14it/s] 30%|‚ñà‚ñà‚ñà       | 992/3257 [00:05<00:12, 183.56it/s] 31%|‚ñà‚ñà‚ñà       | 1011/3257 [00:05<00:12, 173.83it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1029/3257 [00:05<00:12, 172.16it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:05<00:12, 170.15it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1065/3257 [00:06<00:13, 167.67it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1082/3257 [00:06<00:24, 88.23it/s]  34%|‚ñà‚ñà‚ñà‚ñé      | 1098/3257 [00:06<00:21, 100.04it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1116/3257 [00:06<00:18, 114.97it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1131/3257 [00:06<00:18, 117.68it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:06<00:16, 126.88it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1168/3257 [00:06<00:14, 146.07it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1185/3257 [00:07<00:14, 142.54it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1201/3257 [00:07<00:14, 141.57it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1219/3257 [00:07<00:13, 149.82it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:07<00:11, 169.90it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1261/3257 [00:07<00:11, 175.30it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1280/3257 [00:07<00:11, 165.04it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1297/3257 [00:07<00:12, 158.23it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1316/3257 [00:07<00:11, 165.02it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1334/3257 [00:07<00:11, 167.85it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1352/3257 [00:08<00:11, 167.73it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:08<00:11, 167.88it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1387/3257 [00:08<00:11, 164.00it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1406/3257 [00:08<00:10, 170.09it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1425/3257 [00:08<00:10, 175.09it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1443/3257 [00:08<00:10, 172.64it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1465/3257 [00:08<00:09, 185.12it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1484/3257 [00:08<00:09, 186.24it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1507/3257 [00:08<00:08, 198.89it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1527/3257 [00:09<00:09, 179.82it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1546/3257 [00:09<00:10, 169.03it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1564/3257 [00:09<00:09, 171.00it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1582/3257 [00:09<00:09, 169.36it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1603/3257 [00:09<00:09, 178.56it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1622/3257 [00:09<00:09, 180.70it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1641/3257 [00:09<00:09, 177.11it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:09<00:09, 172.94it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1677/3257 [00:09<00:09, 169.57it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1695/3257 [00:10<00:09, 172.23it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:10<00:08, 175.31it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1733/3257 [00:10<00:09, 164.24it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1751/3257 [00:10<00:09, 167.25it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1771/3257 [00:10<00:08, 173.97it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1792/3257 [00:10<00:08, 182.15it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1811/3257 [00:10<00:08, 169.73it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1829/3257 [00:10<00:08, 168.35it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1849/3257 [00:10<00:08, 175.69it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1870/3257 [00:11<00:07, 183.89it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1889/3257 [00:11<00:07, 182.70it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1908/3257 [00:11<00:07, 183.41it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1927/3257 [00:11<00:07, 180.78it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1948/3257 [00:11<00:06, 187.41it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1971/3257 [00:11<00:06, 198.35it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1991/3257 [00:11<00:06, 187.80it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2010/3257 [00:11<00:06, 185.00it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2032/3257 [00:11<00:06, 194.59it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2052/3257 [00:12<00:06, 178.45it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2071/3257 [00:12<00:06, 178.81it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2091/3257 [00:12<00:06, 184.44it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2110/3257 [00:12<00:06, 182.85it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2129/3257 [00:12<00:06, 173.14it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2147/3257 [00:12<00:06, 174.15it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2166/3257 [00:12<00:06, 176.78it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2184/3257 [00:12<00:06, 176.29it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:12<00:05, 185.16it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2225/3257 [00:12<00:05, 185.22it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2244/3257 [00:13<00:05, 184.39it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2265/3257 [00:13<00:05, 189.22it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2285/3257 [00:13<00:05, 190.03it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2305/3257 [00:13<00:05, 190.09it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2329/3257 [00:13<00:04, 203.32it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2353/3257 [00:13<00:04, 213.91it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2375/3257 [00:13<00:04, 208.53it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2398/3257 [00:13<00:04, 212.35it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2420/3257 [00:13<00:04, 198.90it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2441/3257 [00:14<00:08, 97.97it/s]  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2462/3257 [00:14<00:06, 115.25it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2483/3257 [00:14<00:05, 131.69it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2504/3257 [00:14<00:05, 146.55it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2525/3257 [00:14<00:04, 160.71it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2545/3257 [00:14<00:04, 167.47it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2565/3257 [00:15<00:04, 167.09it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2584/3257 [00:15<00:03, 170.12it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2605/3257 [00:15<00:03, 178.71it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:15<00:03, 197.91it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2652/3257 [00:15<00:03, 192.08it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2674/3257 [00:15<00:02, 197.67it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2695/3257 [00:15<00:02, 194.71it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2715/3257 [00:15<00:03, 179.88it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2738/3257 [00:15<00:02, 191.25it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2758/3257 [00:16<00:02, 185.95it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2777/3257 [00:16<00:02, 184.69it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2803/3257 [00:16<00:02, 202.97it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2824/3257 [00:16<00:02, 186.12it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2843/3257 [00:16<00:02, 187.15it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2866/3257 [00:16<00:01, 198.97it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2888/3257 [00:16<00:01, 204.51it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2909/3257 [00:16<00:01, 196.15it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2929/3257 [00:16<00:01, 194.30it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2949/3257 [00:17<00:01, 172.73it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2967/3257 [00:17<00:01, 168.04it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:17<00:01, 163.76it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3009/3257 [00:17<00:01, 183.62it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3028/3257 [00:17<00:01, 178.26it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3050/3257 [00:17<00:01, 188.59it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3075/3257 [00:17<00:00, 202.86it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3096/3257 [00:17<00:00, 198.68it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3119/3257 [00:17<00:00, 206.29it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3140/3257 [00:18<00:00, 199.38it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3161/3257 [00:18<00:00, 195.14it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3181/3257 [00:18<00:00, 185.13it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3204/3257 [00:18<00:00, 194.59it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3224/3257 [00:18<00:00, 186.97it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:18<00:00, 188.04it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 174.31it/s]
2023-02-07 18:30:30.751 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:30:30,752][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d47,n5,w4,mc4,s0.719296,t4>', 'datetime': '2023-02-07T18:30:30.752763', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:30:30,753][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:30:30,754][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:30:31,169][gensim.models.doc2vec][INFO] - collected 21699 word types and 3257 unique tags from a corpus of 3257 examples and 4367244 words
[2023-02-07 18:30:31,169][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:30:31,210][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 14837 unique words (68.38% of original 21699, drops 6862)', 'datetime': '2023-02-07T18:30:31.210056', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:30:31,210][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 4356054 word corpus (99.74% of original 4367244, drops 11190)', 'datetime': '2023-02-07T18:30:31.210470', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:30:31,264][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 21699 items
[2023-02-07 18:30:31,265][gensim.models.word2vec][INFO] - sample=0.719296 downsamples 0 most-common words
[2023-02-07 18:30:31,266][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4356054 word corpus (100.0%% of prior 4356054)', 'datetime': '2023-02-07T18:30:31.266128', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:30:31,355][gensim.models.word2vec][INFO] - estimated required memory for 14837 words and 47 dimensions: 14260928 bytes
[2023-02-07 18:30:31,355][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:30:31,360][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 14837 vocabulary and 47 features, using sg=0 hs=0 sample=0.7192962621598449 negative=5 window=4 shrink_windows=True', 'datetime': '2023-02-07T18:30:31.360564', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:30:32,364][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 51.43% examples, 2292310 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:30:33,242][gensim.models.word2vec][INFO] - EPOCH 0: training on 4367244 raw words (4357583 effective words) took 1.9s, 2318896 effective words/s
[2023-02-07 18:30:34,246][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 63.99% examples, 2832390 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:30:34,762][gensim.models.word2vec][INFO] - EPOCH 1: training on 4367244 raw words (4357583 effective words) took 1.5s, 2869927 effective words/s
[2023-02-07 18:30:35,768][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 53.82% examples, 2399197 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:30:36,565][gensim.models.word2vec][INFO] - EPOCH 2: training on 4367244 raw words (4357583 effective words) took 1.8s, 2420040 effective words/s
[2023-02-07 18:30:37,578][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 65.98% examples, 2903818 words/s, in_qsize 8, out_qsize 1
[2023-02-07 18:30:38,069][gensim.models.word2vec][INFO] - EPOCH 3: training on 4367244 raw words (4357583 effective words) took 1.5s, 2899073 effective words/s
[2023-02-07 18:30:39,071][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 65.55% examples, 2916379 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:30:39,570][gensim.models.word2vec][INFO] - EPOCH 4: training on 4367244 raw words (4357583 effective words) took 1.5s, 2906806 effective words/s
[2023-02-07 18:30:40,573][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 64.94% examples, 2878437 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:30:41,066][gensim.models.word2vec][INFO] - EPOCH 5: training on 4367244 raw words (4357583 effective words) took 1.5s, 2916178 effective words/s
[2023-02-07 18:30:42,070][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 66.10% examples, 2937471 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:30:42,562][gensim.models.word2vec][INFO] - EPOCH 6: training on 4367244 raw words (4357583 effective words) took 1.5s, 2914772 effective words/s
[2023-02-07 18:30:43,566][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 65.09% examples, 2887300 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:30:44,053][gensim.models.word2vec][INFO] - EPOCH 7: training on 4367244 raw words (4357583 effective words) took 1.5s, 2926237 effective words/s
[2023-02-07 18:30:45,055][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 66.96% examples, 2981680 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:30:45,501][gensim.models.word2vec][INFO] - EPOCH 8: training on 4367244 raw words (4357583 effective words) took 1.4s, 3012904 effective words/s
[2023-02-07 18:30:46,504][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 68.07% examples, 3033121 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:30:47,006][gensim.models.word2vec][INFO] - EPOCH 9: training on 4367244 raw words (4357583 effective words) took 1.5s, 2899299 effective words/s
[2023-02-07 18:30:48,012][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 66.84% examples, 2962002 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:30:48,488][gensim.models.word2vec][INFO] - EPOCH 10: training on 4367244 raw words (4357583 effective words) took 1.5s, 2943841 effective words/s
[2023-02-07 18:30:49,492][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 65.09% examples, 2888595 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:30:49,999][gensim.models.word2vec][INFO] - EPOCH 11: training on 4367244 raw words (4357583 effective words) took 1.5s, 2887977 effective words/s
[2023-02-07 18:30:51,003][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 64.81% examples, 2870849 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:30:51,497][gensim.models.word2vec][INFO] - EPOCH 12: training on 4367244 raw words (4357583 effective words) took 1.5s, 2912322 effective words/s
[2023-02-07 18:30:52,501][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 55.45% examples, 2474331 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:30:53,240][gensim.models.word2vec][INFO] - EPOCH 13: training on 4367244 raw words (4357583 effective words) took 1.7s, 2504258 effective words/s
[2023-02-07 18:30:54,243][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 62.70% examples, 2772767 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:30:54,804][gensim.models.word2vec][INFO] - EPOCH 14: training on 4367244 raw words (4357583 effective words) took 1.6s, 2787477 effective words/s
[2023-02-07 18:30:54,805][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 65508660 raw words (65363745 effective words) took 23.4s, 2788037 effective words/s', 'datetime': '2023-02-07T18:30:54.805419', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:30:54.806 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:30:56,915][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183002-awqdsul2/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:30:56.915792', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:30:56,916][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:30:56,941][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183002-awqdsul2/files/../tmp/embedding_model.pt
2023-02-07 18:30:56.941 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:30:58.019 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:30:58.436 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:30:59.097 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.4210016235467355, 'test_mae': 1.1483415625489495, 'test_r2': -2.622166613298999}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.86
wandb: percentage 0.31624
wandb:   test_mae 1.14834
wandb:   test_mse 2.421
wandb:    test_r2 -2.62217
wandb: 
wandb: üöÄ View run glamorous-sweep-7 at: https://wandb.ai/xiaoqiz/mof2vec/runs/awqdsul2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_183002-awqdsul2/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 1ijujfh4 with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 384
wandb: 	model.gensim.alpha: 0.6375248339844838
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 10
wandb: 	model.gensim.sample: 0.25696976604043303
wandb: 	model.gensim.vector_size: 85
wandb: 	model.gensim.window: 6
wandb: 	model.sklearn.learning_rate: 0.0008044196398056318
wandb: 	model.sklearn.max_depth: 98
wandb: 	model.sklearn.min_child_weight: 0.04639619420024314
wandb: 	model.sklearn.n_estimators: 3895
wandb: 	model.sklearn.num_leaves: 228
wandb: 	model.sklearn.reg_alpha: 0.0029987868696989338
wandb: 	model.sklearn.reg_lambda: 0.5054798665276307
wandb: 	model.sklearn.subsample: 0.4987870297736741
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183117-1ijujfh4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/1ijujfh4
2023-02-07 18:31:27.008 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 18:31:27.009 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 384 for sweep.
2023-02-07 18:31:27.009 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.6375248339844838 for sweep.
2023-02-07 18:31:27.010 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 18:31:27.010 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 10 for sweep.
2023-02-07 18:31:27.010 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.25696976604043303 for sweep.
2023-02-07 18:31:27.010 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 85 for sweep.
2023-02-07 18:31:27.011 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 6 for sweep.
2023-02-07 18:31:27.011 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0008044196398056318 for sweep.
2023-02-07 18:31:27.011 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 98 for sweep.
2023-02-07 18:31:27.011 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.04639619420024314 for sweep.
2023-02-07 18:31:27.011 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3895 for sweep.
2023-02-07 18:31:27.012 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 228 for sweep.
2023-02-07 18:31:27.012 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.0029987868696989338 for sweep.
2023-02-07 18:31:27.012 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.5054798665276307 for sweep.
2023-02-07 18:31:27.012 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.4987870297736741 for sweep.
2023-02-07 18:31:27.013 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:31:27.018 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183117-1ijujfh4/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 384, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 85, 'window': 6, 'min_count': 10, 'dm': 0, 'sample': 0.25696976604043303, 'workers': 4, 'alpha': 0.6375248339844838, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3895, 'max_depth': 98, 'num_leaves': 228, 'reg_alpha': 0.0029987868696989338, 'reg_lambda': 0.5054798665276307, 'subsample': 0.4987870297736741, 'min_child_weight': 0.04639619420024314, 'n_jobs': 4, 'learning_rate': 0.0008044196398056318}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:18, 176.78it/s]  1%|          | 40/3257 [00:00<00:16, 199.13it/s]  2%|‚ñè         | 64/3257 [00:00<00:14, 216.24it/s]  3%|‚ñé         | 88/3257 [00:00<00:14, 225.36it/s]  3%|‚ñé         | 111/3257 [00:00<00:14, 213.12it/s]  4%|‚ñç         | 135/3257 [00:00<00:14, 219.97it/s]  5%|‚ñç         | 158/3257 [00:00<00:13, 222.50it/s]  6%|‚ñå         | 181/3257 [00:00<00:14, 217.27it/s]  6%|‚ñã         | 204/3257 [00:00<00:13, 218.68it/s]  7%|‚ñã         | 232/3257 [00:01<00:12, 234.83it/s]  8%|‚ñä         | 257/3257 [00:01<00:12, 235.45it/s]  9%|‚ñä         | 284/3257 [00:01<00:12, 245.12it/s]  9%|‚ñâ         | 309/3257 [00:01<00:12, 241.50it/s] 10%|‚ñà         | 334/3257 [00:01<00:12, 242.82it/s] 11%|‚ñà         | 359/3257 [00:01<00:11, 242.32it/s] 12%|‚ñà‚ñè        | 384/3257 [00:01<00:12, 233.55it/s] 13%|‚ñà‚ñé        | 408/3257 [00:01<00:12, 230.08it/s] 13%|‚ñà‚ñé        | 432/3257 [00:01<00:13, 212.30it/s] 14%|‚ñà‚ñç        | 454/3257 [00:02<00:13, 213.39it/s] 15%|‚ñà‚ñç        | 476/3257 [00:02<00:19, 143.14it/s] 15%|‚ñà‚ñå        | 501/3257 [00:02<00:16, 164.08it/s] 16%|‚ñà‚ñå        | 525/3257 [00:02<00:15, 179.78it/s] 17%|‚ñà‚ñã        | 547/3257 [00:02<00:14, 189.51it/s] 17%|‚ñà‚ñã        | 568/3257 [00:02<00:14, 190.58it/s] 18%|‚ñà‚ñä        | 589/3257 [00:02<00:14, 186.25it/s] 19%|‚ñà‚ñâ        | 611/3257 [00:02<00:13, 192.77it/s] 19%|‚ñà‚ñâ        | 633/3257 [00:03<00:13, 199.31it/s] 20%|‚ñà‚ñà        | 654/3257 [00:03<00:12, 200.86it/s] 21%|‚ñà‚ñà        | 675/3257 [00:03<00:12, 201.71it/s] 21%|‚ñà‚ñà‚ñè       | 696/3257 [00:03<00:13, 196.75it/s] 22%|‚ñà‚ñà‚ñè       | 720/3257 [00:03<00:12, 205.64it/s] 23%|‚ñà‚ñà‚ñé       | 741/3257 [00:03<00:12, 199.94it/s] 24%|‚ñà‚ñà‚ñé       | 767/3257 [00:03<00:11, 216.24it/s] 24%|‚ñà‚ñà‚ñç       | 789/3257 [00:03<00:11, 210.14it/s] 25%|‚ñà‚ñà‚ñç       | 813/3257 [00:03<00:11, 218.10it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:04<00:11, 210.49it/s] 26%|‚ñà‚ñà‚ñã       | 857/3257 [00:04<00:11, 201.03it/s] 27%|‚ñà‚ñà‚ñã       | 878/3257 [00:04<00:12, 197.74it/s] 28%|‚ñà‚ñà‚ñä       | 902/3257 [00:04<00:11, 208.05it/s] 28%|‚ñà‚ñà‚ñä       | 925/3257 [00:04<00:10, 213.67it/s] 29%|‚ñà‚ñà‚ñâ       | 947/3257 [00:04<00:10, 215.16it/s] 30%|‚ñà‚ñà‚ñâ       | 969/3257 [00:04<00:10, 215.83it/s] 30%|‚ñà‚ñà‚ñà       | 991/3257 [00:04<00:10, 213.76it/s] 31%|‚ñà‚ñà‚ñà       | 1013/3257 [00:04<00:10, 210.08it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1035/3257 [00:04<00:10, 206.77it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1056/3257 [00:05<00:10, 202.88it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:05<00:10, 204.24it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1103/3257 [00:05<00:10, 208.44it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1125/3257 [00:05<00:10, 208.94it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1146/3257 [00:05<00:10, 204.62it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1168/3257 [00:05<00:10, 206.57it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1189/3257 [00:05<00:11, 182.38it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1208/3257 [00:05<00:11, 184.15it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1233/3257 [00:05<00:10, 202.12it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1255/3257 [00:06<00:09, 206.77it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1277/3257 [00:06<00:09, 204.70it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1298/3257 [00:06<00:10, 194.75it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1320/3257 [00:06<00:09, 200.13it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1344/3257 [00:06<00:09, 209.74it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1366/3257 [00:06<00:09, 205.48it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1387/3257 [00:06<00:09, 204.87it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1413/3257 [00:06<00:08, 219.99it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1437/3257 [00:06<00:08, 225.67it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1465/3257 [00:07<00:07, 239.90it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1490/3257 [00:07<00:07, 239.44it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1515/3257 [00:07<00:07, 242.41it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1540/3257 [00:07<00:07, 227.31it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1563/3257 [00:07<00:07, 223.31it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1586/3257 [00:07<00:07, 218.41it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1610/3257 [00:07<00:07, 223.68it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:07<00:07, 220.98it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1657/3257 [00:07<00:07, 217.99it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1679/3257 [00:08<00:07, 213.61it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1703/3257 [00:08<00:07, 219.93it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:08<00:07, 216.61it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1750/3257 [00:08<00:10, 140.87it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1774/3257 [00:08<00:09, 160.27it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1799/3257 [00:08<00:08, 179.39it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1821/3257 [00:08<00:07, 188.18it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1842/3257 [00:08<00:07, 192.96it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1865/3257 [00:09<00:06, 200.72it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1888/3257 [00:09<00:06, 207.94it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1915/3257 [00:09<00:05, 224.74it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1939/3257 [00:09<00:05, 228.05it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1967/3257 [00:09<00:05, 241.92it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1992/3257 [00:09<00:05, 236.74it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2016/3257 [00:09<00:05, 236.51it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2040/3257 [00:09<00:05, 233.86it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2064/3257 [00:09<00:05, 226.60it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2088/3257 [00:09<00:05, 229.72it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2112/3257 [00:10<00:05, 227.83it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2135/3257 [00:10<00:05, 218.17it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2157/3257 [00:10<00:05, 215.04it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:10<00:04, 219.03it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:10<00:04, 226.54it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2230/3257 [00:10<00:04, 229.32it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2253/3257 [00:10<00:04, 228.63it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2276/3257 [00:10<00:04, 219.86it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2300/3257 [00:10<00:04, 225.25it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2327/3257 [00:11<00:03, 236.16it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2353/3257 [00:11<00:03, 241.98it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2379/3257 [00:11<00:03, 246.51it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:11<00:03, 246.79it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2430/3257 [00:11<00:03, 243.09it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2455/3257 [00:11<00:03, 234.47it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2481/3257 [00:11<00:03, 239.29it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2510/3257 [00:11<00:02, 252.76it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2537/3257 [00:11<00:02, 256.20it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2563/3257 [00:11<00:02, 242.81it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2588/3257 [00:12<00:02, 233.06it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2619/3257 [00:12<00:02, 253.11it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2645/3257 [00:12<00:02, 252.25it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2671/3257 [00:12<00:02, 246.73it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2696/3257 [00:12<00:02, 235.76it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2720/3257 [00:12<00:02, 227.07it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2750/3257 [00:12<00:02, 245.80it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2775/3257 [00:12<00:02, 236.22it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2805/3257 [00:12<00:01, 250.59it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2831/3257 [00:13<00:01, 233.93it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2857/3257 [00:13<00:01, 238.86it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:13<00:01, 249.62it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2912/3257 [00:13<00:01, 240.64it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2937/3257 [00:13<00:01, 238.92it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2962/3257 [00:13<00:01, 232.62it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2986/3257 [00:13<00:01, 223.68it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:13<00:01, 232.45it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3039/3257 [00:13<00:00, 239.27it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3069/3257 [00:14<00:00, 255.27it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3095/3257 [00:14<00:00, 254.87it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3124/3257 [00:14<00:00, 262.21it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3151/3257 [00:14<00:00, 247.87it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3176/3257 [00:14<00:00, 245.24it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3201/3257 [00:14<00:00, 138.40it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3225/3257 [00:14<00:00, 156.49it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:15<00:00, 181.35it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:15<00:00, 215.54it/s]
2023-02-07 18:31:42.522 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:31:42,523][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d85,n5,mc10,s0.25697,t4>', 'datetime': '2023-02-07T18:31:42.523297', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:31:42,523][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:31:42,523][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:31:42,820][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 18:31:42,821][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:31:42,830][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=10 retains 2697 unique words (40.48% of original 6662, drops 3965)', 'datetime': '2023-02-07T18:31:42.830212', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:31:42,830][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 2897377 word corpus (99.52% of original 2911496, drops 14119)', 'datetime': '2023-02-07T18:31:42.830669', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:31:42,840][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 18:31:42,840][gensim.models.word2vec][INFO] - sample=0.25697 downsamples 0 most-common words
[2023-02-07 18:31:42,840][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2897377 word corpus (100.0%% of prior 2897377)', 'datetime': '2023-02-07T18:31:42.840688', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:31:42,856][gensim.models.word2vec][INFO] - estimated required memory for 2697 words and 85 dimensions: 4941240 bytes
[2023-02-07 18:31:42,856][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:31:42,860][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2697 vocabulary and 85 features, using sg=1 hs=0 sample=0.25696976604043303 negative=5 window=6 shrink_windows=True', 'datetime': '2023-02-07T18:31:42.859975', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:31:43,363][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2900634 effective words) took 0.5s, 5786326 effective words/s
[2023-02-07 18:31:43,909][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2900634 effective words) took 0.5s, 5329925 effective words/s
[2023-02-07 18:31:44,480][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2900634 effective words) took 0.6s, 5101906 effective words/s
[2023-02-07 18:31:45,021][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2900634 effective words) took 0.5s, 5374821 effective words/s
[2023-02-07 18:31:45,575][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2900634 effective words) took 0.6s, 5255660 effective words/s
[2023-02-07 18:31:46,148][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2900634 effective words) took 0.6s, 5078762 effective words/s
[2023-02-07 18:31:46,643][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2900634 effective words) took 0.5s, 5880824 effective words/s
[2023-02-07 18:31:47,182][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2900634 effective words) took 0.5s, 5391655 effective words/s
[2023-02-07 18:31:47,761][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2900634 effective words) took 0.6s, 5026345 effective words/s
[2023-02-07 18:31:48,305][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2900634 effective words) took 0.5s, 5342895 effective words/s
[2023-02-07 18:31:48,852][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2900634 effective words) took 0.5s, 5324427 effective words/s
[2023-02-07 18:31:49,391][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2900634 effective words) took 0.5s, 5397603 effective words/s
[2023-02-07 18:31:49,929][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2900634 effective words) took 0.5s, 5414552 effective words/s
[2023-02-07 18:31:50,481][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2900634 effective words) took 0.6s, 5268120 effective words/s
[2023-02-07 18:31:51,028][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2900634 effective words) took 0.5s, 5315427 effective words/s
[2023-02-07 18:31:51,029][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 43672440 raw words (43509510 effective words) took 8.2s, 5326234 effective words/s', 'datetime': '2023-02-07T18:31:51.029301', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:31:51.029 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:31:51,997][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183117-1ijujfh4/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:31:51.997477', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:31:51,998][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:31:52,007][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183117-1ijujfh4/files/../tmp/embedding_model.pt
2023-02-07 18:31:52.008 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:31:53.118 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:31:53.564 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:31:54.470 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.9254142140666763, 'test_mae': 1.276163100986313, 'test_r2': -3.633133360108726}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.0
wandb: percentage 0.59517
wandb:   test_mae 1.27616
wandb:   test_mse 2.92541
wandb:    test_r2 -3.63313
wandb: 
wandb: üöÄ View run atomic-sweep-8 at: https://wandb.ai/xiaoqiz/mof2vec/runs/1ijujfh4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_183117-1ijujfh4/logs
wandb: Agent Starting Run: eoyhdctd with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 948
wandb: 	model.gensim.alpha: 0.0007204382884835643
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.7589972798056355
wandb: 	model.gensim.vector_size: 219
wandb: 	model.gensim.window: 19
wandb: 	model.sklearn.learning_rate: 0.01049120286786472
wandb: 	model.sklearn.max_depth: 9
wandb: 	model.sklearn.min_child_weight: 0.08445655254621327
wandb: 	model.sklearn.n_estimators: 3066
wandb: 	model.sklearn.num_leaves: 105
wandb: 	model.sklearn.reg_alpha: 0.00958839410712133
wandb: 	model.sklearn.reg_lambda: 0.04046106485200935
wandb: 	model.sklearn.subsample: 0.8322589238455993
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183204-eoyhdctd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sweep-9
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/eoyhdctd
2023-02-07 18:32:13.216 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 18:32:13.217 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 948 for sweep.
2023-02-07 18:32:13.217 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0007204382884835643 for sweep.
2023-02-07 18:32:13.217 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 18:32:13.217 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 18:32:13.218 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7589972798056355 for sweep.
2023-02-07 18:32:13.218 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 219 for sweep.
2023-02-07 18:32:13.218 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 19 for sweep.
2023-02-07 18:32:13.218 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.01049120286786472 for sweep.
2023-02-07 18:32:13.219 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 9 for sweep.
2023-02-07 18:32:13.219 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.08445655254621327 for sweep.
2023-02-07 18:32:13.219 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3066 for sweep.
2023-02-07 18:32:13.219 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 105 for sweep.
2023-02-07 18:32:13.219 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.00958839410712133 for sweep.
2023-02-07 18:32:13.220 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.04046106485200935 for sweep.
2023-02-07 18:32:13.220 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8322589238455993 for sweep.
2023-02-07 18:32:13.220 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:32:13.225 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183204-eoyhdctd/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 948, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 219, 'window': 19, 'min_count': 4, 'dm': 0, 'sample': 0.7589972798056355, 'workers': 4, 'alpha': 0.0007204382884835643, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3066, 'max_depth': 9, 'num_leaves': 105, 'reg_alpha': 0.00958839410712133, 'reg_lambda': 0.04046106485200935, 'subsample': 0.8322589238455993, 'min_child_weight': 0.08445655254621327, 'n_jobs': 4, 'learning_rate': 0.01049120286786472}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 24/3257 [00:00<00:13, 239.12it/s]  1%|‚ñè         | 48/3257 [00:00<00:13, 235.86it/s]  2%|‚ñè         | 72/3257 [00:00<00:13, 236.50it/s]  3%|‚ñé         | 96/3257 [00:00<00:13, 236.07it/s]  4%|‚ñé         | 120/3257 [00:00<00:13, 235.59it/s]  5%|‚ñç         | 148/3257 [00:00<00:12, 250.15it/s]  5%|‚ñå         | 174/3257 [00:00<00:12, 244.09it/s]  6%|‚ñå         | 201/3257 [00:00<00:12, 246.63it/s]  7%|‚ñã         | 231/3257 [00:00<00:11, 259.19it/s]  8%|‚ñä         | 257/3257 [00:01<00:11, 254.03it/s]  9%|‚ñâ         | 286/3257 [00:01<00:11, 262.60it/s] 10%|‚ñâ         | 313/3257 [00:01<00:11, 256.12it/s] 10%|‚ñà         | 341/3257 [00:01<00:11, 262.05it/s] 11%|‚ñà‚ñè        | 369/3257 [00:01<00:10, 266.84it/s] 12%|‚ñà‚ñè        | 396/3257 [00:01<00:11, 250.66it/s] 13%|‚ñà‚ñé        | 422/3257 [00:01<00:11, 244.66it/s] 14%|‚ñà‚ñé        | 447/3257 [00:01<00:12, 221.54it/s] 15%|‚ñà‚ñç        | 473/3257 [00:01<00:12, 230.86it/s] 15%|‚ñà‚ñå        | 497/3257 [00:02<00:11, 231.30it/s] 16%|‚ñà‚ñå        | 522/3257 [00:02<00:11, 236.47it/s] 17%|‚ñà‚ñã        | 546/3257 [00:02<00:11, 235.86it/s] 18%|‚ñà‚ñä        | 570/3257 [00:02<00:11, 231.82it/s] 18%|‚ñà‚ñä        | 594/3257 [00:02<00:11, 230.17it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:02<00:11, 231.26it/s] 20%|‚ñà‚ñâ        | 645/3257 [00:02<00:11, 233.00it/s] 21%|‚ñà‚ñà        | 669/3257 [00:02<00:11, 228.16it/s] 21%|‚ñà‚ñà        | 692/3257 [00:02<00:11, 223.60it/s] 22%|‚ñà‚ñà‚ñè       | 718/3257 [00:02<00:10, 232.74it/s] 23%|‚ñà‚ñà‚ñé       | 742/3257 [00:03<00:11, 220.55it/s] 24%|‚ñà‚ñà‚ñé       | 769/3257 [00:03<00:10, 232.22it/s] 24%|‚ñà‚ñà‚ñç       | 793/3257 [00:03<00:10, 224.51it/s] 25%|‚ñà‚ñà‚ñå       | 818/3257 [00:03<00:10, 227.74it/s] 26%|‚ñà‚ñà‚ñå       | 841/3257 [00:03<00:11, 205.85it/s] 26%|‚ñà‚ñà‚ñã       | 863/3257 [00:03<00:11, 209.36it/s] 27%|‚ñà‚ñà‚ñã       | 885/3257 [00:03<00:11, 206.91it/s] 28%|‚ñà‚ñà‚ñä       | 913/3257 [00:03<00:10, 224.83it/s] 29%|‚ñà‚ñà‚ñä       | 936/3257 [00:03<00:10, 221.96it/s] 30%|‚ñà‚ñà‚ñâ       | 961/3257 [00:04<00:10, 227.69it/s] 30%|‚ñà‚ñà‚ñà       | 984/3257 [00:04<00:10, 226.01it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:04<00:10, 212.92it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1029/3257 [00:04<00:10, 208.20it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1050/3257 [00:04<00:10, 203.78it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1075/3257 [00:04<00:10, 216.59it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1097/3257 [00:04<00:10, 213.78it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1121/3257 [00:04<00:09, 219.41it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1144/3257 [00:04<00:09, 218.70it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1166/3257 [00:05<00:16, 129.17it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1184/3257 [00:05<00:15, 138.03it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1204/3257 [00:05<00:13, 149.77it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1229/3257 [00:05<00:11, 172.62it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1251/3257 [00:05<00:10, 183.42it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1272/3257 [00:05<00:10, 190.00it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1293/3257 [00:05<00:10, 183.24it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1316/3257 [00:06<00:09, 195.27it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1342/3257 [00:06<00:09, 210.92it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1364/3257 [00:06<00:08, 211.32it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1386/3257 [00:06<00:08, 210.24it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1413/3257 [00:06<00:08, 225.67it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1436/3257 [00:06<00:08, 222.92it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1461/3257 [00:06<00:07, 228.97it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1485/3257 [00:06<00:07, 223.81it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1512/3257 [00:06<00:07, 234.43it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1536/3257 [00:07<00:07, 221.90it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1559/3257 [00:07<00:07, 222.20it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1582/3257 [00:07<00:07, 219.75it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1605/3257 [00:07<00:07, 219.45it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1630/3257 [00:07<00:07, 227.41it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1653/3257 [00:07<00:07, 214.52it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1675/3257 [00:07<00:07, 211.67it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1697/3257 [00:07<00:07, 213.53it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1719/3257 [00:07<00:07, 211.29it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1741/3257 [00:07<00:07, 197.27it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1766/3257 [00:08<00:07, 211.33it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1793/3257 [00:08<00:06, 224.34it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1816/3257 [00:08<00:06, 222.44it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1841/3257 [00:08<00:06, 228.47it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1866/3257 [00:08<00:05, 233.95it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1890/3257 [00:08<00:05, 232.68it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:08<00:05, 233.32it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1945/3257 [00:08<00:05, 246.95it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1970/3257 [00:08<00:05, 243.07it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1995/3257 [00:09<00:05, 230.58it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2019/3257 [00:09<00:05, 227.50it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2042/3257 [00:09<00:05, 226.03it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2065/3257 [00:09<00:05, 220.16it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2088/3257 [00:09<00:05, 222.02it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2112/3257 [00:09<00:05, 221.58it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2135/3257 [00:09<00:05, 215.87it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2157/3257 [00:09<00:05, 214.78it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2179/3257 [00:09<00:05, 213.83it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2201/3257 [00:10<00:05, 205.42it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2222/3257 [00:10<00:05, 204.67it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2243/3257 [00:10<00:04, 204.79it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2267/3257 [00:10<00:04, 213.70it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2291/3257 [00:10<00:04, 220.74it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2315/3257 [00:10<00:04, 224.07it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2343/3257 [00:10<00:03, 238.74it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2370/3257 [00:10<00:03, 247.36it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2397/3257 [00:10<00:03, 252.60it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2423/3257 [00:10<00:03, 243.68it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2448/3257 [00:11<00:03, 230.19it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2472/3257 [00:11<00:05, 139.79it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2498/3257 [00:11<00:04, 161.74it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2525/3257 [00:11<00:03, 183.31it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2548/3257 [00:11<00:03, 193.86it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2571/3257 [00:11<00:03, 186.14it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2593/3257 [00:11<00:03, 192.72it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2624/3257 [00:12<00:02, 222.71it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2648/3257 [00:12<00:02, 224.18it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2672/3257 [00:12<00:02, 224.44it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2696/3257 [00:12<00:02, 221.70it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2719/3257 [00:12<00:02, 212.80it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2748/3257 [00:12<00:02, 232.82it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2772/3257 [00:12<00:02, 227.16it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2796/3257 [00:12<00:02, 229.68it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2820/3257 [00:12<00:01, 231.95it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:13<00:01, 230.16it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2879/3257 [00:13<00:01, 262.46it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2906/3257 [00:13<00:01, 246.75it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2931/3257 [00:13<00:01, 245.91it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2956/3257 [00:13<00:01, 238.15it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2981/3257 [00:13<00:01, 238.54it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3010/3257 [00:13<00:00, 251.67it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3036/3257 [00:13<00:00, 251.99it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3066/3257 [00:13<00:00, 264.51it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3093/3257 [00:14<00:00, 256.99it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3123/3257 [00:14<00:00, 266.68it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3150/3257 [00:14<00:00, 250.90it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3176/3257 [00:14<00:00, 244.28it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3201/3257 [00:14<00:00, 242.53it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3226/3257 [00:14<00:00, 231.28it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:14<00:00, 239.50it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:14<00:00, 221.78it/s]
2023-02-07 18:32:28.342 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:32:28,345][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d219,n5,mc4,s0.758997,t4>', 'datetime': '2023-02-07T18:32:28.345262', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:32:28,345][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:32:28,345][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:32:28,613][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 18:32:28,615][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:32:28,628][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 4674 unique words (70.16% of original 6662, drops 1988)', 'datetime': '2023-02-07T18:32:28.628290', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:32:28,629][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 2908210 word corpus (99.89% of original 2911496, drops 3286)', 'datetime': '2023-02-07T18:32:28.629542', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:32:28,645][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 18:32:28,646][gensim.models.word2vec][INFO] - sample=0.758997 downsamples 0 most-common words
[2023-02-07 18:32:28,646][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2908210 word corpus (100.0%% of prior 2908210)', 'datetime': '2023-02-07T18:32:28.646499', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:32:28,673][gensim.models.word2vec][INFO] - estimated required memory for 4674 words and 219 dimensions: 14030380 bytes
[2023-02-07 18:32:28,674][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:32:28,682][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 4674 vocabulary and 219 features, using sg=1 hs=0 sample=0.7589972798056355 negative=5 window=19 shrink_windows=True', 'datetime': '2023-02-07T18:32:28.682346', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:32:29,693][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 42.28% examples, 1257015 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:32:30,696][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 88.03% examples, 1282531 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:30,946][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2911467 effective words) took 2.3s, 1287573 effective words/s
[2023-02-07 18:32:31,950][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 69.20% examples, 2055684 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:32,355][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2911467 effective words) took 1.4s, 2069983 effective words/s
[2023-02-07 18:32:33,360][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 39.36% examples, 1171607 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:34,367][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 83.97% examples, 1228421 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:34,699][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2911467 effective words) took 2.3s, 1242955 effective words/s
[2023-02-07 18:32:35,706][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 66.38% examples, 1966390 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:36,216][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2911467 effective words) took 1.5s, 1921791 effective words/s
[2023-02-07 18:32:37,225][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 43.84% examples, 1303209 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:38,024][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2911467 effective words) took 1.8s, 1612343 effective words/s
[2023-02-07 18:32:39,029][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 66.04% examples, 1960775 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:39,498][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2911467 effective words) took 1.5s, 1977757 effective words/s
[2023-02-07 18:32:40,507][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 43.51% examples, 1292639 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:41,513][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 89.59% examples, 1305433 words/s, in_qsize 8, out_qsize 1
[2023-02-07 18:32:41,680][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2911467 effective words) took 2.2s, 1336249 effective words/s
[2023-02-07 18:32:42,683][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 43.51% examples, 1298594 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:43,687][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 89.75% examples, 1309709 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:43,896][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2911467 effective words) took 2.2s, 1314909 effective words/s
[2023-02-07 18:32:44,901][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 42.52% examples, 1273879 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:32:45,904][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 88.52% examples, 1294934 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:46,097][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2911467 effective words) took 2.2s, 1324407 effective words/s
[2023-02-07 18:32:47,099][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 67.70% examples, 2011393 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:47,541][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2911467 effective words) took 1.4s, 2019058 effective words/s
[2023-02-07 18:32:48,547][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 52.29% examples, 1550662 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:49,217][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2911467 effective words) took 1.7s, 1738961 effective words/s
[2023-02-07 18:32:50,223][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 44.58% examples, 1324966 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:51,003][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2911467 effective words) took 1.8s, 1632743 effective words/s
[2023-02-07 18:32:52,010][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 43.94% examples, 1304976 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:53,011][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 90.48% examples, 1322845 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:53,194][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2911467 effective words) took 2.2s, 1330443 effective words/s
[2023-02-07 18:32:54,199][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 43.94% examples, 1307795 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:32:55,215][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 90.54% examples, 1312841 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:55,397][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2911467 effective words) took 2.2s, 1322997 effective words/s
[2023-02-07 18:32:56,402][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 67.70% examples, 2005921 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:32:56,854][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2911467 effective words) took 1.5s, 2001686 effective words/s
[2023-02-07 18:32:56,854][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 43672440 raw words (43672005 effective words) took 28.2s, 1550192 effective words/s', 'datetime': '2023-02-07T18:32:56.854804', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:32:56.855 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:32:58,678][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183204-eoyhdctd/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:32:58.678364', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:32:58,680][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:32:58,717][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183204-eoyhdctd/files/../tmp/embedding_model.pt
2023-02-07 18:32:58.718 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:33:00.352 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:33:00.959 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:33:02.933 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.5665791710296286, 'test_mae': 1.1814939675615541, 'test_r2': -2.232426526867992}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.36
wandb: percentage 0.29841
wandb:   test_mae 1.18149
wandb:   test_mse 2.56658
wandb:    test_r2 -2.23243
wandb: 
wandb: üöÄ View run classic-sweep-9 at: https://wandb.ai/xiaoqiz/mof2vec/runs/eoyhdctd
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_183204-eoyhdctd/logs
wandb: Agent Starting Run: 36xbs0lo with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 751
wandb: 	model.gensim.alpha: 0.004700579823862644
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.4618260786795027
wandb: 	model.gensim.vector_size: 167
wandb: 	model.gensim.window: 19
wandb: 	model.sklearn.learning_rate: 0.030521215690844476
wandb: 	model.sklearn.max_depth: 61
wandb: 	model.sklearn.min_child_weight: 0.09925517951129684
wandb: 	model.sklearn.n_estimators: 1618
wandb: 	model.sklearn.num_leaves: 344
wandb: 	model.sklearn.reg_alpha: 0.05123129932470001
wandb: 	model.sklearn.reg_lambda: 0.2972148470272353
wandb: 	model.sklearn.subsample: 0.5797839759007302
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183316-36xbs0lo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/36xbs0lo
2023-02-07 18:33:25.069 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 18:33:25.070 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 751 for sweep.
2023-02-07 18:33:25.070 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.004700579823862644 for sweep.
2023-02-07 18:33:25.071 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 18:33:25.071 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 18:33:25.071 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4618260786795027 for sweep.
2023-02-07 18:33:25.071 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 167 for sweep.
2023-02-07 18:33:25.072 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 19 for sweep.
2023-02-07 18:33:25.072 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.030521215690844476 for sweep.
2023-02-07 18:33:25.073 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 61 for sweep.
2023-02-07 18:33:25.073 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.09925517951129684 for sweep.
2023-02-07 18:33:25.073 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1618 for sweep.
2023-02-07 18:33:25.073 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 344 for sweep.
2023-02-07 18:33:25.074 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.05123129932470001 for sweep.
2023-02-07 18:33:25.074 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.2972148470272353 for sweep.
2023-02-07 18:33:25.074 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5797839759007302 for sweep.
2023-02-07 18:33:25.074 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:33:25.083 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183316-36xbs0lo/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 751, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 167, 'window': 19, 'min_count': 3, 'dm': 0, 'sample': 0.4618260786795027, 'workers': 4, 'alpha': 0.004700579823862644, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1618, 'max_depth': 61, 'num_leaves': 344, 'reg_alpha': 0.05123129932470001, 'reg_lambda': 0.2972148470272353, 'subsample': 0.5797839759007302, 'min_child_weight': 0.09925517951129684, 'n_jobs': 4, 'learning_rate': 0.030521215690844476}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 25/3257 [00:00<00:13, 245.07it/s]  2%|‚ñè         | 50/3257 [00:00<00:14, 223.88it/s]  2%|‚ñè         | 76/3257 [00:00<00:13, 236.43it/s]  3%|‚ñé         | 104/3257 [00:00<00:12, 247.26it/s]  4%|‚ñç         | 129/3257 [00:00<00:12, 244.37it/s]  5%|‚ñç         | 157/3257 [00:00<00:12, 254.85it/s]  6%|‚ñå         | 183/3257 [00:00<00:12, 250.67it/s]  6%|‚ñã         | 211/3257 [00:00<00:11, 258.56it/s]  7%|‚ñã         | 240/3257 [00:00<00:11, 267.58it/s]  8%|‚ñä         | 267/3257 [00:01<00:11, 260.70it/s]  9%|‚ñâ         | 298/3257 [00:01<00:10, 274.87it/s] 10%|‚ñà         | 326/3257 [00:01<00:10, 273.22it/s] 11%|‚ñà         | 354/3257 [00:01<00:10, 267.89it/s] 12%|‚ñà‚ñè        | 381/3257 [00:01<00:11, 259.59it/s] 13%|‚ñà‚ñé        | 409/3257 [00:01<00:10, 264.98it/s] 13%|‚ñà‚ñé        | 436/3257 [00:01<00:11, 242.36it/s] 14%|‚ñà‚ñç        | 465/3257 [00:01<00:10, 253.95it/s] 15%|‚ñà‚ñå        | 491/3257 [00:01<00:11, 248.94it/s] 16%|‚ñà‚ñå        | 520/3257 [00:02<00:10, 258.99it/s] 17%|‚ñà‚ñã        | 547/3257 [00:02<00:10, 257.38it/s] 18%|‚ñà‚ñä        | 573/3257 [00:02<00:11, 239.97it/s] 18%|‚ñà‚ñä        | 599/3257 [00:02<00:10, 243.14it/s] 19%|‚ñà‚ñâ        | 624/3257 [00:02<00:10, 240.15it/s] 20%|‚ñà‚ñà        | 652/3257 [00:02<00:10, 250.15it/s] 21%|‚ñà‚ñà        | 678/3257 [00:02<00:10, 252.26it/s] 22%|‚ñà‚ñà‚ñè       | 704/3257 [00:02<00:10, 247.61it/s] 22%|‚ñà‚ñà‚ñè       | 729/3257 [00:02<00:10, 241.87it/s] 23%|‚ñà‚ñà‚ñé       | 754/3257 [00:02<00:10, 242.74it/s] 24%|‚ñà‚ñà‚ñç       | 779/3257 [00:03<00:16, 148.23it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:03<00:14, 169.91it/s] 26%|‚ñà‚ñà‚ñå       | 831/3257 [00:03<00:13, 186.10it/s] 26%|‚ñà‚ñà‚ñå       | 853/3257 [00:03<00:12, 192.61it/s] 27%|‚ñà‚ñà‚ñã       | 877/3257 [00:03<00:11, 203.72it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:03<00:10, 225.73it/s] 29%|‚ñà‚ñà‚ñä       | 933/3257 [00:03<00:09, 234.96it/s] 29%|‚ñà‚ñà‚ñâ       | 959/3257 [00:04<00:09, 239.07it/s] 30%|‚ñà‚ñà‚ñà       | 984/3257 [00:04<00:09, 240.04it/s] 31%|‚ñà‚ñà‚ñà       | 1009/3257 [00:04<00:09, 235.12it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1034/3257 [00:04<00:09, 238.06it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1059/3257 [00:04<00:09, 236.53it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1084/3257 [00:04<00:09, 240.19it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1111/3257 [00:04<00:08, 246.74it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1136/3257 [00:04<00:08, 237.98it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1162/3257 [00:04<00:08, 243.77it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1187/3257 [00:04<00:08, 240.38it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1212/3257 [00:05<00:08, 233.83it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:05<00:08, 251.00it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1269/3257 [00:05<00:07, 256.04it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1295/3257 [00:05<00:08, 238.44it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1323/3257 [00:05<00:07, 248.61it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:05<00:07, 253.30it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1376/3257 [00:05<00:07, 247.56it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1402/3257 [00:05<00:07, 248.61it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:05<00:07, 259.99it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1462/3257 [00:06<00:06, 271.15it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1491/3257 [00:06<00:06, 276.35it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:06<00:06, 273.13it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1547/3257 [00:06<00:06, 255.88it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1574/3257 [00:06<00:06, 257.02it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1600/3257 [00:06<00:06, 244.13it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1625/3257 [00:06<00:06, 234.68it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1649/3257 [00:06<00:06, 232.70it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1675/3257 [00:06<00:06, 238.73it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1701/3257 [00:07<00:06, 243.66it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:07<00:06, 243.15it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1753/3257 [00:07<00:06, 240.93it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1780/3257 [00:07<00:05, 247.93it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1807/3257 [00:07<00:05, 249.54it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1835/3257 [00:07<00:05, 257.57it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1865/3257 [00:07<00:05, 267.13it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1892/3257 [00:07<00:05, 262.05it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1920/3257 [00:07<00:05, 265.19it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1952/3257 [00:07<00:04, 280.98it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1981/3257 [00:08<00:04, 274.46it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2010/3257 [00:08<00:04, 277.53it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2038/3257 [00:08<00:04, 277.15it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2066/3257 [00:08<00:04, 261.90it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2093/3257 [00:08<00:04, 263.89it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:08<00:04, 257.25it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2146/3257 [00:08<00:07, 149.24it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2177/3257 [00:09<00:06, 179.32it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2201/3257 [00:09<00:05, 189.77it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2225/3257 [00:09<00:05, 196.92it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2251/3257 [00:09<00:04, 212.09it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2275/3257 [00:09<00:04, 218.99it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2306/3257 [00:09<00:03, 242.96it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2338/3257 [00:09<00:03, 263.19it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2370/3257 [00:09<00:03, 277.13it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2402/3257 [00:09<00:02, 287.60it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2432/3257 [00:10<00:03, 267.42it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2461/3257 [00:10<00:02, 273.35it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2489/3257 [00:10<00:02, 272.38it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2521/3257 [00:10<00:02, 283.75it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:10<00:02, 285.96it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2580/3257 [00:10<00:02, 271.89it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2611/3257 [00:10<00:02, 282.42it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2643/3257 [00:10<00:02, 286.67it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2672/3257 [00:10<00:02, 276.70it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2700/3257 [00:11<00:02, 258.65it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2730/3257 [00:11<00:01, 267.17it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:11<00:01, 273.68it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2789/3257 [00:11<00:01, 247.41it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2816/3257 [00:11<00:01, 250.57it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2842/3257 [00:11<00:01, 252.00it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2878/3257 [00:11<00:01, 281.18it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2907/3257 [00:11<00:01, 264.98it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2935/3257 [00:11<00:01, 267.52it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2963/3257 [00:12<00:01, 264.49it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2990/3257 [00:12<00:01, 260.77it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3020/3257 [00:12<00:00, 269.26it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3053/3257 [00:12<00:00, 285.79it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3082/3257 [00:12<00:00, 286.46it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3113/3257 [00:12<00:00, 292.29it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3143/3257 [00:12<00:00, 286.49it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3172/3257 [00:12<00:00, 285.87it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3201/3257 [00:12<00:00, 281.82it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3230/3257 [00:12<00:00, 264.83it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 249.24it/s]
2023-02-07 18:33:38.474 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:33:38,477][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d167,n5,mc3,s0.461826,t4>', 'datetime': '2023-02-07T18:33:38.477161', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:33:38,477][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:33:38,478][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:33:38,680][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 18:33:38,680][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:33:38,686][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 2159 unique words (76.59% of original 2819, drops 660)', 'datetime': '2023-02-07T18:33:38.686753', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:33:38,687][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 2182579 word corpus (99.95% of original 2183622, drops 1043)', 'datetime': '2023-02-07T18:33:38.687166', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:33:38,695][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 18:33:38,695][gensim.models.word2vec][INFO] - sample=0.461826 downsamples 0 most-common words
[2023-02-07 18:33:38,695][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2182579 word corpus (100.0%% of prior 2182579)', 'datetime': '2023-02-07T18:33:38.695531', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:33:38,708][gensim.models.word2vec][INFO] - estimated required memory for 2159 words and 167 dimensions: 6791000 bytes
[2023-02-07 18:33:38,708][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:33:38,712][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2159 vocabulary and 167 features, using sg=1 hs=0 sample=0.4618260786795027 negative=5 window=19 shrink_windows=True', 'datetime': '2023-02-07T18:33:38.712799', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:33:39,629][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2185836 effective words) took 0.9s, 2392510 effective words/s
[2023-02-07 18:33:40,635][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 97.51% examples, 2125415 words/s, in_qsize 6, out_qsize 0
[2023-02-07 18:33:40,650][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2185836 effective words) took 1.0s, 2142190 effective words/s
[2023-02-07 18:33:41,657][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 98.43% examples, 2143296 words/s, in_qsize 4, out_qsize 0
[2023-02-07 18:33:41,670][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2185836 effective words) took 1.0s, 2147740 effective words/s
[2023-02-07 18:33:42,673][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 92.66% examples, 2038744 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:33:42,730][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2185836 effective words) took 1.1s, 2067780 effective words/s
[2023-02-07 18:33:43,435][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2185836 effective words) took 0.7s, 3108008 effective words/s
[2023-02-07 18:33:44,439][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 93.86% examples, 2053089 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:33:44,494][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2185836 effective words) took 1.1s, 2066527 effective words/s
[2023-02-07 18:33:45,498][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 94.75% examples, 2074243 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:33:45,549][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2185836 effective words) took 1.1s, 2078340 effective words/s
[2023-02-07 18:33:46,256][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2185836 effective words) took 0.7s, 3100072 effective words/s
[2023-02-07 18:33:47,259][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 91.65% examples, 2016040 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:33:47,339][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2185836 effective words) took 1.1s, 2020979 effective words/s
[2023-02-07 18:33:48,034][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2185836 effective words) took 0.7s, 3153373 effective words/s
[2023-02-07 18:33:49,047][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 96.75% examples, 2094033 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:33:49,071][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2185836 effective words) took 1.0s, 2111078 effective words/s
[2023-02-07 18:33:50,081][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 95.73% examples, 2079607 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:33:50,112][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2185836 effective words) took 1.0s, 2103535 effective words/s
[2023-02-07 18:33:50,815][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2185836 effective words) took 0.7s, 3119974 effective words/s
[2023-02-07 18:33:51,818][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 91.68% examples, 2016826 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:33:51,891][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2185836 effective words) took 1.1s, 2035098 effective words/s
[2023-02-07 18:33:52,600][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2185836 effective words) took 0.7s, 3087655 effective words/s
[2023-02-07 18:33:52,601][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 32754330 raw words (32787540 effective words) took 13.9s, 2360803 effective words/s', 'datetime': '2023-02-07T18:33:52.601548', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:33:52.601 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:33:53,456][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183316-36xbs0lo/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:33:53.456810', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:33:53,458][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:33:53,478][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183316-36xbs0lo/files/../tmp/embedding_model.pt
2023-02-07 18:33:53.479 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:33:54.907 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:33:55.432 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:33:56.712 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.114084438558754, 'test_mae': 1.0874615550645417, 'test_r2': -1.46695080112376}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.56
wandb: percentage 0.23413
wandb:   test_mae 1.08746
wandb:   test_mse 2.11408
wandb:    test_r2 -1.46695
wandb: 
wandb: üöÄ View run icy-sweep-10 at: https://wandb.ai/xiaoqiz/mof2vec/runs/36xbs0lo
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_183316-36xbs0lo/logs
wandb: Agent Starting Run: scvk9xf4 with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 532
wandb: 	model.gensim.alpha: 0.04415860456564781
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 7
wandb: 	model.gensim.sample: 0.5142820220836215
wandb: 	model.gensim.vector_size: 270
wandb: 	model.gensim.window: 7
wandb: 	model.sklearn.learning_rate: 0.25590090215033334
wandb: 	model.sklearn.max_depth: 81
wandb: 	model.sklearn.min_child_weight: 0.03061927891712658
wandb: 	model.sklearn.n_estimators: 726
wandb: 	model.sklearn.num_leaves: 49
wandb: 	model.sklearn.reg_alpha: 0.04612840294199823
wandb: 	model.sklearn.reg_lambda: 0.0026610725062917472
wandb: 	model.sklearn.subsample: 0.9449793401077282
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183408-scvk9xf4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/scvk9xf4
2023-02-07 18:34:17.260 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 18:34:17.261 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 532 for sweep.
2023-02-07 18:34:17.261 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.04415860456564781 for sweep.
2023-02-07 18:34:17.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 18:34:17.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 7 for sweep.
2023-02-07 18:34:17.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5142820220836215 for sweep.
2023-02-07 18:34:17.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 270 for sweep.
2023-02-07 18:34:17.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 7 for sweep.
2023-02-07 18:34:17.263 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.25590090215033334 for sweep.
2023-02-07 18:34:17.263 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 81 for sweep.
2023-02-07 18:34:17.263 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03061927891712658 for sweep.
2023-02-07 18:34:17.263 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 726 for sweep.
2023-02-07 18:34:17.264 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 49 for sweep.
2023-02-07 18:34:17.264 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.04612840294199823 for sweep.
2023-02-07 18:34:17.264 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.0026610725062917472 for sweep.
2023-02-07 18:34:17.264 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.9449793401077282 for sweep.
2023-02-07 18:34:17.265 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:34:17.275 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183408-scvk9xf4/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 532, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 270, 'window': 7, 'min_count': 7, 'dm': 1, 'sample': 0.5142820220836215, 'workers': 4, 'alpha': 0.04415860456564781, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 726, 'max_depth': 81, 'num_leaves': 49, 'reg_alpha': 0.04612840294199823, 'reg_lambda': 0.0026610725062917472, 'subsample': 0.9449793401077282, 'min_child_weight': 0.03061927891712658, 'n_jobs': 4, 'learning_rate': 0.25590090215033334}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 16/3257 [00:00<00:20, 157.52it/s]  1%|          | 34/3257 [00:00<00:19, 165.94it/s]  2%|‚ñè         | 53/3257 [00:00<00:19, 167.59it/s]  2%|‚ñè         | 74/3257 [00:00<00:17, 182.46it/s]  3%|‚ñé         | 94/3257 [00:00<00:16, 186.51it/s]  3%|‚ñé         | 113/3257 [00:00<00:17, 179.42it/s]  4%|‚ñç         | 132/3257 [00:00<00:17, 176.65it/s]  5%|‚ñç         | 151/3257 [00:00<00:17, 177.63it/s]  5%|‚ñå         | 169/3257 [00:00<00:17, 177.83it/s]  6%|‚ñå         | 187/3257 [00:01<00:17, 177.43it/s]  6%|‚ñã         | 205/3257 [00:01<00:17, 174.25it/s]  7%|‚ñã         | 229/3257 [00:01<00:15, 193.41it/s]  8%|‚ñä         | 249/3257 [00:01<00:15, 190.15it/s]  8%|‚ñä         | 269/3257 [00:01<00:16, 184.15it/s]  9%|‚ñâ         | 293/3257 [00:01<00:14, 198.01it/s] 10%|‚ñâ         | 313/3257 [00:01<00:23, 125.15it/s] 10%|‚ñà         | 335/3257 [00:02<00:20, 143.02it/s] 11%|‚ñà         | 355/3257 [00:02<00:18, 155.23it/s] 11%|‚ñà‚ñè        | 374/3257 [00:02<00:17, 162.35it/s] 12%|‚ñà‚ñè        | 393/3257 [00:02<00:18, 156.41it/s] 13%|‚ñà‚ñé        | 413/3257 [00:02<00:17, 166.52it/s] 13%|‚ñà‚ñé        | 431/3257 [00:02<00:18, 149.05it/s] 14%|‚ñà‚ñç        | 448/3257 [00:02<00:18, 152.91it/s] 14%|‚ñà‚ñç        | 469/3257 [00:02<00:16, 166.61it/s] 15%|‚ñà‚ñç        | 487/3257 [00:02<00:16, 166.81it/s] 16%|‚ñà‚ñå        | 510/3257 [00:03<00:15, 180.88it/s] 16%|‚ñà‚ñå        | 529/3257 [00:03<00:15, 176.58it/s] 17%|‚ñà‚ñã        | 547/3257 [00:03<00:15, 176.51it/s] 17%|‚ñà‚ñã        | 565/3257 [00:03<00:16, 163.26it/s] 18%|‚ñà‚ñä        | 582/3257 [00:03<00:16, 158.99it/s] 19%|‚ñà‚ñä        | 604/3257 [00:03<00:15, 170.48it/s] 19%|‚ñà‚ñâ        | 622/3257 [00:03<00:15, 170.61it/s] 20%|‚ñà‚ñâ        | 643/3257 [00:03<00:14, 180.66it/s] 20%|‚ñà‚ñà        | 662/3257 [00:03<00:15, 166.54it/s] 21%|‚ñà‚ñà        | 683/3257 [00:04<00:15, 170.42it/s] 22%|‚ñà‚ñà‚ñè       | 703/3257 [00:04<00:14, 175.71it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:04<00:14, 174.93it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:04<00:15, 167.68it/s] 23%|‚ñà‚ñà‚ñé       | 761/3257 [00:04<00:13, 178.45it/s] 24%|‚ñà‚ñà‚ñç       | 780/3257 [00:04<00:14, 171.88it/s] 25%|‚ñà‚ñà‚ñç       | 801/3257 [00:04<00:13, 181.82it/s] 25%|‚ñà‚ñà‚ñå       | 820/3257 [00:04<00:13, 176.79it/s] 26%|‚ñà‚ñà‚ñå       | 838/3257 [00:04<00:14, 163.21it/s] 26%|‚ñà‚ñà‚ñã       | 855/3257 [00:05<00:15, 156.32it/s] 27%|‚ñà‚ñà‚ñã       | 873/3257 [00:05<00:14, 160.78it/s] 27%|‚ñà‚ñà‚ñã       | 890/3257 [00:05<00:14, 162.75it/s] 28%|‚ñà‚ñà‚ñä       | 909/3257 [00:05<00:13, 169.65it/s] 29%|‚ñà‚ñà‚ñä       | 929/3257 [00:05<00:13, 177.15it/s] 29%|‚ñà‚ñà‚ñâ       | 947/3257 [00:05<00:13, 175.04it/s] 30%|‚ñà‚ñà‚ñâ       | 967/3257 [00:05<00:12, 178.25it/s] 30%|‚ñà‚ñà‚ñà       | 985/3257 [00:05<00:12, 177.01it/s] 31%|‚ñà‚ñà‚ñà       | 1003/3257 [00:05<00:13, 170.47it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1021/3257 [00:06<00:12, 172.24it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1039/3257 [00:06<00:13, 163.35it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1056/3257 [00:06<00:13, 163.96it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1079/3257 [00:06<00:11, 181.61it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1098/3257 [00:06<00:12, 168.48it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:06<00:12, 173.44it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1135/3257 [00:06<00:12, 171.87it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1153/3257 [00:06<00:12, 164.91it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:06<00:12, 167.96it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1188/3257 [00:07<00:13, 152.18it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1204/3257 [00:07<00:13, 154.02it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1222/3257 [00:07<00:12, 160.30it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1243/3257 [00:07<00:11, 173.21it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1261/3257 [00:07<00:11, 173.83it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1279/3257 [00:07<00:12, 164.75it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1296/3257 [00:07<00:12, 160.21it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1315/3257 [00:07<00:11, 166.65it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1334/3257 [00:07<00:11, 171.78it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1352/3257 [00:08<00:11, 167.52it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:08<00:11, 165.99it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1387/3257 [00:08<00:11, 163.00it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1409/3257 [00:08<00:10, 177.03it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1430/3257 [00:08<00:09, 186.29it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1449/3257 [00:08<00:09, 185.71it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1471/3257 [00:08<00:09, 195.08it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1491/3257 [00:08<00:09, 194.23it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1512/3257 [00:08<00:08, 198.61it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1532/3257 [00:08<00:09, 178.38it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1551/3257 [00:09<00:15, 112.79it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1569/3257 [00:09<00:13, 124.78it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1587/3257 [00:09<00:12, 135.52it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1606/3257 [00:09<00:11, 146.80it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1624/3257 [00:09<00:10, 154.47it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1641/3257 [00:09<00:10, 157.11it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1658/3257 [00:09<00:10, 156.46it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1675/3257 [00:10<00:10, 155.03it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1692/3257 [00:10<00:09, 156.97it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1712/3257 [00:10<00:09, 168.22it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1730/3257 [00:10<00:09, 158.53it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1747/3257 [00:10<00:09, 159.42it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1765/3257 [00:10<00:09, 163.94it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1785/3257 [00:10<00:08, 174.04it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1803/3257 [00:10<00:08, 173.47it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1821/3257 [00:10<00:08, 172.49it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1839/3257 [00:11<00:08, 166.41it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1857/3257 [00:11<00:08, 169.16it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1877/3257 [00:11<00:07, 177.03it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1895/3257 [00:11<00:07, 172.33it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:11<00:07, 173.30it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1937/3257 [00:11<00:07, 179.49it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1961/3257 [00:11<00:06, 194.17it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1981/3257 [00:11<00:06, 184.79it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2001/3257 [00:11<00:06, 187.60it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2020/3257 [00:11<00:06, 187.76it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2039/3257 [00:12<00:06, 181.09it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2058/3257 [00:12<00:07, 167.84it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2076/3257 [00:12<00:07, 167.19it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2093/3257 [00:12<00:07, 163.14it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2110/3257 [00:12<00:07, 161.20it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2127/3257 [00:12<00:07, 153.15it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2143/3257 [00:12<00:07, 148.60it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2161/3257 [00:12<00:07, 153.55it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2180/3257 [00:13<00:06, 160.33it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2198/3257 [00:13<00:06, 163.92it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2215/3257 [00:13<00:06, 157.98it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2235/3257 [00:13<00:06, 167.60it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2252/3257 [00:13<00:06, 161.85it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2269/3257 [00:13<00:06, 161.55it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2287/3257 [00:13<00:05, 166.38it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2304/3257 [00:13<00:05, 164.18it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2327/3257 [00:13<00:05, 182.76it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2349/3257 [00:13<00:04, 192.46it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2369/3257 [00:14<00:04, 191.51it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2391/3257 [00:14<00:04, 198.78it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2411/3257 [00:14<00:04, 186.40it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2430/3257 [00:14<00:04, 183.76it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2449/3257 [00:14<00:04, 172.74it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2472/3257 [00:14<00:04, 187.47it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2492/3257 [00:14<00:04, 182.69it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2513/3257 [00:14<00:03, 187.96it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2532/3257 [00:14<00:03, 187.96it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:15<00:03, 178.68it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2570/3257 [00:15<00:04, 161.33it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2587/3257 [00:15<00:04, 160.83it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2606/3257 [00:15<00:03, 168.55it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:15<00:03, 181.09it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2647/3257 [00:15<00:03, 174.17it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2665/3257 [00:15<00:03, 173.53it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2684/3257 [00:15<00:03, 177.70it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2702/3257 [00:16<00:03, 156.18it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2719/3257 [00:16<00:03, 154.24it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2740/3257 [00:16<00:03, 167.57it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2758/3257 [00:16<00:02, 169.81it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2776/3257 [00:16<00:02, 165.23it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2797/3257 [00:16<00:02, 176.87it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2815/3257 [00:16<00:02, 173.62it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2833/3257 [00:16<00:02, 168.08it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2851/3257 [00:16<00:02, 171.21it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2873/3257 [00:16<00:02, 183.26it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2892/3257 [00:17<00:04, 86.05it/s]  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2907/3257 [00:17<00:03, 95.03it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:17<00:02, 112.38it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2944/3257 [00:17<00:02, 118.49it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2961/3257 [00:17<00:02, 129.46it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2977/3257 [00:18<00:02, 135.38it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2993/3257 [00:18<00:01, 141.12it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3013/3257 [00:18<00:01, 156.05it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3031/3257 [00:18<00:01, 161.96it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3050/3257 [00:18<00:01, 169.80it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3072/3257 [00:18<00:01, 184.06it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3091/3257 [00:18<00:00, 178.72it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3113/3257 [00:18<00:00, 189.74it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3133/3257 [00:18<00:00, 179.85it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3152/3257 [00:18<00:00, 170.16it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3170/3257 [00:19<00:00, 170.85it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3188/3257 [00:19<00:00, 164.59it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:19<00:00, 167.88it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3224/3257 [00:19<00:00, 164.38it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:19<00:00, 172.09it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 166.02it/s]
2023-02-07 18:34:37.683 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:34:37,684][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d270,n5,w7,mc7,s0.514282,t4>', 'datetime': '2023-02-07T18:34:37.684811', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:34:37,685][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:34:37,685][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:34:38,159][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 18:34:38,160][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:34:38,202][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 retains 14137 unique words (44.45% of original 31803, drops 17666)', 'datetime': '2023-02-07T18:34:38.201988', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:34:38,202][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 leaves 5044802 word corpus (99.01% of original 5095118, drops 50316)', 'datetime': '2023-02-07T18:34:38.202482', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:34:38,254][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 18:34:38,256][gensim.models.word2vec][INFO] - sample=0.514282 downsamples 0 most-common words
[2023-02-07 18:34:38,257][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5044802 word corpus (100.0%% of prior 5044802)', 'datetime': '2023-02-07T18:34:38.257256', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:34:38,340][gensim.models.word2vec][INFO] - estimated required memory for 14137 words and 270 dimensions: 41773380 bytes
[2023-02-07 18:34:38,340][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:34:38,364][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 14137 vocabulary and 270 features, using sg=0 hs=0 sample=0.5142820220836215 negative=5 window=7 shrink_windows=True', 'datetime': '2023-02-07T18:34:38.364019', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:34:39,370][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 23.79% examples, 1189700 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:34:40,374][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 47.71% examples, 1224563 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:34:41,378][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 72.92% examples, 1239134 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:34:42,382][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 99.11% examples, 1244522 words/s, in_qsize 5, out_qsize 0
[2023-02-07 18:34:42,407][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5036861 effective words) took 4.0s, 1246873 effective words/s
[2023-02-07 18:34:43,422][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 27.88% examples, 1398805 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:34:44,432][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 53.18% examples, 1358346 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:34:45,434][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 79.40% examples, 1338059 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:34:46,145][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5036861 effective words) took 3.7s, 1348493 effective words/s
[2023-02-07 18:34:47,153][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 30.00% examples, 1504635 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:34:48,159][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 55.23% examples, 1414021 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:34:49,169][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 82.16% examples, 1380574 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:34:49,808][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5036861 effective words) took 3.7s, 1375771 effective words/s
[2023-02-07 18:34:50,818][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 25.76% examples, 1290874 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:34:51,824][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 51.70% examples, 1324763 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:34:52,831][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 78.66% examples, 1325679 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:34:53,621][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5036861 effective words) took 3.8s, 1322292 effective words/s
[2023-02-07 18:34:54,630][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 33.59% examples, 1706386 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:34:55,635][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 67.06% examples, 1715693 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:34:56,542][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5036861 effective words) took 2.9s, 1725592 effective words/s
[2023-02-07 18:34:57,550][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 26.53% examples, 1335463 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:34:58,557][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 52.78% examples, 1350637 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:34:59,562][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 79.52% examples, 1344033 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:35:00,291][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5036861 effective words) took 3.7s, 1344169 effective words/s
[2023-02-07 18:35:01,295][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 33.13% examples, 1681413 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:02,298][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 66.93% examples, 1716640 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:03,211][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5036861 effective words) took 2.9s, 1726184 effective words/s
[2023-02-07 18:35:04,218][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 29.08% examples, 1463521 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:05,222][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 62.45% examples, 1588344 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:06,224][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 96.75% examples, 1621546 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:06,310][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5036861 effective words) took 3.1s, 1626426 effective words/s
[2023-02-07 18:35:07,316][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 29.26% examples, 1471710 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:08,325][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 55.76% examples, 1430463 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:09,325][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 84.40% examples, 1425192 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:09,865][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5036861 effective words) took 3.6s, 1417734 effective words/s
[2023-02-07 18:35:10,872][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 26.87% examples, 1357907 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:11,876][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 52.66% examples, 1354011 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:12,891][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 80.47% examples, 1353548 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:35:13,564][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5036861 effective words) took 3.7s, 1362770 effective words/s
[2023-02-07 18:35:14,567][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 34.33% examples, 1749928 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:15,574][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 68.50% examples, 1754434 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:16,419][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5036861 effective words) took 2.9s, 1765449 effective words/s
[2023-02-07 18:35:17,423][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 33.59% examples, 1713899 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:18,427][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 67.30% examples, 1725285 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:19,319][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5036861 effective words) took 2.9s, 1737683 effective words/s
[2023-02-07 18:35:20,328][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 28.25% examples, 1424101 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:21,334][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 55.57% examples, 1426350 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:22,338][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 84.65% examples, 1426320 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:35:22,859][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5036861 effective words) took 3.5s, 1424058 effective words/s
[2023-02-07 18:35:23,868][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 34.48% examples, 1743378 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:35:24,875][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 69.02% examples, 1763062 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:25,709][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5036861 effective words) took 2.8s, 1768374 effective words/s
[2023-02-07 18:35:26,714][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 26.87% examples, 1359279 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:35:27,729][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 53.30% examples, 1366729 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:35:28,741][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 81.58% examples, 1367932 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:35:29,410][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5036861 effective words) took 3.7s, 1361842 effective words/s
[2023-02-07 18:35:29,411][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (75552915 effective words) took 51.0s, 1480097 effective words/s', 'datetime': '2023-02-07T18:35:29.411320', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:35:29.411 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:35:33,482][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183408-scvk9xf4/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:35:33.482425', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:35:33,483][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:35:33,554][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183408-scvk9xf4/files/../tmp/embedding_model.pt
2023-02-07 18:35:33.555 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:35:35.424 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:35:36.099 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:35:38.080 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.504378651102288, 'test_mae': 1.186616126832158, 'test_r2': -2.607014169032375}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.82
wandb: percentage 0.55548
wandb:   test_mae 1.18662
wandb:   test_mse 2.50438
wandb:    test_r2 -2.60701
wandb: 
wandb: üöÄ View run glorious-sweep-11 at: https://wandb.ai/xiaoqiz/mof2vec/runs/scvk9xf4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_183408-scvk9xf4/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: y8xdfft3 with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 1000
wandb: 	model.gensim.alpha: 0.0017342172491650145
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 7
wandb: 	model.gensim.sample: 0.903233813778396
wandb: 	model.gensim.vector_size: 470
wandb: 	model.gensim.window: 4
wandb: 	model.sklearn.learning_rate: 0.028285625577723628
wandb: 	model.sklearn.max_depth: 21
wandb: 	model.sklearn.min_child_weight: 0.06314381863310672
wandb: 	model.sklearn.n_estimators: 4610
wandb: 	model.sklearn.num_leaves: 383
wandb: 	model.sklearn.reg_alpha: 0.014792765207206352
wandb: 	model.sklearn.reg_lambda: 0.014719981283710552
wandb: 	model.sklearn.subsample: 0.24057418897551336
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183556-y8xdfft3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-12
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/y8xdfft3
2023-02-07 18:36:05.506 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 18:36:05.507 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 1000 for sweep.
2023-02-07 18:36:05.507 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0017342172491650145 for sweep.
2023-02-07 18:36:05.507 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 18:36:05.508 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 7 for sweep.
2023-02-07 18:36:05.508 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.903233813778396 for sweep.
2023-02-07 18:36:05.508 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 470 for sweep.
2023-02-07 18:36:05.508 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 4 for sweep.
2023-02-07 18:36:05.509 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.028285625577723628 for sweep.
2023-02-07 18:36:05.509 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 21 for sweep.
2023-02-07 18:36:05.509 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.06314381863310672 for sweep.
2023-02-07 18:36:05.509 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4610 for sweep.
2023-02-07 18:36:05.510 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 383 for sweep.
2023-02-07 18:36:05.510 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.014792765207206352 for sweep.
2023-02-07 18:36:05.510 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.014719981283710552 for sweep.
2023-02-07 18:36:05.510 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.24057418897551336 for sweep.
2023-02-07 18:36:05.511 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:36:05.516 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183556-y8xdfft3/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 1000, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 470, 'window': 4, 'min_count': 7, 'dm': 1, 'sample': 0.903233813778396, 'workers': 4, 'alpha': 0.0017342172491650145, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4610, 'max_depth': 21, 'num_leaves': 383, 'reg_alpha': 0.014792765207206352, 'reg_lambda': 0.014719981283710552, 'subsample': 0.24057418897551336, 'min_child_weight': 0.06314381863310672, 'n_jobs': 4, 'learning_rate': 0.028285625577723628}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 13/3257 [00:00<00:26, 123.43it/s]  1%|          | 29/3257 [00:00<00:23, 138.36it/s]  2%|‚ñè         | 49/3257 [00:00<00:19, 163.95it/s]  2%|‚ñè         | 68/3257 [00:00<00:18, 170.69it/s]  3%|‚ñé         | 90/3257 [00:00<00:16, 187.77it/s]  3%|‚ñé         | 109/3257 [00:00<00:17, 180.88it/s]  4%|‚ñç         | 130/3257 [00:00<00:16, 189.40it/s]  5%|‚ñç         | 152/3257 [00:00<00:15, 196.83it/s]  5%|‚ñå         | 172/3257 [00:00<00:15, 194.98it/s]  6%|‚ñå         | 193/3257 [00:01<00:15, 197.31it/s]  7%|‚ñã         | 216/3257 [00:01<00:15, 202.67it/s]  7%|‚ñã         | 241/3257 [00:01<00:14, 214.65it/s]  8%|‚ñä         | 263/3257 [00:01<00:14, 209.19it/s]  9%|‚ñâ         | 290/3257 [00:01<00:13, 224.96it/s] 10%|‚ñâ         | 313/3257 [00:01<00:13, 218.76it/s] 10%|‚ñà         | 337/3257 [00:01<00:13, 223.79it/s] 11%|‚ñà         | 360/3257 [00:01<00:12, 224.47it/s] 12%|‚ñà‚ñè        | 383/3257 [00:01<00:13, 214.40it/s] 12%|‚ñà‚ñè        | 405/3257 [00:02<00:13, 212.84it/s] 13%|‚ñà‚ñé        | 427/3257 [00:02<00:14, 193.86it/s] 14%|‚ñà‚ñé        | 447/3257 [00:02<00:14, 193.30it/s] 14%|‚ñà‚ñç        | 471/3257 [00:02<00:13, 205.39it/s] 15%|‚ñà‚ñå        | 492/3257 [00:02<00:13, 202.83it/s] 16%|‚ñà‚ñå        | 516/3257 [00:02<00:12, 212.12it/s] 17%|‚ñà‚ñã        | 538/3257 [00:02<00:12, 211.51it/s] 17%|‚ñà‚ñã        | 560/3257 [00:02<00:13, 199.37it/s] 18%|‚ñà‚ñä        | 581/3257 [00:02<00:13, 195.06it/s] 19%|‚ñà‚ñä        | 604/3257 [00:03<00:13, 202.52it/s] 19%|‚ñà‚ñâ        | 625/3257 [00:03<00:12, 203.40it/s] 20%|‚ñà‚ñâ        | 647/3257 [00:03<00:12, 206.63it/s] 21%|‚ñà‚ñà        | 668/3257 [00:03<00:12, 200.11it/s] 21%|‚ñà‚ñà        | 689/3257 [00:03<00:12, 199.45it/s] 22%|‚ñà‚ñà‚ñè       | 713/3257 [00:03<00:12, 209.91it/s] 23%|‚ñà‚ñà‚ñé       | 735/3257 [00:03<00:12, 202.11it/s] 23%|‚ñà‚ñà‚ñé       | 756/3257 [00:03<00:12, 197.16it/s] 24%|‚ñà‚ñà‚ñç       | 776/3257 [00:04<00:20, 122.29it/s] 25%|‚ñà‚ñà‚ñç       | 799/3257 [00:04<00:17, 142.76it/s] 25%|‚ñà‚ñà‚ñå       | 819/3257 [00:04<00:15, 154.97it/s] 26%|‚ñà‚ñà‚ñå       | 838/3257 [00:04<00:15, 160.57it/s] 26%|‚ñà‚ñà‚ñã       | 858/3257 [00:04<00:14, 169.78it/s] 27%|‚ñà‚ñà‚ñã       | 877/3257 [00:04<00:13, 171.65it/s] 28%|‚ñà‚ñà‚ñä       | 901/3257 [00:04<00:12, 189.08it/s] 28%|‚ñà‚ñà‚ñä       | 921/3257 [00:04<00:12, 191.84it/s] 29%|‚ñà‚ñà‚ñâ       | 941/3257 [00:04<00:12, 192.84it/s] 30%|‚ñà‚ñà‚ñâ       | 963/3257 [00:05<00:11, 200.41it/s] 30%|‚ñà‚ñà‚ñà       | 984/3257 [00:05<00:11, 198.09it/s] 31%|‚ñà‚ñà‚ñà       | 1005/3257 [00:05<00:11, 197.36it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1025/3257 [00:05<00:11, 194.05it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1045/3257 [00:05<00:12, 181.86it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:05<00:11, 190.02it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1088/3257 [00:05<00:11, 195.01it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1109/3257 [00:05<00:10, 198.08it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1129/3257 [00:05<00:11, 190.96it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1149/3257 [00:06<00:11, 185.68it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:06<00:10, 194.28it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:06<00:11, 179.28it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1210/3257 [00:06<00:11, 179.40it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1235/3257 [00:06<00:10, 195.90it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1255/3257 [00:06<00:10, 194.46it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:06<00:10, 190.15it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1295/3257 [00:06<00:10, 181.72it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1317/3257 [00:06<00:10, 190.45it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:06<00:09, 197.49it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1359/3257 [00:07<00:09, 196.11it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1379/3257 [00:07<00:09, 195.85it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1399/3257 [00:07<00:09, 192.98it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:07<00:09, 202.65it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1444/3257 [00:07<00:08, 203.43it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1470/3257 [00:07<00:08, 219.43it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1493/3257 [00:07<00:08, 219.75it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1517/3257 [00:07<00:07, 222.76it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1540/3257 [00:07<00:08, 207.69it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1561/3257 [00:08<00:08, 200.90it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1582/3257 [00:08<00:08, 197.89it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1604/3257 [00:08<00:08, 203.98it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1625/3257 [00:08<00:08, 199.70it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1646/3257 [00:08<00:08, 195.81it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1666/3257 [00:08<00:08, 188.10it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1685/3257 [00:08<00:08, 185.57it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1704/3257 [00:08<00:08, 184.76it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1723/3257 [00:08<00:08, 182.10it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1742/3257 [00:09<00:09, 165.03it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1764/3257 [00:09<00:08, 179.53it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1785/3257 [00:09<00:07, 186.92it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:09<00:07, 186.31it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1825/3257 [00:09<00:07, 189.74it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1846/3257 [00:09<00:07, 193.94it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:09<00:06, 201.40it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1890/3257 [00:09<00:06, 201.76it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1913/3257 [00:09<00:06, 209.20it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1934/3257 [00:09<00:06, 209.25it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1961/3257 [00:10<00:05, 223.84it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1984/3257 [00:10<00:10, 122.40it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2003/3257 [00:10<00:09, 133.75it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2023/3257 [00:10<00:08, 145.03it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2041/3257 [00:10<00:07, 152.52it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2059/3257 [00:10<00:07, 157.27it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2079/3257 [00:11<00:07, 167.10it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2098/3257 [00:11<00:06, 169.69it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:11<00:06, 176.98it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2140/3257 [00:11<00:06, 181.50it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2160/3257 [00:11<00:05, 184.75it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:11<00:05, 187.70it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2204/3257 [00:11<00:05, 197.04it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2224/3257 [00:11<00:05, 195.64it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2244/3257 [00:11<00:05, 194.46it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2264/3257 [00:11<00:05, 193.81it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2285/3257 [00:12<00:04, 198.02it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2305/3257 [00:12<00:04, 198.23it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2330/3257 [00:12<00:04, 211.59it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2356/3257 [00:12<00:04, 224.91it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2379/3257 [00:12<00:04, 218.84it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2401/3257 [00:12<00:03, 217.50it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2423/3257 [00:12<00:04, 205.52it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2444/3257 [00:12<00:04, 194.31it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2469/3257 [00:12<00:03, 208.37it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2492/3257 [00:13<00:03, 212.99it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2517/3257 [00:13<00:03, 223.14it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2540/3257 [00:13<00:03, 216.37it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2562/3257 [00:13<00:03, 205.02it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2583/3257 [00:13<00:03, 196.12it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2605/3257 [00:13<00:03, 201.75it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2630/3257 [00:13<00:02, 213.05it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2652/3257 [00:13<00:03, 199.65it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2674/3257 [00:13<00:02, 203.71it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2695/3257 [00:14<00:02, 196.44it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2715/3257 [00:14<00:02, 183.06it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2739/3257 [00:14<00:02, 196.36it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:14<00:02, 197.23it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2782/3257 [00:14<00:02, 200.62it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:14<00:02, 207.66it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2827/3257 [00:14<00:02, 195.39it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2849/3257 [00:14<00:02, 201.82it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2877/3257 [00:14<00:01, 223.54it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2900/3257 [00:15<00:01, 207.81it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2924/3257 [00:15<00:01, 214.45it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2946/3257 [00:15<00:01, 200.63it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2969/3257 [00:15<00:01, 204.12it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2990/3257 [00:15<00:01, 198.69it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3015/3257 [00:15<00:01, 210.22it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3037/3257 [00:15<00:01, 212.54it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3060/3257 [00:15<00:00, 216.57it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3082/3257 [00:15<00:00, 217.35it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3106/3257 [00:16<00:00, 223.66it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3129/3257 [00:16<00:00, 225.30it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3152/3257 [00:16<00:00, 210.44it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3174/3257 [00:16<00:00, 206.17it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3195/3257 [00:16<00:00, 201.24it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3216/3257 [00:16<00:00, 189.86it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3240/3257 [00:16<00:00, 202.77it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 194.36it/s]
2023-02-07 18:36:22.823 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:36:22,824][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d470,n5,w4,mc7,s0.903234,t4>', 'datetime': '2023-02-07T18:36:22.824484', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:36:22,826][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:36:22,826][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:36:23,201][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 18:36:23,202][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:36:23,220][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 retains 6087 unique words (46.60% of original 13061, drops 6974)', 'datetime': '2023-02-07T18:36:23.220571', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:36:23,221][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 leaves 3619677 word corpus (99.46% of original 3639370, drops 19693)', 'datetime': '2023-02-07T18:36:23.221020', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:36:23,243][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 18:36:23,244][gensim.models.word2vec][INFO] - sample=0.903234 downsamples 0 most-common words
[2023-02-07 18:36:23,244][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3619677 word corpus (100.0%% of prior 3619677)', 'datetime': '2023-02-07T18:36:23.244627', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:36:23,280][gensim.models.word2vec][INFO] - estimated required memory for 6087 words and 470 dimensions: 32705180 bytes
[2023-02-07 18:36:23,280][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:36:23,299][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 6087 vocabulary and 470 features, using sg=0 hs=0 sample=0.903233813778396 negative=5 window=4 shrink_windows=True', 'datetime': '2023-02-07T18:36:23.299675', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:36:24,304][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 22.63% examples, 814307 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:36:25,319][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 47.31% examples, 867543 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:26,337][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 73.60% examples, 890624 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:27,188][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3622934 effective words) took 3.9s, 932189 effective words/s
[2023-02-07 18:36:28,194][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 26.50% examples, 960442 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:36:29,194][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 53.02% examples, 980441 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:30,195][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 81.03% examples, 984451 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:36:30,858][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3622934 effective words) took 3.7s, 987878 effective words/s
[2023-02-07 18:36:31,868][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 28.62% examples, 1034262 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:32,875][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 56.40% examples, 1042456 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:33,889][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 86.40% examples, 1041305 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:34,337][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3622934 effective words) took 3.5s, 1042016 effective words/s
[2023-02-07 18:36:35,345][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 27.82% examples, 1009512 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:36,361][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 54.81% examples, 1007994 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:36:37,370][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 83.94% examples, 1013680 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:37,904][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3622934 effective words) took 3.6s, 1016570 effective words/s
[2023-02-07 18:36:38,914][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 28.25% examples, 1025740 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:39,914][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 56.03% examples, 1036831 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:40,926][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 86.37% examples, 1044418 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:41,368][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3622934 effective words) took 3.5s, 1046416 effective words/s
[2023-02-07 18:36:42,373][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 27.11% examples, 985520 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:43,378][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 53.73% examples, 995682 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:44,380][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 82.87% examples, 1008439 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:44,957][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3622934 effective words) took 3.6s, 1010228 effective words/s
[2023-02-07 18:36:45,962][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 36.26% examples, 1332889 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:46,965][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 71.97% examples, 1323285 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:36:47,694][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3622934 effective words) took 2.7s, 1324138 effective words/s
[2023-02-07 18:36:48,700][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 35.31% examples, 1299581 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:49,701][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 71.72% examples, 1321964 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:36:50,439][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3622934 effective words) took 2.7s, 1321037 effective words/s
[2023-02-07 18:36:51,443][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 35.68% examples, 1311477 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:36:52,444][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 71.45% examples, 1318732 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:53,176][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3622934 effective words) took 2.7s, 1325025 effective words/s
[2023-02-07 18:36:54,179][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 28.46% examples, 1032479 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:55,179][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 57.23% examples, 1062484 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:56,185][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 86.77% examples, 1054934 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:36:56,615][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3622934 effective words) took 3.4s, 1054144 effective words/s
[2023-02-07 18:36:57,619][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 32.55% examples, 1189907 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:58,621][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 60.73% examples, 1118667 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:59,632][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 90.02% examples, 1088989 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:36:59,954][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3622934 effective words) took 3.3s, 1086252 effective words/s
[2023-02-07 18:37:00,961][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 35.31% examples, 1298407 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:37:01,964][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 65.55% examples, 1208746 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:37:02,969][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 96.16% examples, 1157704 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:37:03,089][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3622934 effective words) took 3.1s, 1156805 effective words/s
[2023-02-07 18:37:04,095][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 35.31% examples, 1297672 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:37:05,115][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 63.95% examples, 1165136 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:37:06,142][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 94.11% examples, 1120935 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:37:06,324][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3622934 effective words) took 3.2s, 1120715 effective words/s
[2023-02-07 18:37:07,328][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 28.46% examples, 1030737 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:37:08,338][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 63.00% examples, 1152862 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:37:09,326][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3622934 effective words) took 3.0s, 1207418 effective words/s
[2023-02-07 18:37:10,342][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 27.48% examples, 992416 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:37:11,347][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 55.66% examples, 1026698 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:37:12,358][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 84.99% examples, 1026183 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:37:12,861][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3622934 effective words) took 3.5s, 1025612 effective words/s
[2023-02-07 18:37:12,862][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54344010 effective words) took 49.6s, 1096486 effective words/s', 'datetime': '2023-02-07T18:37:12.862149', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:37:12.862 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:37:16,153][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183556-y8xdfft3/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:37:16.153662', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:37:16,154][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:37:16,207][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183556-y8xdfft3/files/../tmp/embedding_model.pt
2023-02-07 18:37:16.207 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:37:18.836 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:37:19.728 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:37:23.306 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.5575294921164318, 'test_mae': 1.1753095006013141, 'test_r2': -2.6159939368571647}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.032 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.032 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.41
wandb: percentage 0.53396
wandb:   test_mae 1.17531
wandb:   test_mse 2.55753
wandb:    test_r2 -2.61599
wandb: 
wandb: üöÄ View run likely-sweep-12 at: https://wandb.ai/xiaoqiz/mof2vec/runs/y8xdfft3
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_183556-y8xdfft3/logs
wandb: Agent Starting Run: 0mprup8m with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 682
wandb: 	model.gensim.alpha: 0.004149257552487723
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.4607965184453554
wandb: 	model.gensim.vector_size: 268
wandb: 	model.gensim.window: 18
wandb: 	model.sklearn.learning_rate: 0.011402531023717404
wandb: 	model.sklearn.max_depth: 91
wandb: 	model.sklearn.min_child_weight: 0.03294349745823755
wandb: 	model.sklearn.n_estimators: 3507
wandb: 	model.sklearn.num_leaves: 276
wandb: 	model.sklearn.reg_alpha: 0.01196803869178061
wandb: 	model.sklearn.reg_lambda: 0.021164729329897432
wandb: 	model.sklearn.subsample: 0.5829830077827851
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183732-0mprup8m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-13
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/0mprup8m
2023-02-07 18:37:40.748 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 18:37:40.749 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 682 for sweep.
2023-02-07 18:37:40.749 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.004149257552487723 for sweep.
2023-02-07 18:37:40.749 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 18:37:40.750 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 18:37:40.750 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4607965184453554 for sweep.
2023-02-07 18:37:40.750 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 268 for sweep.
2023-02-07 18:37:40.750 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 18 for sweep.
2023-02-07 18:37:40.750 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.011402531023717404 for sweep.
2023-02-07 18:37:40.751 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 91 for sweep.
2023-02-07 18:37:40.751 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03294349745823755 for sweep.
2023-02-07 18:37:40.751 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3507 for sweep.
2023-02-07 18:37:40.751 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 276 for sweep.
2023-02-07 18:37:40.751 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.01196803869178061 for sweep.
2023-02-07 18:37:40.752 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.021164729329897432 for sweep.
2023-02-07 18:37:40.752 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5829830077827851 for sweep.
2023-02-07 18:37:40.752 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:37:40.758 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183732-0mprup8m/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 682, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 268, 'window': 18, 'min_count': 3, 'dm': 1, 'sample': 0.4607965184453554, 'workers': 4, 'alpha': 0.004149257552487723, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3507, 'max_depth': 91, 'num_leaves': 276, 'reg_alpha': 0.01196803869178061, 'reg_lambda': 0.021164729329897432, 'subsample': 0.5829830077827851, 'min_child_weight': 0.03294349745823755, 'n_jobs': 4, 'learning_rate': 0.011402531023717404}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 23/3257 [00:00<00:14, 228.37it/s]  1%|‚ñè         | 46/3257 [00:00<00:14, 227.96it/s]  2%|‚ñè         | 69/3257 [00:00<00:14, 223.36it/s]  3%|‚ñé         | 92/3257 [00:00<00:23, 135.67it/s]  4%|‚ñé         | 114/3257 [00:00<00:20, 156.27it/s]  4%|‚ñç         | 140/3257 [00:00<00:17, 182.14it/s]  5%|‚ñç         | 162/3257 [00:00<00:16, 192.05it/s]  6%|‚ñå         | 184/3257 [00:01<00:16, 190.50it/s]  6%|‚ñã         | 205/3257 [00:01<00:16, 184.35it/s]  7%|‚ñã         | 232/3257 [00:01<00:14, 207.27it/s]  8%|‚ñä         | 257/3257 [00:01<00:13, 216.87it/s]  9%|‚ñä         | 284/3257 [00:01<00:12, 230.04it/s]  9%|‚ñâ         | 308/3257 [00:01<00:12, 230.79it/s] 10%|‚ñà         | 332/3257 [00:01<00:12, 230.69it/s] 11%|‚ñà         | 356/3257 [00:01<00:12, 231.08it/s] 12%|‚ñà‚ñè        | 380/3257 [00:01<00:12, 223.80it/s] 12%|‚ñà‚ñè        | 403/3257 [00:01<00:12, 224.34it/s] 13%|‚ñà‚ñé        | 426/3257 [00:02<00:13, 211.18it/s] 14%|‚ñà‚ñç        | 448/3257 [00:02<00:13, 211.65it/s] 15%|‚ñà‚ñç        | 474/3257 [00:02<00:12, 224.11it/s] 15%|‚ñà‚ñå        | 500/3257 [00:02<00:11, 233.01it/s] 16%|‚ñà‚ñå        | 524/3257 [00:02<00:12, 226.57it/s] 17%|‚ñà‚ñã        | 547/3257 [00:02<00:12, 215.95it/s] 17%|‚ñà‚ñã        | 569/3257 [00:02<00:12, 210.63it/s] 18%|‚ñà‚ñä        | 591/3257 [00:02<00:12, 212.54it/s] 19%|‚ñà‚ñâ        | 619/3257 [00:02<00:11, 230.20it/s] 20%|‚ñà‚ñâ        | 643/3257 [00:03<00:11, 229.18it/s] 20%|‚ñà‚ñà        | 667/3257 [00:03<00:11, 221.77it/s] 21%|‚ñà‚ñà        | 690/3257 [00:03<00:11, 221.32it/s] 22%|‚ñà‚ñà‚ñè       | 717/3257 [00:03<00:10, 232.41it/s] 23%|‚ñà‚ñà‚ñé       | 741/3257 [00:03<00:11, 220.02it/s] 24%|‚ñà‚ñà‚ñé       | 769/3257 [00:03<00:10, 233.62it/s] 24%|‚ñà‚ñà‚ñç       | 793/3257 [00:03<00:10, 227.57it/s] 25%|‚ñà‚ñà‚ñå       | 818/3257 [00:03<00:10, 229.85it/s] 26%|‚ñà‚ñà‚ñå       | 842/3257 [00:03<00:11, 212.88it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:04<00:11, 211.53it/s] 27%|‚ñà‚ñà‚ñã       | 886/3257 [00:04<00:11, 212.83it/s] 28%|‚ñà‚ñà‚ñä       | 912/3257 [00:04<00:10, 225.24it/s] 29%|‚ñà‚ñà‚ñä       | 935/3257 [00:04<00:10, 224.77it/s] 29%|‚ñà‚ñà‚ñâ       | 960/3257 [00:04<00:09, 230.59it/s] 30%|‚ñà‚ñà‚ñà       | 984/3257 [00:04<00:09, 232.27it/s] 31%|‚ñà‚ñà‚ñà       | 1008/3257 [00:04<00:09, 226.49it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1032/3257 [00:04<00:09, 227.86it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1055/3257 [00:04<00:09, 224.94it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:04<00:09, 226.82it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1105/3257 [00:05<00:09, 230.22it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1129/3257 [00:05<00:09, 226.56it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1152/3257 [00:05<00:09, 227.21it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1177/3257 [00:05<00:08, 232.83it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1201/3257 [00:05<00:09, 215.54it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1228/3257 [00:05<00:08, 229.57it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1252/3257 [00:05<00:08, 231.61it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1276/3257 [00:05<00:08, 228.68it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1300/3257 [00:05<00:09, 212.81it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1322/3257 [00:06<00:09, 210.79it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1345/3257 [00:06<00:08, 214.44it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1367/3257 [00:06<00:08, 214.45it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1389/3257 [00:06<00:13, 142.94it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1419/3257 [00:06<00:10, 175.31it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1442/3257 [00:06<00:09, 187.60it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1470/3257 [00:06<00:08, 210.10it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1495/3257 [00:06<00:07, 220.50it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:07<00:07, 221.37it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1543/3257 [00:07<00:07, 222.45it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1567/3257 [00:07<00:07, 223.81it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1591/3257 [00:07<00:07, 227.28it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1618/3257 [00:07<00:06, 238.78it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1643/3257 [00:07<00:06, 231.80it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1667/3257 [00:07<00:06, 227.59it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1690/3257 [00:07<00:06, 226.23it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:07<00:06, 232.63it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1739/3257 [00:08<00:06, 221.70it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1764/3257 [00:08<00:06, 229.44it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1791/3257 [00:08<00:06, 240.69it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1816/3257 [00:08<00:06, 232.27it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1841/3257 [00:08<00:06, 235.89it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1868/3257 [00:08<00:05, 244.70it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1893/3257 [00:08<00:05, 235.72it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1919/3257 [00:08<00:05, 240.43it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1950/3257 [00:08<00:05, 258.93it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:08<00:04, 259.66it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:09<00:04, 257.19it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2032/3257 [00:09<00:04, 262.88it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2059/3257 [00:09<00:05, 238.11it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2085/3257 [00:09<00:04, 240.37it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2111/3257 [00:09<00:04, 245.56it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2136/3257 [00:09<00:04, 233.95it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2160/3257 [00:09<00:04, 232.80it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2184/3257 [00:09<00:04, 232.80it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2208/3257 [00:09<00:04, 234.83it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2233/3257 [00:10<00:04, 237.26it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2257/3257 [00:10<00:04, 235.57it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2281/3257 [00:10<00:04, 234.23it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2308/3257 [00:10<00:03, 241.76it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2337/3257 [00:10<00:03, 255.41it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2366/3257 [00:10<00:03, 264.71it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2393/3257 [00:10<00:03, 262.40it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2420/3257 [00:10<00:03, 249.19it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2446/3257 [00:10<00:03, 239.33it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2473/3257 [00:11<00:03, 246.68it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2501/3257 [00:11<00:02, 254.76it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2527/3257 [00:11<00:02, 255.21it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2553/3257 [00:11<00:02, 253.46it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2579/3257 [00:11<00:02, 236.64it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2604/3257 [00:11<00:02, 239.79it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2633/3257 [00:11<00:02, 253.06it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2659/3257 [00:11<00:02, 242.00it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2684/3257 [00:12<00:03, 158.10it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2704/3257 [00:12<00:03, 164.79it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2730/3257 [00:12<00:02, 184.51it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2757/3257 [00:12<00:02, 204.57it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2780/3257 [00:12<00:02, 207.12it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:12<00:02, 217.86it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2830/3257 [00:12<00:02, 212.40it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2853/3257 [00:12<00:01, 215.52it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2882/3257 [00:12<00:01, 235.09it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2907/3257 [00:13<00:01, 226.44it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2933/3257 [00:13<00:01, 233.60it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2957/3257 [00:13<00:01, 222.53it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2981/3257 [00:13<00:01, 226.63it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3009/3257 [00:13<00:01, 240.35it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3034/3257 [00:13<00:00, 231.01it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3058/3257 [00:13<00:00, 230.60it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3085/3257 [00:13<00:00, 237.97it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3112/3257 [00:13<00:00, 244.66it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3137/3257 [00:14<00:00, 246.15it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3162/3257 [00:14<00:00, 243.50it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3187/3257 [00:14<00:00, 238.71it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3213/3257 [00:14<00:00, 238.10it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3241/3257 [00:14<00:00, 249.85it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:14<00:00, 224.58it/s]
2023-02-07 18:37:55.648 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:37:55,649][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d268,n5,w18,mc3,s0.460797,t4>', 'datetime': '2023-02-07T18:37:55.649441', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:37:55,651][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:37:55,651][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:37:55,925][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 18:37:55,926][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:37:55,939][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 4875 unique words (73.18% of original 6662, drops 1787)', 'datetime': '2023-02-07T18:37:55.939398', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:37:55,939][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 2908813 word corpus (99.91% of original 2911496, drops 2683)', 'datetime': '2023-02-07T18:37:55.939843', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:37:55,958][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 18:37:55,959][gensim.models.word2vec][INFO] - sample=0.460797 downsamples 0 most-common words
[2023-02-07 18:37:55,959][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2908813 word corpus (100.0%% of prior 2908813)', 'datetime': '2023-02-07T18:37:55.959276', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:37:55,987][gensim.models.word2vec][INFO] - estimated required memory for 4875 words and 268 dimensions: 17032404 bytes
[2023-02-07 18:37:55,988][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:37:55,997][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 4875 vocabulary and 268 features, using sg=0 hs=0 sample=0.4607965184453554 negative=5 window=18 shrink_windows=True', 'datetime': '2023-02-07T18:37:55.997384', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:37:57,007][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 30.58% examples, 885671 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:37:58,017][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 63.00% examples, 925496 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:37:59,026][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 97.39% examples, 938196 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:37:59,083][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2912070 effective words) took 3.1s, 944196 effective words/s
[2023-02-07 18:38:00,104][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 21.95% examples, 617556 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:01,107][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 42.89% examples, 636252 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:02,120][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 64.81% examples, 633431 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:03,122][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 87.23% examples, 634342 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:03,650][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2912070 effective words) took 4.6s, 638001 effective words/s
[2023-02-07 18:38:04,659][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 32.42% examples, 945126 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:05,679][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 65.49% examples, 960421 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:06,660][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2912070 effective words) took 3.0s, 968023 effective words/s
[2023-02-07 18:38:07,674][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 20.63% examples, 583656 words/s, in_qsize 8, out_qsize 1
[2023-02-07 18:38:08,679][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 41.42% examples, 614762 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:09,700][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 70.46% examples, 690309 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:10,513][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2912070 effective words) took 3.9s, 756029 effective words/s
[2023-02-07 18:38:11,519][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 33.53% examples, 986158 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:12,520][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 63.59% examples, 940198 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:13,527][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 97.39% examples, 942752 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:13,596][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2912070 effective words) took 3.1s, 945099 effective words/s
[2023-02-07 18:38:14,603][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 22.14% examples, 635159 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:15,615][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 44.67% examples, 659716 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:16,628][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 66.20% examples, 648918 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:17,639][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 90.18% examples, 654935 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:18,044][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2912070 effective words) took 4.4s, 655139 effective words/s
[2023-02-07 18:38:19,055][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 22.44% examples, 641307 words/s, in_qsize 8, out_qsize 1
[2023-02-07 18:38:20,064][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 43.51% examples, 644698 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:21,071][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 66.53% examples, 653093 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:22,081][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 90.18% examples, 655819 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:22,488][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2912070 effective words) took 4.4s, 655652 effective words/s
[2023-02-07 18:38:23,501][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 30.89% examples, 893461 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:24,518][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 60.79% examples, 888601 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:25,521][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 82.56% examples, 799208 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:26,315][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2912070 effective words) took 3.8s, 761320 effective words/s
[2023-02-07 18:38:27,334][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 21.00% examples, 590532 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:28,338][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 41.17% examples, 609209 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:29,355][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 63.03% examples, 614537 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:30,355][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 83.51% examples, 609300 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:31,095][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2912070 effective words) took 4.8s, 609617 effective words/s
[2023-02-07 18:38:32,102][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 19.53% examples, 551462 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:33,123][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 38.87% examples, 571669 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:34,134][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 68.28% examples, 669373 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:35,147][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 89.62% examples, 648246 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:35,576][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2912070 effective words) took 4.5s, 650372 effective words/s
[2023-02-07 18:38:36,584][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 31.10% examples, 907241 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:37,584][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 64.69% examples, 957654 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:38,560][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2912070 effective words) took 3.0s, 976886 effective words/s
[2023-02-07 18:38:39,592][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 21.46% examples, 601100 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:40,604][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 54.81% examples, 800460 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:41,615][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 89.75% examples, 860807 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:41,891][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2912070 effective words) took 3.3s, 874803 effective words/s
[2023-02-07 18:38:42,902][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 22.63% examples, 650282 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:43,904][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 55.08% examples, 817653 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:44,907][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 85.85% examples, 838117 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:45,514][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2912070 effective words) took 3.6s, 804560 effective words/s
[2023-02-07 18:38:46,526][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 25.15% examples, 723149 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:47,534][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 45.84% examples, 673467 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:48,544][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 66.47% examples, 652064 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:49,562][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 90.02% examples, 651691 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:38:50,038][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2912070 effective words) took 4.5s, 643861 effective words/s
[2023-02-07 18:38:51,052][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 31.10% examples, 902630 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:52,059][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 64.45% examples, 948318 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:38:53,070][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 99.36% examples, 955503 words/s, in_qsize 2, out_qsize 1
[2023-02-07 18:38:53,073][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2912070 effective words) took 3.0s, 960514 effective words/s
[2023-02-07 18:38:53,073][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 43672440 raw words (43681050 effective words) took 57.1s, 765320 effective words/s', 'datetime': '2023-02-07T18:38:53.073409', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:38:53.073 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:38:55,479][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183732-0mprup8m/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:38:55.479027', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:38:55,481][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:38:55,521][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183732-0mprup8m/files/../tmp/embedding_model.pt
2023-02-07 18:38:55.521 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:38:57.348 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:38:58.019 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:39:01.044 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.4911431151251815, 'test_mae': 1.20064479964671, 'test_r2': -3.144692684957322}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.32
wandb: percentage 0.26824
wandb:   test_mae 1.20064
wandb:   test_mse 2.49114
wandb:    test_r2 -3.14469
wandb: 
wandb: üöÄ View run efficient-sweep-13 at: https://wandb.ai/xiaoqiz/mof2vec/runs/0mprup8m
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_183732-0mprup8m/logs
wandb: Agent Starting Run: tx4ax0ea with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 517
wandb: 	model.gensim.alpha: 0.0029338718729425856
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 8
wandb: 	model.gensim.sample: 0.9109413078155464
wandb: 	model.gensim.vector_size: 339
wandb: 	model.gensim.window: 9
wandb: 	model.sklearn.learning_rate: 0.16657234401215157
wandb: 	model.sklearn.max_depth: 66
wandb: 	model.sklearn.min_child_weight: 0.0926734888589947
wandb: 	model.sklearn.n_estimators: 2394
wandb: 	model.sklearn.num_leaves: 32
wandb: 	model.sklearn.reg_alpha: 0.0807587039166522
wandb: 	model.sklearn.reg_lambda: 0.07853026885999925
wandb: 	model.sklearn.subsample: 0.8038200871317622
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183911-tx4ax0ea
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-14
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/tx4ax0ea
2023-02-07 18:39:20.479 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 18:39:20.479 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 517 for sweep.
2023-02-07 18:39:20.480 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0029338718729425856 for sweep.
2023-02-07 18:39:20.480 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 18:39:20.480 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 8 for sweep.
2023-02-07 18:39:20.480 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.9109413078155464 for sweep.
2023-02-07 18:39:20.481 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 339 for sweep.
2023-02-07 18:39:20.481 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 9 for sweep.
2023-02-07 18:39:20.481 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.16657234401215157 for sweep.
2023-02-07 18:39:20.481 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 66 for sweep.
2023-02-07 18:39:20.481 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0926734888589947 for sweep.
2023-02-07 18:39:20.482 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2394 for sweep.
2023-02-07 18:39:20.482 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 32 for sweep.
2023-02-07 18:39:20.482 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.0807587039166522 for sweep.
2023-02-07 18:39:20.483 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.07853026885999925 for sweep.
2023-02-07 18:39:20.483 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8038200871317622 for sweep.
2023-02-07 18:39:20.483 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:39:20.490 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183911-tx4ax0ea/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 517, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 339, 'window': 9, 'min_count': 8, 'dm': 1, 'sample': 0.9109413078155464, 'workers': 4, 'alpha': 0.0029338718729425856, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2394, 'max_depth': 66, 'num_leaves': 32, 'reg_alpha': 0.0807587039166522, 'reg_lambda': 0.07853026885999925, 'subsample': 0.8038200871317622, 'min_child_weight': 0.0926734888589947, 'n_jobs': 4, 'learning_rate': 0.16657234401215157}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 19/3257 [00:00<00:17, 184.48it/s]  1%|          | 39/3257 [00:00<00:16, 189.55it/s]  2%|‚ñè         | 58/3257 [00:00<00:17, 187.39it/s]  3%|‚ñé         | 82/3257 [00:00<00:15, 204.22it/s]  3%|‚ñé         | 104/3257 [00:00<00:15, 204.37it/s]  4%|‚ñç         | 125/3257 [00:00<00:15, 197.24it/s]  5%|‚ñç         | 150/3257 [00:00<00:14, 211.58it/s]  5%|‚ñå         | 172/3257 [00:00<00:15, 204.53it/s]  6%|‚ñå         | 194/3257 [00:00<00:14, 207.93it/s]  7%|‚ñã         | 218/3257 [00:01<00:14, 214.44it/s]  7%|‚ñã         | 243/3257 [00:01<00:13, 222.91it/s]  8%|‚ñä         | 266/3257 [00:01<00:13, 215.93it/s]  9%|‚ñâ         | 292/3257 [00:01<00:13, 227.79it/s] 10%|‚ñâ         | 315/3257 [00:01<00:13, 220.51it/s] 10%|‚ñà         | 338/3257 [00:01<00:13, 222.07it/s] 11%|‚ñà         | 362/3257 [00:01<00:13, 222.33it/s] 12%|‚ñà‚ñè        | 385/3257 [00:01<00:13, 213.71it/s] 12%|‚ñà‚ñè        | 407/3257 [00:01<00:13, 213.53it/s] 13%|‚ñà‚ñé        | 429/3257 [00:02<00:14, 196.76it/s] 14%|‚ñà‚ñç        | 450/3257 [00:02<00:14, 198.08it/s] 15%|‚ñà‚ñç        | 474/3257 [00:02<00:13, 206.42it/s] 15%|‚ñà‚ñå        | 495/3257 [00:02<00:13, 207.39it/s] 16%|‚ñà‚ñå        | 518/3257 [00:02<00:12, 212.73it/s] 17%|‚ñà‚ñã        | 540/3257 [00:02<00:12, 210.81it/s] 17%|‚ñà‚ñã        | 562/3257 [00:02<00:13, 197.14it/s] 18%|‚ñà‚ñä        | 582/3257 [00:02<00:14, 189.95it/s] 19%|‚ñà‚ñä        | 604/3257 [00:02<00:13, 195.20it/s] 19%|‚ñà‚ñâ        | 624/3257 [00:03<00:13, 196.02it/s] 20%|‚ñà‚ñâ        | 645/3257 [00:03<00:13, 195.57it/s] 20%|‚ñà‚ñà        | 665/3257 [00:03<00:13, 186.26it/s] 21%|‚ñà‚ñà        | 684/3257 [00:03<00:13, 184.71it/s] 22%|‚ñà‚ñà‚ñè       | 705/3257 [00:03<00:13, 191.51it/s] 22%|‚ñà‚ñà‚ñè       | 725/3257 [00:03<00:13, 191.46it/s] 23%|‚ñà‚ñà‚ñé       | 745/3257 [00:03<00:13, 190.01it/s] 24%|‚ñà‚ñà‚ñé       | 769/3257 [00:03<00:12, 204.01it/s] 24%|‚ñà‚ñà‚ñç       | 790/3257 [00:03<00:12, 196.70it/s] 25%|‚ñà‚ñà‚ñç       | 811/3257 [00:03<00:12, 200.00it/s] 26%|‚ñà‚ñà‚ñå       | 832/3257 [00:04<00:12, 194.95it/s] 26%|‚ñà‚ñà‚ñå       | 852/3257 [00:04<00:12, 186.15it/s] 27%|‚ñà‚ñà‚ñã       | 873/3257 [00:04<00:12, 190.92it/s] 27%|‚ñà‚ñà‚ñã       | 893/3257 [00:04<00:12, 187.83it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:04<00:12, 191.89it/s] 29%|‚ñà‚ñà‚ñä       | 934/3257 [00:04<00:18, 125.82it/s] 29%|‚ñà‚ñà‚ñâ       | 956/3257 [00:04<00:15, 145.02it/s] 30%|‚ñà‚ñà‚ñà       | 978/3257 [00:05<00:14, 160.47it/s] 31%|‚ñà‚ñà‚ñà       | 997/3257 [00:05<00:13, 163.65it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1019/3257 [00:05<00:12, 175.66it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1038/3257 [00:05<00:12, 174.33it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1057/3257 [00:05<00:12, 177.57it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:05<00:11, 183.43it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1101/3257 [00:05<00:11, 189.62it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1121/3257 [00:05<00:11, 191.10it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1141/3257 [00:05<00:11, 188.38it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1164/3257 [00:05<00:10, 198.97it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1185/3257 [00:06<00:10, 189.34it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1205/3257 [00:06<00:11, 180.92it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1227/3257 [00:06<00:10, 190.13it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1248/3257 [00:06<00:10, 191.70it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1270/3257 [00:06<00:09, 199.05it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1291/3257 [00:06<00:10, 185.38it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1311/3257 [00:06<00:10, 188.82it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1333/3257 [00:06<00:09, 197.06it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1353/3257 [00:06<00:09, 197.54it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1373/3257 [00:07<00:09, 193.54it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1393/3257 [00:07<00:09, 193.96it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1420/3257 [00:07<00:08, 213.11it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1442/3257 [00:07<00:08, 209.73it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1466/3257 [00:07<00:08, 217.77it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1488/3257 [00:07<00:08, 218.29it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1512/3257 [00:07<00:07, 222.43it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1535/3257 [00:07<00:08, 201.17it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1556/3257 [00:07<00:08, 198.18it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1577/3257 [00:08<00:08, 200.82it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1599/3257 [00:08<00:08, 205.75it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1620/3257 [00:08<00:07, 206.54it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1641/3257 [00:08<00:08, 201.62it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1662/3257 [00:08<00:08, 194.19it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1682/3257 [00:08<00:08, 195.04it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1702/3257 [00:08<00:08, 193.55it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1723/3257 [00:08<00:07, 197.72it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1743/3257 [00:08<00:08, 183.52it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1766/3257 [00:09<00:07, 196.10it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1790/3257 [00:09<00:07, 206.62it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1811/3257 [00:09<00:07, 197.50it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1831/3257 [00:09<00:07, 197.25it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1854/3257 [00:09<00:06, 205.83it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1876/3257 [00:09<00:06, 208.64it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1897/3257 [00:09<00:06, 205.63it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1918/3257 [00:09<00:06, 203.27it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1945/3257 [00:09<00:05, 221.10it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1971/3257 [00:09<00:05, 231.53it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1995/3257 [00:10<00:05, 223.97it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2018/3257 [00:10<00:05, 218.39it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2040/3257 [00:10<00:05, 206.17it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2061/3257 [00:10<00:06, 196.16it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2084/3257 [00:10<00:05, 204.06it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2105/3257 [00:10<00:05, 203.68it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2126/3257 [00:10<00:05, 197.53it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2146/3257 [00:10<00:05, 190.34it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2170/3257 [00:10<00:05, 203.37it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2192/3257 [00:11<00:05, 206.39it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2213/3257 [00:11<00:05, 200.95it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:11<00:04, 209.00it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2258/3257 [00:11<00:04, 208.65it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2279/3257 [00:11<00:07, 127.51it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2301/3257 [00:11<00:06, 144.69it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2328/3257 [00:11<00:05, 172.36it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2355/3257 [00:12<00:04, 195.37it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2378/3257 [00:12<00:04, 201.44it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2404/3257 [00:12<00:03, 214.31it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2428/3257 [00:12<00:03, 207.37it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2450/3257 [00:12<00:04, 197.87it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2476/3257 [00:12<00:03, 213.74it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2503/3257 [00:12<00:03, 227.35it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2527/3257 [00:12<00:03, 227.61it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:12<00:03, 227.02it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2575/3257 [00:13<00:03, 211.92it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2597/3257 [00:13<00:03, 210.17it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2627/3257 [00:13<00:02, 232.95it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2651/3257 [00:13<00:02, 219.70it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2674/3257 [00:13<00:02, 221.05it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2697/3257 [00:13<00:02, 218.57it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2720/3257 [00:13<00:02, 208.88it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2747/3257 [00:13<00:02, 223.60it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2770/3257 [00:13<00:02, 221.37it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2796/3257 [00:14<00:01, 232.11it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2820/3257 [00:14<00:01, 228.73it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:14<00:01, 221.41it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2875/3257 [00:14<00:01, 245.07it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2900/3257 [00:14<00:01, 231.50it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2927/3257 [00:14<00:01, 241.45it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2952/3257 [00:14<00:01, 219.29it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:14<00:01, 223.77it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3002/3257 [00:14<00:01, 232.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3026/3257 [00:15<00:00, 231.08it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3054/3257 [00:15<00:00, 242.64it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3080/3257 [00:15<00:00, 247.45it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3105/3257 [00:15<00:00, 246.72it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3130/3257 [00:15<00:00, 246.58it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3155/3257 [00:15<00:00, 232.89it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3179/3257 [00:15<00:00, 221.82it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3206/3257 [00:15<00:00, 234.13it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3230/3257 [00:15<00:00, 228.02it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3254/3257 [00:15<00:00, 230.29it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:15<00:00, 203.73it/s]
2023-02-07 18:39:36.964 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:39:36,965][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d339,n5,w9,mc8,s0.910941,t4>', 'datetime': '2023-02-07T18:39:36.965354', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:39:36,965][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:39:36,965][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:39:37,314][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 18:39:37,315][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:39:37,333][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 retains 5936 unique words (45.45% of original 13061, drops 7125)', 'datetime': '2023-02-07T18:39:37.333559', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:39:37,334][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 leaves 3618620 word corpus (99.43% of original 3639370, drops 20750)', 'datetime': '2023-02-07T18:39:37.334006', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:39:37,354][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 18:39:37,355][gensim.models.word2vec][INFO] - sample=0.910941 downsamples 0 most-common words
[2023-02-07 18:39:37,355][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3618620 word corpus (100.0%% of prior 3618620)', 'datetime': '2023-02-07T18:39:37.355359', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:39:37,390][gensim.models.word2vec][INFO] - estimated required memory for 5936 words and 339 dimensions: 24134324 bytes
[2023-02-07 18:39:37,390][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:39:37,403][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 5936 vocabulary and 339 features, using sg=0 hs=0 sample=0.9109413078155464 negative=5 window=9 shrink_windows=True', 'datetime': '2023-02-07T18:39:37.403942', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:39:38,411][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 31.01% examples, 1123499 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:39:39,422][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 64.45% examples, 1180961 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:39:40,401][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3621877 effective words) took 3.0s, 1209299 effective words/s
[2023-02-07 18:39:41,404][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 33.47% examples, 1227328 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:39:42,410][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 69.51% examples, 1283266 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:39:43,196][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3621877 effective words) took 2.8s, 1296561 effective words/s
[2023-02-07 18:39:44,205][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 25.33% examples, 911090 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:39:45,211][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 52.38% examples, 963505 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:39:46,214][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 78.88% examples, 958875 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:39:46,990][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3621877 effective words) took 3.8s, 955265 effective words/s
[2023-02-07 18:39:48,002][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 25.94% examples, 935553 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:39:49,002][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 58.46% examples, 1077241 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:39:50,017][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 95.30% examples, 1142883 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:39:50,145][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3621877 effective words) took 3.2s, 1148691 effective words/s
[2023-02-07 18:39:51,163][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 34.23% examples, 1237427 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:39:52,175][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 69.08% examples, 1262044 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:39:52,997][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3621877 effective words) took 2.8s, 1271283 effective words/s
[2023-02-07 18:39:54,003][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 24.72% examples, 885223 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:39:55,009][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 58.89% examples, 1087570 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:39:56,018][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 95.30% examples, 1145338 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:39:56,140][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3621877 effective words) took 3.1s, 1152890 effective words/s
[2023-02-07 18:39:57,148][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 35.34% examples, 1291728 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:39:58,149][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 71.45% examples, 1315536 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:39:58,895][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3621877 effective words) took 2.8s, 1316002 effective words/s
[2023-02-07 18:39:59,908][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 25.48% examples, 915861 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:40:00,923][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 59.66% examples, 1088011 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:40:01,938][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 97.85% examples, 1167667 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:01,986][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3621877 effective words) took 3.1s, 1172295 effective words/s
[2023-02-07 18:40:02,998][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 26.50% examples, 953350 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:04,000][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 52.35% examples, 964263 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:05,017][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 78.72% examples, 951615 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:05,791][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3621877 effective words) took 3.8s, 952585 effective words/s
[2023-02-07 18:40:06,803][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 26.90% examples, 971969 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:07,804][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 53.02% examples, 976738 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:08,821][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 79.55% examples, 963980 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:09,529][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3621877 effective words) took 3.7s, 969741 effective words/s
[2023-02-07 18:40:10,539][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 35.34% examples, 1288950 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:11,540][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 70.77% examples, 1304952 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:12,311][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3621877 effective words) took 2.8s, 1303065 effective words/s
[2023-02-07 18:40:13,314][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 27.60% examples, 1005709 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:14,316][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 63.13% examples, 1162765 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:15,297][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3621877 effective words) took 3.0s, 1213965 effective words/s
[2023-02-07 18:40:16,301][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 26.19% examples, 952548 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:17,306][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 52.29% examples, 964767 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:18,309][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 88.79% examples, 1074398 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:18,616][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3621877 effective words) took 3.3s, 1091997 effective words/s
[2023-02-07 18:40:19,620][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 35.52% examples, 1307186 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:20,623][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 70.83% examples, 1307804 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:21,372][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3621877 effective words) took 2.8s, 1315579 effective words/s
[2023-02-07 18:40:22,376][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 25.64% examples, 933911 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:23,387][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 51.37% examples, 945977 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:40:24,392][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 77.80% examples, 943115 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:40:25,187][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3621877 effective words) took 3.8s, 950006 effective words/s
[2023-02-07 18:40:25,188][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54328155 effective words) took 47.8s, 1136962 effective words/s', 'datetime': '2023-02-07T18:40:25.188034', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:40:25.188 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:40:28,280][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183911-tx4ax0ea/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:40:28.280339', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:40:28,281][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:40:28,321][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_183911-tx4ax0ea/files/../tmp/embedding_model.pt
2023-02-07 18:40:28.322 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:40:30.466 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:40:31.239 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:40:33.900 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.7337161432482673, 'test_mae': 1.2384556250923822, 'test_r2': -3.0518566790975763}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.030 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: | 0.030 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.39
wandb: percentage 0.54552
wandb:   test_mae 1.23846
wandb:   test_mse 2.73372
wandb:    test_r2 -3.05186
wandb: 
wandb: üöÄ View run smooth-sweep-14 at: https://wandb.ai/xiaoqiz/mof2vec/runs/tx4ax0ea
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_183911-tx4ax0ea/logs
wandb: Agent Starting Run: wjmdmkee with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 461
wandb: 	model.gensim.alpha: 0.006286830774213501
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 8
wandb: 	model.gensim.sample: 0.9020642820212594
wandb: 	model.gensim.vector_size: 455
wandb: 	model.gensim.window: 2
wandb: 	model.sklearn.learning_rate: 0.13952920914189065
wandb: 	model.sklearn.max_depth: 19
wandb: 	model.sklearn.min_child_weight: 0.05582481044647703
wandb: 	model.sklearn.n_estimators: 1519
wandb: 	model.sklearn.num_leaves: 334
wandb: 	model.sklearn.reg_alpha: 0.01598715216248334
wandb: 	model.sklearn.reg_lambda: 0.9040559075895536
wandb: 	model.sklearn.subsample: 0.6286020315244896
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184042-wjmdmkee
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/wjmdmkee
2023-02-07 18:40:50.795 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 18:40:50.796 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 461 for sweep.
2023-02-07 18:40:50.796 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.006286830774213501 for sweep.
2023-02-07 18:40:50.797 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 18:40:50.797 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 8 for sweep.
2023-02-07 18:40:50.797 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.9020642820212594 for sweep.
2023-02-07 18:40:50.797 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 455 for sweep.
2023-02-07 18:40:50.798 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 2 for sweep.
2023-02-07 18:40:50.798 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.13952920914189065 for sweep.
2023-02-07 18:40:50.798 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 19 for sweep.
2023-02-07 18:40:50.798 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05582481044647703 for sweep.
2023-02-07 18:40:50.799 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1519 for sweep.
2023-02-07 18:40:50.799 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 334 for sweep.
2023-02-07 18:40:50.800 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.01598715216248334 for sweep.
2023-02-07 18:40:50.800 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.9040559075895536 for sweep.
2023-02-07 18:40:50.800 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6286020315244896 for sweep.
2023-02-07 18:40:50.800 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:40:50.807 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184042-wjmdmkee/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 461, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 455, 'window': 2, 'min_count': 8, 'dm': 0, 'sample': 0.9020642820212594, 'workers': 4, 'alpha': 0.006286830774213501, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1519, 'max_depth': 19, 'num_leaves': 334, 'reg_alpha': 0.01598715216248334, 'reg_lambda': 0.9040559075895536, 'subsample': 0.6286020315244896, 'min_child_weight': 0.05582481044647703, 'n_jobs': 4, 'learning_rate': 0.13952920914189065}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 13/3257 [00:00<00:25, 127.18it/s]  1%|          | 28/3257 [00:00<00:23, 139.42it/s]  1%|‚ñè         | 42/3257 [00:00<00:24, 132.08it/s]  2%|‚ñè         | 56/3257 [00:00<00:24, 130.36it/s]  2%|‚ñè         | 74/3257 [00:00<00:21, 146.19it/s]  3%|‚ñé         | 91/3257 [00:00<00:20, 152.06it/s]  3%|‚ñé         | 107/3257 [00:00<00:22, 141.91it/s]  4%|‚ñç         | 124/3257 [00:00<00:21, 147.27it/s]  4%|‚ñç         | 142/3257 [00:00<00:19, 156.03it/s]  5%|‚ñç         | 158/3257 [00:01<00:20, 149.11it/s]  5%|‚ñå         | 174/3257 [00:01<00:21, 141.28it/s]  6%|‚ñå         | 191/3257 [00:01<00:20, 148.84it/s]  6%|‚ñã         | 207/3257 [00:01<00:20, 149.95it/s]  7%|‚ñã         | 229/3257 [00:01<00:17, 168.70it/s]  8%|‚ñä         | 247/3257 [00:01<00:18, 164.54it/s]  8%|‚ñä         | 264/3257 [00:01<00:19, 151.53it/s]  9%|‚ñâ         | 285/3257 [00:01<00:18, 162.92it/s]  9%|‚ñâ         | 302/3257 [00:01<00:18, 157.70it/s] 10%|‚ñâ         | 318/3257 [00:02<00:18, 156.62it/s] 10%|‚ñà         | 336/3257 [00:02<00:18, 159.45it/s] 11%|‚ñà         | 353/3257 [00:02<00:17, 161.58it/s] 11%|‚ñà‚ñè        | 371/3257 [00:02<00:17, 164.59it/s] 12%|‚ñà‚ñè        | 388/3257 [00:02<00:19, 143.69it/s] 12%|‚ñà‚ñè        | 404/3257 [00:02<00:19, 146.12it/s] 13%|‚ñà‚ñé        | 419/3257 [00:02<00:19, 145.45it/s] 13%|‚ñà‚ñé        | 434/3257 [00:03<00:33, 84.83it/s]  14%|‚ñà‚ñç        | 452/3257 [00:03<00:27, 101.42it/s] 14%|‚ñà‚ñç        | 472/3257 [00:03<00:23, 120.31it/s] 15%|‚ñà‚ñç        | 487/3257 [00:03<00:22, 125.79it/s] 16%|‚ñà‚ñå        | 506/3257 [00:03<00:19, 141.22it/s] 16%|‚ñà‚ñå        | 524/3257 [00:03<00:18, 148.57it/s] 17%|‚ñà‚ñã        | 542/3257 [00:03<00:17, 155.34it/s] 17%|‚ñà‚ñã        | 559/3257 [00:03<00:18, 143.91it/s] 18%|‚ñà‚ñä        | 575/3257 [00:04<00:19, 139.80it/s] 18%|‚ñà‚ñä        | 594/3257 [00:04<00:17, 152.14it/s] 19%|‚ñà‚ñâ        | 612/3257 [00:04<00:16, 158.03it/s] 19%|‚ñà‚ñâ        | 629/3257 [00:04<00:16, 154.86it/s] 20%|‚ñà‚ñâ        | 645/3257 [00:04<00:17, 150.86it/s] 20%|‚ñà‚ñà        | 661/3257 [00:04<00:18, 142.56it/s] 21%|‚ñà‚ñà        | 680/3257 [00:04<00:16, 153.26it/s] 21%|‚ñà‚ñà‚ñè       | 696/3257 [00:04<00:17, 146.17it/s] 22%|‚ñà‚ñà‚ñè       | 716/3257 [00:04<00:15, 159.22it/s] 23%|‚ñà‚ñà‚ñé       | 733/3257 [00:05<00:16, 150.46it/s] 23%|‚ñà‚ñà‚ñé       | 749/3257 [00:05<00:17, 144.56it/s] 24%|‚ñà‚ñà‚ñé       | 769/3257 [00:05<00:15, 157.48it/s] 24%|‚ñà‚ñà‚ñç       | 786/3257 [00:05<00:16, 148.60it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:05<00:16, 153.28it/s] 25%|‚ñà‚ñà‚ñå       | 820/3257 [00:05<00:16, 151.88it/s] 26%|‚ñà‚ñà‚ñå       | 836/3257 [00:05<00:16, 143.33it/s] 26%|‚ñà‚ñà‚ñå       | 851/3257 [00:05<00:17, 138.53it/s] 27%|‚ñà‚ñà‚ñã       | 868/3257 [00:05<00:16, 143.20it/s] 27%|‚ñà‚ñà‚ñã       | 883/3257 [00:06<00:16, 141.14it/s] 28%|‚ñà‚ñà‚ñä       | 898/3257 [00:06<00:16, 140.56it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:06<00:16, 140.49it/s] 29%|‚ñà‚ñà‚ñä       | 932/3257 [00:06<00:15, 147.73it/s] 29%|‚ñà‚ñà‚ñâ       | 949/3257 [00:06<00:15, 151.56it/s] 30%|‚ñà‚ñà‚ñâ       | 965/3257 [00:06<00:14, 153.40it/s] 30%|‚ñà‚ñà‚ñà       | 981/3257 [00:06<00:15, 147.21it/s] 31%|‚ñà‚ñà‚ñà       | 996/3257 [00:06<00:15, 144.37it/s] 31%|‚ñà‚ñà‚ñà       | 1011/3257 [00:06<00:15, 144.07it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:07<00:15, 142.48it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1042/3257 [00:07<00:16, 137.80it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1056/3257 [00:07<00:15, 137.99it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1073/3257 [00:07<00:15, 145.47it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1088/3257 [00:07<00:15, 137.39it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1104/3257 [00:07<00:15, 142.68it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1119/3257 [00:07<00:15, 140.27it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1135/3257 [00:07<00:14, 144.31it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1150/3257 [00:07<00:15, 139.58it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1168/3257 [00:08<00:13, 150.54it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1184/3257 [00:08<00:15, 136.42it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1198/3257 [00:08<00:15, 136.01it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1212/3257 [00:08<00:15, 129.98it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1232/3257 [00:08<00:13, 144.67it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1247/3257 [00:08<00:14, 143.56it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1263/3257 [00:08<00:13, 146.79it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1278/3257 [00:08<00:14, 138.78it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1293/3257 [00:09<00:15, 130.37it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1307/3257 [00:09<00:14, 130.23it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1321/3257 [00:09<00:14, 132.58it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:09<00:13, 141.30it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1354/3257 [00:09<00:13, 136.74it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:09<00:13, 140.11it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1385/3257 [00:09<00:13, 140.25it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1402/3257 [00:09<00:12, 146.52it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:09<00:11, 157.48it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1439/3257 [00:09<00:11, 156.77it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1460/3257 [00:10<00:10, 167.60it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1477/3257 [00:10<00:10, 165.52it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1496/3257 [00:10<00:10, 170.14it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1514/3257 [00:10<00:10, 170.61it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1532/3257 [00:10<00:11, 155.71it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1548/3257 [00:10<00:11, 147.27it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1564/3257 [00:10<00:11, 150.12it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1580/3257 [00:11<00:17, 93.48it/s]  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1597/3257 [00:11<00:15, 107.29it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1616/3257 [00:11<00:13, 123.40it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:11<00:12, 125.11it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1649/3257 [00:11<00:12, 129.02it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1664/3257 [00:11<00:12, 131.44it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1680/3257 [00:11<00:11, 136.88it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1695/3257 [00:11<00:11, 139.59it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1713/3257 [00:11<00:10, 149.24it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1729/3257 [00:12<00:10, 138.99it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1744/3257 [00:12<00:11, 136.78it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:12<00:10, 146.70it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1780/3257 [00:12<00:09, 151.79it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1797/3257 [00:12<00:09, 154.04it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1813/3257 [00:12<00:09, 148.48it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1828/3257 [00:12<00:09, 145.04it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1846/3257 [00:12<00:09, 152.45it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1865/3257 [00:12<00:08, 160.85it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1882/3257 [00:13<00:08, 160.73it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1899/3257 [00:13<00:08, 155.70it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:13<00:08, 156.19it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1934/3257 [00:13<00:08, 162.16it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1954/3257 [00:13<00:07, 171.48it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1972/3257 [00:13<00:07, 170.74it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1990/3257 [00:13<00:07, 160.70it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2007/3257 [00:13<00:07, 158.25it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2025/3257 [00:13<00:07, 164.01it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2042/3257 [00:14<00:07, 155.80it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2058/3257 [00:14<00:08, 148.70it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2074/3257 [00:14<00:07, 150.26it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2090/3257 [00:14<00:07, 146.98it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2105/3257 [00:14<00:08, 143.53it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:14<00:08, 139.19it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2135/3257 [00:14<00:07, 140.46it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2150/3257 [00:14<00:08, 137.19it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2166/3257 [00:14<00:07, 142.78it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:15<00:07, 140.88it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2199/3257 [00:15<00:07, 149.78it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2215/3257 [00:15<00:07, 144.21it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2231/3257 [00:15<00:06, 148.58it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2246/3257 [00:15<00:07, 143.51it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2261/3257 [00:15<00:06, 144.57it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2276/3257 [00:15<00:07, 138.56it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2296/3257 [00:15<00:06, 154.74it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2312/3257 [00:15<00:06, 151.98it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2334/3257 [00:16<00:05, 168.63it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2354/3257 [00:16<00:05, 177.08it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2372/3257 [00:16<00:05, 166.87it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2393/3257 [00:16<00:04, 174.39it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2411/3257 [00:16<00:05, 162.35it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2428/3257 [00:16<00:05, 161.20it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2445/3257 [00:16<00:05, 153.99it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2463/3257 [00:16<00:04, 160.49it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2481/3257 [00:16<00:04, 164.43it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2501/3257 [00:17<00:04, 174.29it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2519/3257 [00:17<00:04, 167.05it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2537/3257 [00:17<00:04, 167.95it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2554/3257 [00:17<00:04, 156.47it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2570/3257 [00:17<00:04, 145.99it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2585/3257 [00:17<00:04, 145.87it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2602/3257 [00:17<00:04, 151.97it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2624/3257 [00:17<00:03, 170.27it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2642/3257 [00:17<00:03, 164.66it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2659/3257 [00:18<00:03, 152.79it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2677/3257 [00:18<00:03, 155.94it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2693/3257 [00:18<00:03, 155.45it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2709/3257 [00:18<00:04, 134.91it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2724/3257 [00:18<00:03, 138.71it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2744/3257 [00:18<00:03, 151.79it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:18<00:03, 150.96it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2777/3257 [00:18<00:03, 147.18it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2797/3257 [00:19<00:02, 160.23it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2814/3257 [00:19<00:02, 158.24it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2830/3257 [00:19<00:02, 145.48it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2846/3257 [00:19<00:02, 148.66it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2865/3257 [00:19<00:02, 159.09it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2883/3257 [00:19<00:02, 163.21it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2900/3257 [00:19<00:02, 144.37it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2916/3257 [00:19<00:02, 144.15it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2931/3257 [00:19<00:02, 143.51it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2946/3257 [00:20<00:02, 133.46it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2960/3257 [00:20<00:03, 74.74it/s]  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:20<00:03, 86.91it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2989/3257 [00:20<00:02, 94.80it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3003/3257 [00:20<00:02, 104.17it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3016/3257 [00:20<00:02, 108.70it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3030/3257 [00:20<00:01, 115.21it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3047/3257 [00:21<00:01, 127.47it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3064/3257 [00:21<00:01, 138.04it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:21<00:01, 138.97it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3094/3257 [00:21<00:01, 136.13it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3110/3257 [00:21<00:01, 141.11it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3127/3257 [00:21<00:00, 146.45it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3142/3257 [00:21<00:00, 145.02it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3157/3257 [00:21<00:00, 142.50it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3173/3257 [00:21<00:00, 145.96it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3188/3257 [00:22<00:00, 140.82it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:22<00:00, 148.40it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3222/3257 [00:22<00:00, 145.51it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3240/3257 [00:22<00:00, 155.01it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3256/3257 [00:22<00:00, 156.00it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:22<00:00, 144.73it/s]
2023-02-07 18:41:14.220 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:41:14,221][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d455,n5,mc8,s0.902064,t4>', 'datetime': '2023-02-07T18:41:14.221328', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:41:14,222][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:41:14,222][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:41:14,857][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 18:41:14,857][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:41:14,930][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 retains 22849 unique words (42.27% of original 54054, drops 31205)', 'datetime': '2023-02-07T18:41:14.930642', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:41:14,931][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 leaves 6458832 word corpus (98.60% of original 6550866, drops 92034)', 'datetime': '2023-02-07T18:41:14.931122', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:41:15,010][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 18:41:15,012][gensim.models.word2vec][INFO] - sample=0.902064 downsamples 0 most-common words
[2023-02-07 18:41:15,012][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6458832 word corpus (100.0%% of prior 6458832)', 'datetime': '2023-02-07T18:41:15.012835', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:41:15,148][gensim.models.word2vec][INFO] - estimated required memory for 22849 words and 455 dimensions: 101174000 bytes
[2023-02-07 18:41:15,149][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:41:15,200][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 22849 vocabulary and 455 features, using sg=1 hs=0 sample=0.9020642820212594 negative=5 window=2 shrink_windows=True', 'datetime': '2023-02-07T18:41:15.200205', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:41:16,205][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 17.13% examples, 1072425 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:17,210][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 34.45% examples, 1112371 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:18,212][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 51.73% examples, 1126741 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:41:19,224][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 69.76% examples, 1137899 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:20,232][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 96.65% examples, 1233295 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:41:20,354][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6404777 effective words) took 5.1s, 1243657 effective words/s
[2023-02-07 18:41:21,358][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 22.14% examples, 1392689 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:41:22,365][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 41.63% examples, 1367062 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:23,367][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 62.85% examples, 1359008 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:24,373][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 83.57% examples, 1347759 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:25,113][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6404777 effective words) took 4.8s, 1346591 effective words/s
[2023-02-07 18:41:26,115][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 20.11% examples, 1266971 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:41:27,116][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 39.39% examples, 1294648 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:41:28,121][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 59.75% examples, 1296053 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:29,127][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 81.09% examples, 1304426 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:29,996][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6404777 effective words) took 4.9s, 1312348 effective words/s
[2023-02-07 18:41:31,003][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 20.97% examples, 1320994 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:41:32,007][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 40.84% examples, 1336909 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:41:33,010][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 62.27% examples, 1341620 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:34,019][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 83.33% examples, 1341979 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:41:34,763][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6404777 effective words) took 4.8s, 1344325 effective words/s
[2023-02-07 18:41:35,779][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 21.37% examples, 1333881 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:36,780][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 41.57% examples, 1358990 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:37,785][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 62.79% examples, 1351631 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:41:38,789][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 84.16% examples, 1352063 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:41:39,475][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6404777 effective words) took 4.7s, 1359733 effective words/s
[2023-02-07 18:41:40,484][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 23.89% examples, 1512559 words/s, in_qsize 7, out_qsize 1
[2023-02-07 18:41:41,493][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 44.09% examples, 1438680 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:42,496][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 65.09% examples, 1409116 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:43,496][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 86.61% examples, 1392702 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:41:44,088][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6404777 effective words) took 4.6s, 1389818 effective words/s
[2023-02-07 18:41:45,097][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 21.55% examples, 1352851 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:46,099][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 41.42% examples, 1354565 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:41:47,100][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 62.97% examples, 1362605 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:48,104][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 84.96% examples, 1367171 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:48,771][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6404777 effective words) took 4.7s, 1368404 effective words/s
[2023-02-07 18:41:49,780][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 28.55% examples, 1820574 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:50,783][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 56.31% examples, 1835632 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:51,786][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 78.72% examples, 1691466 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:41:52,724][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6404777 effective words) took 4.0s, 1621207 effective words/s
[2023-02-07 18:41:53,730][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 21.95% examples, 1378079 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:54,738][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 42.22% examples, 1382964 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:41:55,747][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 64.32% examples, 1387956 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:41:56,756][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 86.18% examples, 1380884 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:41:57,354][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6404777 effective words) took 4.6s, 1384407 effective words/s
[2023-02-07 18:41:58,365][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 22.44% examples, 1407760 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:41:59,368][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 42.98% examples, 1406599 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:00,371][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 65.06% examples, 1406486 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:01,373][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 87.17% examples, 1403839 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:01,922][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6404777 effective words) took 4.6s, 1402863 effective words/s
[2023-02-07 18:42:02,940][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 22.01% examples, 1367564 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:42:03,941][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 42.98% examples, 1403528 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:04,947][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 65.37% examples, 1412026 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:05,948][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 88.09% examples, 1410541 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:06,470][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6404777 effective words) took 4.5s, 1409136 effective words/s
[2023-02-07 18:42:07,474][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 21.95% examples, 1377184 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:08,488][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 42.80% examples, 1398447 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:42:09,490][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 65.37% examples, 1414358 words/s, in_qsize 5, out_qsize 0
[2023-02-07 18:42:10,490][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 88.21% examples, 1413367 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:10,990][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6404777 effective words) took 4.5s, 1417500 effective words/s
[2023-02-07 18:42:12,001][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 22.11% examples, 1384710 words/s, in_qsize 8, out_qsize 1
[2023-02-07 18:42:13,002][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 42.98% examples, 1408007 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:14,009][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 65.15% examples, 1411456 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:15,016][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 87.63% examples, 1405978 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:15,534][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6404777 effective words) took 4.5s, 1410466 effective words/s
[2023-02-07 18:42:16,543][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 22.01% examples, 1379449 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:17,548][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 42.52% examples, 1394159 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:18,550][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 64.66% examples, 1398214 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:19,562][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 86.64% examples, 1389448 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:42:20,125][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6404777 effective words) took 4.6s, 1395756 effective words/s
[2023-02-07 18:42:21,137][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 22.44% examples, 1406765 words/s, in_qsize 7, out_qsize 1
[2023-02-07 18:42:22,137][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 43.75% examples, 1430004 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:23,139][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 65.95% examples, 1433391 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:24,145][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 89.16% examples, 1432357 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:42:24,602][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6404777 effective words) took 4.5s, 1431769 effective words/s
[2023-02-07 18:42:24,603][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (96071655 effective words) took 69.4s, 1384263 effective words/s', 'datetime': '2023-02-07T18:42:24.603448', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:42:24.603 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:42:30,245][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184042-wjmdmkee/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:42:30.245522', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:42:30,246][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:42:30,371][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184042-wjmdmkee/files/../tmp/embedding_model.pt
2023-02-07 18:42:30.371 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:42:32.993 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:42:33.901 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:42:37.166 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1655263615796594, 'test_mae': 1.1064199208256222, 'test_r2': -1.7966094664767676}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.83
wandb: percentage 0.57729
wandb:   test_mae 1.10642
wandb:   test_mse 2.16553
wandb:    test_r2 -1.79661
wandb: 
wandb: üöÄ View run earnest-sweep-15 at: https://wandb.ai/xiaoqiz/mof2vec/runs/wjmdmkee
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_184042-wjmdmkee/logs
wandb: Agent Starting Run: gle5mh70 with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 550
wandb: 	model.gensim.alpha: 0.003921913348544838
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.5520059412831071
wandb: 	model.gensim.vector_size: 283
wandb: 	model.gensim.window: 10
wandb: 	model.sklearn.learning_rate: 0.008912366646710813
wandb: 	model.sklearn.max_depth: 22
wandb: 	model.sklearn.min_child_weight: 0.09431899664343878
wandb: 	model.sklearn.n_estimators: 1405
wandb: 	model.sklearn.num_leaves: 380
wandb: 	model.sklearn.reg_alpha: 0.9799459095800312
wandb: 	model.sklearn.reg_lambda: 0.1803638899032398
wandb: 	model.sklearn.subsample: 0.6887546432966223
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184247-gle5mh70
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-16
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/gle5mh70
2023-02-07 18:42:56.256 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 18:42:56.257 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 550 for sweep.
2023-02-07 18:42:56.257 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.003921913348544838 for sweep.
2023-02-07 18:42:56.258 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 18:42:56.258 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 18:42:56.258 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5520059412831071 for sweep.
2023-02-07 18:42:56.258 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 283 for sweep.
2023-02-07 18:42:56.259 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 10 for sweep.
2023-02-07 18:42:56.259 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.008912366646710813 for sweep.
2023-02-07 18:42:56.259 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 22 for sweep.
2023-02-07 18:42:56.259 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.09431899664343878 for sweep.
2023-02-07 18:42:56.260 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1405 for sweep.
2023-02-07 18:42:56.260 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 380 for sweep.
2023-02-07 18:42:56.260 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.9799459095800312 for sweep.
2023-02-07 18:42:56.260 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.1803638899032398 for sweep.
2023-02-07 18:42:56.261 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6887546432966223 for sweep.
2023-02-07 18:42:56.261 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:42:56.270 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184247-gle5mh70/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 550, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 283, 'window': 10, 'min_count': 4, 'dm': 0, 'sample': 0.5520059412831071, 'workers': 4, 'alpha': 0.003921913348544838, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1405, 'max_depth': 22, 'num_leaves': 380, 'reg_alpha': 0.9799459095800312, 'reg_lambda': 0.1803638899032398, 'subsample': 0.6887546432966223, 'min_child_weight': 0.09431899664343878, 'n_jobs': 4, 'learning_rate': 0.008912366646710813}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 21/3257 [00:00<00:15, 205.63it/s]  1%|‚ñè         | 43/3257 [00:00<00:15, 212.20it/s]  2%|‚ñè         | 65/3257 [00:00<00:14, 213.78it/s]  3%|‚ñé         | 88/3257 [00:00<00:14, 217.66it/s]  3%|‚ñé         | 110/3257 [00:00<00:15, 204.36it/s]  4%|‚ñç         | 135/3257 [00:00<00:14, 216.12it/s]  5%|‚ñç         | 159/3257 [00:00<00:13, 223.09it/s]  6%|‚ñå         | 182/3257 [00:00<00:13, 222.67it/s]  6%|‚ñã         | 205/3257 [00:00<00:13, 223.80it/s]  7%|‚ñã         | 233/3257 [00:01<00:12, 239.00it/s]  8%|‚ñä         | 258/3257 [00:01<00:12, 242.08it/s]  9%|‚ñâ         | 285/3257 [00:01<00:11, 248.21it/s] 10%|‚ñâ         | 310/3257 [00:01<00:12, 239.09it/s] 10%|‚ñà         | 335/3257 [00:01<00:12, 240.77it/s] 11%|‚ñà         | 360/3257 [00:01<00:12, 238.81it/s] 12%|‚ñà‚ñè        | 384/3257 [00:01<00:12, 228.85it/s] 12%|‚ñà‚ñè        | 407/3257 [00:01<00:12, 229.00it/s] 13%|‚ñà‚ñé        | 430/3257 [00:01<00:13, 210.91it/s] 14%|‚ñà‚ñç        | 452/3257 [00:02<00:13, 210.67it/s] 15%|‚ñà‚ñç        | 477/3257 [00:02<00:12, 219.41it/s] 15%|‚ñà‚ñå        | 501/3257 [00:02<00:12, 224.32it/s] 16%|‚ñà‚ñå        | 525/3257 [00:02<00:12, 227.53it/s] 17%|‚ñà‚ñã        | 548/3257 [00:02<00:12, 224.54it/s] 18%|‚ñà‚ñä        | 571/3257 [00:02<00:12, 207.80it/s] 18%|‚ñà‚ñä        | 593/3257 [00:02<00:12, 208.12it/s] 19%|‚ñà‚ñâ        | 618/3257 [00:02<00:12, 219.51it/s] 20%|‚ñà‚ñâ        | 641/3257 [00:02<00:12, 217.13it/s] 20%|‚ñà‚ñà        | 663/3257 [00:02<00:12, 211.31it/s] 21%|‚ñà‚ñà        | 685/3257 [00:03<00:12, 209.96it/s] 22%|‚ñà‚ñà‚ñè       | 707/3257 [00:03<00:12, 211.99it/s] 22%|‚ñà‚ñà‚ñè       | 729/3257 [00:03<00:12, 208.79it/s] 23%|‚ñà‚ñà‚ñé       | 750/3257 [00:03<00:12, 208.62it/s] 24%|‚ñà‚ñà‚ñç       | 774/3257 [00:03<00:11, 215.59it/s] 24%|‚ñà‚ñà‚ñç       | 797/3257 [00:03<00:11, 217.57it/s] 25%|‚ñà‚ñà‚ñå       | 820/3257 [00:03<00:11, 217.67it/s] 26%|‚ñà‚ñà‚ñå       | 842/3257 [00:03<00:11, 206.46it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:03<00:11, 207.14it/s] 27%|‚ñà‚ñà‚ñã       | 886/3257 [00:04<00:11, 209.39it/s] 28%|‚ñà‚ñà‚ñä       | 908/3257 [00:04<00:17, 134.68it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:04<00:15, 153.42it/s] 29%|‚ñà‚ñà‚ñâ       | 955/3257 [00:04<00:13, 172.94it/s] 30%|‚ñà‚ñà‚ñâ       | 977/3257 [00:04<00:12, 183.04it/s] 31%|‚ñà‚ñà‚ñà       | 998/3257 [00:04<00:12, 184.99it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1019/3257 [00:04<00:11, 189.53it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1040/3257 [00:04<00:11, 184.90it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1061/3257 [00:05<00:11, 189.23it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1084/3257 [00:05<00:10, 200.19it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1105/3257 [00:05<00:10, 199.77it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1126/3257 [00:05<00:10, 200.61it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:05<00:10, 199.14it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:05<00:10, 207.43it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1192/3257 [00:05<00:10, 197.86it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1212/3257 [00:05<00:10, 197.06it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1238/3257 [00:05<00:09, 212.82it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1260/3257 [00:06<00:09, 209.10it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1282/3257 [00:06<00:09, 201.70it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1303/3257 [00:06<00:09, 200.61it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:06<00:09, 208.45it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1351/3257 [00:06<00:08, 212.19it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1374/3257 [00:06<00:08, 215.56it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1396/3257 [00:06<00:08, 216.16it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:06<00:08, 226.46it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1446/3257 [00:06<00:07, 226.54it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1472/3257 [00:07<00:07, 235.25it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1497/3257 [00:07<00:07, 238.96it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1521/3257 [00:07<00:07, 229.05it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1544/3257 [00:07<00:07, 220.50it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1567/3257 [00:07<00:07, 223.16it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1590/3257 [00:07<00:07, 223.00it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:07<00:07, 229.68it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1639/3257 [00:07<00:07, 219.35it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1662/3257 [00:07<00:07, 210.78it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1684/3257 [00:07<00:07, 209.95it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1708/3257 [00:08<00:07, 215.60it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1730/3257 [00:08<00:07, 215.29it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1752/3257 [00:08<00:07, 212.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1779/3257 [00:08<00:06, 225.81it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:08<00:06, 232.07it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1829/3257 [00:08<00:06, 223.28it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1855/3257 [00:08<00:06, 232.36it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1882/3257 [00:08<00:05, 241.78it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1907/3257 [00:08<00:05, 240.90it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1932/3257 [00:09<00:05, 239.98it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:09<00:04, 261.62it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1992/3257 [00:09<00:04, 259.30it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2018/3257 [00:09<00:04, 249.06it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2043/3257 [00:09<00:05, 238.54it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2067/3257 [00:09<00:05, 227.00it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2092/3257 [00:09<00:05, 232.11it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2116/3257 [00:09<00:05, 227.70it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2139/3257 [00:10<00:08, 129.16it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2160/3257 [00:10<00:07, 142.67it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2184/3257 [00:10<00:06, 162.52it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2209/3257 [00:10<00:05, 179.99it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2235/3257 [00:10<00:05, 199.23it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2258/3257 [00:10<00:04, 201.14it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2280/3257 [00:10<00:04, 199.43it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2306/3257 [00:10<00:04, 212.79it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2336/3257 [00:11<00:03, 236.42it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2364/3257 [00:11<00:03, 248.00it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2393/3257 [00:11<00:03, 256.96it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2420/3257 [00:11<00:03, 244.50it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2445/3257 [00:11<00:03, 232.86it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2473/3257 [00:11<00:03, 244.02it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2500/3257 [00:11<00:03, 250.43it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2527/3257 [00:11<00:02, 255.38it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2553/3257 [00:11<00:02, 256.52it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2579/3257 [00:12<00:02, 237.68it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2604/3257 [00:12<00:02, 236.82it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2634/3257 [00:12<00:02, 251.23it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2660/3257 [00:12<00:02, 242.33it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2689/3257 [00:12<00:02, 254.70it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2715/3257 [00:12<00:02, 231.47it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2747/3257 [00:12<00:02, 253.20it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2773/3257 [00:12<00:01, 245.48it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2802/3257 [00:12<00:01, 256.33it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2828/3257 [00:13<00:01, 240.93it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2853/3257 [00:13<00:01, 239.67it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2885/3257 [00:13<00:01, 260.92it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2912/3257 [00:13<00:01, 244.38it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2937/3257 [00:13<00:01, 237.26it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2962/3257 [00:13<00:01, 223.89it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:13<00:01, 216.94it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:13<00:01, 227.47it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3038/3257 [00:13<00:00, 235.46it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3067/3257 [00:14<00:00, 249.90it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3093/3257 [00:14<00:00, 241.26it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3120/3257 [00:14<00:00, 247.81it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3145/3257 [00:14<00:00, 229.40it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3170/3257 [00:14<00:00, 233.10it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3194/3257 [00:14<00:00, 234.51it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3218/3257 [00:14<00:00, 222.33it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:14<00:00, 229.20it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:14<00:00, 219.31it/s]
2023-02-07 18:43:11.567 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:43:11,568][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d283,n5,mc4,s0.552006,t4>', 'datetime': '2023-02-07T18:43:11.568715', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:43:11,569][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:43:11,569][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:43:11,841][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 18:43:11,841][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:43:11,855][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 4674 unique words (70.16% of original 6662, drops 1988)', 'datetime': '2023-02-07T18:43:11.854979', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:43:11,855][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 2908210 word corpus (99.89% of original 2911496, drops 3286)', 'datetime': '2023-02-07T18:43:11.855410', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:43:11,873][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 18:43:11,875][gensim.models.word2vec][INFO] - sample=0.552006 downsamples 0 most-common words
[2023-02-07 18:43:11,875][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2908210 word corpus (100.0%% of prior 2908210)', 'datetime': '2023-02-07T18:43:11.875659', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:43:11,902][gensim.models.word2vec][INFO] - estimated required memory for 4674 words and 283 dimensions: 17257260 bytes
[2023-02-07 18:43:11,903][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:43:11,915][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 4674 vocabulary and 283 features, using sg=1 hs=0 sample=0.5520059412831071 negative=5 window=10 shrink_windows=True', 'datetime': '2023-02-07T18:43:11.915266', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:43:12,922][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 63.56% examples, 1878938 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:43:13,421][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2911467 effective words) took 1.5s, 1937397 effective words/s
[2023-02-07 18:43:14,426][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 59.20% examples, 1759035 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:43:15,109][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2911467 effective words) took 1.7s, 1726481 effective words/s
[2023-02-07 18:43:16,114][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 53.70% examples, 1602167 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:43:16,898][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2911467 effective words) took 1.8s, 1629684 effective words/s
[2023-02-07 18:43:17,905][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 54.07% examples, 1607129 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:43:18,686][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2911467 effective words) took 1.8s, 1630269 effective words/s
[2023-02-07 18:43:19,688][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 54.44% examples, 1624066 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:43:20,328][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2911467 effective words) took 1.6s, 1775518 effective words/s
[2023-02-07 18:43:21,338][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 52.26% examples, 1545629 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:43:22,176][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2911467 effective words) took 1.8s, 1577237 effective words/s
[2023-02-07 18:43:23,178][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 66.38% examples, 1974973 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:43:23,760][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2911467 effective words) took 1.6s, 1840237 effective words/s
[2023-02-07 18:43:24,765][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 77.03% examples, 2263746 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:43:25,043][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2911467 effective words) took 1.3s, 2273164 effective words/s
[2023-02-07 18:43:26,050][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 54.44% examples, 1617753 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:43:26,838][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2911467 effective words) took 1.8s, 1624107 effective words/s
[2023-02-07 18:43:27,843][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 77.86% examples, 2282960 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:43:28,117][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2911467 effective words) took 1.3s, 2282084 effective words/s
[2023-02-07 18:43:29,122][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 52.99% examples, 1571127 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:43:29,951][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2911467 effective words) took 1.8s, 1589537 effective words/s
[2023-02-07 18:43:30,955][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 52.63% examples, 1565285 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:43:31,790][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2911467 effective words) took 1.8s, 1585683 effective words/s
[2023-02-07 18:43:32,792][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 77.03% examples, 2268262 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:43:33,072][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2911467 effective words) took 1.3s, 2272477 effective words/s
[2023-02-07 18:43:34,076][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 52.26% examples, 1555772 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:43:34,895][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2911467 effective words) took 1.8s, 1599402 effective words/s
[2023-02-07 18:43:35,899][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 77.03% examples, 2265147 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:43:36,172][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2911467 effective words) took 1.3s, 2283265 effective words/s
[2023-02-07 18:43:36,173][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 43672440 raw words (43672005 effective words) took 24.3s, 1800366 effective words/s', 'datetime': '2023-02-07T18:43:36.173044', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:43:36.173 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:43:37,893][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184247-gle5mh70/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:43:37.893521', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:43:37,894][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:43:37,925][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184247-gle5mh70/files/../tmp/embedding_model.pt
2023-02-07 18:43:37.926 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:43:39.765 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:43:40.432 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:43:42.500 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.172009388436784, 'test_mae': 1.0758839175633605, 'test_r2': -1.5382082177833993}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.74
wandb: percentage 0.29841
wandb:   test_mae 1.07588
wandb:   test_mse 2.17201
wandb:    test_r2 -1.53821
wandb: 
wandb: üöÄ View run glamorous-sweep-16 at: https://wandb.ai/xiaoqiz/mof2vec/runs/gle5mh70
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_184247-gle5mh70/logs
wandb: Agent Starting Run: jk05aj3s with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 615
wandb: 	model.gensim.alpha: 0.1761603743145446
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.7856594792394007
wandb: 	model.gensim.vector_size: 149
wandb: 	model.gensim.window: 13
wandb: 	model.sklearn.learning_rate: 0.001476122562773318
wandb: 	model.sklearn.max_depth: 55
wandb: 	model.sklearn.min_child_weight: 0.03183301005172713
wandb: 	model.sklearn.n_estimators: 1839
wandb: 	model.sklearn.num_leaves: 139
wandb: 	model.sklearn.reg_alpha: 0.03614871104021367
wandb: 	model.sklearn.reg_lambda: 0.005606064409336355
wandb: 	model.sklearn.subsample: 0.6159159290600289
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184356-jk05aj3s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-sweep-17
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/jk05aj3s
2023-02-07 18:44:05.224 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 18:44:05.225 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 615 for sweep.
2023-02-07 18:44:05.225 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.1761603743145446 for sweep.
2023-02-07 18:44:05.226 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 18:44:05.226 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 18:44:05.226 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7856594792394007 for sweep.
2023-02-07 18:44:05.226 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 149 for sweep.
2023-02-07 18:44:05.227 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 13 for sweep.
2023-02-07 18:44:05.227 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.001476122562773318 for sweep.
2023-02-07 18:44:05.227 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 55 for sweep.
2023-02-07 18:44:05.227 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03183301005172713 for sweep.
2023-02-07 18:44:05.227 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1839 for sweep.
2023-02-07 18:44:05.228 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 139 for sweep.
2023-02-07 18:44:05.228 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.03614871104021367 for sweep.
2023-02-07 18:44:05.228 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.005606064409336355 for sweep.
2023-02-07 18:44:05.228 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6159159290600289 for sweep.
2023-02-07 18:44:05.229 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:44:05.241 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184356-jk05aj3s/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 615, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 149, 'window': 13, 'min_count': 2, 'dm': 1, 'sample': 0.7856594792394007, 'workers': 4, 'alpha': 0.1761603743145446, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1839, 'max_depth': 55, 'num_leaves': 139, 'reg_alpha': 0.03614871104021367, 'reg_lambda': 0.005606064409336355, 'subsample': 0.6159159290600289, 'min_child_weight': 0.03183301005172713, 'n_jobs': 4, 'learning_rate': 0.001476122562773318}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 15/3257 [00:00<00:22, 147.01it/s]  1%|          | 33/3257 [00:00<00:19, 165.02it/s]  2%|‚ñè         | 50/3257 [00:00<00:20, 156.24it/s]  2%|‚ñè         | 67/3257 [00:00<00:21, 151.49it/s]  3%|‚ñé         | 87/3257 [00:00<00:18, 167.16it/s]  3%|‚ñé         | 104/3257 [00:00<00:19, 159.72it/s]  4%|‚ñé         | 121/3257 [00:00<00:24, 126.76it/s]  4%|‚ñç         | 135/3257 [00:00<00:24, 129.35it/s]  5%|‚ñç         | 154/3257 [00:01<00:21, 145.14it/s]  5%|‚ñå         | 170/3257 [00:01<00:20, 147.61it/s]  6%|‚ñå         | 186/3257 [00:01<00:20, 148.41it/s]  6%|‚ñå         | 202/3257 [00:01<00:20, 151.02it/s]  7%|‚ñã         | 225/3257 [00:01<00:17, 172.81it/s]  7%|‚ñã         | 243/3257 [00:01<00:17, 174.69it/s]  8%|‚ñä         | 261/3257 [00:01<00:18, 163.25it/s]  9%|‚ñä         | 282/3257 [00:01<00:17, 174.68it/s]  9%|‚ñâ         | 300/3257 [00:01<00:17, 167.43it/s] 10%|‚ñâ         | 317/3257 [00:02<00:17, 165.99it/s] 10%|‚ñà         | 335/3257 [00:02<00:30, 94.58it/s]  11%|‚ñà         | 349/3257 [00:02<00:28, 102.45it/s] 11%|‚ñà         | 366/3257 [00:02<00:24, 116.21it/s] 12%|‚ñà‚ñè        | 381/3257 [00:02<00:24, 119.70it/s] 12%|‚ñà‚ñè        | 396/3257 [00:02<00:22, 125.71it/s] 13%|‚ñà‚ñé        | 411/3257 [00:02<00:21, 130.66it/s] 13%|‚ñà‚ñé        | 426/3257 [00:03<00:23, 119.77it/s] 14%|‚ñà‚ñé        | 442/3257 [00:03<00:21, 128.26it/s] 14%|‚ñà‚ñç        | 459/3257 [00:03<00:20, 136.80it/s] 15%|‚ñà‚ñç        | 476/3257 [00:03<00:19, 142.45it/s] 15%|‚ñà‚ñå        | 491/3257 [00:03<00:19, 144.03it/s] 16%|‚ñà‚ñå        | 509/3257 [00:03<00:17, 153.18it/s] 16%|‚ñà‚ñå        | 525/3257 [00:03<00:18, 148.74it/s] 17%|‚ñà‚ñã        | 543/3257 [00:03<00:17, 154.05it/s] 17%|‚ñà‚ñã        | 559/3257 [00:03<00:18, 142.89it/s] 18%|‚ñà‚ñä        | 574/3257 [00:04<00:19, 136.69it/s] 18%|‚ñà‚ñä        | 589/3257 [00:04<00:19, 139.59it/s] 19%|‚ñà‚ñä        | 604/3257 [00:04<00:18, 140.40it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:04<00:18, 141.71it/s] 20%|‚ñà‚ñâ        | 639/3257 [00:04<00:17, 151.84it/s] 20%|‚ñà‚ñà        | 655/3257 [00:04<00:18, 143.44it/s] 21%|‚ñà‚ñà        | 670/3257 [00:04<00:17, 144.37it/s] 21%|‚ñà‚ñà        | 685/3257 [00:04<00:18, 139.42it/s] 21%|‚ñà‚ñà‚ñè       | 700/3257 [00:04<00:18, 141.75it/s] 22%|‚ñà‚ñà‚ñè       | 719/3257 [00:05<00:16, 153.37it/s] 23%|‚ñà‚ñà‚ñé       | 735/3257 [00:05<00:16, 148.75it/s] 23%|‚ñà‚ñà‚ñé       | 750/3257 [00:05<00:17, 142.94it/s] 24%|‚ñà‚ñà‚ñé       | 769/3257 [00:05<00:15, 155.84it/s] 24%|‚ñà‚ñà‚ñç       | 785/3257 [00:05<00:17, 143.99it/s] 25%|‚ñà‚ñà‚ñç       | 803/3257 [00:05<00:16, 153.31it/s] 25%|‚ñà‚ñà‚ñå       | 819/3257 [00:05<00:16, 148.03it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:05<00:17, 141.11it/s] 26%|‚ñà‚ñà‚ñå       | 850/3257 [00:05<00:18, 131.67it/s] 27%|‚ñà‚ñà‚ñã       | 866/3257 [00:06<00:17, 138.28it/s] 27%|‚ñà‚ñà‚ñã       | 881/3257 [00:06<00:17, 139.67it/s] 28%|‚ñà‚ñà‚ñä       | 898/3257 [00:06<00:16, 147.04it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:06<00:15, 149.63it/s] 29%|‚ñà‚ñà‚ñä       | 932/3257 [00:06<00:14, 157.05it/s] 29%|‚ñà‚ñà‚ñâ       | 949/3257 [00:06<00:14, 159.49it/s] 30%|‚ñà‚ñà‚ñâ       | 966/3257 [00:06<00:14, 162.17it/s] 30%|‚ñà‚ñà‚ñà       | 983/3257 [00:06<00:15, 151.33it/s] 31%|‚ñà‚ñà‚ñà       | 999/3257 [00:06<00:15, 146.20it/s] 31%|‚ñà‚ñà‚ñà       | 1014/3257 [00:07<00:15, 144.11it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1029/3257 [00:07<00:15, 143.99it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1044/3257 [00:07<00:15, 142.28it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1059/3257 [00:07<00:15, 142.43it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1078/3257 [00:07<00:13, 155.74it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1094/3257 [00:07<00:15, 140.91it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1111/3257 [00:07<00:14, 146.55it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1126/3257 [00:07<00:15, 138.06it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1141/3257 [00:07<00:15, 133.86it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1158/3257 [00:08<00:14, 141.38it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:08<00:14, 143.58it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1188/3257 [00:08<00:16, 127.29it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1202/3257 [00:08<00:16, 127.17it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1216/3257 [00:08<00:15, 129.09it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1235/3257 [00:08<00:13, 144.82it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1250/3257 [00:08<00:14, 139.89it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1268/3257 [00:08<00:13, 150.77it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1284/3257 [00:09<00:14, 132.41it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1299/3257 [00:09<00:14, 135.77it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1314/3257 [00:09<00:13, 138.98it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1329/3257 [00:09<00:13, 141.71it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1346/3257 [00:09<00:12, 148.62it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1362/3257 [00:09<00:12, 149.03it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1378/3257 [00:09<00:12, 147.75it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1393/3257 [00:09<00:12, 147.41it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1413/3257 [00:09<00:11, 162.17it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:09<00:10, 167.93it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1451/3257 [00:10<00:10, 173.88it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1469/3257 [00:10<00:10, 172.09it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1487/3257 [00:10<00:10, 164.69it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1507/3257 [00:10<00:10, 174.50it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1525/3257 [00:11<00:26, 66.06it/s]  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1541/3257 [00:11<00:21, 78.13it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1555/3257 [00:11<00:19, 87.38it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1572/3257 [00:11<00:16, 101.72it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1588/3257 [00:11<00:14, 113.23it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1606/3257 [00:11<00:12, 127.56it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1625/3257 [00:11<00:11, 142.16it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1642/3257 [00:11<00:11, 141.79it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:11<00:10, 148.37it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1676/3257 [00:12<00:10, 147.69it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1693/3257 [00:12<00:10, 153.57it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1713/3257 [00:12<00:09, 164.50it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1730/3257 [00:12<00:09, 154.46it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1746/3257 [00:12<00:09, 152.85it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1765/3257 [00:12<00:09, 162.04it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1785/3257 [00:12<00:08, 171.00it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1803/3257 [00:12<00:08, 171.62it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1821/3257 [00:12<00:08, 172.79it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1839/3257 [00:12<00:08, 169.09it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1857/3257 [00:13<00:08, 170.56it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1879/3257 [00:13<00:07, 178.15it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1897/3257 [00:13<00:07, 171.60it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:13<00:07, 174.24it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1936/3257 [00:13<00:07, 181.02it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1961/3257 [00:13<00:06, 197.37it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1981/3257 [00:13<00:06, 186.51it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2000/3257 [00:13<00:06, 186.51it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2019/3257 [00:13<00:06, 184.02it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2038/3257 [00:14<00:06, 185.14it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2057/3257 [00:14<00:07, 168.49it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2075/3257 [00:14<00:06, 168.88it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2093/3257 [00:14<00:06, 168.10it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2110/3257 [00:14<00:06, 165.35it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2127/3257 [00:14<00:07, 159.08it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2144/3257 [00:14<00:07, 157.13it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2162/3257 [00:14<00:06, 162.49it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:14<00:06, 165.44it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2202/3257 [00:15<00:06, 174.88it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2220/3257 [00:15<00:05, 173.05it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2238/3257 [00:15<00:05, 171.16it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2256/3257 [00:15<00:05, 170.97it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2274/3257 [00:15<00:06, 160.90it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2297/3257 [00:15<00:05, 178.52it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2316/3257 [00:15<00:05, 178.28it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2337/3257 [00:15<00:04, 186.41it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:15<00:04, 195.15it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2382/3257 [00:16<00:04, 199.76it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:16<00:04, 196.94it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2425/3257 [00:16<00:04, 195.55it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2445/3257 [00:16<00:04, 181.06it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2468/3257 [00:16<00:04, 193.33it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2489/3257 [00:16<00:03, 197.23it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2514/3257 [00:16<00:03, 208.49it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2539/3257 [00:16<00:03, 218.16it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2561/3257 [00:16<00:03, 184.73it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2581/3257 [00:17<00:04, 168.26it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2599/3257 [00:17<00:04, 163.10it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2621/3257 [00:17<00:03, 175.09it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:17<00:03, 172.64it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2658/3257 [00:17<00:03, 154.18it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2675/3257 [00:17<00:03, 157.07it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2692/3257 [00:17<00:03, 160.42it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2709/3257 [00:17<00:03, 140.98it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2726/3257 [00:18<00:03, 146.78it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2746/3257 [00:18<00:03, 158.01it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2763/3257 [00:18<00:03, 156.01it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2779/3257 [00:18<00:03, 152.79it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2798/3257 [00:18<00:02, 162.85it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2815/3257 [00:18<00:02, 147.44it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2831/3257 [00:18<00:03, 141.15it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2846/3257 [00:18<00:02, 141.84it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2862/3257 [00:18<00:02, 146.39it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:19<00:02, 156.68it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2897/3257 [00:19<00:05, 68.41it/s]  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2913/3257 [00:19<00:04, 81.53it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:19<00:03, 92.14it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2943/3257 [00:19<00:03, 98.68it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2957/3257 [00:20<00:02, 106.79it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2975/3257 [00:20<00:02, 122.55it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2990/3257 [00:20<00:02, 120.25it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3011/3257 [00:20<00:01, 140.00it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3027/3257 [00:20<00:01, 137.62it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3046/3257 [00:20<00:01, 148.84it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3065/3257 [00:20<00:01, 156.94it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3082/3257 [00:20<00:01, 151.27it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3098/3257 [00:20<00:01, 152.81it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3118/3257 [00:21<00:00, 163.62it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3135/3257 [00:21<00:00, 156.39it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3151/3257 [00:21<00:00, 149.03it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3167/3257 [00:21<00:00, 147.43it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3182/3257 [00:21<00:00, 140.16it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3200/3257 [00:21<00:00, 150.66it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3216/3257 [00:21<00:00, 143.52it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3235/3257 [00:21<00:00, 154.87it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:22<00:00, 155.62it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:22<00:00, 147.84it/s]
2023-02-07 18:44:28.214 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:44:28,217][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d149,n5,w13,mc2,s0.785659,t4>', 'datetime': '2023-02-07T18:44:28.217107', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:44:28,217][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:44:28,217][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:44:28,786][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 18:44:28,787][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:44:28,878][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 27186 unique words (85.48% of original 31803, drops 4617)', 'datetime': '2023-02-07T18:44:28.878186', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:44:28,880][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 5090501 word corpus (99.91% of original 5095118, drops 4617)', 'datetime': '2023-02-07T18:44:28.880150', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:44:28,995][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 18:44:28,997][gensim.models.word2vec][INFO] - sample=0.785659 downsamples 0 most-common words
[2023-02-07 18:44:28,997][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5090501 word corpus (100.0%% of prior 5090501)', 'datetime': '2023-02-07T18:44:28.997598', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:44:29,190][gensim.models.word2vec][INFO] - estimated required memory for 27186 words and 149 dimensions: 48591284 bytes
[2023-02-07 18:44:29,190][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:44:29,224][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 27186 vocabulary and 149 features, using sg=0 hs=0 sample=0.7856594792394007 negative=5 window=13 shrink_windows=True', 'datetime': '2023-02-07T18:44:29.224767', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:44:30,228][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 10.47% examples, 500661 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:31,232][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 31.72% examples, 812884 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:32,237][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 57.02% examples, 985966 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:33,236][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 82.16% examples, 1050120 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:33,819][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5082292 effective words) took 4.6s, 1106689 effective words/s
[2023-02-07 18:44:34,824][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 32.21% examples, 1652649 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:35,825][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 58.86% examples, 1526279 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:36,835][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 87.60% examples, 1488709 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:37,233][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5082292 effective words) took 3.4s, 1489402 effective words/s
[2023-02-07 18:44:38,241][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 26.87% examples, 1366568 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:39,252][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 53.85% examples, 1392242 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:40,265][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 81.89% examples, 1386432 words/s, in_qsize 7, out_qsize 1
[2023-02-07 18:44:40,916][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5082292 effective words) took 3.7s, 1380921 effective words/s
[2023-02-07 18:44:41,922][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 28.09% examples, 1431277 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:42,936][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 54.59% examples, 1407772 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:43,937][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 84.28% examples, 1431696 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:44,440][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5082292 effective words) took 3.5s, 1442804 effective words/s
[2023-02-07 18:44:45,452][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 33.71% examples, 1717133 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:44:46,453][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 61.01% examples, 1569568 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:44:47,453][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 89.28% examples, 1516943 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:44:47,819][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5082292 effective words) took 3.4s, 1505770 effective words/s
[2023-02-07 18:44:48,831][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 31.72% examples, 1616533 words/s, in_qsize 6, out_qsize 0
[2023-02-07 18:44:49,846][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 59.75% examples, 1528743 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:44:50,854][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 88.85% examples, 1499790 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:51,227][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5082292 effective words) took 3.4s, 1492607 effective words/s
[2023-02-07 18:44:52,232][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 33.34% examples, 1701077 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:53,238][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 67.79% examples, 1750882 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:54,243][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 99.48% examples, 1677361 words/s, in_qsize 3, out_qsize 1
[2023-02-07 18:44:54,256][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5082292 effective words) took 3.0s, 1679327 effective words/s
[2023-02-07 18:44:55,260][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 28.55% examples, 1460593 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:56,270][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 56.22% examples, 1456242 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:57,276][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 84.99% examples, 1444821 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:57,778][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5082292 effective words) took 3.5s, 1443778 effective words/s
[2023-02-07 18:44:58,787][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 25.70% examples, 1294180 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:44:59,798][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 49.25% examples, 1265862 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:45:00,809][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 75.22% examples, 1284063 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:45:01,649][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5082292 effective words) took 3.9s, 1313820 effective words/s
[2023-02-07 18:45:02,663][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 26.31% examples, 1330237 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:45:03,674][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 53.05% examples, 1366080 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:45:04,683][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 81.12% examples, 1371103 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:45:05,342][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5082292 effective words) took 3.7s, 1377281 effective words/s
[2023-02-07 18:45:06,352][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 27.82% examples, 1408698 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:45:07,354][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 56.37% examples, 1462403 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:45:08,363][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 87.53% examples, 1486403 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:45:08,763][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5082292 effective words) took 3.4s, 1486664 effective words/s
[2023-02-07 18:45:09,766][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 28.86% examples, 1472693 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:45:10,768][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 57.02% examples, 1481241 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:45:11,770][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 86.77% examples, 1481109 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:45:12,197][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5082292 effective words) took 3.4s, 1481092 effective words/s
[2023-02-07 18:45:13,201][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 28.09% examples, 1435088 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:45:14,213][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 56.22% examples, 1454598 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:45:15,215][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 85.66% examples, 1454976 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:45:15,694][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5082292 effective words) took 3.5s, 1454232 effective words/s
[2023-02-07 18:45:16,707][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 34.48% examples, 1752531 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:45:17,709][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 68.01% examples, 1755441 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:45:18,597][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5082292 effective words) took 2.9s, 1751927 effective words/s
[2023-02-07 18:45:19,602][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 25.76% examples, 1305588 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:45:20,603][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 52.93% examples, 1378292 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:45:21,603][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 82.16% examples, 1401233 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:45:22,204][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5082292 effective words) took 3.6s, 1409726 effective words/s
[2023-02-07 18:45:22,205][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (76234380 effective words) took 53.0s, 1438922 effective words/s', 'datetime': '2023-02-07T18:45:22.205523', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:45:22.205 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:45:26,382][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184356-jk05aj3s/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:45:26.382119', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:45:26,383][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:45:26,481][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184356-jk05aj3s/files/../tmp/embedding_model.pt
2023-02-07 18:45:26.482 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:45:28.374 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:45:29.030 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:45:30.431 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.5348524843957176, 'test_mae': 1.2111874947201553, 'test_r2': -4.378489280819311}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.0
wandb: percentage 0.14517
wandb:   test_mae 1.21119
wandb:   test_mse 2.53485
wandb:    test_r2 -4.37849
wandb: 
wandb: üöÄ View run frosty-sweep-17 at: https://wandb.ai/xiaoqiz/mof2vec/runs/jk05aj3s
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_184356-jk05aj3s/logs
wandb: Agent Starting Run: sr9egueh with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 64
wandb: 	model.gensim.alpha: 0.019457215486459872
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.26530974061733104
wandb: 	model.gensim.vector_size: 186
wandb: 	model.gensim.window: 14
wandb: 	model.sklearn.learning_rate: 0.003867908789158869
wandb: 	model.sklearn.max_depth: 25
wandb: 	model.sklearn.min_child_weight: 0.025581048578865763
wandb: 	model.sklearn.n_estimators: 4916
wandb: 	model.sklearn.num_leaves: 236
wandb: 	model.sklearn.reg_alpha: 0.5865220646542464
wandb: 	model.sklearn.reg_lambda: 0.03205004143604296
wandb: 	model.sklearn.subsample: 0.9537561082134108
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184542-sr9egueh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-18
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/sr9egueh
2023-02-07 18:45:50.435 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 18:45:50.435 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 64 for sweep.
2023-02-07 18:45:50.436 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.019457215486459872 for sweep.
2023-02-07 18:45:50.436 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 18:45:50.436 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 18:45:50.436 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.26530974061733104 for sweep.
2023-02-07 18:45:50.437 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 186 for sweep.
2023-02-07 18:45:50.437 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 14 for sweep.
2023-02-07 18:45:50.437 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.003867908789158869 for sweep.
2023-02-07 18:45:50.437 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 25 for sweep.
2023-02-07 18:45:50.438 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.025581048578865763 for sweep.
2023-02-07 18:45:50.438 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4916 for sweep.
2023-02-07 18:45:50.438 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 236 for sweep.
2023-02-07 18:45:50.438 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.5865220646542464 for sweep.
2023-02-07 18:45:50.439 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.03205004143604296 for sweep.
2023-02-07 18:45:50.439 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.9537561082134108 for sweep.
2023-02-07 18:45:50.439 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:45:50.446 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184542-sr9egueh/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 64, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 186, 'window': 14, 'min_count': 5, 'dm': 1, 'sample': 0.26530974061733104, 'workers': 4, 'alpha': 0.019457215486459872, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4916, 'max_depth': 25, 'num_leaves': 236, 'reg_alpha': 0.5865220646542464, 'reg_lambda': 0.03205004143604296, 'subsample': 0.9537561082134108, 'min_child_weight': 0.025581048578865763, 'n_jobs': 4, 'learning_rate': 0.003867908789158869}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 19/3257 [00:00<00:17, 189.13it/s]  1%|‚ñè         | 41/3257 [00:00<00:15, 202.59it/s]  2%|‚ñè         | 62/3257 [00:00<00:15, 201.79it/s]  3%|‚ñé         | 87/3257 [00:00<00:14, 218.70it/s]  3%|‚ñé         | 109/3257 [00:00<00:15, 199.15it/s]  4%|‚ñç         | 131/3257 [00:00<00:15, 204.10it/s]  5%|‚ñç         | 155/3257 [00:00<00:14, 214.54it/s]  5%|‚ñå         | 177/3257 [00:00<00:14, 207.08it/s]  6%|‚ñå         | 200/3257 [00:00<00:14, 213.54it/s]  7%|‚ñã         | 225/3257 [00:01<00:13, 224.03it/s]  8%|‚ñä         | 249/3257 [00:01<00:13, 228.25it/s]  8%|‚ñä         | 272/3257 [00:01<00:13, 226.50it/s]  9%|‚ñâ         | 298/3257 [00:01<00:12, 235.42it/s] 10%|‚ñâ         | 322/3257 [00:01<00:12, 231.84it/s] 11%|‚ñà         | 346/3257 [00:01<00:13, 221.36it/s] 11%|‚ñà‚ñè        | 369/3257 [00:01<00:12, 223.02it/s] 12%|‚ñà‚ñè        | 392/3257 [00:01<00:13, 211.41it/s] 13%|‚ñà‚ñé        | 417/3257 [00:01<00:12, 220.09it/s] 14%|‚ñà‚ñé        | 440/3257 [00:02<00:14, 196.58it/s] 14%|‚ñà‚ñç        | 464/3257 [00:02<00:13, 207.73it/s] 15%|‚ñà‚ñç        | 486/3257 [00:02<00:13, 202.84it/s] 16%|‚ñà‚ñå        | 509/3257 [00:02<00:13, 210.17it/s] 16%|‚ñà‚ñã        | 531/3257 [00:02<00:13, 200.52it/s] 17%|‚ñà‚ñã        | 552/3257 [00:02<00:14, 191.97it/s] 18%|‚ñà‚ñä        | 572/3257 [00:02<00:15, 172.21it/s] 18%|‚ñà‚ñä        | 594/3257 [00:02<00:14, 183.16it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:02<00:13, 190.23it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:03<00:13, 195.94it/s] 20%|‚ñà‚ñà        | 657/3257 [00:03<00:14, 185.15it/s] 21%|‚ñà‚ñà        | 679/3257 [00:03<00:13, 193.85it/s] 21%|‚ñà‚ñà‚ñè       | 699/3257 [00:03<00:13, 187.81it/s] 22%|‚ñà‚ñà‚ñè       | 720/3257 [00:03<00:13, 189.17it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:03<00:13, 181.38it/s] 23%|‚ñà‚ñà‚ñé       | 761/3257 [00:03<00:13, 189.09it/s] 24%|‚ñà‚ñà‚ñç       | 781/3257 [00:03<00:13, 181.96it/s] 25%|‚ñà‚ñà‚ñç       | 803/3257 [00:03<00:12, 190.74it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:04<00:13, 177.72it/s] 26%|‚ñà‚ñà‚ñå       | 842/3257 [00:04<00:13, 172.91it/s] 26%|‚ñà‚ñà‚ñã       | 861/3257 [00:04<00:13, 175.63it/s] 27%|‚ñà‚ñà‚ñã       | 879/3257 [00:04<00:13, 172.59it/s] 28%|‚ñà‚ñà‚ñä       | 897/3257 [00:04<00:24, 96.00it/s]  28%|‚ñà‚ñà‚ñä       | 914/3257 [00:04<00:21, 108.66it/s] 29%|‚ñà‚ñà‚ñä       | 934/3257 [00:05<00:18, 126.84it/s] 29%|‚ñà‚ñà‚ñâ       | 956/3257 [00:05<00:15, 146.96it/s] 30%|‚ñà‚ñà‚ñâ       | 975/3257 [00:05<00:14, 157.08it/s] 31%|‚ñà‚ñà‚ñà       | 994/3257 [00:05<00:14, 157.80it/s] 31%|‚ñà‚ñà‚ñà       | 1012/3257 [00:05<00:14, 159.45it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1031/3257 [00:05<00:13, 166.78it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1050/3257 [00:05<00:12, 171.48it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1072/3257 [00:05<00:11, 183.30it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:05<00:12, 173.54it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1112/3257 [00:05<00:11, 183.20it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1131/3257 [00:06<00:12, 172.02it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1149/3257 [00:06<00:12, 173.64it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:06<00:11, 182.93it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1189/3257 [00:06<00:12, 165.83it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1208/3257 [00:06<00:11, 170.95it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1232/3257 [00:06<00:10, 185.95it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1251/3257 [00:06<00:10, 183.50it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1272/3257 [00:06<00:10, 188.69it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1292/3257 [00:07<00:11, 178.62it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1312/3257 [00:07<00:10, 183.68it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1331/3257 [00:07<00:10, 184.16it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:07<00:10, 184.76it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:07<00:10, 188.12it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1389/3257 [00:07<00:09, 186.96it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1414/3257 [00:07<00:09, 204.71it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1435/3257 [00:07<00:08, 204.47it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1459/3257 [00:07<00:08, 212.85it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1481/3257 [00:07<00:08, 210.93it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1504/3257 [00:08<00:08, 216.47it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1526/3257 [00:08<00:08, 197.59it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1547/3257 [00:08<00:08, 191.47it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1569/3257 [00:08<00:08, 197.21it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1589/3257 [00:08<00:08, 195.52it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1610/3257 [00:08<00:08, 198.76it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1632/3257 [00:08<00:07, 203.15it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1653/3257 [00:08<00:08, 184.37it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1672/3257 [00:08<00:08, 183.04it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1692/3257 [00:09<00:08, 185.36it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:09<00:07, 193.24it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1735/3257 [00:09<00:08, 180.95it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1758/3257 [00:09<00:07, 192.71it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1780/3257 [00:09<00:07, 198.78it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1801/3257 [00:09<00:07, 201.30it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1822/3257 [00:09<00:07, 202.87it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1843/3257 [00:09<00:07, 194.95it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1865/3257 [00:09<00:06, 200.96it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1886/3257 [00:10<00:06, 198.39it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1909/3257 [00:10<00:06, 205.97it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1930/3257 [00:10<00:06, 196.33it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1958/3257 [00:10<00:05, 217.07it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1980/3257 [00:10<00:06, 212.50it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:10<00:05, 215.30it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2029/3257 [00:10<00:05, 221.00it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2052/3257 [00:10<00:05, 201.11it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2073/3257 [00:10<00:05, 198.65it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2094/3257 [00:11<00:05, 201.58it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2115/3257 [00:11<00:05, 201.12it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2136/3257 [00:11<00:05, 187.48it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2156/3257 [00:11<00:05, 186.59it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:11<00:05, 192.14it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2198/3257 [00:11<00:05, 191.27it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2218/3257 [00:12<00:11, 87.73it/s]  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:12<00:10, 100.97it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2253/3257 [00:12<00:08, 113.14it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2270/3257 [00:12<00:08, 122.53it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2290/3257 [00:12<00:06, 139.53it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2308/3257 [00:12<00:06, 147.08it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2331/3257 [00:12<00:05, 166.76it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2354/3257 [00:12<00:04, 182.29it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2374/3257 [00:12<00:04, 180.71it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2396/3257 [00:13<00:04, 191.08it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2416/3257 [00:13<00:04, 179.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2435/3257 [00:13<00:04, 174.62it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2453/3257 [00:13<00:04, 166.01it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2475/3257 [00:13<00:04, 179.41it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2495/3257 [00:13<00:04, 183.42it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2517/3257 [00:13<00:03, 193.31it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2539/3257 [00:13<00:03, 198.80it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2560/3257 [00:13<00:03, 188.99it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2580/3257 [00:14<00:03, 178.06it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2599/3257 [00:14<00:03, 177.18it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2623/3257 [00:14<00:03, 192.95it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2643/3257 [00:14<00:03, 190.66it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2663/3257 [00:14<00:03, 184.92it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2683/3257 [00:14<00:03, 188.02it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2702/3257 [00:14<00:03, 170.86it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2720/3257 [00:14<00:03, 172.49it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2744/3257 [00:14<00:02, 188.09it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2769/3257 [00:15<00:02, 200.61it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2802/3257 [00:15<00:01, 235.36it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2826/3257 [00:15<00:01, 230.25it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2853/3257 [00:15<00:01, 240.17it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:15<00:01, 262.68it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2913/3257 [00:15<00:01, 256.26it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2940/3257 [00:15<00:01, 259.02it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2967/3257 [00:15<00:01, 251.61it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2993/3257 [00:15<00:01, 246.66it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3022/3257 [00:16<00:00, 256.29it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3054/3257 [00:16<00:00, 274.26it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3085/3257 [00:16<00:00, 280.14it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3116/3257 [00:16<00:00, 287.69it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3145/3257 [00:16<00:00, 278.30it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3173/3257 [00:16<00:00, 270.37it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3201/3257 [00:16<00:00, 255.98it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3227/3257 [00:16<00:00, 238.28it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:16<00:00, 241.95it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 192.28it/s]
2023-02-07 18:46:08.036 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:46:08,038][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d186,n5,w14,mc5,s0.26531,t4>', 'datetime': '2023-02-07T18:46:08.038056', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:46:08,038][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:46:08,038][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:46:08,402][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 18:46:08,404][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:46:08,427][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 6948 unique words (53.20% of original 13061, drops 6113)', 'datetime': '2023-02-07T18:46:08.427779', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:46:08,428][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 3624596 word corpus (99.59% of original 3639370, drops 14774)', 'datetime': '2023-02-07T18:46:08.428255', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:46:08,456][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 18:46:08,457][gensim.models.word2vec][INFO] - sample=0.26531 downsamples 0 most-common words
[2023-02-07 18:46:08,457][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3624596 word corpus (100.0%% of prior 3624596)', 'datetime': '2023-02-07T18:46:08.457912', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:46:08,503][gensim.models.word2vec][INFO] - estimated required memory for 6948 words and 186 dimensions: 16887232 bytes
[2023-02-07 18:46:08,504][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:46:08,513][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 6948 vocabulary and 186 features, using sg=0 hs=0 sample=0.26530974061733104 negative=5 window=14 shrink_windows=True', 'datetime': '2023-02-07T18:46:08.513543', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:46:09,518][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 25.33% examples, 916295 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:10,528][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 51.12% examples, 942852 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:11,545][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 78.26% examples, 946793 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:46:12,317][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3627853 effective words) took 3.8s, 954156 effective words/s
[2023-02-07 18:46:13,323][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 27.11% examples, 986984 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:14,329][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 53.21% examples, 987309 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:15,344][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 81.03% examples, 979305 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:16,003][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3627853 effective words) took 3.7s, 985056 effective words/s
[2023-02-07 18:46:17,015][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 25.94% examples, 937628 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:18,034][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 50.84% examples, 931374 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:19,043][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 78.45% examples, 947597 words/s, in_qsize 8, out_qsize 1
[2023-02-07 18:46:19,804][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3627853 effective words) took 3.8s, 955150 effective words/s
[2023-02-07 18:46:20,815][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 27.66% examples, 998756 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:21,823][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 52.38% examples, 963454 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:22,824][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 79.95% examples, 971761 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:23,543][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3627853 effective words) took 3.7s, 970972 effective words/s
[2023-02-07 18:46:24,567][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 26.50% examples, 945534 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:25,575][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 52.13% examples, 954424 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:26,580][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 79.98% examples, 967121 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:27,278][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3627853 effective words) took 3.7s, 972565 effective words/s
[2023-02-07 18:46:28,282][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 34.94% examples, 1283819 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:46:29,283][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 63.92% examples, 1178993 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:30,293][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 91.03% examples, 1103479 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:46:30,588][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3627853 effective words) took 3.3s, 1096684 effective words/s
[2023-02-07 18:46:31,590][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 35.89% examples, 1319334 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:32,591][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 71.45% examples, 1321484 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:46:33,297][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3627853 effective words) took 2.7s, 1340346 effective words/s
[2023-02-07 18:46:34,299][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 32.85% examples, 1200949 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:35,303][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 61.25% examples, 1129192 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:36,308][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 90.02% examples, 1091889 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:36,625][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3627853 effective words) took 3.3s, 1090635 effective words/s
[2023-02-07 18:46:37,630][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 31.13% examples, 1136953 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:38,635][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 58.21% examples, 1075590 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:39,651][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 87.57% examples, 1059752 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:40,072][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3627853 effective words) took 3.4s, 1053242 effective words/s
[2023-02-07 18:46:41,082][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 27.11% examples, 980811 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:42,090][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 53.82% examples, 992996 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:43,090][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 81.98% examples, 993948 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:43,708][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3627853 effective words) took 3.6s, 998205 effective words/s
[2023-02-07 18:46:44,713][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 27.76% examples, 1014571 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:45,713][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 55.51% examples, 1032675 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:46:46,714][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 84.28% examples, 1027507 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:47,247][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3627853 effective words) took 3.5s, 1026013 effective words/s
[2023-02-07 18:46:48,250][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 30.15% examples, 1098744 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:49,257][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 57.35% examples, 1060004 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:46:50,274][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 86.15% examples, 1041678 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:50,751][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3627853 effective words) took 3.5s, 1035976 effective words/s
[2023-02-07 18:46:51,760][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 26.19% examples, 949900 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:52,761][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 52.63% examples, 972261 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:53,779][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 81.64% examples, 988000 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:54,371][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3627853 effective words) took 3.6s, 1002832 effective words/s
[2023-02-07 18:46:55,375][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 27.66% examples, 1004643 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:56,388][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 54.81% examples, 1012098 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:57,390][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 83.33% examples, 1013270 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:57,950][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3627853 effective words) took 3.6s, 1014234 effective words/s
[2023-02-07 18:46:58,969][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 27.45% examples, 982223 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:46:59,971][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 53.21% examples, 982464 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:47:00,983][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 83.11% examples, 1005564 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:47:01,561][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3627853 effective words) took 3.6s, 1005149 effective words/s
[2023-02-07 18:47:01,562][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54417795 effective words) took 53.0s, 1025812 effective words/s', 'datetime': '2023-02-07T18:47:01.562555', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:47:01.562 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:47:04,526][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184542-sr9egueh/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:47:04.526526', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:47:04,527][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:47:04,583][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184542-sr9egueh/files/../tmp/embedding_model.pt
2023-02-07 18:47:04.584 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:47:06.193 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:47:06.806 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:47:08.281 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.598077802330591, 'test_mae': 1.2129610248861342, 'test_r2': -2.9315197011008904}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.65
wandb: percentage 0.46803
wandb:   test_mae 1.21296
wandb:   test_mse 2.59808
wandb:    test_r2 -2.93152
wandb: 
wandb: üöÄ View run winter-sweep-18 at: https://wandb.ai/xiaoqiz/mof2vec/runs/sr9egueh
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_184542-sr9egueh/logs
wandb: Agent Starting Run: 5lyag3o7 with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 857
wandb: 	model.gensim.alpha: 0.025641090785641245
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 7
wandb: 	model.gensim.sample: 0.5240391051724502
wandb: 	model.gensim.vector_size: 212
wandb: 	model.gensim.window: 8
wandb: 	model.sklearn.learning_rate: 0.0007235127088208448
wandb: 	model.sklearn.max_depth: 24
wandb: 	model.sklearn.min_child_weight: 0.03031077404731729
wandb: 	model.sklearn.n_estimators: 3489
wandb: 	model.sklearn.num_leaves: 441
wandb: 	model.sklearn.reg_alpha: 0.012910110456756482
wandb: 	model.sklearn.reg_lambda: 0.0910649434349205
wandb: 	model.sklearn.subsample: 0.5186191449107358
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184718-5lyag3o7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-19
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/5lyag3o7
2023-02-07 18:47:27.277 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 18:47:27.278 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 857 for sweep.
2023-02-07 18:47:27.278 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.025641090785641245 for sweep.
2023-02-07 18:47:27.278 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 18:47:27.278 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 7 for sweep.
2023-02-07 18:47:27.279 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5240391051724502 for sweep.
2023-02-07 18:47:27.279 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 212 for sweep.
2023-02-07 18:47:27.279 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 8 for sweep.
2023-02-07 18:47:27.279 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0007235127088208448 for sweep.
2023-02-07 18:47:27.280 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 24 for sweep.
2023-02-07 18:47:27.280 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03031077404731729 for sweep.
2023-02-07 18:47:27.280 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3489 for sweep.
2023-02-07 18:47:27.280 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 441 for sweep.
2023-02-07 18:47:27.281 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.012910110456756482 for sweep.
2023-02-07 18:47:27.281 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.0910649434349205 for sweep.
2023-02-07 18:47:27.282 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5186191449107358 for sweep.
2023-02-07 18:47:27.282 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:47:27.290 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184718-5lyag3o7/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 857, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 212, 'window': 8, 'min_count': 7, 'dm': 0, 'sample': 0.5240391051724502, 'workers': 4, 'alpha': 0.025641090785641245, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3489, 'max_depth': 24, 'num_leaves': 441, 'reg_alpha': 0.012910110456756482, 'reg_lambda': 0.0910649434349205, 'subsample': 0.5186191449107358, 'min_child_weight': 0.03031077404731729, 'n_jobs': 4, 'learning_rate': 0.0007235127088208448}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 32/3257 [00:00<00:10, 314.05it/s]  2%|‚ñè         | 64/3257 [00:00<00:10, 313.58it/s]  3%|‚ñé         | 97/3257 [00:00<00:09, 317.72it/s]  4%|‚ñç         | 129/3257 [00:00<00:09, 315.30it/s]  5%|‚ñå         | 163/3257 [00:00<00:09, 320.13it/s]  6%|‚ñå         | 199/3257 [00:00<00:09, 331.86it/s]  7%|‚ñã         | 237/3257 [00:00<00:08, 342.39it/s]  8%|‚ñä         | 272/3257 [00:00<00:08, 337.40it/s] 10%|‚ñâ         | 310/3257 [00:00<00:08, 349.03it/s] 11%|‚ñà         | 345/3257 [00:01<00:08, 342.82it/s] 12%|‚ñà‚ñè        | 380/3257 [00:01<00:08, 338.01it/s] 13%|‚ñà‚ñé        | 414/3257 [00:01<00:08, 335.78it/s] 14%|‚ñà‚ñç        | 448/3257 [00:01<00:14, 195.37it/s] 15%|‚ñà‚ñç        | 480/3257 [00:01<00:12, 218.72it/s] 16%|‚ñà‚ñå        | 517/3257 [00:01<00:10, 251.18it/s] 17%|‚ñà‚ñã        | 550/3257 [00:01<00:10, 268.86it/s] 18%|‚ñà‚ñä        | 582/3257 [00:02<00:09, 270.60it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:02<00:08, 293.37it/s] 20%|‚ñà‚ñà        | 654/3257 [00:02<00:08, 303.60it/s] 21%|‚ñà‚ñà        | 687/3257 [00:02<00:08, 298.62it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:02<00:08, 308.41it/s] 23%|‚ñà‚ñà‚ñé       | 754/3257 [00:02<00:08, 308.15it/s] 24%|‚ñà‚ñà‚ñç       | 788/3257 [00:02<00:07, 316.38it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:02<00:07, 320.92it/s] 26%|‚ñà‚ñà‚ñã       | 857/3257 [00:02<00:07, 325.33it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:02<00:07, 327.82it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:03<00:06, 347.18it/s] 30%|‚ñà‚ñà‚ñâ       | 967/3257 [00:03<00:06, 349.95it/s] 31%|‚ñà‚ñà‚ñà       | 1003/3257 [00:03<00:06, 343.33it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1038/3257 [00:03<00:06, 335.96it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1072/3257 [00:03<00:06, 334.85it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1106/3257 [00:03<00:06, 335.84it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1140/3257 [00:03<00:06, 319.04it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:03<00:06, 317.97it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1205/3257 [00:03<00:07, 281.49it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1237/3257 [00:04<00:06, 289.50it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1270/3257 [00:04<00:06, 299.55it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1301/3257 [00:04<00:06, 282.62it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1336/3257 [00:04<00:06, 300.39it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:04<00:06, 306.71it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1402/3257 [00:04<00:06, 291.03it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1436/3257 [00:04<00:05, 304.15it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1467/3257 [00:04<00:05, 305.78it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1500/3257 [00:04<00:05, 312.35it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1532/3257 [00:05<00:05, 303.37it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1563/3257 [00:05<00:05, 299.20it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1594/3257 [00:05<00:05, 300.76it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1625/3257 [00:05<00:05, 302.98it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1657/3257 [00:05<00:05, 303.61it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1688/3257 [00:05<00:08, 175.25it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1719/3257 [00:05<00:07, 200.56it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1748/3257 [00:05<00:06, 218.71it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1786/3257 [00:06<00:05, 256.35it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1821/3257 [00:06<00:05, 277.69it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1854/3257 [00:06<00:04, 289.84it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1892/3257 [00:06<00:04, 311.78it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1926/3257 [00:06<00:04, 317.53it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1966/3257 [00:06<00:03, 339.46it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2002/3257 [00:06<00:03, 340.48it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2037/3257 [00:06<00:03, 342.01it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2072/3257 [00:06<00:03, 327.01it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2107/3257 [00:07<00:03, 330.84it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2141/3257 [00:07<00:03, 326.90it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2176/3257 [00:07<00:03, 332.44it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2210/3257 [00:07<00:03, 319.30it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2243/3257 [00:07<00:03, 315.84it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2277/3257 [00:07<00:03, 321.13it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2314/3257 [00:07<00:02, 335.13it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2356/3257 [00:07<00:02, 359.20it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2393/3257 [00:07<00:02, 354.86it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2429/3257 [00:07<00:02, 338.32it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2464/3257 [00:08<00:02, 329.34it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2498/3257 [00:08<00:02, 326.49it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2535/3257 [00:08<00:02, 337.55it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2569/3257 [00:08<00:02, 328.73it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2605/3257 [00:08<00:01, 334.48it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2643/3257 [00:08<00:01, 343.92it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2678/3257 [00:08<00:01, 342.55it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2713/3257 [00:08<00:01, 327.25it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2751/3257 [00:08<00:01, 339.19it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2786/3257 [00:09<00:01, 317.62it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2819/3257 [00:09<00:01, 312.35it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2855/3257 [00:09<00:01, 324.38it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2894/3257 [00:09<00:01, 340.51it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2929/3257 [00:09<00:00, 342.51it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2964/3257 [00:09<00:00, 334.80it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2998/3257 [00:09<00:00, 332.47it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3037/3257 [00:09<00:00, 346.36it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:09<00:00, 363.70it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3117/3257 [00:10<00:00, 367.37it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3154/3257 [00:10<00:00, 353.09it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3190/3257 [00:10<00:00, 348.49it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3228/3257 [00:10<00:00, 357.48it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:10<00:00, 305.39it/s]
2023-02-07 18:47:38.157 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:47:38,159][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d212,n5,mc7,s0.524039,t4>', 'datetime': '2023-02-07T18:47:38.159201', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:47:38,159][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:47:38,161][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:47:38,309][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 18:47:38,310][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:47:38,312][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 retains 528 unique words (57.14% of original 924, drops 396)', 'datetime': '2023-02-07T18:47:38.312198', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:47:38,312][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 leaves 1454417 word corpus (99.91% of original 1455748, drops 1331)', 'datetime': '2023-02-07T18:47:38.312441', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:47:38,314][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 18:47:38,314][gensim.models.word2vec][INFO] - sample=0.524039 downsamples 0 most-common words
[2023-02-07 18:47:38,314][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1454417 word corpus (100.0%% of prior 1454417)', 'datetime': '2023-02-07T18:47:38.314744', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:47:38,318][gensim.models.word2vec][INFO] - estimated required memory for 528 words and 212 dimensions: 4572824 bytes
[2023-02-07 18:47:38,318][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:47:38,325][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 528 vocabulary and 212 features, using sg=1 hs=0 sample=0.5240391051724502 negative=5 window=8 shrink_windows=True', 'datetime': '2023-02-07T18:47:38.325309', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:47:39,264][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1457674 effective words) took 0.9s, 1557807 effective words/s
[2023-02-07 18:47:40,188][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1457674 effective words) took 0.9s, 1580482 effective words/s
[2023-02-07 18:47:41,034][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1457674 effective words) took 0.8s, 1728629 effective words/s
[2023-02-07 18:47:41,830][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1457674 effective words) took 0.8s, 1838274 effective words/s
[2023-02-07 18:47:42,609][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1457674 effective words) took 0.8s, 1877507 effective words/s
[2023-02-07 18:47:43,384][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1457674 effective words) took 0.8s, 1886431 effective words/s
[2023-02-07 18:47:43,853][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1457674 effective words) took 0.5s, 3149605 effective words/s
[2023-02-07 18:47:44,744][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1457674 effective words) took 0.9s, 1639404 effective words/s
[2023-02-07 18:47:45,503][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1457674 effective words) took 0.8s, 1928021 effective words/s
[2023-02-07 18:47:46,257][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1457674 effective words) took 0.8s, 1937828 effective words/s
[2023-02-07 18:47:47,019][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1457674 effective words) took 0.8s, 1917817 effective words/s
[2023-02-07 18:47:47,474][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1457674 effective words) took 0.5s, 3211192 effective words/s
[2023-02-07 18:47:48,235][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1457674 effective words) took 0.8s, 1919653 effective words/s
[2023-02-07 18:47:48,992][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1457674 effective words) took 0.8s, 1933734 effective words/s
[2023-02-07 18:47:49,796][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1457674 effective words) took 0.8s, 1817253 effective words/s
[2023-02-07 18:47:49,797][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 21836220 raw words (21865110 effective words) took 11.5s, 1906276 effective words/s', 'datetime': '2023-02-07T18:47:49.796942', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:47:49.797 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:47:50,613][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184718-5lyag3o7/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:47:50.613467', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:47:50,614][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:47:50,628][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184718-5lyag3o7/files/../tmp/embedding_model.pt
2023-02-07 18:47:50.629 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:47:52.150 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:47:52.784 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:47:54.669 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2831666753200355, 'test_mae': 1.1263608648141765, 'test_r2': -2.154125102896266}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.72
wandb: percentage 0.42857
wandb:   test_mae 1.12636
wandb:   test_mse 2.28317
wandb:    test_r2 -2.15413
wandb: 
wandb: üöÄ View run cerulean-sweep-19 at: https://wandb.ai/xiaoqiz/mof2vec/runs/5lyag3o7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_184718-5lyag3o7/logs
wandb: Agent Starting Run: 2e4w09cc with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 558
wandb: 	model.gensim.alpha: 0.003833651692391031
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 9
wandb: 	model.gensim.sample: 0.94925555425279
wandb: 	model.gensim.vector_size: 21
wandb: 	model.gensim.window: 5
wandb: 	model.sklearn.learning_rate: 0.7698785173829422
wandb: 	model.sklearn.max_depth: 70
wandb: 	model.sklearn.min_child_weight: 0.06393101947332978
wandb: 	model.sklearn.n_estimators: 1682
wandb: 	model.sklearn.num_leaves: 280
wandb: 	model.sklearn.reg_alpha: 0.03701762150929451
wandb: 	model.sklearn.reg_lambda: 0.00830348039653822
wandb: 	model.sklearn.subsample: 0.5486329125968268
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184806-2e4w09cc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/2e4w09cc
2023-02-07 18:48:15.356 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 18:48:15.356 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 558 for sweep.
2023-02-07 18:48:15.357 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.003833651692391031 for sweep.
2023-02-07 18:48:15.357 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 18:48:15.357 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 9 for sweep.
2023-02-07 18:48:15.357 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.94925555425279 for sweep.
2023-02-07 18:48:15.358 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 21 for sweep.
2023-02-07 18:48:15.358 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 5 for sweep.
2023-02-07 18:48:15.358 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.7698785173829422 for sweep.
2023-02-07 18:48:15.358 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 70 for sweep.
2023-02-07 18:48:15.359 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.06393101947332978 for sweep.
2023-02-07 18:48:15.359 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1682 for sweep.
2023-02-07 18:48:15.359 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 280 for sweep.
2023-02-07 18:48:15.360 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.03701762150929451 for sweep.
2023-02-07 18:48:15.360 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.00830348039653822 for sweep.
2023-02-07 18:48:15.361 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5486329125968268 for sweep.
2023-02-07 18:48:15.361 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:48:15.370 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184806-2e4w09cc/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 558, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 21, 'window': 5, 'min_count': 9, 'dm': 1, 'sample': 0.94925555425279, 'workers': 4, 'alpha': 0.003833651692391031, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1682, 'max_depth': 70, 'num_leaves': 280, 'reg_alpha': 0.03701762150929451, 'reg_lambda': 0.00830348039653822, 'subsample': 0.5486329125968268, 'min_child_weight': 0.06393101947332978, 'n_jobs': 4, 'learning_rate': 0.7698785173829422}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 19/3257 [00:00<00:17, 186.36it/s]  1%|‚ñè         | 41/3257 [00:00<00:16, 194.60it/s]  2%|‚ñè         | 61/3257 [00:00<00:16, 194.63it/s]  2%|‚ñè         | 81/3257 [00:00<00:16, 192.23it/s]  3%|‚ñé         | 101/3257 [00:00<00:17, 181.80it/s]  4%|‚ñé         | 121/3257 [00:00<00:17, 184.06it/s]  4%|‚ñç         | 146/3257 [00:00<00:15, 202.39it/s]  5%|‚ñå         | 167/3257 [00:00<00:15, 194.14it/s]  6%|‚ñå         | 187/3257 [00:00<00:16, 183.85it/s]  6%|‚ñã         | 209/3257 [00:01<00:15, 192.79it/s]  7%|‚ñã         | 236/3257 [00:01<00:14, 211.59it/s]  8%|‚ñä         | 258/3257 [00:01<00:14, 213.76it/s]  9%|‚ñâ         | 287/3257 [00:01<00:12, 233.21it/s] 10%|‚ñâ         | 312/3257 [00:01<00:12, 236.96it/s] 10%|‚ñà         | 338/3257 [00:01<00:12, 241.14it/s] 11%|‚ñà         | 363/3257 [00:01<00:12, 228.77it/s] 12%|‚ñà‚ñè        | 387/3257 [00:01<00:12, 223.98it/s] 13%|‚ñà‚ñé        | 415/3257 [00:01<00:11, 237.99it/s] 13%|‚ñà‚ñé        | 439/3257 [00:02<00:12, 216.79it/s] 14%|‚ñà‚ñç        | 467/3257 [00:02<00:12, 230.07it/s] 15%|‚ñà‚ñå        | 493/3257 [00:02<00:11, 235.85it/s] 16%|‚ñà‚ñå        | 520/3257 [00:02<00:11, 244.79it/s] 17%|‚ñà‚ñã        | 546/3257 [00:02<00:11, 241.10it/s] 18%|‚ñà‚ñä        | 571/3257 [00:02<00:12, 221.61it/s] 18%|‚ñà‚ñä        | 594/3257 [00:02<00:12, 220.43it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:02<00:11, 226.15it/s] 20%|‚ñà‚ñâ        | 645/3257 [00:02<00:11, 229.23it/s] 21%|‚ñà‚ñà        | 669/3257 [00:03<00:11, 225.36it/s] 21%|‚ñà‚ñà‚ñè       | 693/3257 [00:03<00:11, 228.35it/s] 22%|‚ñà‚ñà‚ñè       | 719/3257 [00:03<00:10, 232.90it/s] 23%|‚ñà‚ñà‚ñé       | 743/3257 [00:03<00:11, 225.00it/s] 24%|‚ñà‚ñà‚ñé       | 768/3257 [00:03<00:10, 228.71it/s] 24%|‚ñà‚ñà‚ñç       | 791/3257 [00:03<00:10, 227.71it/s] 25%|‚ñà‚ñà‚ñç       | 814/3257 [00:03<00:10, 223.40it/s] 26%|‚ñà‚ñà‚ñå       | 837/3257 [00:03<00:11, 202.25it/s] 26%|‚ñà‚ñà‚ñã       | 858/3257 [00:03<00:12, 193.83it/s] 27%|‚ñà‚ñà‚ñã       | 878/3257 [00:04<00:13, 170.06it/s] 28%|‚ñà‚ñà‚ñä       | 896/3257 [00:04<00:14, 160.24it/s] 28%|‚ñà‚ñà‚ñä       | 916/3257 [00:04<00:13, 169.02it/s] 29%|‚ñà‚ñà‚ñâ       | 940/3257 [00:04<00:12, 186.81it/s] 30%|‚ñà‚ñà‚ñâ       | 963/3257 [00:04<00:11, 197.04it/s] 30%|‚ñà‚ñà‚ñà       | 984/3257 [00:04<00:11, 196.20it/s] 31%|‚ñà‚ñà‚ñà       | 1004/3257 [00:04<00:13, 172.97it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1022/3257 [00:04<00:13, 166.42it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1040/3257 [00:05<00:13, 166.20it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1059/3257 [00:05<00:12, 172.16it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1077/3257 [00:05<00:19, 112.96it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1094/3257 [00:05<00:17, 124.36it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:05<00:14, 142.67it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1134/3257 [00:05<00:14, 142.74it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1153/3257 [00:05<00:13, 153.17it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1175/3257 [00:05<00:12, 170.16it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1194/3257 [00:06<00:12, 169.70it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1213/3257 [00:06<00:11, 173.28it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1239/3257 [00:06<00:10, 196.48it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1260/3257 [00:06<00:10, 185.28it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1280/3257 [00:06<00:11, 177.57it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1299/3257 [00:06<00:10, 178.43it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1321/3257 [00:06<00:10, 187.67it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1344/3257 [00:06<00:09, 196.36it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1364/3257 [00:06<00:09, 189.95it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:07<00:10, 184.78it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1408/3257 [00:07<00:09, 199.94it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:07<00:08, 207.42it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1453/3257 [00:07<00:08, 201.86it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1474/3257 [00:07<00:08, 200.43it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1497/3257 [00:07<00:08, 208.20it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1518/3257 [00:07<00:08, 205.19it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1539/3257 [00:07<00:09, 178.79it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1558/3257 [00:08<00:09, 172.77it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1578/3257 [00:08<00:09, 177.80it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1601/3257 [00:08<00:08, 191.17it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1623/3257 [00:08<00:08, 198.00it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1644/3257 [00:08<00:08, 199.11it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1665/3257 [00:08<00:08, 191.62it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1688/3257 [00:08<00:07, 201.37it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1709/3257 [00:08<00:07, 197.61it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1729/3257 [00:08<00:07, 193.53it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1749/3257 [00:08<00:08, 177.99it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1768/3257 [00:09<00:08, 166.87it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1792/3257 [00:09<00:07, 185.21it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1811/3257 [00:09<00:07, 186.14it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1830/3257 [00:09<00:08, 174.47it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1848/3257 [00:09<00:08, 171.21it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1873/3257 [00:09<00:07, 191.43it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1894/3257 [00:09<00:06, 195.46it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:09<00:06, 201.06it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1938/3257 [00:09<00:06, 205.43it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:10<00:05, 218.87it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1988/3257 [00:10<00:05, 220.26it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2012/3257 [00:10<00:05, 223.59it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2035/3257 [00:10<00:05, 213.29it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2057/3257 [00:10<00:06, 195.24it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2082/3257 [00:10<00:05, 207.09it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2105/3257 [00:10<00:05, 212.64it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2127/3257 [00:10<00:05, 211.81it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2149/3257 [00:10<00:05, 201.81it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2172/3257 [00:11<00:05, 207.58it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2194/3257 [00:11<00:05, 210.44it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2216/3257 [00:11<00:04, 209.29it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2238/3257 [00:11<00:04, 211.52it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2260/3257 [00:11<00:04, 213.20it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2282/3257 [00:11<00:04, 202.43it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2303/3257 [00:11<00:04, 199.18it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2328/3257 [00:11<00:04, 213.31it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2359/3257 [00:11<00:03, 240.75it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2384/3257 [00:12<00:03, 239.07it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2409/3257 [00:12<00:03, 234.78it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2433/3257 [00:12<00:06, 129.38it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2457/3257 [00:12<00:05, 149.34it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2483/3257 [00:12<00:04, 170.95it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2511/3257 [00:12<00:03, 195.57it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2539/3257 [00:12<00:03, 215.29it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2564/3257 [00:13<00:03, 215.71it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2588/3257 [00:13<00:03, 213.16it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2619/3257 [00:13<00:02, 236.93it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2645/3257 [00:13<00:02, 237.33it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:13<00:02, 227.14it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2694/3257 [00:13<00:02, 214.67it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2717/3257 [00:13<00:02, 200.90it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2748/3257 [00:13<00:02, 228.87it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2772/3257 [00:13<00:02, 223.42it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2801/3257 [00:14<00:01, 240.31it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2826/3257 [00:14<00:01, 220.45it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2851/3257 [00:14<00:01, 225.47it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2885/3257 [00:14<00:01, 256.23it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2912/3257 [00:14<00:01, 243.10it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2937/3257 [00:14<00:01, 241.59it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2962/3257 [00:14<00:01, 221.47it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:14<00:01, 214.25it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:15<00:01, 225.80it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3040/3257 [00:15<00:00, 240.24it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3070/3257 [00:15<00:00, 254.68it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3096/3257 [00:15<00:00, 251.87it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3122/3257 [00:15<00:00, 249.49it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3148/3257 [00:15<00:00, 227.03it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3174/3257 [00:15<00:00, 235.34it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3198/3257 [00:15<00:00, 234.69it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3222/3257 [00:15<00:00, 232.44it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3251/3257 [00:15<00:00, 247.87it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 203.36it/s]
2023-02-07 18:48:31.928 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:48:31,929][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d21,n5,w5,mc9,s0.949256,t4>', 'datetime': '2023-02-07T18:48:31.929673', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:48:31,930][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:48:31,930][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:48:32,274][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 18:48:32,274][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:48:32,292][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=9 retains 5109 unique words (39.12% of original 13061, drops 7952)', 'datetime': '2023-02-07T18:48:32.292573', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:48:32,293][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=9 leaves 3612004 word corpus (99.25% of original 3639370, drops 27366)', 'datetime': '2023-02-07T18:48:32.293051', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:48:32,312][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 18:48:32,312][gensim.models.word2vec][INFO] - sample=0.949256 downsamples 0 most-common words
[2023-02-07 18:48:32,313][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3612004 word corpus (100.0%% of prior 3612004)', 'datetime': '2023-02-07T18:48:32.313001', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:48:32,346][gensim.models.word2vec][INFO] - estimated required memory for 5109 words and 21 dimensions: 4337800 bytes
[2023-02-07 18:48:32,346][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:48:32,348][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 5109 vocabulary and 21 features, using sg=0 hs=0 sample=0.94925555425279 negative=5 window=5 shrink_windows=True', 'datetime': '2023-02-07T18:48:32.348101', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:48:33,350][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 59.20% examples, 2189150 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:48:33,957][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3615261 effective words) took 1.6s, 2248033 effective words/s
[2023-02-07 18:48:34,961][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 76.94% examples, 2808410 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:48:35,260][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3615261 effective words) took 1.3s, 2781248 effective words/s
[2023-02-07 18:48:36,263][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 75.68% examples, 2771992 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:48:36,567][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3615261 effective words) took 1.3s, 2768874 effective words/s
[2023-02-07 18:48:37,575][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 78.88% examples, 2869201 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:48:37,825][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3615261 effective words) took 1.3s, 2877881 effective words/s
[2023-02-07 18:48:38,833][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 78.08% examples, 2842839 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:48:39,105][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3615261 effective words) took 1.3s, 2835284 effective words/s
[2023-02-07 18:48:40,108][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 74.06% examples, 2719177 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:48:40,513][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3615261 effective words) took 1.4s, 2570644 effective words/s
[2023-02-07 18:48:41,517][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 30.43% examples, 1103403 words/s, in_qsize 8, out_qsize 2
[2023-02-07 18:48:42,522][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 83.60% examples, 1522983 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:48:42,857][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3615261 effective words) took 2.3s, 1543651 effective words/s
[2023-02-07 18:48:43,859][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 64.66% examples, 2380770 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:48:44,360][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3615261 effective words) took 1.5s, 2408377 effective words/s
[2023-02-07 18:48:45,362][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 65.34% examples, 2409164 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:48:45,876][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3615261 effective words) took 1.5s, 2387590 effective words/s
[2023-02-07 18:48:46,879][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 61.34% examples, 2251808 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:48:47,440][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3615261 effective words) took 1.6s, 2315054 effective words/s
[2023-02-07 18:48:48,443][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 79.00% examples, 2877863 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:48:48,729][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3615261 effective words) took 1.3s, 2806892 effective words/s
[2023-02-07 18:48:49,733][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 81.98% examples, 2981533 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:48:49,941][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3615261 effective words) took 1.2s, 2987841 effective words/s
[2023-02-07 18:48:50,943][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 63.37% examples, 2332413 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:48:51,635][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3615261 effective words) took 1.7s, 2136071 effective words/s
[2023-02-07 18:48:52,642][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 65.34% examples, 2404860 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:48:53,145][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3615261 effective words) took 1.5s, 2400434 effective words/s
[2023-02-07 18:48:54,147][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 65.77% examples, 2426112 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:48:54,548][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3615261 effective words) took 1.4s, 2578734 effective words/s
[2023-02-07 18:48:54,549][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54228915 effective words) took 22.2s, 2442659 effective words/s', 'datetime': '2023-02-07T18:48:54.549215', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:48:54.549 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:48:56,251][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184806-2e4w09cc/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:48:56.251208', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:48:56,252][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:48:56,258][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184806-2e4w09cc/files/../tmp/embedding_model.pt
2023-02-07 18:48:56.259 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:48:57.213 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:48:57.588 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:49:02.349 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.470560987235629, 'test_mae': 1.1576213891249425, 'test_r2': -3.437283747207247}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.54
wandb: percentage 0.60884
wandb:   test_mae 1.15762
wandb:   test_mse 2.47056
wandb:    test_r2 -3.43728
wandb: 
wandb: üöÄ View run charmed-sweep-20 at: https://wandb.ai/xiaoqiz/mof2vec/runs/2e4w09cc
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_184806-2e4w09cc/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: qzbaeofk with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 663
wandb: 	model.gensim.alpha: 0.07063468715087132
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 10
wandb: 	model.gensim.sample: 0.9627642517609818
wandb: 	model.gensim.vector_size: 423
wandb: 	model.gensim.window: 18
wandb: 	model.sklearn.learning_rate: 0.0020052824923944856
wandb: 	model.sklearn.max_depth: 47
wandb: 	model.sklearn.min_child_weight: 0.05382436513939201
wandb: 	model.sklearn.n_estimators: 2611
wandb: 	model.sklearn.num_leaves: 202
wandb: 	model.sklearn.reg_alpha: 0.16153796581347826
wandb: 	model.sklearn.reg_lambda: 0.010343880814607447
wandb: 	model.sklearn.subsample: 0.6015300588923433
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184920-qzbaeofk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-21
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/qzbaeofk
2023-02-07 18:49:30.540 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 18:49:30.540 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 663 for sweep.
2023-02-07 18:49:30.541 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.07063468715087132 for sweep.
2023-02-07 18:49:30.541 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 18:49:30.542 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 10 for sweep.
2023-02-07 18:49:30.542 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.9627642517609818 for sweep.
2023-02-07 18:49:30.543 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 423 for sweep.
2023-02-07 18:49:30.543 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 18 for sweep.
2023-02-07 18:49:30.543 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0020052824923944856 for sweep.
2023-02-07 18:49:30.543 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 47 for sweep.
2023-02-07 18:49:30.544 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05382436513939201 for sweep.
2023-02-07 18:49:30.544 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2611 for sweep.
2023-02-07 18:49:30.544 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 202 for sweep.
2023-02-07 18:49:30.544 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.16153796581347826 for sweep.
2023-02-07 18:49:30.545 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.010343880814607447 for sweep.
2023-02-07 18:49:30.545 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6015300588923433 for sweep.
2023-02-07 18:49:30.545 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:49:30.551 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184920-qzbaeofk/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 663, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 423, 'window': 18, 'min_count': 10, 'dm': 1, 'sample': 0.9627642517609818, 'workers': 4, 'alpha': 0.07063468715087132, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2611, 'max_depth': 47, 'num_leaves': 202, 'reg_alpha': 0.16153796581347826, 'reg_lambda': 0.010343880814607447, 'subsample': 0.6015300588923433, 'min_child_weight': 0.05382436513939201, 'n_jobs': 4, 'learning_rate': 0.0020052824923944856}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 21/3257 [00:00<00:15, 206.13it/s]  1%|‚ñè         | 42/3257 [00:00<00:15, 203.06it/s]  2%|‚ñè         | 67/3257 [00:00<00:15, 209.00it/s]  3%|‚ñé         | 91/3257 [00:00<00:14, 218.83it/s]  3%|‚ñé         | 113/3257 [00:00<00:15, 204.74it/s]  4%|‚ñç         | 134/3257 [00:00<00:15, 196.73it/s]  5%|‚ñç         | 154/3257 [00:00<00:16, 192.47it/s]  5%|‚ñå         | 177/3257 [00:00<00:15, 202.74it/s]  6%|‚ñã         | 208/3257 [00:00<00:13, 234.06it/s]  7%|‚ñã         | 244/3257 [00:01<00:11, 269.97it/s]  9%|‚ñä         | 277/3257 [00:01<00:10, 287.06it/s]  9%|‚ñâ         | 309/3257 [00:01<00:09, 295.15it/s] 10%|‚ñà         | 339/3257 [00:01<00:10, 285.74it/s] 11%|‚ñà‚ñè        | 368/3257 [00:01<00:10, 286.44it/s] 12%|‚ñà‚ñè        | 398/3257 [00:01<00:09, 287.25it/s] 13%|‚ñà‚ñé        | 427/3257 [00:01<00:10, 273.99it/s] 14%|‚ñà‚ñç        | 455/3257 [00:01<00:10, 268.61it/s] 15%|‚ñà‚ñç        | 487/3257 [00:01<00:09, 281.63it/s] 16%|‚ñà‚ñå        | 522/3257 [00:02<00:09, 296.58it/s] 17%|‚ñà‚ñã        | 552/3257 [00:02<00:13, 195.78it/s] 18%|‚ñà‚ñä        | 577/3257 [00:02<00:13, 197.44it/s] 19%|‚ñà‚ñä        | 607/3257 [00:02<00:12, 220.09it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:02<00:11, 235.39it/s] 20%|‚ñà‚ñà        | 663/3257 [00:02<00:11, 223.93it/s] 21%|‚ñà‚ñà        | 689/3257 [00:02<00:11, 232.10it/s] 22%|‚ñà‚ñà‚ñè       | 717/3257 [00:02<00:10, 243.14it/s] 23%|‚ñà‚ñà‚ñé       | 743/3257 [00:03<00:11, 224.37it/s] 24%|‚ñà‚ñà‚ñé       | 772/3257 [00:03<00:10, 238.68it/s] 25%|‚ñà‚ñà‚ñç       | 799/3257 [00:03<00:10, 243.20it/s] 25%|‚ñà‚ñà‚ñå       | 824/3257 [00:03<00:10, 240.93it/s] 26%|‚ñà‚ñà‚ñå       | 849/3257 [00:03<00:10, 232.46it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:03<00:09, 239.59it/s] 28%|‚ñà‚ñà‚ñä       | 900/3257 [00:03<00:09, 240.66it/s] 28%|‚ñà‚ñà‚ñä       | 925/3257 [00:03<00:09, 241.74it/s] 29%|‚ñà‚ñà‚ñâ       | 951/3257 [00:03<00:09, 245.96it/s] 30%|‚ñà‚ñà‚ñà       | 978/3257 [00:04<00:09, 251.19it/s] 31%|‚ñà‚ñà‚ñà       | 1004/3257 [00:04<00:09, 247.40it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1029/3257 [00:04<00:09, 241.74it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1054/3257 [00:04<00:09, 237.95it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:04<00:09, 237.99it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1107/3257 [00:04<00:08, 243.88it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1132/3257 [00:04<00:09, 234.25it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1158/3257 [00:04<00:08, 238.70it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1182/3257 [00:04<00:08, 235.11it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1206/3257 [00:05<00:09, 222.19it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1235/3257 [00:05<00:08, 239.26it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1260/3257 [00:05<00:08, 240.76it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1285/3257 [00:05<00:08, 232.40it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1313/3257 [00:05<00:08, 242.74it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1342/3257 [00:05<00:07, 254.69it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1368/3257 [00:05<00:07, 250.39it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1394/3257 [00:05<00:07, 243.51it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:05<00:07, 252.67it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1453/3257 [00:06<00:06, 263.55it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1484/3257 [00:06<00:06, 275.68it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1518/3257 [00:06<00:05, 293.20it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1548/3257 [00:06<00:07, 238.01it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1574/3257 [00:06<00:07, 224.66it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1598/3257 [00:06<00:07, 214.06it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1621/3257 [00:06<00:07, 213.78it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1643/3257 [00:06<00:07, 210.36it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1665/3257 [00:06<00:07, 200.26it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1686/3257 [00:07<00:07, 198.22it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1707/3257 [00:07<00:07, 201.17it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:07<00:13, 116.30it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1747/3257 [00:07<00:11, 129.37it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1768/3257 [00:07<00:10, 145.07it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1792/3257 [00:07<00:08, 164.12it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1812/3257 [00:08<00:08, 164.47it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1831/3257 [00:08<00:08, 168.80it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1854/3257 [00:08<00:07, 184.12it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1877/3257 [00:08<00:07, 195.71it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1898/3257 [00:08<00:07, 192.57it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1919/3257 [00:08<00:06, 195.42it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1946/3257 [00:08<00:06, 216.28it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1972/3257 [00:08<00:05, 228.62it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1996/3257 [00:08<00:05, 218.02it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2019/3257 [00:08<00:05, 213.07it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2041/3257 [00:09<00:06, 201.86it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2062/3257 [00:09<00:06, 192.26it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2083/3257 [00:09<00:06, 194.67it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2103/3257 [00:09<00:06, 190.80it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2123/3257 [00:09<00:06, 186.09it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2143/3257 [00:09<00:05, 189.46it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2167/3257 [00:09<00:05, 202.54it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2188/3257 [00:09<00:05, 190.70it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2208/3257 [00:09<00:05, 191.99it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2228/3257 [00:10<00:05, 186.76it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2248/3257 [00:10<00:05, 188.40it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2269/3257 [00:10<00:05, 193.57it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2290/3257 [00:10<00:04, 196.88it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2310/3257 [00:10<00:04, 192.71it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2337/3257 [00:10<00:04, 213.78it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2366/3257 [00:10<00:03, 234.98it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2400/3257 [00:10<00:03, 264.84it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2428/3257 [00:10<00:03, 268.59it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2455/3257 [00:11<00:03, 265.81it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2491/3257 [00:11<00:02, 292.69it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2527/3257 [00:11<00:02, 310.83it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2559/3257 [00:11<00:02, 310.19it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2591/3257 [00:11<00:02, 298.45it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:11<00:01, 326.10it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2664/3257 [00:11<00:01, 316.08it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2696/3257 [00:11<00:01, 311.86it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2728/3257 [00:11<00:01, 307.52it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2764/3257 [00:11<00:01, 321.31it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2797/3257 [00:12<00:01, 314.66it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2829/3257 [00:12<00:01, 292.19it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2862/3257 [00:12<00:01, 300.89it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2893/3257 [00:12<00:01, 291.31it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2923/3257 [00:12<00:01, 289.90it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2953/3257 [00:12<00:01, 268.14it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2981/3257 [00:12<00:01, 263.70it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:12<00:00, 274.45it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3041/3257 [00:12<00:00, 278.49it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3073/3257 [00:13<00:00, 290.00it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3103/3257 [00:13<00:00, 155.43it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3132/3257 [00:13<00:00, 178.71it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3158/3257 [00:13<00:00, 194.70it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3183/3257 [00:13<00:00, 205.77it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3212/3257 [00:13<00:00, 224.70it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3242/3257 [00:13<00:00, 243.43it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:14<00:00, 231.88it/s]
2023-02-07 18:49:44.988 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:49:44,990][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d423,n5,w18,mc10,s0.962764,t4>', 'datetime': '2023-02-07T18:49:44.990286', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:49:44,990][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:49:44,991][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:49:45,261][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 18:49:45,262][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:49:45,270][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=10 retains 2697 unique words (40.48% of original 6662, drops 3965)', 'datetime': '2023-02-07T18:49:45.270596', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:49:45,270][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 2897377 word corpus (99.52% of original 2911496, drops 14119)', 'datetime': '2023-02-07T18:49:45.270975', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:49:45,280][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 18:49:45,281][gensim.models.word2vec][INFO] - sample=0.962764 downsamples 0 most-common words
[2023-02-07 18:49:45,282][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2897377 word corpus (100.0%% of prior 2897377)', 'datetime': '2023-02-07T18:49:45.282251', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:49:45,301][gensim.models.word2vec][INFO] - estimated required memory for 2697 words and 423 dimensions: 16637392 bytes
[2023-02-07 18:49:45,301][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:49:45,314][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2697 vocabulary and 423 features, using sg=0 hs=0 sample=0.9627642517609818 negative=5 window=18 shrink_windows=True', 'datetime': '2023-02-07T18:49:45.314728', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:49:46,330][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 13.69% examples, 381842 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:49:47,336][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 26.83% examples, 387230 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:49:48,350][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 40.77% examples, 401340 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:49:49,383][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 55.57% examples, 406805 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:49:50,395][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 70.53% examples, 411676 words/s, in_qsize 8, out_qsize 1
[2023-02-07 18:49:51,408][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 88.03% examples, 421839 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:49:52,122][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2900634 effective words) took 6.8s, 426225 effective words/s
[2023-02-07 18:49:53,147][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 15.11% examples, 414194 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:49:54,173][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 31.10% examples, 444026 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:49:55,184][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 46.76% examples, 451957 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:49:56,210][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 61.77% examples, 446156 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:49:57,224][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 75.90% examples, 437667 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:49:58,245][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 90.73% examples, 433190 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:49:58,799][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2900634 effective words) took 6.7s, 434658 effective words/s
[2023-02-07 18:49:59,802][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 24.50% examples, 708796 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:00,819][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 48.60% examples, 712754 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:01,819][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 64.45% examples, 631720 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:02,820][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 80.93% examples, 588520 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:03,830][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 96.84% examples, 558665 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:03,992][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2900634 effective words) took 5.2s, 558810 effective words/s
[2023-02-07 18:50:05,008][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 13.08% examples, 364116 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:06,034][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 28.52% examples, 406355 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:07,082][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 42.52% examples, 411882 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:08,090][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 60.27% examples, 435848 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:09,115][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 78.69% examples, 450677 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:10,115][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 95.15% examples, 451468 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:10,407][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2900634 effective words) took 6.4s, 452281 effective words/s
[2023-02-07 18:50:11,430][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 15.81% examples, 433960 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:12,431][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 30.89% examples, 445279 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:13,447][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 45.84% examples, 445943 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:14,451][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 61.22% examples, 446820 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:15,456][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 76.17% examples, 444198 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:16,467][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 94.69% examples, 454695 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:16,653][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2900634 effective words) took 6.2s, 464637 effective words/s
[2023-02-07 18:50:17,657][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 24.50% examples, 708505 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:18,680][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 41.66% examples, 614126 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:19,708][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 57.05% examples, 554771 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:20,719][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 72.52% examples, 526276 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:21,736][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 97.61% examples, 558784 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:21,803][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2900634 effective words) took 5.1s, 563566 effective words/s
[2023-02-07 18:50:22,812][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 15.38% examples, 429469 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:23,822][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 30.09% examples, 434011 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:24,834][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 50.29% examples, 491587 words/s, in_qsize 8, out_qsize 1
[2023-02-07 18:50:25,835][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 65.77% examples, 483620 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:26,839][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 83.51% examples, 486799 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:27,723][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2900634 effective words) took 5.9s, 490116 effective words/s
[2023-02-07 18:50:28,759][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 17.50% examples, 478904 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:29,769][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 33.34% examples, 478349 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:30,771][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 49.55% examples, 481571 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:31,819][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 65.77% examples, 476343 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:32,822][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 84.65% examples, 486622 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:33,635][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2900634 effective words) took 5.9s, 490873 effective words/s
[2023-02-07 18:50:34,643][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 16.79% examples, 466287 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:35,645][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 32.42% examples, 473015 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:36,646][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 48.60% examples, 477964 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:37,683][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 65.40% examples, 477364 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:38,698][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 83.94% examples, 486278 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:39,537][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2900634 effective words) took 5.9s, 491685 effective words/s
[2023-02-07 18:50:40,563][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 18.30% examples, 501796 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:41,581][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 34.48% examples, 497195 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:42,604][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 50.84% examples, 491279 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:43,625][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 76.17% examples, 548716 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:44,528][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2900634 effective words) took 5.0s, 581535 effective words/s
[2023-02-07 18:50:45,542][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 26.22% examples, 755961 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:46,587][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 41.42% examples, 600679 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:47,605][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 57.35% examples, 553413 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:48,628][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 75.10% examples, 539916 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:49,657][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 95.03% examples, 539042 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:49,925][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2900634 effective words) took 5.4s, 537667 effective words/s
[2023-02-07 18:50:50,951][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 17.35% examples, 474084 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:51,953][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 32.91% examples, 477982 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:52,969][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 49.22% examples, 478731 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:53,991][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 66.10% examples, 481876 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:55,010][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 86.83% examples, 500021 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:50:55,478][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2900634 effective words) took 5.6s, 522528 effective words/s
[2023-02-07 18:50:56,498][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 17.50% examples, 486520 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:57,501][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 32.51% examples, 472714 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:58,502][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 47.84% examples, 469579 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:50:59,523][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 63.89% examples, 467121 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:51:00,532][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 87.23% examples, 504962 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:51:00,992][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2900634 effective words) took 5.5s, 526353 effective words/s
[2023-02-07 18:51:02,017][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 16.79% examples, 458050 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:51:03,032][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 33.68% examples, 484382 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:51:04,037][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 50.84% examples, 494761 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:51:05,079][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 68.22% examples, 495109 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:51:06,099][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 85.32% examples, 489557 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:51:06,934][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2900634 effective words) took 5.9s, 488313 effective words/s
[2023-02-07 18:51:07,942][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 19.28% examples, 549604 words/s, in_qsize 7, out_qsize 1
[2023-02-07 18:51:08,948][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 34.73% examples, 509433 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:51:09,954][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 50.84% examples, 498926 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:51:10,965][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 66.96% examples, 492994 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:51:11,972][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 83.51% examples, 486717 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:51:12,939][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2900634 effective words) took 6.0s, 483322 effective words/s
[2023-02-07 18:51:12,941][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 43672440 raw words (43509510 effective words) took 87.6s, 496538 effective words/s', 'datetime': '2023-02-07T18:51:12.941005', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:51:12.941 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:51:15,583][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184920-qzbaeofk/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:51:15.583590', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:51:15,584][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:51:15,624][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_184920-qzbaeofk/files/../tmp/embedding_model.pt
2023-02-07 18:51:15.625 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:51:18.049 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:51:18.947 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:51:21.860 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.9247852977998416, 'test_mae': 1.3199371716273274, 'test_r2': -5.225434996999469}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.01
wandb: percentage 0.59517
wandb:   test_mae 1.31994
wandb:   test_mse 2.92479
wandb:    test_r2 -5.22543
wandb: 
wandb: üöÄ View run rich-sweep-21 at: https://wandb.ai/xiaoqiz/mof2vec/runs/qzbaeofk
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_184920-qzbaeofk/logs
wandb: Agent Starting Run: rdgq1luv with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 860
wandb: 	model.gensim.alpha: 0.003062938473440792
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.3497257583290872
wandb: 	model.gensim.vector_size: 238
wandb: 	model.gensim.window: 16
wandb: 	model.sklearn.learning_rate: 0.045294611352848194
wandb: 	model.sklearn.max_depth: 40
wandb: 	model.sklearn.min_child_weight: 0.039915891567628585
wandb: 	model.sklearn.n_estimators: 18
wandb: 	model.sklearn.num_leaves: 479
wandb: 	model.sklearn.reg_alpha: 0.1640157842624526
wandb: 	model.sklearn.reg_lambda: 0.10191576124052852
wandb: 	model.sklearn.subsample: 0.391450811637736
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185132-rdgq1luv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-22
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/rdgq1luv
2023-02-07 18:51:40.692 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 18:51:40.692 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 860 for sweep.
2023-02-07 18:51:40.693 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.003062938473440792 for sweep.
2023-02-07 18:51:40.693 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 18:51:40.694 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 18:51:40.694 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3497257583290872 for sweep.
2023-02-07 18:51:40.694 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 238 for sweep.
2023-02-07 18:51:40.694 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 16 for sweep.
2023-02-07 18:51:40.695 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.045294611352848194 for sweep.
2023-02-07 18:51:40.695 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 40 for sweep.
2023-02-07 18:51:40.695 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.039915891567628585 for sweep.
2023-02-07 18:51:40.695 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 18 for sweep.
2023-02-07 18:51:40.695 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 479 for sweep.
2023-02-07 18:51:40.696 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.1640157842624526 for sweep.
2023-02-07 18:51:40.696 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.10191576124052852 for sweep.
2023-02-07 18:51:40.696 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.391450811637736 for sweep.
2023-02-07 18:51:40.696 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:51:40.705 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185132-rdgq1luv/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 860, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 238, 'window': 16, 'min_count': 6, 'dm': 0, 'sample': 0.3497257583290872, 'workers': 4, 'alpha': 0.003062938473440792, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 18, 'max_depth': 40, 'num_leaves': 479, 'reg_alpha': 0.1640157842624526, 'reg_lambda': 0.10191576124052852, 'subsample': 0.391450811637736, 'min_child_weight': 0.039915891567628585, 'n_jobs': 4, 'learning_rate': 0.045294611352848194}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 14/3257 [00:00<00:23, 138.66it/s]  1%|          | 30/3257 [00:00<00:21, 150.61it/s]  1%|‚ñè         | 48/3257 [00:00<00:19, 163.36it/s]  2%|‚ñè         | 67/3257 [00:00<00:19, 160.62it/s]  3%|‚ñé         | 84/3257 [00:00<00:19, 163.41it/s]  3%|‚ñé         | 101/3257 [00:00<00:19, 160.18it/s]  4%|‚ñé         | 118/3257 [00:00<00:20, 154.96it/s]  4%|‚ñç         | 137/3257 [00:00<00:18, 165.01it/s]  5%|‚ñç         | 157/3257 [00:00<00:17, 174.20it/s]  5%|‚ñå         | 175/3257 [00:01<00:18, 166.21it/s]  6%|‚ñå         | 195/3257 [00:01<00:17, 175.59it/s]  7%|‚ñã         | 216/3257 [00:01<00:16, 181.75it/s]  7%|‚ñã         | 237/3257 [00:01<00:16, 184.76it/s]  8%|‚ñä         | 257/3257 [00:01<00:16, 185.52it/s]  9%|‚ñä         | 281/3257 [00:01<00:14, 199.66it/s]  9%|‚ñâ         | 302/3257 [00:01<00:15, 189.11it/s] 10%|‚ñâ         | 323/3257 [00:01<00:15, 194.51it/s] 11%|‚ñà         | 343/3257 [00:01<00:15, 184.81it/s] 11%|‚ñà         | 363/3257 [00:02<00:15, 185.60it/s] 12%|‚ñà‚ñè        | 382/3257 [00:02<00:16, 175.80it/s] 12%|‚ñà‚ñè        | 400/3257 [00:02<00:16, 175.21it/s] 13%|‚ñà‚ñé        | 420/3257 [00:02<00:15, 178.67it/s] 13%|‚ñà‚ñé        | 438/3257 [00:02<00:18, 155.90it/s] 14%|‚ñà‚ñç        | 457/3257 [00:02<00:17, 163.79it/s] 15%|‚ñà‚ñç        | 476/3257 [00:02<00:16, 168.89it/s] 15%|‚ñà‚ñå        | 494/3257 [00:02<00:16, 171.12it/s] 16%|‚ñà‚ñå        | 514/3257 [00:02<00:15, 178.88it/s] 16%|‚ñà‚ñã        | 533/3257 [00:03<00:15, 175.97it/s] 17%|‚ñà‚ñã        | 551/3257 [00:03<00:15, 177.09it/s] 17%|‚ñà‚ñã        | 569/3257 [00:03<00:16, 167.79it/s] 18%|‚ñà‚ñä        | 586/3257 [00:03<00:16, 163.65it/s] 19%|‚ñà‚ñä        | 604/3257 [00:03<00:15, 167.55it/s] 19%|‚ñà‚ñâ        | 622/3257 [00:03<00:15, 169.17it/s] 20%|‚ñà‚ñâ        | 642/3257 [00:03<00:14, 177.98it/s] 20%|‚ñà‚ñà        | 660/3257 [00:03<00:16, 161.59it/s] 21%|‚ñà‚ñà        | 682/3257 [00:03<00:14, 176.91it/s] 22%|‚ñà‚ñà‚ñè       | 701/3257 [00:04<00:14, 171.17it/s] 22%|‚ñà‚ñà‚ñè       | 721/3257 [00:04<00:14, 175.98it/s] 23%|‚ñà‚ñà‚ñé       | 739/3257 [00:04<00:15, 165.58it/s] 23%|‚ñà‚ñà‚ñé       | 760/3257 [00:04<00:14, 176.10it/s] 24%|‚ñà‚ñà‚ñç       | 778/3257 [00:04<00:14, 171.55it/s] 24%|‚ñà‚ñà‚ñç       | 797/3257 [00:04<00:13, 176.63it/s] 25%|‚ñà‚ñà‚ñå       | 815/3257 [00:04<00:14, 173.83it/s] 26%|‚ñà‚ñà‚ñå       | 833/3257 [00:04<00:14, 170.19it/s] 26%|‚ñà‚ñà‚ñå       | 851/3257 [00:04<00:14, 164.73it/s] 27%|‚ñà‚ñà‚ñã       | 870/3257 [00:05<00:13, 171.36it/s] 27%|‚ñà‚ñà‚ñã       | 888/3257 [00:05<00:13, 170.73it/s] 28%|‚ñà‚ñà‚ñä       | 908/3257 [00:05<00:13, 177.61it/s] 29%|‚ñà‚ñà‚ñä       | 929/3257 [00:05<00:12, 185.73it/s] 29%|‚ñà‚ñà‚ñâ       | 948/3257 [00:05<00:12, 183.65it/s] 30%|‚ñà‚ñà‚ñâ       | 967/3257 [00:05<00:12, 184.01it/s] 30%|‚ñà‚ñà‚ñà       | 986/3257 [00:05<00:13, 174.51it/s] 31%|‚ñà‚ñà‚ñà       | 1006/3257 [00:05<00:12, 180.08it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1025/3257 [00:05<00:12, 175.13it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1043/3257 [00:06<00:13, 162.40it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1061/3257 [00:06<00:13, 166.55it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:06<00:12, 169.21it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1098/3257 [00:06<00:12, 172.18it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:06<00:12, 172.08it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1135/3257 [00:06<00:12, 167.80it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1152/3257 [00:07<00:25, 83.93it/s]  36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:07<00:20, 101.28it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1186/3257 [00:07<00:18, 110.26it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1201/3257 [00:07<00:17, 115.73it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1219/3257 [00:07<00:15, 129.80it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:07<00:13, 153.75it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1261/3257 [00:07<00:12, 163.08it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1279/3257 [00:07<00:12, 158.99it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1297/3257 [00:07<00:12, 159.10it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1318/3257 [00:07<00:11, 171.92it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:08<00:10, 180.90it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1358/3257 [00:08<00:10, 174.50it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1376/3257 [00:08<00:10, 173.77it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1394/3257 [00:08<00:10, 173.97it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1417/3257 [00:08<00:09, 189.77it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1437/3257 [00:08<00:09, 187.89it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1460/3257 [00:08<00:09, 197.03it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1480/3257 [00:08<00:09, 194.93it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1502/3257 [00:08<00:08, 200.34it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1523/3257 [00:09<00:09, 183.01it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1542/3257 [00:09<00:09, 178.17it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1561/3257 [00:09<00:09, 173.11it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1579/3257 [00:09<00:09, 173.53it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1599/3257 [00:09<00:09, 179.47it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:09<00:08, 183.67it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1638/3257 [00:09<00:09, 174.99it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1656/3257 [00:09<00:09, 174.92it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1674/3257 [00:09<00:09, 164.64it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1691/3257 [00:10<00:09, 163.19it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1711/3257 [00:10<00:08, 173.30it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1729/3257 [00:10<00:09, 164.71it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1746/3257 [00:10<00:09, 163.85it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1766/3257 [00:10<00:08, 171.65it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1786/3257 [00:10<00:08, 177.83it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1804/3257 [00:10<00:08, 173.62it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1822/3257 [00:10<00:08, 173.63it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1840/3257 [00:10<00:08, 166.42it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1858/3257 [00:11<00:08, 166.10it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1880/3257 [00:11<00:07, 178.09it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1899/3257 [00:11<00:07, 179.81it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1918/3257 [00:11<00:07, 179.01it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1941/3257 [00:11<00:06, 192.92it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:11<00:06, 199.62it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1985/3257 [00:11<00:06, 190.85it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2006/3257 [00:11<00:06, 194.73it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2028/3257 [00:11<00:06, 200.61it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2049/3257 [00:12<00:06, 178.95it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2068/3257 [00:12<00:06, 172.28it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2087/3257 [00:12<00:06, 175.64it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2105/3257 [00:12<00:06, 176.14it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2123/3257 [00:12<00:06, 165.77it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2142/3257 [00:12<00:06, 169.45it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2160/3257 [00:12<00:06, 170.05it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2180/3257 [00:12<00:06, 176.10it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2199/3257 [00:12<00:05, 179.44it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2218/3257 [00:13<00:05, 173.80it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2237/3257 [00:13<00:05, 177.09it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2255/3257 [00:13<00:05, 177.02it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2273/3257 [00:13<00:05, 168.69it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2296/3257 [00:13<00:05, 185.80it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2315/3257 [00:13<00:05, 182.11it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2339/3257 [00:13<00:04, 198.35it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2361/3257 [00:13<00:04, 203.79it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2382/3257 [00:14<00:09, 95.70it/s]  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2404/3257 [00:14<00:07, 115.24it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2422/3257 [00:14<00:06, 124.19it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2439/3257 [00:14<00:06, 130.64it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2456/3257 [00:14<00:05, 138.88it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2478/3257 [00:14<00:04, 155.93it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2501/3257 [00:14<00:04, 174.30it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2521/3257 [00:14<00:04, 179.83it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2541/3257 [00:15<00:03, 183.57it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2561/3257 [00:15<00:03, 178.73it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2580/3257 [00:15<00:03, 170.68it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2598/3257 [00:15<00:03, 172.85it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2623/3257 [00:15<00:03, 191.91it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2643/3257 [00:15<00:03, 183.05it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2662/3257 [00:15<00:03, 173.92it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2684/3257 [00:15<00:03, 184.39it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2703/3257 [00:16<00:03, 167.38it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2721/3257 [00:16<00:03, 164.78it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2744/3257 [00:16<00:02, 180.02it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2763/3257 [00:16<00:02, 178.84it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2782/3257 [00:16<00:02, 180.45it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2805/3257 [00:16<00:02, 193.37it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2825/3257 [00:16<00:02, 180.77it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:16<00:02, 179.84it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2871/3257 [00:16<00:01, 204.45it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2892/3257 [00:17<00:01, 198.92it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2913/3257 [00:17<00:01, 187.88it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2933/3257 [00:17<00:01, 185.91it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2952/3257 [00:17<00:01, 175.65it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2972/3257 [00:17<00:01, 181.99it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2991/3257 [00:17<00:01, 175.85it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3014/3257 [00:17<00:01, 189.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3035/3257 [00:17<00:01, 194.61it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3057/3257 [00:17<00:01, 199.76it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3080/3257 [00:18<00:00, 206.24it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3101/3257 [00:18<00:00, 202.88it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3123/3257 [00:18<00:00, 207.77it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3144/3257 [00:18<00:00, 188.22it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3164/3257 [00:18<00:00, 185.42it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3183/3257 [00:18<00:00, 176.42it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3204/3257 [00:18<00:00, 185.33it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3223/3257 [00:18<00:00, 178.99it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:18<00:00, 193.00it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 171.81it/s]
2023-02-07 18:52:00.458 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:52:00,459][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d238,n5,mc6,s0.349726,t4>', 'datetime': '2023-02-07T18:52:00.459788', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:52:00,460][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:52:00,460][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:52:00,980][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 18:52:00,982][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:52:01,029][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 15588 unique words (49.01% of original 31803, drops 16215)', 'datetime': '2023-02-07T18:52:01.029282', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:52:01,030][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 5053508 word corpus (99.18% of original 5095118, drops 41610)', 'datetime': '2023-02-07T18:52:01.030887', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:52:01,086][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 18:52:01,088][gensim.models.word2vec][INFO] - sample=0.349726 downsamples 0 most-common words
[2023-02-07 18:52:01,089][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5053508 word corpus (100.0%% of prior 5053508)', 'datetime': '2023-02-07T18:52:01.089153', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:52:01,179][gensim.models.word2vec][INFO] - estimated required memory for 15588 words and 238 dimensions: 41225616 bytes
[2023-02-07 18:52:01,180][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:52:01,199][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 15588 vocabulary and 238 features, using sg=1 hs=0 sample=0.3497257583290872 negative=5 window=16 shrink_windows=True', 'datetime': '2023-02-07T18:52:01.199813', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:52:02,206][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 32.91% examples, 1669950 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:03,212][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 65.21% examples, 1666325 words/s, in_qsize 8, out_qsize 1
[2023-02-07 18:52:04,215][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 99.72% examples, 1671541 words/s, in_qsize 1, out_qsize 1
[2023-02-07 18:52:04,221][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5045550 effective words) took 3.0s, 1671206 effective words/s
[2023-02-07 18:52:05,228][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 42.40% examples, 2196302 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:06,229][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 84.65% examples, 2148893 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:06,654][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5045550 effective words) took 2.4s, 2075771 effective words/s
[2023-02-07 18:52:07,657][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 35.06% examples, 1796301 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:08,658][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 69.88% examples, 1805440 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:09,437][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5045550 effective words) took 2.8s, 1814714 effective words/s
[2023-02-07 18:52:10,447][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 42.28% examples, 2179756 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:52:11,447][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 77.77% examples, 1976083 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:12,037][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5045550 effective words) took 2.6s, 1942225 effective words/s
[2023-02-07 18:52:13,046][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 44.89% examples, 2304874 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:14,051][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 92.14% examples, 2328899 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:14,200][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5045550 effective words) took 2.2s, 2337639 effective words/s
[2023-02-07 18:52:15,210][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 41.85% examples, 2152228 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:16,212][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 90.33% examples, 2279678 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:16,404][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5045550 effective words) took 2.2s, 2290437 effective words/s
[2023-02-07 18:52:17,411][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 35.28% examples, 1804479 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:18,414][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 80.75% examples, 2049036 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:18,812][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5045550 effective words) took 2.4s, 2099649 effective words/s
[2023-02-07 18:52:19,821][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 35.49% examples, 1819185 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:20,826][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 71.54% examples, 1833934 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:21,503][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5045550 effective words) took 2.7s, 1878016 effective words/s
[2023-02-07 18:52:22,506][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 36.35% examples, 1865749 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:23,511][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 73.63% examples, 1877101 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:24,177][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5045550 effective words) took 2.7s, 1888814 effective words/s
[2023-02-07 18:52:25,184][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 41.08% examples, 2119836 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:26,183][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 89.62% examples, 2270668 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:26,393][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5045550 effective words) took 2.2s, 2279580 effective words/s
[2023-02-07 18:52:27,397][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 46.21% examples, 2368392 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:28,399][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 96.04% examples, 2420712 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:28,469][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5045550 effective words) took 2.1s, 2434030 effective words/s
[2023-02-07 18:52:29,473][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 37.80% examples, 1950413 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:30,473][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 75.22% examples, 1927811 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:31,090][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5045550 effective words) took 2.6s, 1926392 effective words/s
[2023-02-07 18:52:32,095][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 37.21% examples, 1921647 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:33,101][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 75.13% examples, 1916057 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:33,714][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5045550 effective words) took 2.6s, 1924431 effective words/s
[2023-02-07 18:52:34,726][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 38.29% examples, 1963109 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:35,730][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 77.49% examples, 1965046 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:36,255][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5045550 effective words) took 2.5s, 1988530 effective words/s
[2023-02-07 18:52:37,261][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 36.94% examples, 1909047 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:38,266][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 74.27% examples, 1894471 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:52:38,922][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5045550 effective words) took 2.7s, 1895389 effective words/s
[2023-02-07 18:52:38,924][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (75683250 effective words) took 37.7s, 2006241 effective words/s', 'datetime': '2023-02-07T18:52:38.924166', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:52:38.924 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:52:41,611][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185132-rdgq1luv/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:52:41.611522', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:52:41,612][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:52:41,672][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185132-rdgq1luv/files/../tmp/embedding_model.pt
2023-02-07 18:52:41.673 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:52:43.285 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:52:43.874 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:52:45.479 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.3718961828208696, 'test_mae': 1.0996813932905098, 'test_r2': -1.7969163354889357}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.62
wandb: percentage 0.50986
wandb:   test_mae 1.09968
wandb:   test_mse 2.3719
wandb:    test_r2 -1.79692
wandb: 
wandb: üöÄ View run amber-sweep-22 at: https://wandb.ai/xiaoqiz/mof2vec/runs/rdgq1luv
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_185132-rdgq1luv/logs
wandb: Agent Starting Run: leit8al1 with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 853
wandb: 	model.gensim.alpha: 0.0003567648758661738
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.4113369219035044
wandb: 	model.gensim.vector_size: 133
wandb: 	model.gensim.window: 13
wandb: 	model.sklearn.learning_rate: 0.03021025219482641
wandb: 	model.sklearn.max_depth: 53
wandb: 	model.sklearn.min_child_weight: 0.0978621365025731
wandb: 	model.sklearn.n_estimators: 249
wandb: 	model.sklearn.num_leaves: 490
wandb: 	model.sklearn.reg_alpha: 0.16497501803612538
wandb: 	model.sklearn.reg_lambda: 0.1017712480759865
wandb: 	model.sklearn.subsample: 0.4686319165937982
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185259-leit8al1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-23
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/leit8al1
2023-02-07 18:53:07.846 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 18:53:07.847 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 853 for sweep.
2023-02-07 18:53:07.847 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0003567648758661738 for sweep.
2023-02-07 18:53:07.847 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 18:53:07.847 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 18:53:07.848 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4113369219035044 for sweep.
2023-02-07 18:53:07.848 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 133 for sweep.
2023-02-07 18:53:07.848 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 13 for sweep.
2023-02-07 18:53:07.848 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.03021025219482641 for sweep.
2023-02-07 18:53:07.849 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 53 for sweep.
2023-02-07 18:53:07.849 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0978621365025731 for sweep.
2023-02-07 18:53:07.849 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 249 for sweep.
2023-02-07 18:53:07.849 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 490 for sweep.
2023-02-07 18:53:07.850 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.16497501803612538 for sweep.
2023-02-07 18:53:07.850 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.1017712480759865 for sweep.
2023-02-07 18:53:07.850 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.4686319165937982 for sweep.
2023-02-07 18:53:07.850 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:53:07.863 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185259-leit8al1/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 853, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 133, 'window': 13, 'min_count': 4, 'dm': 0, 'sample': 0.4113369219035044, 'workers': 4, 'alpha': 0.0003567648758661738, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 249, 'max_depth': 53, 'num_leaves': 490, 'reg_alpha': 0.16497501803612538, 'reg_lambda': 0.1017712480759865, 'subsample': 0.4686319165937982, 'min_child_weight': 0.0978621365025731, 'n_jobs': 4, 'learning_rate': 0.03021025219482641}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 22/3257 [00:00<00:15, 215.16it/s]  2%|‚ñè         | 49/3257 [00:00<00:13, 240.76it/s]  2%|‚ñè         | 77/3257 [00:00<00:12, 256.44it/s]  3%|‚ñé         | 106/3257 [00:00<00:12, 262.20it/s]  4%|‚ñç         | 138/3257 [00:00<00:11, 281.58it/s]  5%|‚ñå         | 169/3257 [00:00<00:10, 288.25it/s]  6%|‚ñå         | 198/3257 [00:00<00:11, 260.23it/s]  7%|‚ñã         | 225/3257 [00:00<00:11, 257.15it/s]  8%|‚ñä         | 252/3257 [00:00<00:12, 242.12it/s]  9%|‚ñä         | 277/3257 [00:01<00:12, 239.78it/s]  9%|‚ñâ         | 303/3257 [00:01<00:12, 243.55it/s] 10%|‚ñà         | 331/3257 [00:01<00:11, 252.02it/s] 11%|‚ñà         | 359/3257 [00:01<00:11, 258.53it/s] 12%|‚ñà‚ñè        | 385/3257 [00:01<00:11, 253.46it/s] 13%|‚ñà‚ñé        | 414/3257 [00:01<00:10, 263.95it/s] 14%|‚ñà‚ñé        | 441/3257 [00:01<00:11, 249.54it/s] 14%|‚ñà‚ñç        | 472/3257 [00:01<00:10, 264.36it/s] 15%|‚ñà‚ñå        | 499/3257 [00:01<00:10, 263.59it/s] 16%|‚ñà‚ñå        | 527/3257 [00:02<00:10, 266.80it/s] 17%|‚ñà‚ñã        | 557/3257 [00:02<00:09, 274.12it/s] 18%|‚ñà‚ñä        | 585/3257 [00:02<00:10, 257.48it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:02<00:09, 268.87it/s] 20%|‚ñà‚ñâ        | 643/3257 [00:02<00:14, 175.96it/s] 21%|‚ñà‚ñà        | 668/3257 [00:02<00:13, 190.69it/s] 21%|‚ñà‚ñà‚ñè       | 696/3257 [00:02<00:12, 210.42it/s] 22%|‚ñà‚ñà‚ñè       | 724/3257 [00:02<00:11, 226.50it/s] 23%|‚ñà‚ñà‚ñé       | 752/3257 [00:03<00:10, 239.55it/s] 24%|‚ñà‚ñà‚ñç       | 780/3257 [00:03<00:09, 249.95it/s] 25%|‚ñà‚ñà‚ñç       | 808/3257 [00:03<00:09, 258.14it/s] 26%|‚ñà‚ñà‚ñå       | 836/3257 [00:03<00:09, 260.78it/s] 27%|‚ñà‚ñà‚ñã       | 865/3257 [00:03<00:08, 268.86it/s] 27%|‚ñà‚ñà‚ñã       | 893/3257 [00:03<00:08, 266.42it/s] 28%|‚ñà‚ñà‚ñä       | 924/3257 [00:03<00:08, 277.64it/s] 29%|‚ñà‚ñà‚ñâ       | 953/3257 [00:03<00:08, 273.40it/s] 30%|‚ñà‚ñà‚ñà       | 981/3257 [00:03<00:08, 270.06it/s] 31%|‚ñà‚ñà‚ñà       | 1009/3257 [00:04<00:08, 264.33it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1036/3257 [00:04<00:08, 262.53it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1063/3257 [00:04<00:08, 248.47it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:04<00:08, 253.08it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:04<00:08, 254.06it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1143/3257 [00:04<00:08, 253.46it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1172/3257 [00:04<00:07, 262.15it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1199/3257 [00:04<00:08, 252.86it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1226/3257 [00:04<00:07, 256.28it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1255/3257 [00:04<00:07, 265.55it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1282/3257 [00:05<00:07, 258.60it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1308/3257 [00:05<00:07, 249.24it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:05<00:07, 265.85it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1366/3257 [00:05<00:07, 258.46it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1393/3257 [00:05<00:07, 260.90it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:05<00:06, 271.82it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1456/3257 [00:05<00:06, 286.63it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1486/3257 [00:05<00:06, 288.07it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:05<00:05, 290.39it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1549/3257 [00:06<00:06, 278.13it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1577/3257 [00:06<00:06, 274.00it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1606/3257 [00:06<00:05, 276.45it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:06<00:05, 272.92it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1662/3257 [00:06<00:06, 258.88it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1689/3257 [00:06<00:06, 247.09it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1717/3257 [00:06<00:06, 253.55it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1743/3257 [00:06<00:06, 246.32it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1773/3257 [00:06<00:05, 260.37it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1804/3257 [00:07<00:05, 272.86it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1832/3257 [00:07<00:05, 267.67it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1866/3257 [00:07<00:04, 288.01it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1896/3257 [00:07<00:04, 288.03it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1927/3257 [00:07<00:04, 294.32it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:07<00:04, 317.77it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1997/3257 [00:07<00:04, 312.11it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2029/3257 [00:07<00:03, 311.29it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2061/3257 [00:07<00:04, 283.59it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2090/3257 [00:08<00:06, 190.30it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2119/3257 [00:08<00:05, 209.74it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2144/3257 [00:08<00:05, 216.85it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:08<00:04, 243.15it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:08<00:04, 251.27it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:08<00:03, 263.10it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2265/3257 [00:08<00:03, 268.64it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2296/3257 [00:08<00:03, 277.38it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2326/3257 [00:08<00:03, 283.70it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:09<00:02, 299.90it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2394/3257 [00:09<00:02, 309.02it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2426/3257 [00:09<00:02, 295.66it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2456/3257 [00:09<00:02, 287.10it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2489/3257 [00:09<00:02, 298.13it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2524/3257 [00:09<00:02, 312.41it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2556/3257 [00:09<00:02, 308.03it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2587/3257 [00:09<00:02, 289.81it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2622/3257 [00:09<00:02, 305.57it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2653/3257 [00:10<00:02, 295.49it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2685/3257 [00:10<00:01, 301.93it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2716/3257 [00:10<00:01, 278.80it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2752/3257 [00:10<00:01, 300.29it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2783/3257 [00:10<00:01, 296.15it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2816/3257 [00:10<00:01, 303.13it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2847/3257 [00:10<00:01, 299.95it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2885/3257 [00:10<00:01, 322.51it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2918/3257 [00:10<00:01, 309.40it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:11<00:01, 297.46it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2981/3257 [00:11<00:00, 300.46it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3013/3257 [00:11<00:00, 305.60it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3047/3257 [00:11<00:00, 313.59it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3081/3257 [00:11<00:00, 318.47it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3113/3257 [00:11<00:00, 299.78it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3144/3257 [00:11<00:00, 291.45it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3177/3257 [00:11<00:00, 296.70it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3213/3257 [00:11<00:00, 309.70it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:12<00:00, 328.88it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:12<00:00, 270.80it/s]
2023-02-07 18:53:20.193 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:53:20,194][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d133,n5,mc4,s0.411337,t4>', 'datetime': '2023-02-07T18:53:20.194642', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:53:20,195][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:53:20,195][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:53:20,381][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 18:53:20,382][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:53:20,387][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 2073 unique words (73.54% of original 2819, drops 746)', 'datetime': '2023-02-07T18:53:20.387638', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:53:20,387][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 2182321 word corpus (99.94% of original 2183622, drops 1301)', 'datetime': '2023-02-07T18:53:20.387985', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:53:20,395][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 18:53:20,395][gensim.models.word2vec][INFO] - sample=0.411337 downsamples 0 most-common words
[2023-02-07 18:53:20,395][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2182321 word corpus (100.0%% of prior 2182321)', 'datetime': '2023-02-07T18:53:20.395516', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:53:20,407][gensim.models.word2vec][INFO] - estimated required memory for 2073 words and 133 dimensions: 5626296 bytes
[2023-02-07 18:53:20,407][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:53:20,411][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2073 vocabulary and 133 features, using sg=1 hs=0 sample=0.4113369219035044 negative=5 window=13 shrink_windows=True', 'datetime': '2023-02-07T18:53:20.411160', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:53:21,414][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 76.73% examples, 1695736 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:53:21,599][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2185578 effective words) took 1.2s, 1842911 effective words/s
[2023-02-07 18:53:22,606][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 82.87% examples, 1821940 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:53:22,801][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2185578 effective words) took 1.2s, 1822316 effective words/s
[2023-02-07 18:53:23,644][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2185578 effective words) took 0.8s, 2598397 effective words/s
[2023-02-07 18:53:24,649][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 79.77% examples, 1760125 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:53:24,872][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2185578 effective words) took 1.2s, 1784176 effective words/s
[2023-02-07 18:53:25,879][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 80.47% examples, 1765336 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:53:26,105][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2185578 effective words) took 1.2s, 1775040 effective words/s
[2023-02-07 18:53:27,112][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 82.68% examples, 1816880 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:53:27,240][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2185578 effective words) took 1.1s, 1934416 effective words/s
[2023-02-07 18:53:28,052][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2185578 effective words) took 0.8s, 2698307 effective words/s
[2023-02-07 18:53:28,879][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2185578 effective words) took 0.8s, 2648532 effective words/s
[2023-02-07 18:53:29,881][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 78.60% examples, 1734672 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:53:30,059][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2185578 effective words) took 1.2s, 1854288 effective words/s
[2023-02-07 18:53:31,069][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 79.00% examples, 1732425 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:53:31,307][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2185578 effective words) took 1.2s, 1755201 effective words/s
[2023-02-07 18:53:32,100][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2185578 effective words) took 0.8s, 2760805 effective words/s
[2023-02-07 18:53:33,104][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 80.32% examples, 1770432 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:53:33,255][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2185578 effective words) took 1.2s, 1896175 effective words/s
[2023-02-07 18:53:34,263][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 79.34% examples, 1751215 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:53:34,501][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2185578 effective words) took 1.2s, 1761218 effective words/s
[2023-02-07 18:53:35,302][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2185578 effective words) took 0.8s, 2737366 effective words/s
[2023-02-07 18:53:36,312][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 79.43% examples, 1740594 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:53:36,475][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2185578 effective words) took 1.2s, 1867355 effective words/s
[2023-02-07 18:53:36,475][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 32754330 raw words (32783670 effective words) took 16.1s, 2040776 effective words/s', 'datetime': '2023-02-07T18:53:36.475875', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:53:36.476 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:53:37,282][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185259-leit8al1/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:53:37.282058', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:53:37,284][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:53:37,291][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185259-leit8al1/files/../tmp/embedding_model.pt
2023-02-07 18:53:37.291 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:53:38.491 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:53:38.964 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:53:40.251 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.6788812952880283, 'test_mae': 1.2120918930562476, 'test_r2': -2.2827048477745047}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.03
wandb: percentage 0.26463
wandb:   test_mae 1.21209
wandb:   test_mse 2.67888
wandb:    test_r2 -2.2827
wandb: 
wandb: üöÄ View run whole-sweep-23 at: https://wandb.ai/xiaoqiz/mof2vec/runs/leit8al1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_185259-leit8al1/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: hr3j7o3q with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 285
wandb: 	model.gensim.alpha: 0.0005407214957627774
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.2479652327384197
wandb: 	model.gensim.vector_size: 273
wandb: 	model.gensim.window: 12
wandb: 	model.sklearn.learning_rate: 0.0011209642238139829
wandb: 	model.sklearn.max_depth: 95
wandb: 	model.sklearn.min_child_weight: 0.08941516577559085
wandb: 	model.sklearn.n_estimators: 3024
wandb: 	model.sklearn.num_leaves: 438
wandb: 	model.sklearn.reg_alpha: 0.004717072965972622
wandb: 	model.sklearn.reg_lambda: 0.01522147774815272
wandb: 	model.sklearn.subsample: 0.3427020868621641
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185358-hr3j7o3q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-24
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/hr3j7o3q
2023-02-07 18:54:07.164 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 18:54:07.165 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 285 for sweep.
2023-02-07 18:54:07.165 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0005407214957627774 for sweep.
2023-02-07 18:54:07.166 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 18:54:07.166 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 18:54:07.166 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2479652327384197 for sweep.
2023-02-07 18:54:07.166 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 273 for sweep.
2023-02-07 18:54:07.167 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 12 for sweep.
2023-02-07 18:54:07.167 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0011209642238139829 for sweep.
2023-02-07 18:54:07.167 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 95 for sweep.
2023-02-07 18:54:07.168 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.08941516577559085 for sweep.
2023-02-07 18:54:07.168 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3024 for sweep.
2023-02-07 18:54:07.168 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 438 for sweep.
2023-02-07 18:54:07.168 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.004717072965972622 for sweep.
2023-02-07 18:54:07.168 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.01522147774815272 for sweep.
2023-02-07 18:54:07.169 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3427020868621641 for sweep.
2023-02-07 18:54:07.169 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:54:07.177 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185358-hr3j7o3q/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 285, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 273, 'window': 12, 'min_count': 3, 'dm': 0, 'sample': 0.2479652327384197, 'workers': 4, 'alpha': 0.0005407214957627774, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3024, 'max_depth': 95, 'num_leaves': 438, 'reg_alpha': 0.004717072965972622, 'reg_lambda': 0.01522147774815272, 'subsample': 0.3427020868621641, 'min_child_weight': 0.08941516577559085, 'n_jobs': 4, 'learning_rate': 0.0011209642238139829}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 25/3257 [00:00<00:13, 246.78it/s]  2%|‚ñè         | 50/3257 [00:00<00:12, 248.41it/s]  2%|‚ñè         | 78/3257 [00:00<00:12, 260.39it/s]  3%|‚ñé         | 105/3257 [00:00<00:12, 246.57it/s]  4%|‚ñç         | 130/3257 [00:00<00:18, 166.43it/s]  5%|‚ñç         | 158/3257 [00:00<00:16, 193.14it/s]  6%|‚ñå         | 183/3257 [00:00<00:14, 207.25it/s]  7%|‚ñã         | 215/3257 [00:00<00:12, 237.60it/s]  8%|‚ñä         | 248/3257 [00:01<00:11, 260.55it/s]  9%|‚ñä         | 279/3257 [00:01<00:10, 274.21it/s]  9%|‚ñâ         | 308/3257 [00:01<00:10, 273.15it/s] 10%|‚ñà         | 339/3257 [00:01<00:10, 280.75it/s] 11%|‚ñà‚ñè        | 370/3257 [00:01<00:10, 288.60it/s] 12%|‚ñà‚ñè        | 400/3257 [00:01<00:10, 276.30it/s] 13%|‚ñà‚ñé        | 429/3257 [00:01<00:10, 259.84it/s] 14%|‚ñà‚ñç        | 458/3257 [00:01<00:10, 267.26it/s] 15%|‚ñà‚ñç        | 486/3257 [00:01<00:10, 269.25it/s] 16%|‚ñà‚ñå        | 519/3257 [00:02<00:09, 284.97it/s] 17%|‚ñà‚ñã        | 548/3257 [00:02<00:09, 281.75it/s] 18%|‚ñà‚ñä        | 577/3257 [00:02<00:10, 260.49it/s] 19%|‚ñà‚ñä        | 610/3257 [00:02<00:09, 279.01it/s] 20%|‚ñà‚ñâ        | 639/3257 [00:02<00:09, 279.85it/s] 21%|‚ñà‚ñà        | 668/3257 [00:02<00:09, 274.66it/s] 21%|‚ñà‚ñà‚ñè       | 697/3257 [00:02<00:09, 278.98it/s] 22%|‚ñà‚ñà‚ñè       | 726/3257 [00:02<00:09, 278.49it/s] 23%|‚ñà‚ñà‚ñé       | 754/3257 [00:02<00:09, 267.67it/s] 24%|‚ñà‚ñà‚ñç       | 781/3257 [00:03<00:09, 261.65it/s] 25%|‚ñà‚ñà‚ñç       | 812/3257 [00:03<00:08, 272.08it/s] 26%|‚ñà‚ñà‚ñå       | 840/3257 [00:03<00:09, 265.31it/s] 27%|‚ñà‚ñà‚ñã       | 867/3257 [00:03<00:09, 261.88it/s] 27%|‚ñà‚ñà‚ñã       | 894/3257 [00:03<00:08, 263.98it/s] 28%|‚ñà‚ñà‚ñä       | 927/3257 [00:03<00:08, 280.90it/s] 29%|‚ñà‚ñà‚ñâ       | 956/3257 [00:03<00:08, 282.83it/s] 30%|‚ñà‚ñà‚ñà       | 985/3257 [00:03<00:08, 279.58it/s] 31%|‚ñà‚ñà‚ñà       | 1014/3257 [00:03<00:08, 268.42it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1041/3257 [00:03<00:08, 253.54it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1071/3257 [00:04<00:08, 263.13it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1098/3257 [00:04<00:08, 260.02it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1125/3257 [00:04<00:08, 261.95it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1152/3257 [00:04<00:08, 256.67it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1178/3257 [00:04<00:08, 255.19it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1204/3257 [00:04<00:08, 236.30it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1232/3257 [00:04<00:08, 247.26it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1258/3257 [00:04<00:07, 250.28it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1284/3257 [00:04<00:08, 244.41it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1309/3257 [00:05<00:07, 245.70it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:05<00:07, 259.05it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1366/3257 [00:05<00:07, 244.81it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1391/3257 [00:05<00:07, 242.96it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1422/3257 [00:05<00:07, 258.97it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1449/3257 [00:05<00:10, 168.79it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1477/3257 [00:05<00:09, 190.42it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:05<00:07, 222.82it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1537/3257 [00:06<00:07, 219.58it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1562/3257 [00:06<00:07, 225.90it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1589/3257 [00:06<00:07, 236.17it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:06<00:06, 251.08it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1646/3257 [00:06<00:06, 251.26it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1672/3257 [00:06<00:06, 242.03it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1700/3257 [00:06<00:06, 249.07it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:06<00:06, 250.41it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1754/3257 [00:06<00:06, 244.00it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1782/3257 [00:07<00:05, 252.92it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1808/3257 [00:07<00:05, 247.49it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1837/3257 [00:07<00:05, 258.89it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1867/3257 [00:07<00:05, 269.08it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1895/3257 [00:07<00:05, 265.21it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1922/3257 [00:07<00:05, 263.10it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1957/3257 [00:07<00:04, 287.66it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1986/3257 [00:07<00:04, 279.35it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2015/3257 [00:07<00:04, 278.61it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2043/3257 [00:08<00:04, 278.43it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2071/3257 [00:08<00:04, 264.48it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2098/3257 [00:08<00:04, 263.59it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2125/3257 [00:08<00:04, 260.52it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2152/3257 [00:08<00:04, 260.76it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:08<00:04, 266.20it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2209/3257 [00:08<00:03, 265.55it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2238/3257 [00:08<00:03, 268.20it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2265/3257 [00:08<00:03, 264.42it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2292/3257 [00:08<00:03, 264.59it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2323/3257 [00:09<00:03, 276.96it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:09<00:03, 294.07it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2390/3257 [00:09<00:02, 294.75it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2420/3257 [00:09<00:03, 277.16it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2448/3257 [00:09<00:03, 251.08it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2478/3257 [00:09<00:02, 260.90it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2512/3257 [00:09<00:02, 282.40it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2542/3257 [00:09<00:02, 286.24it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2572/3257 [00:09<00:02, 272.72it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2600/3257 [00:10<00:02, 272.01it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2635/3257 [00:10<00:02, 293.77it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2665/3257 [00:10<00:02, 281.53it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2694/3257 [00:10<00:02, 271.58it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2722/3257 [00:10<00:02, 246.92it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:10<00:01, 266.92it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2787/3257 [00:10<00:01, 277.08it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2816/3257 [00:10<00:01, 277.62it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2845/3257 [00:11<00:02, 166.11it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2884/3257 [00:11<00:01, 208.59it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2912/3257 [00:11<00:01, 219.86it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2941/3257 [00:11<00:01, 235.82it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2969/3257 [00:11<00:01, 235.14it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2996/3257 [00:11<00:01, 238.60it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3028/3257 [00:11<00:00, 258.36it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3064/3257 [00:11<00:00, 284.88it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3095/3257 [00:12<00:00, 290.39it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3128/3257 [00:12<00:00, 299.18it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3159/3257 [00:12<00:00, 286.89it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3189/3257 [00:12<00:00, 286.14it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3219/3257 [00:12<00:00, 285.25it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:12<00:00, 296.37it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:12<00:00, 258.14it/s]
2023-02-07 18:54:20.156 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:54:20,157][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d273,n5,mc3,s0.247965,t4>', 'datetime': '2023-02-07T18:54:20.157826', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:54:20,158][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:54:20,158][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:54:20,445][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 18:54:20,446][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:54:20,458][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 4875 unique words (73.18% of original 6662, drops 1787)', 'datetime': '2023-02-07T18:54:20.458473', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:54:20,459][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 2908813 word corpus (99.91% of original 2911496, drops 2683)', 'datetime': '2023-02-07T18:54:20.459614', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:54:20,475][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 18:54:20,476][gensim.models.word2vec][INFO] - sample=0.247965 downsamples 0 most-common words
[2023-02-07 18:54:20,476][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2908813 word corpus (100.0%% of prior 2908813)', 'datetime': '2023-02-07T18:54:20.476453', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:54:20,503][gensim.models.word2vec][INFO] - estimated required memory for 4875 words and 273 dimensions: 17292544 bytes
[2023-02-07 18:54:20,504][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:54:20,512][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 4875 vocabulary and 273 features, using sg=1 hs=0 sample=0.2479652327384197 negative=5 window=12 shrink_windows=True', 'datetime': '2023-02-07T18:54:20.512511', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:54:21,519][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 45.44% examples, 1343867 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:22,527][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 96.71% examples, 1402019 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:22,587][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2912070 effective words) took 2.1s, 1405217 effective words/s
[2023-02-07 18:54:23,595][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 46.58% examples, 1369604 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:24,597][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 94.38% examples, 1372355 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:54:24,699][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2912070 effective words) took 2.1s, 1380152 effective words/s
[2023-02-07 18:54:25,704][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 45.07% examples, 1335713 words/s, in_qsize 1, out_qsize 0
[2023-02-07 18:54:26,709][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 92.63% examples, 1352751 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:26,842][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2912070 effective words) took 2.1s, 1360198 effective words/s
[2023-02-07 18:54:27,845][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 45.84% examples, 1358211 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:28,846][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 93.92% examples, 1371560 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:28,962][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2912070 effective words) took 2.1s, 1374429 effective words/s
[2023-02-07 18:54:29,971][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 45.07% examples, 1331096 words/s, in_qsize 5, out_qsize 1
[2023-02-07 18:54:30,972][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 92.11% examples, 1348108 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:54:31,111][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2912070 effective words) took 2.1s, 1356315 effective words/s
[2023-02-07 18:54:32,121][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 45.41% examples, 1339727 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:54:33,124][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 92.11% examples, 1346572 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:33,274][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2912070 effective words) took 2.2s, 1348312 effective words/s
[2023-02-07 18:54:34,279][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 46.30% examples, 1365214 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:35,284][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 94.38% examples, 1372092 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:54:35,381][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2912070 effective words) took 2.1s, 1383128 effective words/s
[2023-02-07 18:54:36,384][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 24.90% examples, 719751 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:54:37,392][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 52.63% examples, 779898 words/s, in_qsize 5, out_qsize 2
[2023-02-07 18:54:38,372][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2912070 effective words) took 3.0s, 974120 effective words/s
[2023-02-07 18:54:39,380][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 46.58% examples, 1371962 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:40,168][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2912070 effective words) took 1.8s, 1624345 effective words/s
[2023-02-07 18:54:41,183][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 43.94% examples, 1296232 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:42,197][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 91.53% examples, 1326652 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:42,354][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2912070 effective words) took 2.2s, 1334201 effective words/s
[2023-02-07 18:54:43,359][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 45.01% examples, 1334914 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:44,364][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 92.63% examples, 1352632 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:44,489][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2912070 effective words) took 2.1s, 1365061 effective words/s
[2023-02-07 18:54:45,495][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 45.41% examples, 1344492 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:46,497][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 92.63% examples, 1354016 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:46,642][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2912070 effective words) took 2.2s, 1353956 effective words/s
[2023-02-07 18:54:47,652][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 44.18% examples, 1310614 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:48,662][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 92.57% examples, 1341713 words/s, in_qsize 8, out_qsize 1
[2023-02-07 18:54:48,804][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2912070 effective words) took 2.2s, 1348306 effective words/s
[2023-02-07 18:54:49,808][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 66.38% examples, 1973262 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:50,253][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2912070 effective words) took 1.4s, 2012434 effective words/s
[2023-02-07 18:54:51,261][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 46.58% examples, 1375983 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:54:52,006][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2912070 effective words) took 1.8s, 1663145 effective words/s
[2023-02-07 18:54:52,007][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 43672440 raw words (43681050 effective words) took 31.5s, 1386976 effective words/s', 'datetime': '2023-02-07T18:54:52.007386', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:54:52.009 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:54:53,571][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185358-hr3j7o3q/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:54:53.570851', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:54:53,573][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:54:53,610][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185358-hr3j7o3q/files/../tmp/embedding_model.pt
2023-02-07 18:54:53.610 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:54:55.308 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:54:55.911 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:54:57.929 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.464659528964478, 'test_mae': 1.164923270028713, 'test_r2': -2.385978911222827}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.22
wandb: percentage 0.26824
wandb:   test_mae 1.16492
wandb:   test_mse 2.46466
wandb:    test_r2 -2.38598
wandb: 
wandb: üöÄ View run glorious-sweep-24 at: https://wandb.ai/xiaoqiz/mof2vec/runs/hr3j7o3q
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_185358-hr3j7o3q/logs
wandb: Agent Starting Run: 5xdlx96c with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 567
wandb: 	model.gensim.alpha: 0.008149792938706055
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.3430680032532035
wandb: 	model.gensim.vector_size: 49
wandb: 	model.gensim.window: 2
wandb: 	model.sklearn.learning_rate: 0.03973785494633141
wandb: 	model.sklearn.max_depth: 10
wandb: 	model.sklearn.min_child_weight: 0.03430219121849018
wandb: 	model.sklearn.n_estimators: 2272
wandb: 	model.sklearn.num_leaves: 341
wandb: 	model.sklearn.reg_alpha: 0.11921823300352732
wandb: 	model.sklearn.reg_lambda: 0.13888277017783426
wandb: 	model.sklearn.subsample: 0.9142223690852892
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185512-5xdlx96c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-25
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/5xdlx96c
2023-02-07 18:55:21.190 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 18:55:21.190 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 567 for sweep.
2023-02-07 18:55:21.191 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.008149792938706055 for sweep.
2023-02-07 18:55:21.191 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 18:55:21.191 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 18:55:21.192 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3430680032532035 for sweep.
2023-02-07 18:55:21.192 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 49 for sweep.
2023-02-07 18:55:21.192 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 2 for sweep.
2023-02-07 18:55:21.192 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.03973785494633141 for sweep.
2023-02-07 18:55:21.193 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 10 for sweep.
2023-02-07 18:55:21.193 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03430219121849018 for sweep.
2023-02-07 18:55:21.193 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2272 for sweep.
2023-02-07 18:55:21.193 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 341 for sweep.
2023-02-07 18:55:21.193 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.11921823300352732 for sweep.
2023-02-07 18:55:21.194 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.13888277017783426 for sweep.
2023-02-07 18:55:21.194 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.9142223690852892 for sweep.
2023-02-07 18:55:21.194 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:55:21.201 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185512-5xdlx96c/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 567, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 49, 'window': 2, 'min_count': 3, 'dm': 1, 'sample': 0.3430680032532035, 'workers': 4, 'alpha': 0.008149792938706055, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2272, 'max_depth': 10, 'num_leaves': 341, 'reg_alpha': 0.11921823300352732, 'reg_lambda': 0.13888277017783426, 'subsample': 0.9142223690852892, 'min_child_weight': 0.03430219121849018, 'n_jobs': 4, 'learning_rate': 0.03973785494633141}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 22/3257 [00:00<00:15, 213.28it/s]  1%|‚ñè         | 45/3257 [00:00<00:14, 220.46it/s]  2%|‚ñè         | 68/3257 [00:00<00:14, 215.67it/s]  3%|‚ñé         | 94/3257 [00:00<00:13, 228.88it/s]  4%|‚ñé         | 117/3257 [00:00<00:14, 223.38it/s]  4%|‚ñç         | 145/3257 [00:00<00:12, 239.89it/s]  5%|‚ñå         | 170/3257 [00:00<00:13, 224.87it/s]  6%|‚ñå         | 193/3257 [00:00<00:13, 222.77it/s]  7%|‚ñã         | 218/3257 [00:00<00:13, 229.31it/s]  8%|‚ñä         | 245/3257 [00:01<00:12, 240.50it/s]  8%|‚ñä         | 270/3257 [00:01<00:12, 236.53it/s]  9%|‚ñâ         | 300/3257 [00:01<00:11, 249.18it/s] 10%|‚ñà         | 328/3257 [00:01<00:11, 251.61it/s] 11%|‚ñà         | 354/3257 [00:01<00:11, 248.36it/s] 12%|‚ñà‚ñè        | 379/3257 [00:01<00:12, 239.72it/s] 12%|‚ñà‚ñè        | 404/3257 [00:01<00:12, 237.05it/s] 13%|‚ñà‚ñé        | 428/3257 [00:01<00:13, 217.52it/s] 14%|‚ñà‚ñç        | 451/3257 [00:01<00:12, 220.54it/s] 15%|‚ñà‚ñç        | 475/3257 [00:02<00:12, 225.66it/s] 15%|‚ñà‚ñå        | 501/3257 [00:02<00:11, 233.07it/s] 16%|‚ñà‚ñå        | 525/3257 [00:02<00:11, 233.84it/s] 17%|‚ñà‚ñã        | 549/3257 [00:02<00:12, 225.38it/s] 18%|‚ñà‚ñä        | 572/3257 [00:02<00:13, 198.33it/s] 18%|‚ñà‚ñä        | 600/3257 [00:02<00:12, 217.44it/s] 19%|‚ñà‚ñâ        | 623/3257 [00:02<00:11, 220.33it/s] 20%|‚ñà‚ñâ        | 648/3257 [00:02<00:11, 227.20it/s] 21%|‚ñà‚ñà        | 672/3257 [00:02<00:11, 222.45it/s] 21%|‚ñà‚ñà‚ñè       | 695/3257 [00:03<00:11, 222.76it/s] 22%|‚ñà‚ñà‚ñè       | 721/3257 [00:03<00:10, 232.51it/s] 23%|‚ñà‚ñà‚ñé       | 745/3257 [00:03<00:10, 232.33it/s] 24%|‚ñà‚ñà‚ñé       | 773/3257 [00:03<00:10, 241.91it/s] 25%|‚ñà‚ñà‚ñç       | 798/3257 [00:03<00:14, 165.36it/s] 25%|‚ñà‚ñà‚ñå       | 821/3257 [00:03<00:13, 178.37it/s] 26%|‚ñà‚ñà‚ñå       | 842/3257 [00:03<00:13, 184.72it/s] 27%|‚ñà‚ñà‚ñã       | 866/3257 [00:03<00:12, 198.38it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:04<00:11, 209.97it/s] 28%|‚ñà‚ñà‚ñä       | 920/3257 [00:04<00:10, 230.09it/s] 29%|‚ñà‚ñà‚ñâ       | 945/3257 [00:04<00:10, 228.78it/s] 30%|‚ñà‚ñà‚ñâ       | 970/3257 [00:04<00:09, 232.82it/s] 31%|‚ñà‚ñà‚ñà       | 994/3257 [00:04<00:09, 232.41it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1019/3257 [00:04<00:09, 235.71it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1043/3257 [00:04<00:09, 224.22it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1069/3257 [00:04<00:09, 233.80it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1093/3257 [00:04<00:09, 226.24it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1119/3257 [00:04<00:09, 231.18it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1143/3257 [00:05<00:09, 229.54it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1169/3257 [00:05<00:08, 237.92it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1193/3257 [00:05<00:09, 213.58it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1215/3257 [00:05<00:09, 209.35it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1241/3257 [00:05<00:09, 222.49it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1265/3257 [00:05<00:08, 226.30it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1288/3257 [00:05<00:09, 214.51it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1311/3257 [00:05<00:08, 217.50it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1336/3257 [00:05<00:08, 226.06it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1359/3257 [00:06<00:08, 223.92it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1382/3257 [00:06<00:08, 217.45it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1407/3257 [00:06<00:08, 225.40it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1434/3257 [00:06<00:07, 236.05it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1463/3257 [00:06<00:07, 250.50it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1490/3257 [00:06<00:06, 254.73it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1517/3257 [00:06<00:06, 256.70it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1543/3257 [00:06<00:07, 241.12it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1568/3257 [00:06<00:07, 239.16it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1593/3257 [00:07<00:07, 236.44it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:07<00:06, 238.72it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1643/3257 [00:07<00:06, 234.03it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1667/3257 [00:07<00:06, 229.25it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1690/3257 [00:07<00:06, 226.55it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:07<00:06, 229.81it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1739/3257 [00:07<00:07, 216.20it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1765/3257 [00:07<00:06, 226.65it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1792/3257 [00:07<00:06, 237.81it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1816/3257 [00:08<00:06, 233.20it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1842/3257 [00:08<00:05, 238.65it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:08<00:05, 243.96it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1894/3257 [00:08<00:05, 236.27it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1918/3257 [00:08<00:05, 234.71it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1944/3257 [00:08<00:05, 241.36it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1970/3257 [00:08<00:05, 246.06it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1995/3257 [00:08<00:05, 237.78it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2019/3257 [00:08<00:05, 233.38it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2043/3257 [00:09<00:08, 143.74it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2062/3257 [00:09<00:07, 152.07it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2087/3257 [00:09<00:06, 172.92it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2112/3257 [00:09<00:06, 188.38it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2134/3257 [00:09<00:05, 188.81it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2157/3257 [00:09<00:05, 199.12it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:09<00:05, 208.09it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:09<00:04, 216.55it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2229/3257 [00:10<00:04, 218.00it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2252/3257 [00:10<00:04, 217.40it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2275/3257 [00:10<00:04, 208.58it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2301/3257 [00:10<00:04, 220.27it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2329/3257 [00:10<00:03, 236.39it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2359/3257 [00:10<00:03, 253.05it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2385/3257 [00:10<00:03, 248.13it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2411/3257 [00:10<00:03, 242.75it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2436/3257 [00:10<00:03, 228.35it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2460/3257 [00:11<00:03, 231.37it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2484/3257 [00:11<00:03, 230.31it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2511/3257 [00:11<00:03, 240.95it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2537/3257 [00:11<00:02, 243.72it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2562/3257 [00:11<00:03, 228.14it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2586/3257 [00:11<00:03, 220.14it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2613/3257 [00:11<00:02, 233.11it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2638/3257 [00:11<00:02, 235.76it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2662/3257 [00:11<00:02, 221.66it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2688/3257 [00:12<00:02, 232.19it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2712/3257 [00:12<00:02, 212.92it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2738/3257 [00:12<00:02, 223.15it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2763/3257 [00:12<00:02, 228.28it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2788/3257 [00:12<00:02, 233.35it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2813/3257 [00:12<00:01, 233.93it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2837/3257 [00:12<00:01, 226.25it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2868/3257 [00:12<00:01, 248.92it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2894/3257 [00:12<00:01, 242.91it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2924/3257 [00:12<00:01, 257.60it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:13<00:01, 243.23it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:13<00:01, 244.69it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3003/3257 [00:13<00:01, 249.61it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3029/3257 [00:13<00:00, 250.83it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3061/3257 [00:13<00:00, 269.04it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3089/3257 [00:13<00:00, 266.08it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3120/3257 [00:13<00:00, 276.69it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3148/3257 [00:13<00:00, 259.37it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3175/3257 [00:13<00:00, 253.41it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3201/3257 [00:14<00:00, 245.64it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3226/3257 [00:14<00:00, 234.45it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:14<00:00, 237.72it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:14<00:00, 227.35it/s]
2023-02-07 18:55:36.022 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:55:36,023][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d49,n5,w2,mc3,s0.343068,t4>', 'datetime': '2023-02-07T18:55:36.023880', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:55:36,024][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:55:36,024][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:55:36,338][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 18:55:36,339][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:55:36,363][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 9372 unique words (71.76% of original 13061, drops 3689)', 'datetime': '2023-02-07T18:55:36.362996', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:55:36,363][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 3633898 word corpus (99.85% of original 3639370, drops 5472)', 'datetime': '2023-02-07T18:55:36.363432', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:55:36,394][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 18:55:36,394][gensim.models.word2vec][INFO] - sample=0.343068 downsamples 0 most-common words
[2023-02-07 18:55:36,395][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3633898 word corpus (100.0%% of prior 3633898)', 'datetime': '2023-02-07T18:55:36.395111', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:55:36,446][gensim.models.word2vec][INFO] - estimated required memory for 9372 words and 49 dimensions: 9649596 bytes
[2023-02-07 18:55:36,447][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:55:36,449][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 9372 vocabulary and 49 features, using sg=0 hs=0 sample=0.3430680032532035 negative=5 window=2 shrink_windows=True', 'datetime': '2023-02-07T18:55:36.449935', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:55:37,455][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 68.04% examples, 2522967 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:55:37,817][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3637155 effective words) took 1.4s, 2664213 effective words/s
[2023-02-07 18:55:38,819][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 64.35% examples, 2385529 words/s, in_qsize 6, out_qsize 0
[2023-02-07 18:55:39,315][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3637155 effective words) took 1.5s, 2431387 effective words/s
[2023-02-07 18:55:40,318][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 81.21% examples, 2976786 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:55:40,518][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3637155 effective words) took 1.2s, 3031071 effective words/s
[2023-02-07 18:55:41,528][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 87.23% examples, 3176260 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:55:41,697][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3637155 effective words) took 1.2s, 3091099 effective words/s
[2023-02-07 18:55:42,702][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 70.28% examples, 2604937 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:55:43,071][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3637155 effective words) took 1.4s, 2649221 effective words/s
[2023-02-07 18:55:44,083][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 86.31% examples, 3131161 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:55:44,219][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3637155 effective words) took 1.1s, 3171919 effective words/s
[2023-02-07 18:55:45,221][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 93.40% examples, 3413282 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:55:45,285][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3637155 effective words) took 1.1s, 3417005 effective words/s
[2023-02-07 18:55:46,287][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 83.11% examples, 3054102 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:55:46,466][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3637155 effective words) took 1.2s, 3084501 effective words/s
[2023-02-07 18:55:47,469][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 71.69% examples, 2653443 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:55:47,820][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3637155 effective words) took 1.4s, 2689124 effective words/s
[2023-02-07 18:55:48,823][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 70.28% examples, 2612347 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:55:49,208][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3637155 effective words) took 1.4s, 2623081 effective words/s
[2023-02-07 18:55:50,218][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 74.06% examples, 2714843 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:55:50,538][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3637155 effective words) took 1.3s, 2739239 effective words/s
[2023-02-07 18:55:51,540][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 82.87% examples, 3043398 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:55:51,739][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3637155 effective words) took 1.2s, 3030983 effective words/s
[2023-02-07 18:55:52,745][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 74.79% examples, 2754389 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:55:53,063][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3637155 effective words) took 1.3s, 2751157 effective words/s
[2023-02-07 18:55:54,068][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 77.86% examples, 2848966 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:55:54,340][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3637155 effective words) took 1.3s, 2852021 effective words/s
[2023-02-07 18:55:55,343][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 88.61% examples, 3236879 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:55:55,466][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3637155 effective words) took 1.1s, 3236663 effective words/s
[2023-02-07 18:55:55,467][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54557325 effective words) took 19.0s, 2868914 effective words/s', 'datetime': '2023-02-07T18:55:55.467061', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:55:55.468 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:55:57,113][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185512-5xdlx96c/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:55:57.113611', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:55:57,114][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:55:57,136][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185512-5xdlx96c/files/../tmp/embedding_model.pt
2023-02-07 18:55:57.136 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:55:58.142 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:55:58.528 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:55:59.517 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.4512184142644458, 'test_mae': 1.1402906026488364, 'test_r2': -2.676039458749151}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.76
wandb: percentage 0.28244
wandb:   test_mae 1.14029
wandb:   test_mse 2.45122
wandb:    test_r2 -2.67604
wandb: 
wandb: üöÄ View run eager-sweep-25 at: https://wandb.ai/xiaoqiz/mof2vec/runs/5xdlx96c
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_185512-5xdlx96c/logs
wandb: Agent Starting Run: 566132mm with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 266
wandb: 	model.gensim.alpha: 0.03679457028908976
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.3063690520267625
wandb: 	model.gensim.vector_size: 178
wandb: 	model.gensim.window: 9
wandb: 	model.sklearn.learning_rate: 0.9391520918265251
wandb: 	model.sklearn.max_depth: 8
wandb: 	model.sklearn.min_child_weight: 0.02710703016681106
wandb: 	model.sklearn.n_estimators: 2435
wandb: 	model.sklearn.num_leaves: 84
wandb: 	model.sklearn.reg_alpha: 0.009932330850405229
wandb: 	model.sklearn.reg_lambda: 0.04237356048359797
wandb: 	model.sklearn.subsample: 0.3179346772313803
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185610-566132mm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-26
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/566132mm
2023-02-07 18:56:18.456 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 18:56:18.457 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 266 for sweep.
2023-02-07 18:56:18.457 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.03679457028908976 for sweep.
2023-02-07 18:56:18.458 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 18:56:18.458 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 18:56:18.458 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3063690520267625 for sweep.
2023-02-07 18:56:18.458 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 178 for sweep.
2023-02-07 18:56:18.459 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 9 for sweep.
2023-02-07 18:56:18.459 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.9391520918265251 for sweep.
2023-02-07 18:56:18.459 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 8 for sweep.
2023-02-07 18:56:18.459 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.02710703016681106 for sweep.
2023-02-07 18:56:18.460 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2435 for sweep.
2023-02-07 18:56:18.460 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 84 for sweep.
2023-02-07 18:56:18.460 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.009932330850405229 for sweep.
2023-02-07 18:56:18.460 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.04237356048359797 for sweep.
2023-02-07 18:56:18.461 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3179346772313803 for sweep.
2023-02-07 18:56:18.461 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:56:18.469 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185610-566132mm/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 266, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 178, 'window': 9, 'min_count': 4, 'dm': 0, 'sample': 0.3063690520267625, 'workers': 4, 'alpha': 0.03679457028908976, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2435, 'max_depth': 8, 'num_leaves': 84, 'reg_alpha': 0.009932330850405229, 'reg_lambda': 0.04237356048359797, 'subsample': 0.3179346772313803, 'min_child_weight': 0.02710703016681106, 'n_jobs': 4, 'learning_rate': 0.9391520918265251}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 28/3257 [00:00<00:11, 272.99it/s]  2%|‚ñè         | 57/3257 [00:00<00:11, 282.64it/s]  3%|‚ñé         | 86/3257 [00:00<00:22, 143.70it/s]  3%|‚ñé         | 109/3257 [00:00<00:19, 163.20it/s]  4%|‚ñç         | 140/3257 [00:00<00:15, 200.83it/s]  5%|‚ñå         | 171/3257 [00:00<00:13, 228.74it/s]  6%|‚ñã         | 204/3257 [00:00<00:11, 255.65it/s]  7%|‚ñã         | 244/3257 [00:01<00:10, 294.38it/s]  8%|‚ñä         | 276/3257 [00:01<00:10, 295.85it/s] 10%|‚ñâ         | 310/3257 [00:01<00:09, 307.52it/s] 11%|‚ñà         | 342/3257 [00:01<00:09, 307.56it/s] 12%|‚ñà‚ñè        | 377/3257 [00:01<00:09, 315.35it/s] 13%|‚ñà‚ñé        | 410/3257 [00:01<00:08, 318.21it/s] 14%|‚ñà‚ñé        | 443/3257 [00:01<00:09, 304.22it/s] 15%|‚ñà‚ñç        | 474/3257 [00:01<00:09, 302.42it/s] 16%|‚ñà‚ñå        | 505/3257 [00:01<00:09, 290.64it/s] 16%|‚ñà‚ñã        | 535/3257 [00:02<00:09, 278.03it/s] 17%|‚ñà‚ñã        | 564/3257 [00:02<00:09, 278.16it/s] 18%|‚ñà‚ñä        | 596/3257 [00:02<00:09, 288.59it/s] 19%|‚ñà‚ñâ        | 632/3257 [00:02<00:08, 308.76it/s] 20%|‚ñà‚ñà        | 664/3257 [00:02<00:08, 309.52it/s] 21%|‚ñà‚ñà‚ñè       | 696/3257 [00:02<00:08, 310.64it/s] 22%|‚ñà‚ñà‚ñè       | 730/3257 [00:02<00:07, 318.14it/s] 23%|‚ñà‚ñà‚ñé       | 762/3257 [00:02<00:08, 307.40it/s] 25%|‚ñà‚ñà‚ñç       | 799/3257 [00:02<00:07, 322.60it/s] 26%|‚ñà‚ñà‚ñå       | 834/3257 [00:02<00:07, 328.64it/s] 27%|‚ñà‚ñà‚ñã       | 868/3257 [00:03<00:07, 330.70it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:03<00:06, 338.59it/s] 29%|‚ñà‚ñà‚ñâ       | 942/3257 [00:03<00:06, 347.26it/s] 30%|‚ñà‚ñà‚ñà       | 980/3257 [00:03<00:06, 356.42it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1019/3257 [00:03<00:06, 363.40it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1056/3257 [00:03<00:06, 349.27it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1092/3257 [00:03<00:06, 337.94it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1126/3257 [00:03<00:06, 333.55it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1160/3257 [00:03<00:06, 309.75it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1192/3257 [00:04<00:07, 289.91it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1222/3257 [00:04<00:07, 282.93it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1257/3257 [00:04<00:06, 299.29it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1288/3257 [00:04<00:06, 296.78it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1323/3257 [00:04<00:06, 311.58it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1357/3257 [00:04<00:05, 318.49it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1390/3257 [00:04<00:08, 210.27it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1431/3257 [00:04<00:07, 252.02it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1469/3257 [00:05<00:06, 281.24it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1507/3257 [00:05<00:05, 303.78it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1541/3257 [00:05<00:05, 304.12it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1575/3257 [00:05<00:05, 310.79it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:05<00:04, 334.47it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1650/3257 [00:05<00:04, 329.32it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1684/3257 [00:05<00:04, 326.84it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1719/3257 [00:05<00:04, 332.26it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1753/3257 [00:05<00:04, 322.35it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1790/3257 [00:06<00:04, 335.28it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1824/3257 [00:06<00:04, 322.20it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1858/3257 [00:06<00:04, 324.73it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1893/3257 [00:06<00:04, 330.63it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1928/3257 [00:06<00:03, 333.72it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1969/3257 [00:06<00:03, 353.68it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2005/3257 [00:06<00:03, 345.53it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2040/3257 [00:06<00:03, 336.84it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2074/3257 [00:06<00:03, 331.28it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2108/3257 [00:06<00:03, 329.58it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2142/3257 [00:07<00:03, 304.94it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2173/3257 [00:07<00:03, 301.90it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2208/3257 [00:07<00:03, 313.97it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2240/3257 [00:07<00:03, 313.60it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2272/3257 [00:07<00:03, 311.63it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2309/3257 [00:07<00:02, 326.86it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2353/3257 [00:07<00:02, 356.96it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2392/3257 [00:07<00:02, 366.45it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2429/3257 [00:07<00:02, 362.81it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2466/3257 [00:08<00:02, 363.82it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2505/3257 [00:08<00:02, 371.43it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2543/3257 [00:08<00:01, 365.03it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2580/3257 [00:08<00:01, 354.37it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2622/3257 [00:08<00:01, 372.56it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2660/3257 [00:08<00:01, 360.14it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2697/3257 [00:08<00:01, 360.62it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2734/3257 [00:08<00:01, 353.40it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2770/3257 [00:08<00:01, 353.75it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2808/3257 [00:08<00:01, 360.90it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2845/3257 [00:09<00:01, 224.13it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2888/3257 [00:09<00:01, 265.29it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2925/3257 [00:09<00:01, 288.76it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2960/3257 [00:09<00:01, 295.64it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2995/3257 [00:09<00:00, 309.05it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3032/3257 [00:09<00:00, 324.39it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3071/3257 [00:09<00:00, 341.82it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3110/3257 [00:09<00:00, 353.82it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3147/3257 [00:10<00:00, 351.50it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3184/3257 [00:10<00:00, 351.08it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3220/3257 [00:10<00:00, 348.70it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:10<00:00, 313.23it/s]
2023-02-07 18:56:29.076 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:56:29,077][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d178,n5,mc4,s0.306369,t4>', 'datetime': '2023-02-07T18:56:29.077056', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:56:29,077][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:56:29,078][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:56:29,206][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 18:56:29,206][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:56:29,208][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 742 unique words (80.30% of original 924, drops 182)', 'datetime': '2023-02-07T18:56:29.208429', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:56:29,208][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 1455401 word corpus (99.98% of original 1455748, drops 347)', 'datetime': '2023-02-07T18:56:29.208647', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:56:29,212][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 18:56:29,212][gensim.models.word2vec][INFO] - sample=0.306369 downsamples 0 most-common words
[2023-02-07 18:56:29,212][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455401 word corpus (100.0%% of prior 1455401)', 'datetime': '2023-02-07T18:56:29.212632', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:56:29,217][gensim.models.word2vec][INFO] - estimated required memory for 742 words and 178 dimensions: 4397992 bytes
[2023-02-07 18:56:29,217][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:56:29,220][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 742 vocabulary and 178 features, using sg=1 hs=0 sample=0.3063690520267625 negative=5 window=9 shrink_windows=True', 'datetime': '2023-02-07T18:56:29.220239', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:56:29,866][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458658 effective words) took 0.6s, 2264237 effective words/s
[2023-02-07 18:56:30,407][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458658 effective words) took 0.5s, 2705875 effective words/s
[2023-02-07 18:56:30,948][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458658 effective words) took 0.5s, 2703747 effective words/s
[2023-02-07 18:56:31,498][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458658 effective words) took 0.5s, 2660502 effective words/s
[2023-02-07 18:56:32,034][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458658 effective words) took 0.5s, 2729418 effective words/s
[2023-02-07 18:56:32,403][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458658 effective words) took 0.4s, 3966919 effective words/s
[2023-02-07 18:56:32,802][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458658 effective words) took 0.4s, 3670164 effective words/s
[2023-02-07 18:56:33,368][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458658 effective words) took 0.6s, 2581279 effective words/s
[2023-02-07 18:56:33,759][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458658 effective words) took 0.4s, 3747871 effective words/s
[2023-02-07 18:56:34,152][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458658 effective words) took 0.4s, 3712872 effective words/s
[2023-02-07 18:56:34,554][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458658 effective words) took 0.4s, 3642920 effective words/s
[2023-02-07 18:56:35,128][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458658 effective words) took 0.6s, 2549660 effective words/s
[2023-02-07 18:56:35,728][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458658 effective words) took 0.6s, 2437445 effective words/s
[2023-02-07 18:56:36,120][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458658 effective words) took 0.4s, 3728153 effective words/s
[2023-02-07 18:56:36,732][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458658 effective words) took 0.6s, 2391883 effective words/s
[2023-02-07 18:56:36,732][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 21836220 raw words (21879870 effective words) took 7.5s, 2912582 effective words/s', 'datetime': '2023-02-07T18:56:36.732820', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:56:36.733 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:56:37,342][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185610-566132mm/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:56:37.342564', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:56:37,343][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:56:37,352][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185610-566132mm/files/../tmp/embedding_model.pt
2023-02-07 18:56:37.353 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:56:38.658 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:56:39.159 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:56:40.632 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2816346268660164, 'test_mae': 1.0799694013013361, 'test_r2': -2.16066850561039}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.76
wandb: percentage 0.19697
wandb:   test_mae 1.07997
wandb:   test_mse 2.28163
wandb:    test_r2 -2.16067
wandb: 
wandb: üöÄ View run balmy-sweep-26 at: https://wandb.ai/xiaoqiz/mof2vec/runs/566132mm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_185610-566132mm/logs
wandb: Agent Starting Run: voilyhju with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 373
wandb: 	model.gensim.alpha: 0.22285004202343808
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 8
wandb: 	model.gensim.sample: 0.3870214115860875
wandb: 	model.gensim.vector_size: 233
wandb: 	model.gensim.window: 14
wandb: 	model.sklearn.learning_rate: 0.0019671666338440877
wandb: 	model.sklearn.max_depth: 20
wandb: 	model.sklearn.min_child_weight: 0.05822270444467512
wandb: 	model.sklearn.n_estimators: 660
wandb: 	model.sklearn.num_leaves: 65
wandb: 	model.sklearn.reg_alpha: 0.2754522774894252
wandb: 	model.sklearn.reg_lambda: 0.0996208871251472
wandb: 	model.sklearn.subsample: 0.6400825892773787
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185653-voilyhju
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-27
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/voilyhju
2023-02-07 18:57:02.358 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 18:57:02.359 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 373 for sweep.
2023-02-07 18:57:02.359 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.22285004202343808 for sweep.
2023-02-07 18:57:02.359 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 18:57:02.360 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 8 for sweep.
2023-02-07 18:57:02.360 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3870214115860875 for sweep.
2023-02-07 18:57:02.361 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 233 for sweep.
2023-02-07 18:57:02.361 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 14 for sweep.
2023-02-07 18:57:02.361 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0019671666338440877 for sweep.
2023-02-07 18:57:02.361 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 20 for sweep.
2023-02-07 18:57:02.361 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05822270444467512 for sweep.
2023-02-07 18:57:02.362 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 660 for sweep.
2023-02-07 18:57:02.362 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 65 for sweep.
2023-02-07 18:57:02.362 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.2754522774894252 for sweep.
2023-02-07 18:57:02.362 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.0996208871251472 for sweep.
2023-02-07 18:57:02.363 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6400825892773787 for sweep.
2023-02-07 18:57:02.363 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:57:02.369 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185653-voilyhju/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 373, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 233, 'window': 14, 'min_count': 8, 'dm': 1, 'sample': 0.3870214115860875, 'workers': 4, 'alpha': 0.22285004202343808, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 660, 'max_depth': 20, 'num_leaves': 65, 'reg_alpha': 0.2754522774894252, 'reg_lambda': 0.0996208871251472, 'subsample': 0.6400825892773787, 'min_child_weight': 0.05822270444467512, 'n_jobs': 4, 'learning_rate': 0.0019671666338440877}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 26/3257 [00:00<00:12, 256.60it/s]  2%|‚ñè         | 52/3257 [00:00<00:12, 254.88it/s]  2%|‚ñè         | 80/3257 [00:00<00:12, 264.48it/s]  3%|‚ñé         | 110/3257 [00:00<00:11, 276.97it/s]  5%|‚ñç         | 148/3257 [00:00<00:09, 312.26it/s]  6%|‚ñå         | 180/3257 [00:00<00:09, 314.10it/s]  7%|‚ñã         | 216/3257 [00:00<00:09, 327.82it/s]  8%|‚ñä         | 253/3257 [00:00<00:08, 339.68it/s]  9%|‚ñâ         | 291/3257 [00:00<00:08, 351.01it/s] 10%|‚ñà         | 328/3257 [00:01<00:08, 356.65it/s] 11%|‚ñà         | 364/3257 [00:01<00:08, 354.85it/s] 12%|‚ñà‚ñè        | 400/3257 [00:01<00:08, 344.89it/s] 13%|‚ñà‚ñé        | 435/3257 [00:01<00:08, 327.72it/s] 14%|‚ñà‚ñç        | 472/3257 [00:01<00:08, 338.13it/s] 16%|‚ñà‚ñå        | 509/3257 [00:01<00:07, 346.67it/s] 17%|‚ñà‚ñã        | 544/3257 [00:01<00:07, 346.44it/s] 18%|‚ñà‚ñä        | 579/3257 [00:01<00:08, 319.76it/s] 19%|‚ñà‚ñâ        | 612/3257 [00:01<00:08, 319.79it/s] 20%|‚ñà‚ñâ        | 645/3257 [00:01<00:08, 321.29it/s] 21%|‚ñà‚ñà        | 680/3257 [00:02<00:07, 327.82it/s] 22%|‚ñà‚ñà‚ñè       | 715/3257 [00:02<00:07, 331.59it/s] 23%|‚ñà‚ñà‚ñé       | 749/3257 [00:02<00:07, 322.81it/s] 24%|‚ñà‚ñà‚ñç       | 785/3257 [00:02<00:10, 230.38it/s] 25%|‚ñà‚ñà‚ñå       | 820/3257 [00:02<00:09, 256.60it/s] 26%|‚ñà‚ñà‚ñå       | 850/3257 [00:02<00:09, 266.22it/s] 27%|‚ñà‚ñà‚ñã       | 884/3257 [00:02<00:08, 283.22it/s] 28%|‚ñà‚ñà‚ñä       | 922/3257 [00:02<00:07, 307.64it/s] 29%|‚ñà‚ñà‚ñâ       | 957/3257 [00:03<00:07, 319.11it/s] 30%|‚ñà‚ñà‚ñà       | 992/3257 [00:03<00:06, 327.63it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:03<00:06, 333.53it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1062/3257 [00:03<00:06, 337.14it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1099/3257 [00:03<00:06, 344.82it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1136/3257 [00:03<00:06, 350.75it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:03<00:05, 355.33it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1209/3257 [00:03<00:06, 339.98it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1248/3257 [00:03<00:05, 352.84it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1284/3257 [00:03<00:05, 353.77it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1323/3257 [00:04<00:05, 362.82it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1362/3257 [00:04<00:05, 369.05it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1400/3257 [00:04<00:05, 356.32it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1436/3257 [00:04<00:05, 356.40it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1479/3257 [00:04<00:04, 375.57it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:04<00:04, 381.87it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1558/3257 [00:04<00:04, 371.29it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1596/3257 [00:04<00:04, 370.38it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:04<00:04, 362.33it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1671/3257 [00:05<00:04, 343.09it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1707/3257 [00:05<00:04, 344.97it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1742/3257 [00:05<00:04, 331.90it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1779/3257 [00:05<00:04, 340.43it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1814/3257 [00:05<00:04, 303.95it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1852/3257 [00:05<00:04, 321.04it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1889/3257 [00:05<00:04, 334.04it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1925/3257 [00:05<00:03, 338.10it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1970/3257 [00:05<00:03, 369.79it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2008/3257 [00:06<00:05, 247.72it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2044/3257 [00:06<00:04, 270.54it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2077/3257 [00:06<00:04, 283.31it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2112/3257 [00:06<00:03, 298.29it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2145/3257 [00:06<00:03, 302.04it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2182/3257 [00:06<00:03, 318.75it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2220/3257 [00:06<00:03, 334.50it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2255/3257 [00:06<00:02, 335.45it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2291/3257 [00:07<00:02, 339.97it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2326/3257 [00:07<00:02, 338.17it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2366/3257 [00:07<00:02, 355.10it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2402/3257 [00:07<00:02, 350.82it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2438/3257 [00:07<00:02, 329.39it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2477/3257 [00:07<00:02, 345.85it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2520/3257 [00:07<00:01, 368.67it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2559/3257 [00:07<00:01, 371.35it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2597/3257 [00:07<00:01, 358.52it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2639/3257 [00:07<00:01, 375.09it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2677/3257 [00:08<00:01, 367.91it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2714/3257 [00:08<00:01, 365.59it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2758/3257 [00:08<00:01, 385.18it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2802/3257 [00:08<00:01, 399.77it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2843/3257 [00:08<00:01, 363.99it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2883/3257 [00:08<00:01, 370.10it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2921/3257 [00:08<00:00, 364.77it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2958/3257 [00:08<00:00, 357.94it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3002/3257 [00:08<00:00, 378.94it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3045/3257 [00:09<00:00, 392.41it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3087/3257 [00:09<00:00, 396.94it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3130/3257 [00:09<00:00, 405.17it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3171/3257 [00:09<00:00, 392.64it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3211/3257 [00:09<00:00, 390.30it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3255/3257 [00:09<00:00, 402.60it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 339.88it/s]
2023-02-07 18:57:12.164 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:57:12,165][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d233,n5,w14,mc8,s0.387021,t4>', 'datetime': '2023-02-07T18:57:12.165262', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:57:12,165][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:57:12,165][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:57:12,295][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 18:57:12,295][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:57:12,297][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 retains 515 unique words (55.74% of original 924, drops 409)', 'datetime': '2023-02-07T18:57:12.297248', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:57:12,297][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 leaves 1454326 word corpus (99.90% of original 1455748, drops 1422)', 'datetime': '2023-02-07T18:57:12.297461', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:57:12,299][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 18:57:12,299][gensim.models.word2vec][INFO] - sample=0.387021 downsamples 0 most-common words
[2023-02-07 18:57:12,299][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1454326 word corpus (100.0%% of prior 1454326)', 'datetime': '2023-02-07T18:57:12.299552', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:57:12,304][gensim.models.word2vec][INFO] - estimated required memory for 515 words and 233 dimensions: 4904384 bytes
[2023-02-07 18:57:12,304][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:57:12,308][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 515 vocabulary and 233 features, using sg=0 hs=0 sample=0.3870214115860875 negative=5 window=14 shrink_windows=True', 'datetime': '2023-02-07T18:57:12.307996', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:57:13,335][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 45.90% examples, 664594 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:57:14,338][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 80.04% examples, 579474 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:57:14,834][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1457583 effective words) took 2.5s, 577544 effective words/s
[2023-02-07 18:57:15,840][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 32.05% examples, 467984 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:57:16,857][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 66.56% examples, 490502 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:57:17,699][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1457583 effective words) took 2.9s, 509189 effective words/s
[2023-02-07 18:57:18,710][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 33.13% examples, 483235 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:57:19,713][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 67.21% examples, 497449 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:57:20,360][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1457583 effective words) took 2.7s, 548161 effective words/s
[2023-02-07 18:57:21,362][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 58.06% examples, 864304 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:57:21,998][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1457583 effective words) took 1.6s, 891326 effective words/s
[2023-02-07 18:57:23,021][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 59.69% examples, 864641 words/s, in_qsize 8, out_qsize 1
[2023-02-07 18:57:23,642][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1457583 effective words) took 1.6s, 887161 effective words/s
[2023-02-07 18:57:24,649][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 36.48% examples, 542272 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:57:25,651][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 89.07% examples, 652810 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:57:25,913][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1457583 effective words) took 2.3s, 642178 effective words/s
[2023-02-07 18:57:26,929][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 59.26% examples, 872154 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:57:27,544][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1457583 effective words) took 1.6s, 894695 effective words/s
[2023-02-07 18:57:28,563][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 59.26% examples, 872183 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:57:29,166][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1457583 effective words) took 1.6s, 901704 effective words/s
[2023-02-07 18:57:30,194][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 62.39% examples, 898713 words/s, in_qsize 6, out_qsize 1
[2023-02-07 18:57:30,743][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1457583 effective words) took 1.6s, 925772 effective words/s
[2023-02-07 18:57:31,747][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 57.14% examples, 853769 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:57:32,411][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1457583 effective words) took 1.7s, 875523 effective words/s
[2023-02-07 18:57:33,417][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 57.91% examples, 861897 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:57:34,055][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1457583 effective words) took 1.6s, 888226 effective words/s
[2023-02-07 18:57:35,060][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 32.51% examples, 478331 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:57:36,061][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 92.63% examples, 678583 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:57:36,155][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1457583 effective words) took 2.1s, 694652 effective words/s
[2023-02-07 18:57:37,171][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 32.51% examples, 470426 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:57:38,178][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 65.34% examples, 480905 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:57:39,171][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1457583 effective words) took 3.0s, 483699 effective words/s
[2023-02-07 18:57:40,174][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 33.07% examples, 485378 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:57:41,180][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 66.41% examples, 493127 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:57:41,738][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1457583 effective words) took 2.6s, 568326 effective words/s
[2023-02-07 18:57:42,747][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 59.26% examples, 878580 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:57:43,376][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1457583 effective words) took 1.6s, 891128 effective words/s
[2023-02-07 18:57:43,377][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 21836220 raw words (21863745 effective words) took 31.1s, 703704 effective words/s', 'datetime': '2023-02-07T18:57:43.377925', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:57:43.378 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:57:44,523][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185653-voilyhju/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:57:44.523082', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:57:44,524][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:57:44,541][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185653-voilyhju/files/../tmp/embedding_model.pt
2023-02-07 18:57:44.542 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:57:46.202 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:57:46.799 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:57:48.360 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.9521687059560766, 'test_mae': 1.2960954956850324, 'test_r2': -5.66480640753242}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.0
wandb: percentage 0.44264
wandb:   test_mae 1.2961
wandb:   test_mse 2.95217
wandb:    test_r2 -5.66481
wandb: 
wandb: üöÄ View run robust-sweep-27 at: https://wandb.ai/xiaoqiz/mof2vec/runs/voilyhju
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_185653-voilyhju/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ix0yebbh with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 759
wandb: 	model.gensim.alpha: 0.0017788954363694692
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.31769982601219243
wandb: 	model.gensim.vector_size: 269
wandb: 	model.gensim.window: 8
wandb: 	model.sklearn.learning_rate: 0.00036444829915926015
wandb: 	model.sklearn.max_depth: 58
wandb: 	model.sklearn.min_child_weight: 0.07914543350507125
wandb: 	model.sklearn.n_estimators: 2802
wandb: 	model.sklearn.num_leaves: 156
wandb: 	model.sklearn.reg_alpha: 0.23846142103496404
wandb: 	model.sklearn.reg_lambda: 0.02561515868184193
wandb: 	model.sklearn.subsample: 0.8183675306688099
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185808-ix0yebbh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-28
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/ix0yebbh
2023-02-07 18:58:15.985 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 18:58:15.986 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 759 for sweep.
2023-02-07 18:58:15.986 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0017788954363694692 for sweep.
2023-02-07 18:58:15.987 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 18:58:15.987 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 18:58:15.987 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.31769982601219243 for sweep.
2023-02-07 18:58:15.987 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 269 for sweep.
2023-02-07 18:58:15.987 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 8 for sweep.
2023-02-07 18:58:15.988 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.00036444829915926015 for sweep.
2023-02-07 18:58:15.988 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 58 for sweep.
2023-02-07 18:58:15.988 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.07914543350507125 for sweep.
2023-02-07 18:58:15.989 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2802 for sweep.
2023-02-07 18:58:15.989 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 156 for sweep.
2023-02-07 18:58:15.989 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.23846142103496404 for sweep.
2023-02-07 18:58:15.989 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.02561515868184193 for sweep.
2023-02-07 18:58:15.991 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8183675306688099 for sweep.
2023-02-07 18:58:15.991 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:58:16.002 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185808-ix0yebbh/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 759, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 269, 'window': 8, 'min_count': 5, 'dm': 1, 'sample': 0.31769982601219243, 'workers': 4, 'alpha': 0.0017788954363694692, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2802, 'max_depth': 58, 'num_leaves': 156, 'reg_alpha': 0.23846142103496404, 'reg_lambda': 0.02561515868184193, 'subsample': 0.8183675306688099, 'min_child_weight': 0.07914543350507125, 'n_jobs': 4, 'learning_rate': 0.00036444829915926015}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 21/3257 [00:00<00:15, 207.13it/s]  1%|‚ñè         | 48/3257 [00:00<00:13, 243.51it/s]  2%|‚ñè         | 73/3257 [00:00<00:20, 154.43it/s]  3%|‚ñé         | 99/3257 [00:00<00:17, 184.12it/s]  4%|‚ñç         | 125/3257 [00:00<00:15, 204.28it/s]  5%|‚ñç         | 155/3257 [00:00<00:13, 230.93it/s]  6%|‚ñå         | 180/3257 [00:00<00:13, 231.30it/s]  6%|‚ñã         | 207/3257 [00:00<00:12, 241.18it/s]  7%|‚ñã         | 240/3257 [00:01<00:11, 265.71it/s]  8%|‚ñä         | 268/3257 [00:01<00:11, 262.25it/s]  9%|‚ñâ         | 300/3257 [00:01<00:10, 276.00it/s] 10%|‚ñà         | 332/3257 [00:01<00:10, 284.04it/s] 11%|‚ñà         | 361/3257 [00:01<00:10, 283.30it/s] 12%|‚ñà‚ñè        | 390/3257 [00:01<00:10, 268.55it/s] 13%|‚ñà‚ñé        | 420/3257 [00:01<00:10, 271.95it/s] 14%|‚ñà‚ñç        | 448/3257 [00:01<00:11, 249.68it/s] 15%|‚ñà‚ñç        | 475/3257 [00:01<00:10, 254.16it/s] 15%|‚ñà‚ñå        | 503/3257 [00:02<00:10, 259.59it/s] 16%|‚ñà‚ñã        | 530/3257 [00:02<00:10, 255.06it/s] 17%|‚ñà‚ñã        | 558/3257 [00:02<00:10, 251.17it/s] 18%|‚ñà‚ñä        | 584/3257 [00:02<00:11, 242.39it/s] 19%|‚ñà‚ñâ        | 614/3257 [00:02<00:10, 256.92it/s] 20%|‚ñà‚ñâ        | 640/3257 [00:02<00:10, 251.90it/s] 20%|‚ñà‚ñà        | 666/3257 [00:02<00:10, 242.99it/s] 21%|‚ñà‚ñà        | 691/3257 [00:02<00:11, 228.44it/s] 22%|‚ñà‚ñà‚ñè       | 715/3257 [00:02<00:11, 221.12it/s] 23%|‚ñà‚ñà‚ñé       | 738/3257 [00:03<00:12, 209.57it/s] 24%|‚ñà‚ñà‚ñé       | 767/3257 [00:03<00:10, 230.12it/s] 24%|‚ñà‚ñà‚ñç       | 791/3257 [00:03<00:10, 229.68it/s] 25%|‚ñà‚ñà‚ñå       | 815/3257 [00:03<00:10, 230.55it/s] 26%|‚ñà‚ñà‚ñå       | 839/3257 [00:03<00:10, 226.13it/s] 26%|‚ñà‚ñà‚ñã       | 862/3257 [00:03<00:10, 221.77it/s] 27%|‚ñà‚ñà‚ñã       | 885/3257 [00:03<00:11, 212.75it/s] 28%|‚ñà‚ñà‚ñä       | 912/3257 [00:03<00:10, 226.20it/s] 29%|‚ñà‚ñà‚ñä       | 936/3257 [00:03<00:10, 229.05it/s] 30%|‚ñà‚ñà‚ñâ       | 964/3257 [00:04<00:09, 241.74it/s] 30%|‚ñà‚ñà‚ñà       | 989/3257 [00:04<00:09, 241.28it/s] 31%|‚ñà‚ñà‚ñà       | 1014/3257 [00:04<00:09, 237.41it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1038/3257 [00:04<00:09, 234.39it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1062/3257 [00:04<00:09, 235.25it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1088/3257 [00:04<00:09, 239.22it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1115/3257 [00:04<00:08, 247.05it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1140/3257 [00:04<00:08, 238.12it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1166/3257 [00:04<00:08, 242.96it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:05<00:13, 149.74it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1213/3257 [00:05<00:12, 162.80it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1241/3257 [00:05<00:10, 188.26it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1265/3257 [00:05<00:10, 198.65it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1288/3257 [00:05<00:10, 192.12it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1313/3257 [00:05<00:09, 204.12it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1341/3257 [00:05<00:08, 222.69it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1365/3257 [00:05<00:08, 225.27it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1389/3257 [00:06<00:08, 225.25it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1422/3257 [00:06<00:07, 252.12it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1449/3257 [00:06<00:07, 256.28it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1478/3257 [00:06<00:06, 265.94it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1509/3257 [00:06<00:06, 276.18it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1537/3257 [00:06<00:06, 254.10it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1563/3257 [00:06<00:06, 250.50it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1589/3257 [00:06<00:06, 250.07it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1617/3257 [00:06<00:06, 255.79it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1643/3257 [00:07<00:06, 252.14it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1669/3257 [00:07<00:06, 243.94it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1694/3257 [00:07<00:06, 241.22it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1723/3257 [00:07<00:06, 253.79it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1749/3257 [00:07<00:06, 246.12it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1777/3257 [00:07<00:05, 253.49it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:07<00:05, 259.24it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1832/3257 [00:07<00:05, 250.40it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1859/3257 [00:07<00:05, 255.87it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1889/3257 [00:07<00:05, 266.31it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:08<00:05, 263.49it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1948/3257 [00:08<00:04, 279.22it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1978/3257 [00:08<00:04, 280.25it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2007/3257 [00:08<00:04, 280.35it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2036/3257 [00:08<00:04, 280.43it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2065/3257 [00:08<00:04, 257.33it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2093/3257 [00:08<00:04, 261.87it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:08<00:04, 250.78it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2146/3257 [00:08<00:04, 242.58it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2176/3257 [00:09<00:04, 257.90it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2203/3257 [00:09<00:04, 257.57it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2229/3257 [00:09<00:04, 254.35it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2255/3257 [00:09<00:04, 245.34it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2280/3257 [00:09<00:04, 240.22it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2308/3257 [00:09<00:03, 249.47it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2341/3257 [00:09<00:03, 271.71it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2372/3257 [00:09<00:03, 281.78it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2403/3257 [00:09<00:02, 288.16it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2432/3257 [00:10<00:03, 266.12it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2459/3257 [00:10<00:04, 169.67it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2487/3257 [00:10<00:04, 191.72it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2519/3257 [00:10<00:03, 220.29it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2549/3257 [00:10<00:02, 238.85it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2577/3257 [00:10<00:02, 236.13it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2605/3257 [00:10<00:02, 246.61it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2638/3257 [00:10<00:02, 267.13it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2667/3257 [00:11<00:02, 260.32it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2695/3257 [00:11<00:02, 257.65it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2722/3257 [00:11<00:02, 240.64it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2754/3257 [00:11<00:01, 261.17it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2781/3257 [00:11<00:01, 260.23it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2813/3257 [00:11<00:01, 273.47it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2841/3257 [00:11<00:01, 265.66it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2880/3257 [00:11<00:01, 298.64it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2911/3257 [00:11<00:01, 282.90it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2940/3257 [00:12<00:01, 282.94it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2969/3257 [00:12<00:01, 271.45it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2997/3257 [00:12<00:00, 270.89it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3026/3257 [00:12<00:00, 275.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3061/3257 [00:12<00:00, 295.35it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3092/3257 [00:12<00:00, 298.31it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3129/3257 [00:12<00:00, 316.16it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3161/3257 [00:12<00:00, 304.72it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3192/3257 [00:12<00:00, 305.46it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3223/3257 [00:13<00:00, 297.95it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 309.29it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 247.86it/s]
2023-02-07 18:58:29.544 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:58:29,545][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d269,n5,w8,mc5,s0.3177,t4>', 'datetime': '2023-02-07T18:58:29.545857', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:58:29,546][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:58:29,546][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:58:29,823][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 18:58:29,823][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:58:29,833][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 3705 unique words (55.61% of original 6662, drops 2957)', 'datetime': '2023-02-07T18:58:29.833635', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:58:29,835][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 2904334 word corpus (99.75% of original 2911496, drops 7162)', 'datetime': '2023-02-07T18:58:29.834978', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:58:29,847][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 18:58:29,848][gensim.models.word2vec][INFO] - sample=0.3177 downsamples 0 most-common words
[2023-02-07 18:58:29,848][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2904334 word corpus (100.0%% of prior 2904334)', 'datetime': '2023-02-07T18:58:29.848576', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:58:29,870][gensim.models.word2vec][INFO] - estimated required memory for 3705 words and 269 dimensions: 13981592 bytes
[2023-02-07 18:58:29,870][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:58:29,878][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 3705 vocabulary and 269 features, using sg=0 hs=0 sample=0.31769982601219243 negative=5 window=8 shrink_windows=True', 'datetime': '2023-02-07T18:58:29.878391', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:58:30,884][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 36.05% examples, 1060526 words/s, in_qsize 5, out_qsize 0
[2023-02-07 18:58:31,891][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 74.15% examples, 1089915 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:32,489][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2907591 effective words) took 2.6s, 1114720 effective words/s
[2023-02-07 18:58:33,494][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 39.12% examples, 1161455 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:34,497][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 79.52% examples, 1168101 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:58:34,980][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2907591 effective words) took 2.5s, 1168544 effective words/s
[2023-02-07 18:58:35,984][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 52.29% examples, 1551647 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:36,833][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2907591 effective words) took 1.9s, 1570295 effective words/s
[2023-02-07 18:58:37,836][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 37.70% examples, 1126544 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:58:38,836][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 78.63% examples, 1155955 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:39,345][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2907591 effective words) took 2.5s, 1158593 effective words/s
[2023-02-07 18:58:40,350][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 39.27% examples, 1170994 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:41,353][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 80.69% examples, 1182370 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:41,787][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2907591 effective words) took 2.4s, 1191936 effective words/s
[2023-02-07 18:58:42,796][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 39.79% examples, 1183757 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:43,801][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 79.77% examples, 1164834 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:44,309][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2907591 effective words) took 2.5s, 1153864 effective words/s
[2023-02-07 18:58:45,330][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 38.90% examples, 1144977 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:58:46,340][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 79.28% examples, 1150704 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:46,833][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2907591 effective words) took 2.5s, 1153757 effective words/s
[2023-02-07 18:58:47,844][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 39.12% examples, 1160757 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:48,842][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 77.86% examples, 1139563 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:49,368][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2907591 effective words) took 2.5s, 1148584 effective words/s
[2023-02-07 18:58:50,384][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 37.95% examples, 1111488 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:51,393][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 77.86% examples, 1129794 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:51,913][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2907591 effective words) took 2.5s, 1143542 effective words/s
[2023-02-07 18:58:52,930][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 39.02% examples, 1149180 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:58:53,932][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 80.01% examples, 1165913 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:54,378][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2907591 effective words) took 2.5s, 1180716 effective words/s
[2023-02-07 18:58:55,392][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 53.30% examples, 1573209 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:56,401][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 97.30% examples, 1402452 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:56,450][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2907591 effective words) took 2.1s, 1403928 effective words/s
[2023-02-07 18:58:57,460][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 40.13% examples, 1190787 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:58,467][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 81.82% examples, 1190979 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:58:58,896][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2907591 effective words) took 2.4s, 1189838 effective words/s
[2023-02-07 18:58:59,902][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 40.47% examples, 1206356 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:59:00,904][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 85.78% examples, 1253525 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:59:01,178][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2907591 effective words) took 2.3s, 1275537 effective words/s
[2023-02-07 18:59:02,186][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 39.27% examples, 1168133 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:59:03,189][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 81.21% examples, 1185706 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:59:03,623][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2907591 effective words) took 2.4s, 1190252 effective words/s
[2023-02-07 18:59:04,627][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 52.99% examples, 1573083 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:59:05,436][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2907591 effective words) took 1.8s, 1607213 effective words/s
[2023-02-07 18:59:05,438][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 43672440 raw words (43613865 effective words) took 35.6s, 1226526 effective words/s', 'datetime': '2023-02-07T18:59:05.438245', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 18:59:05.438 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 18:59:07,335][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185808-ix0yebbh/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T18:59:07.335453', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 18:59:07,337][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 18:59:07,359][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185808-ix0yebbh/files/../tmp/embedding_model.pt
2023-02-07 18:59:07.359 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 18:59:09.105 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 18:59:09.748 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 18:59:12.097 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.695731788873439, 'test_mae': 1.2374419135690033, 'test_r2': -3.1157492530049495}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.4
wandb: percentage 0.44386
wandb:   test_mae 1.23744
wandb:   test_mse 2.69573
wandb:    test_r2 -3.11575
wandb: 
wandb: üöÄ View run upbeat-sweep-28 at: https://wandb.ai/xiaoqiz/mof2vec/runs/ix0yebbh
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_185808-ix0yebbh/logs
wandb: Agent Starting Run: lksam6du with config:
wandb: 	data.data.wl_step: 5
wandb: 	data.nn.batch_size: 119
wandb: 	model.gensim.alpha: 0.0003607724746741335
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.7807904400167742
wandb: 	model.gensim.vector_size: 245
wandb: 	model.gensim.window: 8
wandb: 	model.sklearn.learning_rate: 0.0011766978156404963
wandb: 	model.sklearn.max_depth: 53
wandb: 	model.sklearn.min_child_weight: 0.059733357180404574
wandb: 	model.sklearn.n_estimators: 4531
wandb: 	model.sklearn.num_leaves: 295
wandb: 	model.sklearn.reg_alpha: 0.12875355094865606
wandb: 	model.sklearn.reg_lambda: 0.08514534552504888
wandb: 	model.sklearn.subsample: 0.3964997271295817
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185926-lksam6du
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-sweep-29
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/lksam6du
2023-02-07 18:59:34.259 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 5 for sweep.
2023-02-07 18:59:34.259 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 119 for sweep.
2023-02-07 18:59:34.260 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0003607724746741335 for sweep.
2023-02-07 18:59:34.260 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 18:59:34.260 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 18:59:34.260 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7807904400167742 for sweep.
2023-02-07 18:59:34.261 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 245 for sweep.
2023-02-07 18:59:34.261 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 8 for sweep.
2023-02-07 18:59:34.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0011766978156404963 for sweep.
2023-02-07 18:59:34.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 53 for sweep.
2023-02-07 18:59:34.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.059733357180404574 for sweep.
2023-02-07 18:59:34.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4531 for sweep.
2023-02-07 18:59:34.263 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 295 for sweep.
2023-02-07 18:59:34.263 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.12875355094865606 for sweep.
2023-02-07 18:59:34.263 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.08514534552504888 for sweep.
2023-02-07 18:59:34.263 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3964997271295817 for sweep.
2023-02-07 18:59:34.263 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 18:59:34.274 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 5}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185926-lksam6du/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 119, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 245, 'window': 8, 'min_count': 1, 'dm': 0, 'sample': 0.7807904400167742, 'workers': 4, 'alpha': 0.0003607724746741335, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4531, 'max_depth': 53, 'num_leaves': 295, 'reg_alpha': 0.12875355094865606, 'reg_lambda': 0.08514534552504888, 'subsample': 0.3964997271295817, 'min_child_weight': 0.059733357180404574, 'n_jobs': 4, 'learning_rate': 0.0011766978156404963}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:18, 177.58it/s]  1%|          | 39/3257 [00:00<00:16, 192.25it/s]  2%|‚ñè         | 59/3257 [00:00<00:17, 184.22it/s]  3%|‚ñé         | 82/3257 [00:00<00:15, 198.55it/s]  3%|‚ñé         | 103/3257 [00:00<00:15, 201.23it/s]  4%|‚ñç         | 124/3257 [00:00<00:16, 188.58it/s]  5%|‚ñç         | 148/3257 [00:00<00:15, 203.68it/s]  5%|‚ñå         | 169/3257 [00:00<00:15, 195.94it/s]  6%|‚ñå         | 191/3257 [00:00<00:15, 200.94it/s]  7%|‚ñã         | 213/3257 [00:01<00:14, 206.43it/s]  7%|‚ñã         | 237/3257 [00:01<00:14, 212.72it/s]  8%|‚ñä         | 259/3257 [00:01<00:14, 213.75it/s]  9%|‚ñâ         | 285/3257 [00:01<00:13, 225.16it/s]  9%|‚ñâ         | 308/3257 [00:01<00:13, 217.51it/s] 10%|‚ñà         | 332/3257 [00:01<00:13, 220.24it/s] 11%|‚ñà         | 355/3257 [00:01<00:13, 219.40it/s] 12%|‚ñà‚ñè        | 377/3257 [00:01<00:14, 203.76it/s] 12%|‚ñà‚ñè        | 398/3257 [00:01<00:14, 202.62it/s] 13%|‚ñà‚ñé        | 421/3257 [00:02<00:13, 209.19it/s] 14%|‚ñà‚ñé        | 443/3257 [00:02<00:14, 189.08it/s] 14%|‚ñà‚ñç        | 465/3257 [00:02<00:14, 194.82it/s] 15%|‚ñà‚ñç        | 485/3257 [00:02<00:14, 192.33it/s] 16%|‚ñà‚ñå        | 510/3257 [00:02<00:13, 205.63it/s] 16%|‚ñà‚ñã        | 531/3257 [00:02<00:13, 203.11it/s] 17%|‚ñà‚ñã        | 552/3257 [00:02<00:13, 204.90it/s] 18%|‚ñà‚ñä        | 573/3257 [00:02<00:14, 180.26it/s] 18%|‚ñà‚ñä        | 599/3257 [00:02<00:13, 197.13it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:13, 200.00it/s] 20%|‚ñà‚ñâ        | 643/3257 [00:03<00:12, 207.41it/s] 20%|‚ñà‚ñà        | 665/3257 [00:03<00:13, 193.97it/s] 21%|‚ñà‚ñà        | 685/3257 [00:03<00:13, 193.09it/s] 22%|‚ñà‚ñà‚ñè       | 709/3257 [00:03<00:12, 204.12it/s] 22%|‚ñà‚ñà‚ñè       | 730/3257 [00:03<00:19, 132.74it/s] 23%|‚ñà‚ñà‚ñé       | 751/3257 [00:03<00:16, 147.51it/s] 24%|‚ñà‚ñà‚ñé       | 773/3257 [00:04<00:15, 162.26it/s] 24%|‚ñà‚ñà‚ñç       | 794/3257 [00:04<00:14, 173.49it/s] 25%|‚ñà‚ñà‚ñå       | 815/3257 [00:04<00:13, 182.46it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:04<00:13, 184.50it/s] 26%|‚ñà‚ñà‚ñã       | 855/3257 [00:04<00:13, 184.02it/s] 27%|‚ñà‚ñà‚ñã       | 876/3257 [00:04<00:12, 188.16it/s] 28%|‚ñà‚ñà‚ñä       | 897/3257 [00:04<00:12, 192.94it/s] 28%|‚ñà‚ñà‚ñä       | 919/3257 [00:04<00:11, 200.09it/s] 29%|‚ñà‚ñà‚ñâ       | 940/3257 [00:04<00:11, 200.62it/s] 30%|‚ñà‚ñà‚ñâ       | 964/3257 [00:04<00:10, 210.11it/s] 30%|‚ñà‚ñà‚ñà       | 986/3257 [00:05<00:11, 204.73it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:05<00:11, 198.06it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:05<00:11, 198.18it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:05<00:11, 194.56it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1069/3257 [00:05<00:10, 201.40it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1090/3257 [00:05<00:10, 198.99it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1112/3257 [00:05<00:10, 202.08it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1133/3257 [00:05<00:10, 194.93it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1154/3257 [00:05<00:10, 196.85it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1176/3257 [00:06<00:10, 201.65it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1197/3257 [00:06<00:11, 187.15it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1216/3257 [00:06<00:11, 184.42it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1240/3257 [00:06<00:10, 199.08it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1261/3257 [00:06<00:10, 195.42it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1281/3257 [00:06<00:10, 186.59it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1302/3257 [00:06<00:10, 190.05it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1327/3257 [00:06<00:09, 205.04it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1348/3257 [00:06<00:09, 206.25it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1369/3257 [00:07<00:09, 200.12it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1390/3257 [00:07<00:09, 193.91it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1416/3257 [00:07<00:08, 211.73it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1438/3257 [00:07<00:08, 208.33it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1463/3257 [00:07<00:08, 218.97it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1486/3257 [00:07<00:08, 220.67it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1512/3257 [00:07<00:07, 231.72it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1536/3257 [00:07<00:08, 206.89it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1558/3257 [00:07<00:08, 203.84it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1579/3257 [00:08<00:08, 203.67it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1602/3257 [00:08<00:07, 209.18it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1624/3257 [00:08<00:07, 209.96it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1646/3257 [00:08<00:07, 203.37it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1667/3257 [00:08<00:08, 196.59it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1687/3257 [00:08<00:08, 193.54it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1710/3257 [00:08<00:07, 203.12it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:08<00:07, 198.60it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1753/3257 [00:08<00:07, 202.21it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1779/3257 [00:08<00:06, 215.46it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1803/3257 [00:09<00:06, 221.25it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1826/3257 [00:09<00:06, 215.02it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1849/3257 [00:09<00:06, 218.78it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1876/3257 [00:09<00:06, 230.03it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1900/3257 [00:09<00:05, 228.78it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1923/3257 [00:09<00:05, 229.03it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1954/3257 [00:09<00:05, 251.83it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1980/3257 [00:09<00:05, 237.63it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:09<00:05, 233.99it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2030/3257 [00:10<00:05, 241.16it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2055/3257 [00:10<00:05, 219.19it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2078/3257 [00:10<00:05, 219.29it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2101/3257 [00:10<00:08, 136.19it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2121/3257 [00:10<00:07, 148.25it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2142/3257 [00:10<00:06, 160.68it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2163/3257 [00:10<00:06, 171.61it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2184/3257 [00:11<00:05, 179.99it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2209/3257 [00:11<00:05, 192.68it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2235/3257 [00:11<00:04, 209.48it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2258/3257 [00:11<00:04, 209.54it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2280/3257 [00:11<00:04, 203.35it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2303/3257 [00:11<00:04, 209.52it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2331/3257 [00:11<00:04, 228.95it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2359/3257 [00:11<00:03, 242.64it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2384/3257 [00:11<00:03, 233.91it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2408/3257 [00:11<00:03, 229.63it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2432/3257 [00:12<00:03, 218.17it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2455/3257 [00:12<00:03, 219.98it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2481/3257 [00:12<00:03, 229.44it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2508/3257 [00:12<00:03, 240.28it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2535/3257 [00:12<00:02, 246.33it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2560/3257 [00:12<00:03, 230.55it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2584/3257 [00:12<00:03, 214.76it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2608/3257 [00:12<00:02, 219.78it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2636/3257 [00:12<00:02, 234.58it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2660/3257 [00:13<00:02, 221.81it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2685/3257 [00:13<00:02, 228.39it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2709/3257 [00:13<00:02, 205.69it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:13<00:02, 210.57it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2757/3257 [00:13<00:02, 220.32it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2780/3257 [00:13<00:02, 218.02it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2807/3257 [00:13<00:01, 226.31it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2830/3257 [00:13<00:01, 215.32it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2854/3257 [00:13<00:01, 220.29it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2885/3257 [00:14<00:01, 244.76it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2910/3257 [00:14<00:01, 225.51it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2933/3257 [00:14<00:01, 225.75it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2956/3257 [00:14<00:01, 211.68it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2979/3257 [00:14<00:01, 216.22it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3004/3257 [00:14<00:01, 225.41it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3027/3257 [00:14<00:01, 224.74it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3054/3257 [00:14<00:00, 235.65it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3080/3257 [00:14<00:00, 239.50it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3105/3257 [00:15<00:00, 240.79it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3130/3257 [00:15<00:00, 242.70it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3155/3257 [00:15<00:00, 229.70it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3179/3257 [00:15<00:00, 219.91it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3206/3257 [00:15<00:00, 232.76it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3230/3257 [00:15<00:00, 227.97it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:15<00:00, 224.61it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:15<00:00, 206.83it/s]
2023-02-07 18:59:50.616 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 18:59:50,618][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d245,n5,s0.78079,t4>', 'datetime': '2023-02-07T18:59:50.618353', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 18:59:50,618][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 18:59:50,618][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 18:59:50,988][gensim.models.doc2vec][INFO] - collected 21699 word types and 3257 unique tags from a corpus of 3257 examples and 4367244 words
[2023-02-07 18:59:50,989][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 18:59:51,042][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 21699 unique words (100.00% of original 21699, drops 0)', 'datetime': '2023-02-07T18:59:51.042792', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:59:51,043][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 4367244 word corpus (100.00% of original 4367244, drops 0)', 'datetime': '2023-02-07T18:59:51.043206', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:59:51,113][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 21699 items
[2023-02-07 18:59:51,114][gensim.models.word2vec][INFO] - sample=0.78079 downsamples 0 most-common words
[2023-02-07 18:59:51,114][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4367244 word corpus (100.0%% of prior 4367244)', 'datetime': '2023-02-07T18:59:51.114593', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 18:59:51,235][gensim.models.word2vec][INFO] - estimated required memory for 21699 words and 245 dimensions: 57222800 bytes
[2023-02-07 18:59:51,235][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 18:59:51,261][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 21699 vocabulary and 245 features, using sg=1 hs=0 sample=0.7807904400167742 negative=5 window=8 shrink_windows=True', 'datetime': '2023-02-07T18:59:51.261493', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 18:59:52,276][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 38.50% examples, 1722012 words/s, in_qsize 8, out_qsize 0
[2023-02-07 18:59:53,284][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 75.90% examples, 1677131 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:59:53,903][gensim.models.word2vec][INFO] - EPOCH 0: training on 4367244 raw words (4368669 effective words) took 2.6s, 1662740 effective words/s
[2023-02-07 18:59:54,914][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 35.92% examples, 1582740 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:59:55,918][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 71.66% examples, 1586413 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:59:56,659][gensim.models.word2vec][INFO] - EPOCH 1: training on 4367244 raw words (4368669 effective words) took 2.8s, 1586993 effective words/s
[2023-02-07 18:59:57,670][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 35.22% examples, 1553167 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:59:58,672][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 69.54% examples, 1545844 words/s, in_qsize 7, out_qsize 0
[2023-02-07 18:59:59,483][gensim.models.word2vec][INFO] - EPOCH 2: training on 4367244 raw words (4368669 effective words) took 2.8s, 1548884 effective words/s
[2023-02-07 19:00:00,492][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 46.70% examples, 2059383 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:01,495][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 89.53% examples, 1958232 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:01,759][gensim.models.word2vec][INFO] - EPOCH 3: training on 4367244 raw words (4368669 effective words) took 2.3s, 1921153 effective words/s
[2023-02-07 19:00:02,765][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 35.92% examples, 1589100 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:03,775][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 71.66% examples, 1585374 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:04,511][gensim.models.word2vec][INFO] - EPOCH 4: training on 4367244 raw words (4368669 effective words) took 2.7s, 1589101 effective words/s
[2023-02-07 19:00:05,514][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 34.69% examples, 1537520 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:06,532][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 69.97% examples, 1553979 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:00:07,277][gensim.models.word2vec][INFO] - EPOCH 5: training on 4367244 raw words (4368669 effective words) took 2.8s, 1580902 effective words/s
[2023-02-07 19:00:08,279][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 47.47% examples, 2120862 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:09,282][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 85.23% examples, 1877566 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:09,644][gensim.models.word2vec][INFO] - EPOCH 6: training on 4367244 raw words (4368669 effective words) took 2.4s, 1847371 effective words/s
[2023-02-07 19:00:10,656][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 45.81% examples, 2017652 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:00:11,664][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 93.00% examples, 2026722 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:00:11,798][gensim.models.word2vec][INFO] - EPOCH 7: training on 4367244 raw words (4368669 effective words) took 2.2s, 2029849 effective words/s
[2023-02-07 19:00:12,809][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 35.92% examples, 1582724 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:13,818][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 72.03% examples, 1591757 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:14,531][gensim.models.word2vec][INFO] - EPOCH 8: training on 4367244 raw words (4368669 effective words) took 2.7s, 1600172 effective words/s
[2023-02-07 19:00:15,571][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 33.71% examples, 1442849 words/s, in_qsize 4, out_qsize 6
[2023-02-07 19:00:16,577][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 68.90% examples, 1507548 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:17,441][gensim.models.word2vec][INFO] - EPOCH 9: training on 4367244 raw words (4368669 effective words) took 2.9s, 1503589 effective words/s
[2023-02-07 19:00:18,452][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 35.49% examples, 1563659 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:19,454][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 70.22% examples, 1560212 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:00:20,240][gensim.models.word2vec][INFO] - EPOCH 10: training on 4367244 raw words (4368669 effective words) took 2.8s, 1562591 effective words/s
[2023-02-07 19:00:21,246][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 34.39% examples, 1515171 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:22,255][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 67.92% examples, 1507245 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:23,189][gensim.models.word2vec][INFO] - EPOCH 11: training on 4367244 raw words (4368669 effective words) took 2.9s, 1482518 effective words/s
[2023-02-07 19:00:24,197][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 33.25% examples, 1471788 words/s, in_qsize 0, out_qsize 1
[2023-02-07 19:00:25,230][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 66.38% examples, 1455394 words/s, in_qsize 8, out_qsize 8
[2023-02-07 19:00:26,079][gensim.models.word2vec][INFO] - EPOCH 12: training on 4367244 raw words (4368669 effective words) took 2.9s, 1514070 effective words/s
[2023-02-07 19:00:27,084][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 33.53% examples, 1483612 words/s, in_qsize 3, out_qsize 2
[2023-02-07 19:00:28,089][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 68.90% examples, 1536838 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:28,884][gensim.models.word2vec][INFO] - EPOCH 13: training on 4367244 raw words (4368669 effective words) took 2.8s, 1559280 effective words/s
[2023-02-07 19:00:29,888][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 36.17% examples, 1603574 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:30,889][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 71.38% examples, 1588969 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:00:31,632][gensim.models.word2vec][INFO] - EPOCH 14: training on 4367244 raw words (4368669 effective words) took 2.7s, 1590881 effective words/s
[2023-02-07 19:00:31,633][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 65508660 raw words (65530035 effective words) took 40.4s, 1623210 effective words/s', 'datetime': '2023-02-07T19:00:31.633510', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:00:31.633 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:00:36,531][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185926-lksam6du/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:00:36.531705', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:00:36,532][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:00:36,612][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_185926-lksam6du/files/../tmp/embedding_model.pt
2023-02-07 19:00:36.612 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:00:38.240 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:00:38.806 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:00:40.412 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.600865873370549, 'test_mae': 1.2114344337695147, 'test_r2': -2.5417107216923127}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.07
wandb: percentage 0.0
wandb:   test_mae 1.21143
wandb:   test_mse 2.60087
wandb:    test_r2 -2.54171
wandb: 
wandb: üöÄ View run desert-sweep-29 at: https://wandb.ai/xiaoqiz/mof2vec/runs/lksam6du
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_185926-lksam6du/logs
wandb: Agent Starting Run: a4ci9huq with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 580
wandb: 	model.gensim.alpha: 0.0016480033569169032
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.22545150883226572
wandb: 	model.gensim.vector_size: 261
wandb: 	model.gensim.window: 20
wandb: 	model.sklearn.learning_rate: 0.3986394988901701
wandb: 	model.sklearn.max_depth: 30
wandb: 	model.sklearn.min_child_weight: 0.059216452021591766
wandb: 	model.sklearn.n_estimators: 4779
wandb: 	model.sklearn.num_leaves: 424
wandb: 	model.sklearn.reg_alpha: 0.05754480631017193
wandb: 	model.sklearn.reg_lambda: 0.7993317547631401
wandb: 	model.sklearn.subsample: 0.22510768709311196
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190051-a4ci9huq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-30
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/a4ci9huq
2023-02-07 19:00:59.792 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 19:00:59.793 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 580 for sweep.
2023-02-07 19:00:59.793 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0016480033569169032 for sweep.
2023-02-07 19:00:59.794 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 19:00:59.794 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 19:00:59.794 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.22545150883226572 for sweep.
2023-02-07 19:00:59.794 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 261 for sweep.
2023-02-07 19:00:59.794 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 20 for sweep.
2023-02-07 19:00:59.795 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.3986394988901701 for sweep.
2023-02-07 19:00:59.795 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 30 for sweep.
2023-02-07 19:00:59.795 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.059216452021591766 for sweep.
2023-02-07 19:00:59.795 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4779 for sweep.
2023-02-07 19:00:59.796 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 424 for sweep.
2023-02-07 19:00:59.796 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.05754480631017193 for sweep.
2023-02-07 19:00:59.796 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.7993317547631401 for sweep.
2023-02-07 19:00:59.796 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.22510768709311196 for sweep.
2023-02-07 19:00:59.797 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:00:59.802 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190051-a4ci9huq/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 580, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 261, 'window': 20, 'min_count': 5, 'dm': 1, 'sample': 0.22545150883226572, 'workers': 4, 'alpha': 0.0016480033569169032, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4779, 'max_depth': 30, 'num_leaves': 424, 'reg_alpha': 0.05754480631017193, 'reg_lambda': 0.7993317547631401, 'subsample': 0.22510768709311196, 'min_child_weight': 0.059216452021591766, 'n_jobs': 4, 'learning_rate': 0.3986394988901701}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:20, 160.11it/s]  1%|          | 34/3257 [00:00<00:19, 164.86it/s]  2%|‚ñè         | 53/3257 [00:00<00:18, 170.19it/s]  2%|‚ñè         | 73/3257 [00:00<00:17, 181.07it/s]  3%|‚ñé         | 93/3257 [00:00<00:16, 187.14it/s]  3%|‚ñé         | 112/3257 [00:00<00:17, 178.14it/s]  4%|‚ñç         | 133/3257 [00:00<00:16, 186.70it/s]  5%|‚ñç         | 152/3257 [00:00<00:23, 130.27it/s]  5%|‚ñå         | 171/3257 [00:01<00:21, 142.04it/s]  6%|‚ñå         | 190/3257 [00:01<00:19, 153.45it/s]  6%|‚ñã         | 210/3257 [00:01<00:18, 164.34it/s]  7%|‚ñã         | 234/3257 [00:01<00:16, 183.96it/s]  8%|‚ñä         | 254/3257 [00:01<00:16, 183.21it/s]  8%|‚ñä         | 275/3257 [00:01<00:15, 190.33it/s]  9%|‚ñâ         | 297/3257 [00:01<00:15, 194.89it/s] 10%|‚ñâ         | 317/3257 [00:01<00:15, 189.90it/s] 10%|‚ñà         | 338/3257 [00:01<00:15, 194.52it/s] 11%|‚ñà         | 360/3257 [00:02<00:14, 199.88it/s] 12%|‚ñà‚ñè        | 381/3257 [00:02<00:15, 182.34it/s] 12%|‚ñà‚ñè        | 400/3257 [00:02<00:15, 179.83it/s] 13%|‚ñà‚ñé        | 421/3257 [00:02<00:15, 185.51it/s] 14%|‚ñà‚ñé        | 440/3257 [00:02<00:16, 166.34it/s] 14%|‚ñà‚ñç        | 462/3257 [00:02<00:15, 180.08it/s] 15%|‚ñà‚ñç        | 481/3257 [00:02<00:15, 181.93it/s] 16%|‚ñà‚ñå        | 505/3257 [00:02<00:14, 195.88it/s] 16%|‚ñà‚ñå        | 526/3257 [00:02<00:13, 199.69it/s] 17%|‚ñà‚ñã        | 548/3257 [00:03<00:13, 204.88it/s] 17%|‚ñà‚ñã        | 569/3257 [00:03<00:13, 197.65it/s] 18%|‚ñà‚ñä        | 589/3257 [00:03<00:13, 191.12it/s] 19%|‚ñà‚ñâ        | 613/3257 [00:03<00:12, 204.48it/s] 20%|‚ñà‚ñâ        | 636/3257 [00:03<00:12, 208.24it/s] 20%|‚ñà‚ñà        | 657/3257 [00:03<00:13, 187.39it/s] 21%|‚ñà‚ñà        | 680/3257 [00:03<00:13, 197.23it/s] 22%|‚ñà‚ñà‚ñè       | 701/3257 [00:03<00:13, 189.17it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:03<00:13, 192.09it/s] 23%|‚ñà‚ñà‚ñé       | 742/3257 [00:04<00:13, 185.92it/s] 24%|‚ñà‚ñà‚ñé       | 766/3257 [00:04<00:12, 200.37it/s] 24%|‚ñà‚ñà‚ñç       | 787/3257 [00:04<00:12, 194.68it/s] 25%|‚ñà‚ñà‚ñç       | 808/3257 [00:04<00:12, 198.35it/s] 25%|‚ñà‚ñà‚ñå       | 829/3257 [00:04<00:12, 193.11it/s] 26%|‚ñà‚ñà‚ñå       | 849/3257 [00:04<00:14, 171.19it/s] 27%|‚ñà‚ñà‚ñã       | 870/3257 [00:04<00:13, 178.97it/s] 27%|‚ñà‚ñà‚ñã       | 889/3257 [00:04<00:13, 181.27it/s] 28%|‚ñà‚ñà‚ñä       | 910/3257 [00:04<00:12, 188.25it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:05<00:11, 193.98it/s] 29%|‚ñà‚ñà‚ñâ       | 953/3257 [00:05<00:11, 200.25it/s] 30%|‚ñà‚ñà‚ñâ       | 974/3257 [00:05<00:11, 201.27it/s] 31%|‚ñà‚ñà‚ñà       | 995/3257 [00:05<00:11, 194.56it/s] 31%|‚ñà‚ñà‚ñà       | 1015/3257 [00:05<00:11, 195.80it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1035/3257 [00:05<00:11, 191.97it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1055/3257 [00:05<00:11, 188.65it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:05<00:11, 191.56it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1101/3257 [00:05<00:10, 196.11it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1121/3257 [00:06<00:11, 192.24it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1141/3257 [00:06<00:11, 188.97it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1166/3257 [00:06<00:10, 203.06it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1187/3257 [00:06<00:10, 188.22it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1207/3257 [00:06<00:11, 177.26it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1231/3257 [00:06<00:10, 193.42it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1251/3257 [00:06<00:10, 188.12it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1273/3257 [00:06<00:10, 196.35it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1293/3257 [00:06<00:10, 180.82it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1312/3257 [00:07<00:16, 119.49it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1333/3257 [00:07<00:14, 137.13it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1351/3257 [00:07<00:13, 146.01it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:07<00:12, 155.04it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1389/3257 [00:07<00:11, 162.18it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1417/3257 [00:07<00:09, 193.17it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1438/3257 [00:07<00:09, 191.86it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1463/3257 [00:07<00:08, 207.66it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1485/3257 [00:08<00:08, 207.94it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:08<00:07, 221.06it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1534/3257 [00:08<00:08, 201.29it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1555/3257 [00:08<00:08, 195.70it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1576/3257 [00:08<00:08, 188.17it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1596/3257 [00:08<00:08, 188.29it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:08<00:08, 196.87it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1639/3257 [00:08<00:08, 195.73it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:08<00:08, 190.56it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1679/3257 [00:09<00:08, 185.89it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1698/3257 [00:09<00:08, 184.56it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1717/3257 [00:09<00:08, 182.80it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1736/3257 [00:09<00:08, 172.87it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1758/3257 [00:09<00:08, 184.90it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1777/3257 [00:09<00:07, 186.26it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1798/3257 [00:09<00:07, 192.15it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1818/3257 [00:09<00:07, 190.72it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1838/3257 [00:09<00:07, 186.34it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1857/3257 [00:10<00:07, 178.37it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1877/3257 [00:10<00:07, 183.35it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1896/3257 [00:10<00:07, 178.01it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1917/3257 [00:10<00:07, 184.01it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1940/3257 [00:10<00:06, 196.02it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:10<00:06, 208.99it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1987/3257 [00:10<00:06, 201.56it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2009/3257 [00:10<00:06, 205.38it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2033/3257 [00:10<00:05, 209.82it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2055/3257 [00:11<00:06, 189.96it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2075/3257 [00:11<00:06, 188.48it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2095/3257 [00:11<00:06, 190.03it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2115/3257 [00:11<00:06, 188.18it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2134/3257 [00:11<00:06, 173.86it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2152/3257 [00:11<00:06, 173.69it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2172/3257 [00:11<00:06, 179.49it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2191/3257 [00:11<00:05, 181.24it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2210/3257 [00:11<00:05, 177.16it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2232/3257 [00:12<00:05, 189.20it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2252/3257 [00:12<00:05, 180.09it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2271/3257 [00:12<00:05, 167.43it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2292/3257 [00:12<00:05, 178.43it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2312/3257 [00:12<00:05, 184.12it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2340/3257 [00:12<00:04, 209.23it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2366/3257 [00:12<00:03, 223.05it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2390/3257 [00:12<00:03, 225.53it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2413/3257 [00:12<00:03, 214.20it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2435/3257 [00:13<00:03, 207.34it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2456/3257 [00:13<00:03, 206.98it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2481/3257 [00:13<00:03, 216.56it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2507/3257 [00:13<00:03, 226.62it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2531/3257 [00:13<00:03, 229.52it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2555/3257 [00:13<00:03, 213.36it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2577/3257 [00:13<00:03, 205.55it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2598/3257 [00:14<00:05, 117.10it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2626/3257 [00:14<00:04, 146.19it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2646/3257 [00:14<00:03, 154.61it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2666/3257 [00:14<00:03, 163.47it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2689/3257 [00:14<00:03, 179.08it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2710/3257 [00:14<00:03, 169.85it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2733/3257 [00:14<00:02, 184.07it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:14<00:02, 195.69it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2777/3257 [00:14<00:02, 192.95it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2803/3257 [00:15<00:02, 210.06it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2825/3257 [00:15<00:02, 195.70it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2846/3257 [00:15<00:02, 197.67it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2874/3257 [00:15<00:01, 219.33it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2897/3257 [00:15<00:01, 207.18it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2920/3257 [00:15<00:01, 211.39it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2942/3257 [00:15<00:01, 208.26it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2964/3257 [00:15<00:01, 196.50it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2984/3257 [00:15<00:01, 186.58it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3010/3257 [00:16<00:01, 204.37it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3031/3257 [00:16<00:01, 199.14it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3055/3257 [00:16<00:00, 209.91it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3080/3257 [00:16<00:00, 218.06it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3104/3257 [00:16<00:00, 221.40it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3128/3257 [00:16<00:00, 225.28it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3151/3257 [00:16<00:00, 213.42it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3173/3257 [00:16<00:00, 213.82it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3195/3257 [00:16<00:00, 213.45it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3217/3257 [00:17<00:00, 199.83it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3243/3257 [00:17<00:00, 215.55it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:17<00:00, 188.97it/s]
2023-02-07 19:01:17.736 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:01:17,737][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d261,n5,w20,mc5,s0.225452,t4>', 'datetime': '2023-02-07T19:01:17.737862', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:01:17,738][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:01:17,739][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:01:18,187][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 19:01:18,188][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:01:18,233][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 16108 unique words (50.65% of original 31803, drops 15695)', 'datetime': '2023-02-07T19:01:18.233884', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:01:18,234][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5056108 word corpus (99.23% of original 5095118, drops 39010)', 'datetime': '2023-02-07T19:01:18.234286', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:01:18,288][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 19:01:18,289][gensim.models.word2vec][INFO] - sample=0.225452 downsamples 0 most-common words
[2023-02-07 19:01:18,290][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5056108 word corpus (100.0%% of prior 5056108)', 'datetime': '2023-02-07T19:01:18.290067', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:01:18,381][gensim.models.word2vec][INFO] - estimated required memory for 16108 words and 261 dimensions: 45739212 bytes
[2023-02-07 19:01:18,381][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:01:18,401][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 16108 vocabulary and 261 features, using sg=0 hs=0 sample=0.22545150883226572 negative=5 window=20 shrink_windows=True', 'datetime': '2023-02-07T19:01:18.401420', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:01:19,411][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 17.32% examples, 849348 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:01:20,414][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 32.73% examples, 831710 words/s, in_qsize 5, out_qsize 2
[2023-02-07 19:01:21,421][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 49.00% examples, 837531 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:22,421][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 65.86% examples, 846116 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:23,423][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 83.85% examples, 851981 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:24,277][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5048130 effective words) took 5.9s, 859495 effective words/s
[2023-02-07 19:01:25,296][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 22.41% examples, 1100453 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:01:26,305][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 43.84% examples, 1119538 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:27,316][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 65.86% examples, 1118955 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:01:28,317][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 83.85% examples, 1058614 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:29,193][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5048130 effective words) took 4.9s, 1027209 effective words/s
[2023-02-07 19:01:30,198][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 17.13% examples, 843362 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:31,202][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 32.55% examples, 830006 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:32,216][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 47.80% examples, 815547 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:33,241][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 65.09% examples, 828786 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:34,249][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 84.22% examples, 849851 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:35,035][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5048130 effective words) took 5.8s, 864582 effective words/s
[2023-02-07 19:01:36,050][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 18.36% examples, 896280 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:01:37,054][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 34.76% examples, 883226 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:01:38,067][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 52.63% examples, 896288 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:01:39,070][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 69.67% examples, 891701 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:40,070][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 88.52% examples, 893596 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:40,693][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5048130 effective words) took 5.7s, 892595 effective words/s
[2023-02-07 19:01:41,697][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 17.96% examples, 889963 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:42,709][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 35.06% examples, 893521 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:43,715][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 52.81% examples, 905724 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:44,717][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 70.65% examples, 907475 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:45,718][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 89.93% examples, 909502 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:46,281][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5048130 effective words) took 5.6s, 903744 effective words/s
[2023-02-07 19:01:47,291][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 16.98% examples, 820317 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:48,298][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 32.91% examples, 833015 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:49,312][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 49.00% examples, 834664 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:50,313][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 66.66% examples, 852103 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:01:51,318][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 85.82% examples, 867526 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:52,065][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5048130 effective words) took 5.8s, 873307 effective words/s
[2023-02-07 19:01:53,071][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 17.72% examples, 867905 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:01:54,081][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 34.60% examples, 881242 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:55,088][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 51.37% examples, 877815 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:01:56,092][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 68.35% examples, 876403 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:01:57,097][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 86.67% examples, 876846 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:57,768][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5048130 effective words) took 5.7s, 885379 effective words/s
[2023-02-07 19:01:58,774][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 18.73% examples, 922357 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:01:59,775][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 35.06% examples, 898198 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:00,792][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 52.13% examples, 890960 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:01,793][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 69.67% examples, 894107 words/s, in_qsize 7, out_qsize 1
[2023-02-07 19:02:02,798][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 87.84% examples, 888594 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:03,406][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5048130 effective words) took 5.6s, 895929 effective words/s
[2023-02-07 19:02:04,419][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 18.15% examples, 891336 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:05,437][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 35.06% examples, 887021 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:06,440][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 51.55% examples, 878108 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:02:07,442][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 70.06% examples, 895788 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:08,456][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 88.79% examples, 892815 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:09,047][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5048130 effective words) took 5.6s, 895229 effective words/s
[2023-02-07 19:02:10,053][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 18.15% examples, 893569 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:11,056][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 35.40% examples, 905975 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:12,073][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 53.70% examples, 919457 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:02:13,073][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 72.52% examples, 924581 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:02:14,085][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 91.80% examples, 927651 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:02:14,463][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5048130 effective words) took 5.4s, 932438 effective words/s
[2023-02-07 19:02:15,494][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 18.11% examples, 874711 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:02:16,513][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 35.49% examples, 892446 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:02:17,513][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 53.05% examples, 900497 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:18,530][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 69.82% examples, 886964 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:19,539][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 89.87% examples, 898385 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:20,070][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5048130 effective words) took 5.6s, 900640 effective words/s
[2023-02-07 19:02:21,077][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 18.18% examples, 892481 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:22,078][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 33.93% examples, 862203 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:23,080][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 50.97% examples, 872644 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:02:24,086][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 68.13% examples, 874807 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:25,092][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 85.66% examples, 867974 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:25,871][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5048130 effective words) took 5.8s, 870424 effective words/s
[2023-02-07 19:02:26,885][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 17.53% examples, 863295 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:27,886][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 33.16% examples, 842860 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:28,896][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 49.83% examples, 848582 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:29,903][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 66.47% examples, 849921 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:30,924][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 84.46% examples, 851823 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:31,759][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5048130 effective words) took 5.9s, 857651 effective words/s
[2023-02-07 19:02:32,767][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 23.18% examples, 1156817 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:33,768][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 45.13% examples, 1158106 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:34,769][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 67.85% examples, 1164293 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:02:35,778][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 92.35% examples, 1167581 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:36,069][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5048130 effective words) took 4.3s, 1171829 effective words/s
[2023-02-07 19:02:37,082][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 20.97% examples, 1037440 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:38,084][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 42.77% examples, 1105682 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:39,097][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 65.95% examples, 1122618 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:02:40,100][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 84.03% examples, 1063169 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:02:40,927][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5048130 effective words) took 4.9s, 1039453 effective words/s
[2023-02-07 19:02:40,928][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (75721950 effective words) took 82.5s, 917547 effective words/s', 'datetime': '2023-02-07T19:02:40.928322', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:02:40.928 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:02:45,209][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190051-a4ci9huq/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:02:45.209493', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:02:45,210][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:02:45,278][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190051-a4ci9huq/files/../tmp/embedding_model.pt
2023-02-07 19:02:45.279 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:02:47.099 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:02:47.743 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:02:49.503 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.7338700208859863, 'test_mae': 1.2384520198398503, 'test_r2': -4.0377177759102345}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.17
wandb: percentage 0.49351
wandb:   test_mae 1.23845
wandb:   test_mse 2.73387
wandb:    test_r2 -4.03772
wandb: 
wandb: üöÄ View run tough-sweep-30 at: https://wandb.ai/xiaoqiz/mof2vec/runs/a4ci9huq
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_190051-a4ci9huq/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 33z2iamm with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 738
wandb: 	model.gensim.alpha: 0.004711752160997893
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.2441928188313125
wandb: 	model.gensim.vector_size: 318
wandb: 	model.gensim.window: 19
wandb: 	model.sklearn.learning_rate: 0.0008738058191264793
wandb: 	model.sklearn.max_depth: 50
wandb: 	model.sklearn.min_child_weight: 0.06651293609001255
wandb: 	model.sklearn.n_estimators: 709
wandb: 	model.sklearn.num_leaves: 42
wandb: 	model.sklearn.reg_alpha: 0.010918307385799964
wandb: 	model.sklearn.reg_lambda: 0.030575615852620048
wandb: 	model.sklearn.subsample: 0.8894336434087518
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190307-33z2iamm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-31
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/33z2iamm
2023-02-07 19:03:16.249 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 19:03:16.250 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 738 for sweep.
2023-02-07 19:03:16.250 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.004711752160997893 for sweep.
2023-02-07 19:03:16.250 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:03:16.251 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 19:03:16.251 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2441928188313125 for sweep.
2023-02-07 19:03:16.252 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 318 for sweep.
2023-02-07 19:03:16.252 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 19 for sweep.
2023-02-07 19:03:16.252 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0008738058191264793 for sweep.
2023-02-07 19:03:16.252 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 50 for sweep.
2023-02-07 19:03:16.253 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.06651293609001255 for sweep.
2023-02-07 19:03:16.253 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 709 for sweep.
2023-02-07 19:03:16.253 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 42 for sweep.
2023-02-07 19:03:16.253 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.010918307385799964 for sweep.
2023-02-07 19:03:16.253 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.030575615852620048 for sweep.
2023-02-07 19:03:16.254 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8894336434087518 for sweep.
2023-02-07 19:03:16.254 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:03:16.262 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190307-33z2iamm/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 738, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 318, 'window': 19, 'min_count': 3, 'dm': 0, 'sample': 0.2441928188313125, 'workers': 4, 'alpha': 0.004711752160997893, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 709, 'max_depth': 50, 'num_leaves': 42, 'reg_alpha': 0.010918307385799964, 'reg_lambda': 0.030575615852620048, 'subsample': 0.8894336434087518, 'min_child_weight': 0.06651293609001255, 'n_jobs': 4, 'learning_rate': 0.0008738058191264793}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 15/3257 [00:00<00:21, 147.79it/s]  1%|          | 33/3257 [00:00<00:19, 163.41it/s]  2%|‚ñè         | 51/3257 [00:00<00:19, 167.18it/s]  2%|‚ñè         | 68/3257 [00:00<00:19, 165.55it/s]  3%|‚ñé         | 88/3257 [00:00<00:18, 176.01it/s]  3%|‚ñé         | 106/3257 [00:00<00:20, 155.58it/s]  4%|‚ñç         | 124/3257 [00:00<00:19, 161.90it/s]  4%|‚ñç         | 145/3257 [00:00<00:17, 175.71it/s]  5%|‚ñå         | 163/3257 [00:00<00:18, 164.52it/s]  6%|‚ñå         | 180/3257 [00:01<00:18, 164.91it/s]  6%|‚ñå         | 200/3257 [00:01<00:17, 173.42it/s]  7%|‚ñã         | 220/3257 [00:01<00:16, 179.76it/s]  7%|‚ñã         | 239/3257 [00:01<00:16, 180.76it/s]  8%|‚ñä         | 258/3257 [00:01<00:16, 182.49it/s]  9%|‚ñä         | 279/3257 [00:01<00:15, 188.81it/s]  9%|‚ñâ         | 298/3257 [00:01<00:15, 186.24it/s] 10%|‚ñâ         | 317/3257 [00:01<00:15, 184.01it/s] 10%|‚ñà         | 338/3257 [00:01<00:15, 189.35it/s] 11%|‚ñà         | 358/3257 [00:02<00:15, 191.32it/s] 12%|‚ñà‚ñè        | 378/3257 [00:02<00:16, 175.72it/s] 12%|‚ñà‚ñè        | 396/3257 [00:02<00:16, 175.77it/s] 13%|‚ñà‚ñé        | 417/3257 [00:02<00:15, 185.15it/s] 13%|‚ñà‚ñé        | 436/3257 [00:02<00:17, 159.21it/s] 14%|‚ñà‚ñç        | 458/3257 [00:02<00:16, 173.26it/s] 15%|‚ñà‚ñç        | 477/3257 [00:02<00:15, 175.55it/s] 15%|‚ñà‚ñå        | 496/3257 [00:02<00:15, 177.79it/s] 16%|‚ñà‚ñå        | 516/3257 [00:02<00:15, 182.70it/s] 16%|‚ñà‚ñã        | 535/3257 [00:03<00:14, 181.72it/s] 17%|‚ñà‚ñã        | 556/3257 [00:03<00:14, 187.05it/s] 18%|‚ñà‚ñä        | 575/3257 [00:03<00:16, 163.62it/s] 18%|‚ñà‚ñä        | 599/3257 [00:03<00:14, 179.44it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:14, 183.58it/s] 20%|‚ñà‚ñâ        | 639/3257 [00:03<00:27, 95.25it/s]  20%|‚ñà‚ñà        | 654/3257 [00:04<00:26, 99.87it/s] 21%|‚ñà‚ñà        | 668/3257 [00:04<00:24, 104.76it/s] 21%|‚ñà‚ñà        | 683/3257 [00:04<00:23, 111.23it/s] 21%|‚ñà‚ñà‚ñè       | 698/3257 [00:04<00:21, 118.84it/s] 22%|‚ñà‚ñà‚ñè       | 716/3257 [00:04<00:19, 132.57it/s] 22%|‚ñà‚ñà‚ñè       | 731/3257 [00:04<00:19, 129.63it/s] 23%|‚ñà‚ñà‚ñé       | 746/3257 [00:04<00:18, 132.77it/s] 23%|‚ñà‚ñà‚ñé       | 763/3257 [00:04<00:17, 141.50it/s] 24%|‚ñà‚ñà‚ñç       | 778/3257 [00:04<00:17, 138.42it/s] 24%|‚ñà‚ñà‚ñç       | 793/3257 [00:05<00:17, 139.76it/s] 25%|‚ñà‚ñà‚ñç       | 808/3257 [00:05<00:17, 140.87it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:05<00:17, 142.28it/s] 26%|‚ñà‚ñà‚ñå       | 838/3257 [00:05<00:16, 143.42it/s] 26%|‚ñà‚ñà‚ñå       | 853/3257 [00:05<00:16, 141.97it/s] 27%|‚ñà‚ñà‚ñã       | 871/3257 [00:05<00:15, 150.16it/s] 27%|‚ñà‚ñà‚ñã       | 888/3257 [00:05<00:15, 154.52it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:05<00:14, 160.74it/s] 28%|‚ñà‚ñà‚ñä       | 925/3257 [00:05<00:13, 167.24it/s] 29%|‚ñà‚ñà‚ñâ       | 942/3257 [00:06<00:14, 160.61it/s] 29%|‚ñà‚ñà‚ñâ       | 960/3257 [00:06<00:13, 164.61it/s] 30%|‚ñà‚ñà‚ñâ       | 977/3257 [00:06<00:13, 164.41it/s] 31%|‚ñà‚ñà‚ñà       | 994/3257 [00:06<00:15, 150.64it/s] 31%|‚ñà‚ñà‚ñà       | 1010/3257 [00:06<00:15, 142.49it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1026/3257 [00:06<00:15, 145.72it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1041/3257 [00:06<00:16, 133.41it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1056/3257 [00:06<00:16, 137.43it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1073/3257 [00:06<00:14, 145.63it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1088/3257 [00:07<00:15, 143.47it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1103/3257 [00:07<00:14, 145.16it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1118/3257 [00:07<00:14, 145.47it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1133/3257 [00:07<00:15, 140.57it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1148/3257 [00:07<00:15, 135.29it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1166/3257 [00:07<00:14, 147.09it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1181/3257 [00:07<00:15, 135.28it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1195/3257 [00:07<00:15, 131.48it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1209/3257 [00:07<00:15, 129.63it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1228/3257 [00:08<00:14, 144.84it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1243/3257 [00:08<00:13, 145.69it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1258/3257 [00:08<00:13, 143.78it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1274/3257 [00:08<00:13, 147.95it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1289/3257 [00:08<00:15, 130.24it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1304/3257 [00:08<00:14, 133.91it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1321/3257 [00:08<00:13, 141.19it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:08<00:12, 151.55it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1355/3257 [00:08<00:13, 139.50it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:09<00:13, 135.47it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:09<00:14, 132.52it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1401/3257 [00:09<00:13, 142.38it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1421/3257 [00:09<00:11, 158.24it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1438/3257 [00:09<00:11, 153.39it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1459/3257 [00:09<00:10, 167.82it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1477/3257 [00:09<00:10, 163.69it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1494/3257 [00:09<00:10, 165.04it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:09<00:10, 167.57it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1530/3257 [00:10<00:11, 154.98it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1546/3257 [00:10<00:12, 141.61it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1562/3257 [00:10<00:11, 146.33it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1577/3257 [00:10<00:11, 144.04it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1594/3257 [00:10<00:11, 149.26it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1612/3257 [00:10<00:10, 155.81it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1630/3257 [00:10<00:10, 160.90it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1647/3257 [00:10<00:11, 143.04it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1662/3257 [00:10<00:11, 138.24it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1677/3257 [00:11<00:11, 138.68it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1692/3257 [00:11<00:11, 139.97it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1708/3257 [00:11<00:10, 145.30it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1725/3257 [00:11<00:10, 152.09it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1741/3257 [00:11<00:11, 130.38it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1760/3257 [00:11<00:10, 142.39it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1776/3257 [00:11<00:10, 144.97it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1794/3257 [00:11<00:09, 152.44it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1810/3257 [00:12<00:10, 143.93it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1826/3257 [00:12<00:09, 145.39it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1844/3257 [00:12<00:09, 152.68it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1860/3257 [00:12<00:09, 151.12it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1876/3257 [00:12<00:21, 63.10it/s]  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1891/3257 [00:13<00:18, 75.04it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1910/3257 [00:13<00:14, 93.10it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1924/3257 [00:13<00:13, 101.63it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1945/3257 [00:13<00:10, 124.40it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:13<00:09, 139.99it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1982/3257 [00:13<00:08, 144.06it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1999/3257 [00:13<00:08, 148.68it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2016/3257 [00:13<00:08, 149.24it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2034/3257 [00:13<00:07, 154.93it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2051/3257 [00:14<00:08, 147.24it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2067/3257 [00:14<00:08, 146.75it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2084/3257 [00:14<00:07, 151.71it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2100/3257 [00:14<00:07, 147.63it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2119/3257 [00:14<00:07, 158.94it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2136/3257 [00:14<00:07, 152.03it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2154/3257 [00:14<00:07, 157.41it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2177/3257 [00:14<00:06, 175.88it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2198/3257 [00:14<00:05, 183.69it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2218/3257 [00:15<00:05, 186.14it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2238/3257 [00:15<00:05, 185.75it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2259/3257 [00:15<00:05, 191.91it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2279/3257 [00:15<00:05, 186.65it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2301/3257 [00:15<00:04, 194.28it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2328/3257 [00:15<00:04, 215.66it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2357/3257 [00:15<00:03, 235.36it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2381/3257 [00:15<00:03, 229.13it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:15<00:03, 221.90it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2428/3257 [00:15<00:03, 221.39it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2451/3257 [00:16<00:04, 200.24it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2476/3257 [00:16<00:03, 210.98it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2499/3257 [00:16<00:03, 216.00it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2521/3257 [00:16<00:03, 214.68it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2543/3257 [00:16<00:03, 212.67it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2565/3257 [00:16<00:03, 198.16it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2586/3257 [00:16<00:03, 187.30it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2607/3257 [00:16<00:03, 191.51it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:16<00:03, 203.61it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2652/3257 [00:17<00:03, 193.27it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2673/3257 [00:17<00:02, 195.07it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2693/3257 [00:17<00:02, 195.19it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2713/3257 [00:17<00:03, 169.17it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:17<00:03, 171.84it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2753/3257 [00:17<00:02, 181.36it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2772/3257 [00:17<00:02, 172.50it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2796/3257 [00:17<00:02, 189.41it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2816/3257 [00:18<00:02, 178.56it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2835/3257 [00:18<00:02, 170.91it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2858/3257 [00:18<00:02, 186.35it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2885/3257 [00:18<00:01, 207.13it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2907/3257 [00:18<00:01, 185.00it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:18<00:01, 186.40it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2948/3257 [00:18<00:01, 176.75it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2967/3257 [00:18<00:01, 179.01it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2986/3257 [00:18<00:01, 169.89it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3010/3257 [00:19<00:01, 186.79it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3030/3257 [00:19<00:01, 181.78it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:19<00:01, 182.01it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3075/3257 [00:19<00:00, 199.99it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3096/3257 [00:19<00:00, 193.52it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3120/3257 [00:19<00:00, 204.26it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3141/3257 [00:19<00:00, 195.11it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3161/3257 [00:19<00:00, 189.33it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3181/3257 [00:19<00:00, 179.39it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3204/3257 [00:20<00:00, 191.04it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3224/3257 [00:20<00:00, 187.30it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3247/3257 [00:20<00:00, 199.00it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:20<00:00, 159.95it/s]
2023-02-07 19:03:37.463 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:03:37,464][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d318,n5,mc3,s0.244193,t4>', 'datetime': '2023-02-07T19:03:37.464864', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:03:37,465][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:03:37,466][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:03:37,972][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 19:03:37,973][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:03:38,044][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 22723 unique words (71.45% of original 31803, drops 9080)', 'datetime': '2023-02-07T19:03:38.043964', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:03:38,044][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 5081575 word corpus (99.73% of original 5095118, drops 13543)', 'datetime': '2023-02-07T19:03:38.044419', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:03:38,134][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 19:03:38,135][gensim.models.word2vec][INFO] - sample=0.244193 downsamples 0 most-common words
[2023-02-07 19:03:38,135][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5081575 word corpus (100.0%% of prior 5081575)', 'datetime': '2023-02-07T19:03:38.135720', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:03:38,279][gensim.models.word2vec][INFO] - estimated required memory for 22723 words and 318 dimensions: 73963116 bytes
[2023-02-07 19:03:38,279][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:03:38,318][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 22723 vocabulary and 318 features, using sg=1 hs=0 sample=0.2441928188313125 negative=5 window=19 shrink_windows=True', 'datetime': '2023-02-07T19:03:38.318703', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:03:39,342][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 11.08% examples, 514669 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:03:40,356][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 29.84% examples, 744011 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:03:41,375][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 48.36% examples, 822610 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:03:42,376][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 72.43% examples, 919722 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:03:43,390][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 99.11% examples, 992806 words/s, in_qsize 5, out_qsize 0
[2023-02-07 19:03:43,408][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5073452 effective words) took 5.1s, 997194 effective words/s
[2023-02-07 19:03:44,418][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 30.00% examples, 1511476 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:03:45,425][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 60.27% examples, 1541455 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:03:46,426][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 92.26% examples, 1563312 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:03:46,646][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5073452 effective words) took 3.2s, 1568072 effective words/s
[2023-02-07 19:03:47,656][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 29.26% examples, 1476213 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:03:48,668][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 56.22% examples, 1448311 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:03:49,670][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 84.28% examples, 1428778 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:03:50,203][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5073452 effective words) took 3.6s, 1427400 effective words/s
[2023-02-07 19:03:51,208][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 25.45% examples, 1286978 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:03:52,208][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 51.06% examples, 1322512 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:03:53,210][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 78.66% examples, 1342821 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:03:53,952][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5073452 effective words) took 3.7s, 1354542 effective words/s
[2023-02-07 19:03:54,958][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 27.88% examples, 1420506 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:03:55,960][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 54.44% examples, 1409526 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:03:56,964][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 82.50% examples, 1403453 words/s, in_qsize 6, out_qsize 2
[2023-02-07 19:03:57,547][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5073452 effective words) took 3.6s, 1412163 effective words/s
[2023-02-07 19:03:58,550][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 30.18% examples, 1529897 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:03:59,556][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 60.27% examples, 1548845 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:00,558][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 90.94% examples, 1543533 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:00,828][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5073452 effective words) took 3.3s, 1547792 effective words/s
[2023-02-07 19:04:01,833][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 30.06% examples, 1527200 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:04:02,834][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 59.47% examples, 1536455 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:03,838][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 90.51% examples, 1536989 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:04,130][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5073452 effective words) took 3.3s, 1537083 effective words/s
[2023-02-07 19:04:05,138][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 30.00% examples, 1517364 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:06,143][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 59.75% examples, 1537150 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:07,156][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 90.67% examples, 1533317 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:07,441][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5073452 effective words) took 3.3s, 1534284 effective words/s
[2023-02-07 19:04:08,454][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 28.19% examples, 1422308 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:09,456][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 54.59% examples, 1410487 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:10,460][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 84.46% examples, 1434422 words/s, in_qsize 2, out_qsize 0
[2023-02-07 19:04:10,946][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5073452 effective words) took 3.5s, 1449130 effective words/s
[2023-02-07 19:04:11,949][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 37.86% examples, 1965140 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:12,955][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 77.49% examples, 1980248 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:13,501][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5073452 effective words) took 2.6s, 1987949 effective words/s
[2023-02-07 19:04:14,505][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 27.82% examples, 1414060 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:04:15,510][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 54.59% examples, 1413352 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:04:16,514][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 82.65% examples, 1406404 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:04:17,102][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5073452 effective words) took 3.6s, 1409565 effective words/s
[2023-02-07 19:04:18,106][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 28.09% examples, 1432300 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:19,107][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 54.10% examples, 1403398 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:04:20,108][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 82.47% examples, 1403262 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:20,706][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5073452 effective words) took 3.6s, 1408626 effective words/s
[2023-02-07 19:04:21,720][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 29.08% examples, 1465517 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:22,729][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 57.02% examples, 1467356 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:23,730][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 86.67% examples, 1467893 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:24,151][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5073452 effective words) took 3.4s, 1474506 effective words/s
[2023-02-07 19:04:25,158][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 31.19% examples, 1585731 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:26,160][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 62.76% examples, 1610753 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:27,170][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 94.90% examples, 1600848 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:27,323][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5073452 effective words) took 3.2s, 1600702 effective words/s
[2023-02-07 19:04:28,332][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 30.52% examples, 1547079 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:29,339][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 60.27% examples, 1541934 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:30,340][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 91.10% examples, 1542376 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:04:30,614][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5073452 effective words) took 3.3s, 1542172 effective words/s
[2023-02-07 19:04:30,615][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (76101780 effective words) took 52.3s, 1455201 effective words/s', 'datetime': '2023-02-07T19:04:30.615613', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:04:30.616 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:04:34,395][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190307-33z2iamm/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:04:34.395533', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:04:34,397][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:04:34,574][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190307-33z2iamm/files/../tmp/embedding_model.pt
2023-02-07 19:04:34.575 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:04:36.598 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:04:37.291 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:04:39.658 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2395038003675323, 'test_mae': 1.1138745851406222, 'test_r2': -1.9257764998294098}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.76
wandb: percentage 0.28551
wandb:   test_mae 1.11387
wandb:   test_mse 2.2395
wandb:    test_r2 -1.92578
wandb: 
wandb: üöÄ View run eager-sweep-31 at: https://wandb.ai/xiaoqiz/mof2vec/runs/33z2iamm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_190307-33z2iamm/logs
wandb: Agent Starting Run: v1hk4knc with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 855
wandb: 	model.gensim.alpha: 0.01607024095946575
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.38714706290059697
wandb: 	model.gensim.vector_size: 165
wandb: 	model.gensim.window: 16
wandb: 	model.sklearn.learning_rate: 0.0006280732473434745
wandb: 	model.sklearn.max_depth: 8
wandb: 	model.sklearn.min_child_weight: 0.01406361546534358
wandb: 	model.sklearn.n_estimators: 762
wandb: 	model.sklearn.num_leaves: 92
wandb: 	model.sklearn.reg_alpha: 0.004350150323128607
wandb: 	model.sklearn.reg_lambda: 0.5207821949350411
wandb: 	model.sklearn.subsample: 0.8164112904511074
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190451-v1hk4knc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-32
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/v1hk4knc
2023-02-07 19:04:59.089 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 19:04:59.090 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 855 for sweep.
2023-02-07 19:04:59.090 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.01607024095946575 for sweep.
2023-02-07 19:04:59.090 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 19:04:59.090 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 19:04:59.091 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.38714706290059697 for sweep.
2023-02-07 19:04:59.091 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 165 for sweep.
2023-02-07 19:04:59.091 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 16 for sweep.
2023-02-07 19:04:59.091 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0006280732473434745 for sweep.
2023-02-07 19:04:59.092 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 8 for sweep.
2023-02-07 19:04:59.092 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.01406361546534358 for sweep.
2023-02-07 19:04:59.092 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 762 for sweep.
2023-02-07 19:04:59.093 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 92 for sweep.
2023-02-07 19:04:59.093 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.004350150323128607 for sweep.
2023-02-07 19:04:59.093 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.5207821949350411 for sweep.
2023-02-07 19:04:59.093 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8164112904511074 for sweep.
2023-02-07 19:04:59.093 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:04:59.099 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190451-v1hk4knc/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 855, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 165, 'window': 16, 'min_count': 4, 'dm': 1, 'sample': 0.38714706290059697, 'workers': 4, 'alpha': 0.01607024095946575, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 762, 'max_depth': 8, 'num_leaves': 92, 'reg_alpha': 0.004350150323128607, 'reg_lambda': 0.5207821949350411, 'subsample': 0.8164112904511074, 'min_child_weight': 0.01406361546534358, 'n_jobs': 4, 'learning_rate': 0.0006280732473434745}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 4/3257 [00:00<02:52, 18.91it/s]  1%|          | 19/3257 [00:00<00:45, 71.22it/s]  1%|          | 35/3257 [00:00<00:31, 102.41it/s]  2%|‚ñè         | 50/3257 [00:00<00:27, 118.00it/s]  2%|‚ñè         | 67/3257 [00:00<00:24, 127.63it/s]  3%|‚ñé         | 88/3257 [00:00<00:20, 152.12it/s]  3%|‚ñé         | 105/3257 [00:00<00:21, 146.50it/s]  4%|‚ñé         | 121/3257 [00:00<00:21, 146.41it/s]  4%|‚ñç         | 140/3257 [00:01<00:19, 158.56it/s]  5%|‚ñç         | 158/3257 [00:01<00:19, 159.15it/s]  5%|‚ñå         | 175/3257 [00:01<00:19, 154.37it/s]  6%|‚ñå         | 193/3257 [00:01<00:18, 161.41it/s]  6%|‚ñã         | 211/3257 [00:01<00:18, 165.91it/s]  7%|‚ñã         | 233/3257 [00:01<00:16, 180.59it/s]  8%|‚ñä         | 252/3257 [00:01<00:16, 178.08it/s]  8%|‚ñä         | 270/3257 [00:01<00:18, 165.88it/s]  9%|‚ñâ         | 292/3257 [00:01<00:16, 180.38it/s] 10%|‚ñâ         | 311/3257 [00:02<00:16, 175.08it/s] 10%|‚ñà         | 329/3257 [00:02<00:16, 174.30it/s] 11%|‚ñà         | 347/3257 [00:02<00:17, 170.89it/s] 11%|‚ñà         | 366/3257 [00:02<00:16, 174.47it/s] 12%|‚ñà‚ñè        | 384/3257 [00:02<00:17, 167.76it/s] 12%|‚ñà‚ñè        | 401/3257 [00:02<00:17, 164.35it/s] 13%|‚ñà‚ñé        | 420/3257 [00:02<00:17, 164.12it/s] 13%|‚ñà‚ñé        | 437/3257 [00:02<00:19, 144.01it/s] 14%|‚ñà‚ñç        | 455/3257 [00:02<00:18, 152.81it/s] 15%|‚ñà‚ñç        | 474/3257 [00:03<00:17, 160.48it/s] 15%|‚ñà‚ñå        | 491/3257 [00:03<00:17, 160.83it/s] 16%|‚ñà‚ñå        | 511/3257 [00:03<00:16, 170.71it/s] 16%|‚ñà‚ñå        | 529/3257 [00:03<00:16, 163.82it/s] 17%|‚ñà‚ñã        | 547/3257 [00:03<00:16, 167.51it/s] 17%|‚ñà‚ñã        | 564/3257 [00:03<00:17, 152.88it/s] 18%|‚ñà‚ñä        | 580/3257 [00:03<00:17, 151.36it/s] 18%|‚ñà‚ñä        | 600/3257 [00:03<00:16, 163.17it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:16, 159.43it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:04<00:16, 159.24it/s] 20%|‚ñà‚ñà        | 654/3257 [00:04<00:17, 148.20it/s] 21%|‚ñà‚ñà        | 670/3257 [00:04<00:18, 139.41it/s] 21%|‚ñà‚ñà        | 685/3257 [00:04<00:19, 130.94it/s] 21%|‚ñà‚ñà‚ñè       | 699/3257 [00:04<00:19, 129.59it/s] 22%|‚ñà‚ñà‚ñè       | 716/3257 [00:04<00:18, 140.04it/s] 22%|‚ñà‚ñà‚ñè       | 731/3257 [00:04<00:19, 127.93it/s] 23%|‚ñà‚ñà‚ñé       | 745/3257 [00:04<00:19, 126.43it/s] 23%|‚ñà‚ñà‚ñé       | 761/3257 [00:05<00:18, 134.65it/s] 24%|‚ñà‚ñà‚ñç       | 775/3257 [00:05<00:18, 131.40it/s] 24%|‚ñà‚ñà‚ñç       | 789/3257 [00:05<00:19, 129.37it/s] 25%|‚ñà‚ñà‚ñç       | 805/3257 [00:05<00:17, 137.04it/s] 25%|‚ñà‚ñà‚ñå       | 819/3257 [00:05<00:18, 134.43it/s] 26%|‚ñà‚ñà‚ñå       | 833/3257 [00:05<00:18, 129.30it/s] 26%|‚ñà‚ñà‚ñå       | 847/3257 [00:05<00:19, 121.89it/s] 26%|‚ñà‚ñà‚ñã       | 861/3257 [00:05<00:18, 126.32it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:05<00:18, 126.27it/s] 27%|‚ñà‚ñà‚ñã       | 889/3257 [00:06<00:18, 129.89it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:06<00:17, 135.37it/s] 28%|‚ñà‚ñà‚ñä       | 919/3257 [00:06<00:17, 134.69it/s] 29%|‚ñà‚ñà‚ñä       | 933/3257 [00:06<00:17, 133.23it/s] 29%|‚ñà‚ñà‚ñâ       | 948/3257 [00:06<00:16, 137.81it/s] 30%|‚ñà‚ñà‚ñâ       | 964/3257 [00:06<00:16, 141.68it/s] 30%|‚ñà‚ñà‚ñà       | 979/3257 [00:06<00:16, 135.83it/s] 30%|‚ñà‚ñà‚ñà       | 993/3257 [00:06<00:17, 132.66it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:06<00:17, 129.80it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1024/3257 [00:07<00:15, 140.23it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1039/3257 [00:07<00:17, 127.32it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1053/3257 [00:07<00:17, 126.13it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:07<00:16, 129.07it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1081/3257 [00:07<00:16, 129.66it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1095/3257 [00:07<00:16, 131.18it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1109/3257 [00:07<00:16, 128.73it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1122/3257 [00:07<00:17, 124.53it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1135/3257 [00:07<00:17, 119.78it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1148/3257 [00:08<00:18, 113.46it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1165/3257 [00:08<00:16, 128.38it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1179/3257 [00:08<00:17, 120.75it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1192/3257 [00:08<00:17, 116.87it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1204/3257 [00:08<00:17, 116.95it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1217/3257 [00:08<00:17, 117.58it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1233/3257 [00:08<00:15, 127.84it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1246/3257 [00:09<00:35, 56.01it/s]  39%|‚ñà‚ñà‚ñà‚ñä      | 1258/3257 [00:09<00:31, 64.35it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1272/3257 [00:09<00:25, 76.87it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1284/3257 [00:09<00:24, 81.20it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1297/3257 [00:09<00:21, 90.55it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1311/3257 [00:09<00:19, 101.61it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1327/3257 [00:09<00:16, 114.67it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1342/3257 [00:10<00:15, 122.48it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1356/3257 [00:10<00:15, 120.70it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:10<00:15, 124.43it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:10<00:14, 125.12it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1401/3257 [00:10<00:13, 135.70it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1420/3257 [00:10<00:12, 149.73it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1436/3257 [00:10<00:12, 144.31it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1453/3257 [00:10<00:12, 149.12it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1471/3257 [00:10<00:11, 157.07it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1487/3257 [00:11<00:11, 149.11it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1506/3257 [00:11<00:10, 160.44it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1523/3257 [00:11<00:12, 143.35it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1538/3257 [00:11<00:12, 136.10it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1552/3257 [00:11<00:12, 135.16it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1566/3257 [00:11<00:12, 135.19it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1580/3257 [00:11<00:12, 132.18it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1596/3257 [00:11<00:12, 137.11it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1613/3257 [00:11<00:11, 145.45it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1628/3257 [00:12<00:11, 146.15it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1643/3257 [00:12<00:11, 134.65it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1657/3257 [00:12<00:12, 132.84it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1671/3257 [00:12<00:12, 130.02it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1685/3257 [00:12<00:12, 129.63it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1701/3257 [00:12<00:11, 137.07it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:12<00:11, 137.13it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1729/3257 [00:12<00:11, 130.03it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1743/3257 [00:12<00:12, 122.49it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1759/3257 [00:13<00:11, 130.82it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1773/3257 [00:13<00:11, 131.87it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1787/3257 [00:13<00:10, 133.81it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1801/3257 [00:13<00:10, 132.73it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1815/3257 [00:13<00:11, 130.24it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1829/3257 [00:13<00:11, 128.99it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1846/3257 [00:13<00:10, 136.97it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1863/3257 [00:13<00:09, 145.27it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1879/3257 [00:13<00:09, 146.48it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1894/3257 [00:14<00:09, 141.00it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1912/3257 [00:14<00:08, 150.96it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1928/3257 [00:14<00:09, 145.01it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1948/3257 [00:14<00:08, 156.78it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:14<00:08, 158.86it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1981/3257 [00:14<00:08, 150.68it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1999/3257 [00:14<00:08, 155.93it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2021/3257 [00:14<00:07, 173.83it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2039/3257 [00:14<00:06, 174.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2057/3257 [00:15<00:07, 165.00it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2077/3257 [00:15<00:06, 173.71it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2095/3257 [00:15<00:06, 175.48it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2114/3257 [00:15<00:06, 178.96it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2133/3257 [00:15<00:06, 168.48it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2151/3257 [00:15<00:06, 168.53it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2173/3257 [00:15<00:05, 180.67it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2192/3257 [00:15<00:05, 182.94it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2211/3257 [00:15<00:06, 169.70it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2233/3257 [00:16<00:05, 181.77it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2252/3257 [00:16<00:05, 181.57it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2271/3257 [00:16<00:05, 167.07it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2291/3257 [00:16<00:05, 175.45it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2309/3257 [00:16<00:05, 172.06it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2333/3257 [00:16<00:04, 190.57it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2359/3257 [00:16<00:04, 209.13it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2381/3257 [00:16<00:04, 193.74it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2403/3257 [00:16<00:04, 199.32it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2424/3257 [00:17<00:04, 184.64it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2443/3257 [00:17<00:04, 167.62it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2462/3257 [00:17<00:04, 173.26it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2481/3257 [00:17<00:04, 175.58it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2503/3257 [00:17<00:04, 187.71it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2523/3257 [00:17<00:03, 186.55it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2542/3257 [00:17<00:03, 180.78it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2561/3257 [00:17<00:04, 169.08it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2579/3257 [00:17<00:04, 159.69it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2596/3257 [00:18<00:04, 158.00it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2619/3257 [00:18<00:03, 174.25it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2638/3257 [00:18<00:03, 177.78it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2656/3257 [00:18<00:03, 170.17it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2674/3257 [00:18<00:06, 87.45it/s]  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2693/3257 [00:18<00:05, 104.35it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2708/3257 [00:19<00:05, 106.31it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2726/3257 [00:19<00:04, 120.10it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2747/3257 [00:19<00:03, 138.86it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2764/3257 [00:19<00:03, 144.59it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2781/3257 [00:19<00:03, 149.96it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2802/3257 [00:19<00:02, 165.77it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2820/3257 [00:19<00:02, 160.81it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2837/3257 [00:19<00:02, 155.18it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2856/3257 [00:19<00:02, 164.30it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:20<00:02, 184.66it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2900/3257 [00:20<00:02, 165.65it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2918/3257 [00:20<00:02, 168.97it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2936/3257 [00:20<00:01, 167.59it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2954/3257 [00:20<00:01, 159.19it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2972/3257 [00:20<00:01, 164.33it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2989/3257 [00:20<00:01, 155.49it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3010/3257 [00:20<00:01, 170.17it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3028/3257 [00:21<00:01, 160.77it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:21<00:01, 169.52it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3072/3257 [00:21<00:01, 184.67it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3091/3257 [00:21<00:00, 176.60it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3112/3257 [00:21<00:00, 184.10it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3131/3257 [00:21<00:00, 179.39it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3150/3257 [00:21<00:00, 169.74it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3168/3257 [00:21<00:00, 170.41it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3186/3257 [00:21<00:00, 164.53it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3205/3257 [00:22<00:00, 171.34it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3223/3257 [00:22<00:00, 160.52it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:22<00:00, 169.53it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:22<00:00, 145.87it/s]
2023-02-07 19:05:22.367 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:05:22,368][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d165,n5,w16,mc4,s0.387147,t4>', 'datetime': '2023-02-07T19:05:22.368708', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:05:22,369][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:05:22,369][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:05:22,974][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 19:05:22,975][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:05:23,065][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 29175 unique words (68.32% of original 42701, drops 13526)', 'datetime': '2023-02-07T19:05:23.065618', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:05:23,066][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 5800681 word corpus (99.62% of original 5822992, drops 22311)', 'datetime': '2023-02-07T19:05:23.066099', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:05:23,178][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 19:05:23,179][gensim.models.word2vec][INFO] - sample=0.387147 downsamples 0 most-common words
[2023-02-07 19:05:23,180][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5800681 word corpus (100.0%% of prior 5800681)', 'datetime': '2023-02-07T19:05:23.179983', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:05:23,363][gensim.models.word2vec][INFO] - estimated required memory for 29175 words and 165 dimensions: 55899520 bytes
[2023-02-07 19:05:23,364][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:05:23,390][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 29175 vocabulary and 165 features, using sg=0 hs=0 sample=0.38714706290059697 negative=5 window=16 shrink_windows=True', 'datetime': '2023-02-07T19:05:23.390543', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:05:24,404][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 21.12% examples, 1194971 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:25,403][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 40.56% examples, 1196892 words/s, in_qsize 7, out_qsize 1
[2023-02-07 19:05:26,401][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 62.85% examples, 1225751 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:27,402][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 84.59% examples, 1228782 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:28,165][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5774537 effective words) took 4.8s, 1209967 effective words/s
[2023-02-07 19:05:29,182][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 19.37% examples, 1078401 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:30,202][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 38.78% examples, 1124926 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:05:31,208][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 59.23% examples, 1147756 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:32,210][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 81.55% examples, 1174160 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:33,002][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5774537 effective words) took 4.8s, 1194425 effective words/s
[2023-02-07 19:05:34,007][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 22.20% examples, 1265750 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:05:35,015][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 43.14% examples, 1273640 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:05:36,027][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 65.64% examples, 1279133 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:05:37,029][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 88.79% examples, 1281366 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:05:37,582][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5774537 effective words) took 4.6s, 1261484 effective words/s
[2023-02-07 19:05:38,588][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 24.32% examples, 1390265 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:39,595][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 44.43% examples, 1306958 words/s, in_qsize 6, out_qsize 0
[2023-02-07 19:05:40,595][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 67.45% examples, 1319809 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:41,604][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 91.62% examples, 1327453 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:41,929][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5774537 effective words) took 4.3s, 1329604 effective words/s
[2023-02-07 19:05:42,936][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 23.37% examples, 1331657 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:43,937][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 46.61% examples, 1362824 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:44,937][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 70.22% examples, 1378031 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:45,942][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 95.79% examples, 1379560 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:46,103][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5774537 effective words) took 4.2s, 1384092 effective words/s
[2023-02-07 19:05:47,107][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 24.81% examples, 1423177 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:48,113][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 47.74% examples, 1405589 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:49,123][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 72.77% examples, 1414613 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:05:50,130][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 98.68% examples, 1419430 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:50,168][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5774537 effective words) took 4.1s, 1421697 effective words/s
[2023-02-07 19:05:51,169][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 24.50% examples, 1407625 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:52,173][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 46.76% examples, 1372927 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:53,175][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 70.22% examples, 1378024 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:54,181][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 95.18% examples, 1372544 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:54,369][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5774537 effective words) took 4.2s, 1374930 effective words/s
[2023-02-07 19:05:55,381][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 28.58% examples, 1641922 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:05:56,382][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 51.34% examples, 1507597 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:57,382][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 75.13% examples, 1463030 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:05:58,369][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5774537 effective words) took 4.0s, 1444674 effective words/s
[2023-02-07 19:05:59,372][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 28.00% examples, 1619033 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:00,377][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 55.14% examples, 1624536 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:06:01,378][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 79.37% examples, 1542903 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:02,148][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5774537 effective words) took 3.8s, 1528808 effective words/s
[2023-02-07 19:06:03,157][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 28.89% examples, 1664583 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:04,158][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 58.58% examples, 1719203 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:05,159][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 89.96% examples, 1734828 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:05,472][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5774537 effective words) took 3.3s, 1738317 effective words/s
[2023-02-07 19:06:06,485][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 25.79% examples, 1472198 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:07,487][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 50.54% examples, 1476648 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:08,488][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 75.90% examples, 1473004 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:09,401][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5774537 effective words) took 3.9s, 1470168 effective words/s
[2023-02-07 19:06:10,412][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 29.87% examples, 1721329 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:06:11,412][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 58.95% examples, 1733040 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:12,418][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 86.95% examples, 1681253 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:12,905][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5774537 effective words) took 3.5s, 1649003 effective words/s
[2023-02-07 19:06:13,909][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 25.45% examples, 1464591 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:14,922][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 50.17% examples, 1466932 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:06:15,929][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 76.14% examples, 1475178 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:16,816][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5774537 effective words) took 3.9s, 1477593 effective words/s
[2023-02-07 19:06:17,821][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 22.32% examples, 1263958 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:06:18,827][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 45.56% examples, 1332482 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:19,828][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 68.71% examples, 1346448 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:20,836][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 95.64% examples, 1375217 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:20,998][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5774537 effective words) took 4.2s, 1381211 effective words/s
[2023-02-07 19:06:22,000][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 25.15% examples, 1441833 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:23,008][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 49.55% examples, 1450668 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:06:24,013][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 74.52% examples, 1447731 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:06:24,964][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5774537 effective words) took 4.0s, 1456792 effective words/s
[2023-02-07 19:06:24,965][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86618055 effective words) took 61.6s, 1406727 effective words/s', 'datetime': '2023-02-07T19:06:24.965240', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:06:24.966 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:06:29,100][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190451-v1hk4knc/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:06:29.100080', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:06:29,101][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:06:29,241][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190451-v1hk4knc/files/../tmp/embedding_model.pt
2023-02-07 19:06:29.241 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:06:30.816 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:06:31.354 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:06:32.808 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2429800077911124, 'test_mae': 1.1376469570734864, 'test_r2': -2.3673089938110983}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.58
wandb: percentage 0.31676
wandb:   test_mae 1.13765
wandb:   test_mse 2.24298
wandb:    test_r2 -2.36731
wandb: 
wandb: üöÄ View run hearty-sweep-32 at: https://wandb.ai/xiaoqiz/mof2vec/runs/v1hk4knc
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_190451-v1hk4knc/logs
wandb: Agent Starting Run: f65zorqb with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 453
wandb: 	model.gensim.alpha: 0.4788092962094456
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.4787882618109933
wandb: 	model.gensim.vector_size: 173
wandb: 	model.gensim.window: 8
wandb: 	model.sklearn.learning_rate: 0.009923726687549332
wandb: 	model.sklearn.max_depth: 40
wandb: 	model.sklearn.min_child_weight: 0.027922938706908942
wandb: 	model.sklearn.n_estimators: 1472
wandb: 	model.sklearn.num_leaves: 454
wandb: 	model.sklearn.reg_alpha: 0.2170077291921552
wandb: 	model.sklearn.reg_lambda: 0.04819789092271322
wandb: 	model.sklearn.subsample: 0.21518902413321545
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190644-f65zorqb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-33
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/f65zorqb
2023-02-07 19:06:52.969 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 19:06:52.970 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 453 for sweep.
2023-02-07 19:06:52.970 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.4788092962094456 for sweep.
2023-02-07 19:06:52.970 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 19:06:52.972 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 19:06:52.973 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4787882618109933 for sweep.
2023-02-07 19:06:52.973 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 173 for sweep.
2023-02-07 19:06:52.973 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 8 for sweep.
2023-02-07 19:06:52.973 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.009923726687549332 for sweep.
2023-02-07 19:06:52.974 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 40 for sweep.
2023-02-07 19:06:52.974 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.027922938706908942 for sweep.
2023-02-07 19:06:52.974 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1472 for sweep.
2023-02-07 19:06:52.975 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 454 for sweep.
2023-02-07 19:06:52.975 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.2170077291921552 for sweep.
2023-02-07 19:06:52.975 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.04819789092271322 for sweep.
2023-02-07 19:06:52.976 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.21518902413321545 for sweep.
2023-02-07 19:06:52.976 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:06:52.982 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190644-f65zorqb/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 453, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 173, 'window': 8, 'min_count': 1, 'dm': 1, 'sample': 0.4787882618109933, 'workers': 4, 'alpha': 0.4788092962094456, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1472, 'max_depth': 40, 'num_leaves': 454, 'reg_alpha': 0.2170077291921552, 'reg_lambda': 0.04819789092271322, 'subsample': 0.21518902413321545, 'min_child_weight': 0.027922938706908942, 'n_jobs': 4, 'learning_rate': 0.009923726687549332}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:18, 170.52it/s]  1%|          | 38/3257 [00:00<00:17, 185.29it/s]  2%|‚ñè         | 57/3257 [00:00<00:17, 179.24it/s]  2%|‚ñè         | 78/3257 [00:00<00:16, 189.81it/s]  3%|‚ñé         | 98/3257 [00:00<00:16, 185.90it/s]  4%|‚ñé         | 117/3257 [00:00<00:17, 180.64it/s]  4%|‚ñç         | 138/3257 [00:00<00:16, 189.14it/s]  5%|‚ñç         | 158/3257 [00:00<00:16, 188.55it/s]  5%|‚ñå         | 177/3257 [00:00<00:16, 183.55it/s]  6%|‚ñå         | 196/3257 [00:01<00:16, 184.83it/s]  7%|‚ñã         | 216/3257 [00:01<00:16, 186.57it/s]  7%|‚ñã         | 237/3257 [00:01<00:15, 189.96it/s]  8%|‚ñä         | 258/3257 [00:01<00:15, 194.27it/s]  9%|‚ñä         | 278/3257 [00:01<00:15, 194.02it/s]  9%|‚ñâ         | 298/3257 [00:01<00:15, 194.76it/s] 10%|‚ñâ         | 318/3257 [00:01<00:15, 191.06it/s] 10%|‚ñà         | 338/3257 [00:01<00:15, 188.44it/s] 11%|‚ñà         | 359/3257 [00:01<00:14, 193.79it/s] 12%|‚ñà‚ñè        | 379/3257 [00:02<00:15, 186.62it/s] 12%|‚ñà‚ñè        | 398/3257 [00:02<00:15, 184.86it/s] 13%|‚ñà‚ñé        | 420/3257 [00:02<00:15, 188.56it/s] 13%|‚ñà‚ñé        | 439/3257 [00:02<00:17, 160.19it/s] 14%|‚ñà‚ñç        | 458/3257 [00:02<00:16, 167.64it/s] 15%|‚ñà‚ñç        | 477/3257 [00:02<00:16, 173.51it/s] 15%|‚ñà‚ñå        | 495/3257 [00:02<00:15, 173.45it/s] 16%|‚ñà‚ñå        | 516/3257 [00:02<00:15, 182.02it/s] 16%|‚ñà‚ñã        | 535/3257 [00:02<00:15, 180.94it/s] 17%|‚ñà‚ñã        | 556/3257 [00:03<00:14, 187.90it/s] 18%|‚ñà‚ñä        | 575/3257 [00:03<00:16, 164.62it/s] 18%|‚ñà‚ñä        | 597/3257 [00:03<00:14, 177.64it/s] 19%|‚ñà‚ñâ        | 619/3257 [00:03<00:14, 187.42it/s] 20%|‚ñà‚ñâ        | 639/3257 [00:03<00:14, 183.41it/s] 20%|‚ñà‚ñà        | 658/3257 [00:03<00:15, 171.95it/s] 21%|‚ñà‚ñà        | 678/3257 [00:03<00:14, 177.47it/s] 21%|‚ñà‚ñà‚ñè       | 697/3257 [00:03<00:15, 167.04it/s] 22%|‚ñà‚ñà‚ñè       | 717/3257 [00:03<00:14, 172.69it/s] 23%|‚ñà‚ñà‚ñé       | 735/3257 [00:04<00:15, 167.15it/s] 23%|‚ñà‚ñà‚ñé       | 752/3257 [00:04<00:15, 164.12it/s] 24%|‚ñà‚ñà‚ñé       | 772/3257 [00:04<00:14, 172.75it/s] 24%|‚ñà‚ñà‚ñç       | 791/3257 [00:04<00:14, 175.14it/s] 25%|‚ñà‚ñà‚ñç       | 809/3257 [00:04<00:13, 176.15it/s] 26%|‚ñà‚ñà‚ñå       | 832/3257 [00:04<00:12, 188.75it/s] 26%|‚ñà‚ñà‚ñå       | 854/3257 [00:04<00:12, 196.43it/s] 27%|‚ñà‚ñà‚ñã       | 876/3257 [00:04<00:11, 201.65it/s] 28%|‚ñà‚ñà‚ñä       | 904/3257 [00:04<00:10, 224.13it/s] 28%|‚ñà‚ñà‚ñä       | 927/3257 [00:05<00:17, 133.43it/s] 29%|‚ñà‚ñà‚ñâ       | 952/3257 [00:05<00:14, 155.45it/s] 30%|‚ñà‚ñà‚ñà       | 979/3257 [00:05<00:12, 176.43it/s] 31%|‚ñà‚ñà‚ñà       | 1001/3257 [00:05<00:12, 186.00it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1025/3257 [00:05<00:11, 198.45it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:05<00:11, 196.58it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1073/3257 [00:05<00:10, 212.40it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1096/3257 [00:05<00:10, 213.92it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1122/3257 [00:06<00:09, 225.53it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1146/3257 [00:06<00:09, 227.83it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1175/3257 [00:06<00:08, 243.28it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1200/3257 [00:06<00:08, 230.59it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1227/3257 [00:06<00:08, 240.57it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1255/3257 [00:06<00:08, 248.78it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1281/3257 [00:06<00:08, 242.40it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1308/3257 [00:06<00:07, 248.19it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1335/3257 [00:06<00:07, 252.79it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1361/3257 [00:07<00:08, 234.47it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1385/3257 [00:07<00:08, 215.30it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1417/3257 [00:07<00:07, 241.77it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1444/3257 [00:07<00:07, 248.64it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1471/3257 [00:07<00:07, 253.72it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1500/3257 [00:07<00:06, 261.56it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1527/3257 [00:07<00:06, 254.18it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1553/3257 [00:07<00:06, 245.28it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1580/3257 [00:07<00:06, 251.70it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1610/3257 [00:08<00:06, 264.80it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1637/3257 [00:08<00:06, 256.88it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1663/3257 [00:08<00:06, 249.11it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1689/3257 [00:08<00:06, 249.82it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1716/3257 [00:08<00:06, 252.17it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1742/3257 [00:08<00:06, 239.68it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1771/3257 [00:08<00:05, 252.23it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1798/3257 [00:08<00:05, 255.91it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1825/3257 [00:08<00:05, 255.69it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1851/3257 [00:09<00:05, 252.47it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1879/3257 [00:09<00:05, 259.17it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1907/3257 [00:09<00:05, 264.93it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1934/3257 [00:09<00:05, 263.99it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1969/3257 [00:09<00:04, 287.02it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1998/3257 [00:09<00:04, 284.64it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2027/3257 [00:09<00:04, 284.52it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2056/3257 [00:09<00:04, 258.29it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2083/3257 [00:09<00:04, 260.62it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2110/3257 [00:09<00:04, 262.03it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2137/3257 [00:10<00:04, 251.14it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2163/3257 [00:10<00:04, 248.44it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2191/3257 [00:10<00:04, 257.29it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2217/3257 [00:10<00:04, 257.58it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2243/3257 [00:10<00:04, 253.43it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2269/3257 [00:10<00:03, 252.35it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2295/3257 [00:11<00:08, 119.83it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2315/3257 [00:11<00:07, 132.28it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2341/3257 [00:11<00:05, 155.79it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2367/3257 [00:11<00:05, 174.72it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2394/3257 [00:11<00:04, 195.32it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2418/3257 [00:11<00:04, 193.79it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2441/3257 [00:11<00:04, 191.62it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2464/3257 [00:11<00:03, 199.81it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2486/3257 [00:11<00:03, 204.69it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2513/3257 [00:12<00:03, 220.94it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2537/3257 [00:12<00:03, 223.74it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2561/3257 [00:12<00:03, 211.34it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2583/3257 [00:12<00:03, 202.98it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2607/3257 [00:12<00:03, 211.84it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2634/3257 [00:12<00:02, 225.50it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2657/3257 [00:12<00:02, 218.64it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2680/3257 [00:12<00:02, 221.26it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2703/3257 [00:12<00:02, 200.21it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2724/3257 [00:13<00:02, 201.01it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2751/3257 [00:13<00:02, 215.13it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2773/3257 [00:13<00:02, 207.94it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2799/3257 [00:13<00:02, 222.19it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2822/3257 [00:13<00:02, 210.79it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:13<00:02, 197.35it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2874/3257 [00:13<00:01, 223.37it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2897/3257 [00:13<00:01, 211.39it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2920/3257 [00:13<00:01, 215.12it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2943/3257 [00:14<00:01, 206.00it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2966/3257 [00:14<00:01, 211.83it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2988/3257 [00:14<00:01, 213.27it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3018/3257 [00:14<00:01, 234.18it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:14<00:00, 254.01it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3081/3257 [00:14<00:00, 272.71it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3110/3257 [00:14<00:00, 276.66it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3139/3257 [00:14<00:00, 278.01it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3167/3257 [00:14<00:00, 271.60it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3196/3257 [00:15<00:00, 275.25it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3224/3257 [00:15<00:00, 265.39it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3254/3257 [00:15<00:00, 273.13it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:15<00:00, 213.59it/s]
2023-02-07 19:07:08.828 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:07:08,830][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d173,n5,w8,s0.478788,t4>', 'datetime': '2023-02-07T19:07:08.829975', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:07:08,830][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:07:08,830][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:07:09,181][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 19:07:09,182][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:07:09,218][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 13061 unique words (100.00% of original 13061, drops 0)', 'datetime': '2023-02-07T19:07:09.218713', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:07:09,219][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 3639370 word corpus (100.00% of original 3639370, drops 0)', 'datetime': '2023-02-07T19:07:09.219540', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:07:09,265][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 19:07:09,266][gensim.models.word2vec][INFO] - sample=0.478788 downsamples 0 most-common words
[2023-02-07 19:07:09,266][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3639370 word corpus (100.0%% of prior 3639370)', 'datetime': '2023-02-07T19:07:09.266215', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:07:09,336][gensim.models.word2vec][INFO] - estimated required memory for 13061 words and 173 dimensions: 27512168 bytes
[2023-02-07 19:07:09,336][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:07:09,353][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 13061 vocabulary and 173 features, using sg=0 hs=0 sample=0.4787882618109933 negative=5 window=8 shrink_windows=True', 'datetime': '2023-02-07T19:07:09.353301', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:07:10,363][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 20.85% examples, 740930 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:07:11,375][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 41.05% examples, 761926 words/s, in_qsize 8, out_qsize 2
[2023-02-07 19:07:12,380][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 78.72% examples, 957932 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:07:12,920][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3642627 effective words) took 3.6s, 1021776 effective words/s
[2023-02-07 19:07:13,928][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 40.41% examples, 1506792 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:07:14,930][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 84.99% examples, 1558762 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:07:15,222][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3642627 effective words) took 2.3s, 1584814 effective words/s
[2023-02-07 19:07:16,225][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 43.02% examples, 1609914 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:07:17,229][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 90.42% examples, 1654646 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:07:17,428][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3642627 effective words) took 2.2s, 1652480 effective words/s
[2023-02-07 19:07:18,432][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 55.39% examples, 2063346 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:07:19,225][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3642627 effective words) took 1.8s, 2030487 effective words/s
[2023-02-07 19:07:20,232][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 51.61% examples, 1915787 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:07:21,236][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 95.30% examples, 1731078 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:07:21,343][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3642627 effective words) took 2.1s, 1721581 effective words/s
[2023-02-07 19:07:22,352][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 51.86% examples, 1918707 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:07:23,228][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3642627 effective words) took 1.9s, 1934669 effective words/s
[2023-02-07 19:07:24,241][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 44.12% examples, 1631377 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:07:25,254][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 82.25% examples, 1492550 words/s, in_qsize 8, out_qsize 2
[2023-02-07 19:07:25,599][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3642627 effective words) took 2.4s, 1537644 effective words/s
[2023-02-07 19:07:26,609][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 52.38% examples, 1937475 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:07:27,612][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 90.36% examples, 1651836 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:07:27,858][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3642627 effective words) took 2.3s, 1614933 effective words/s
[2023-02-07 19:07:28,872][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 51.86% examples, 1911507 words/s, in_qsize 6, out_qsize 3
[2023-02-07 19:07:29,765][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3642627 effective words) took 1.9s, 1912773 effective words/s
[2023-02-07 19:07:30,769][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 48.76% examples, 1813364 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:07:31,687][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3642627 effective words) took 1.9s, 1897941 effective words/s
[2023-02-07 19:07:32,689][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 51.37% examples, 1913304 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:07:33,575][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3642627 effective words) took 1.9s, 1930460 effective words/s
[2023-02-07 19:07:34,582][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 49.62% examples, 1834151 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:07:35,549][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3642627 effective words) took 2.0s, 1848460 effective words/s
[2023-02-07 19:07:36,553][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 49.31% examples, 1829920 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:07:37,485][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3642627 effective words) took 1.9s, 1883494 effective words/s
[2023-02-07 19:07:38,490][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 50.72% examples, 1879194 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:07:39,414][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3642627 effective words) took 1.9s, 1889258 effective words/s
[2023-02-07 19:07:40,419][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 48.76% examples, 1809739 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:07:41,385][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3642627 effective words) took 2.0s, 1850223 effective words/s
[2023-02-07 19:07:41,386][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54639405 effective words) took 32.0s, 1705726 effective words/s', 'datetime': '2023-02-07T19:07:41.386615', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:07:41.386 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:07:45,013][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190644-f65zorqb/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:07:45.013683', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:07:45,014][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:07:45,067][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190644-f65zorqb/files/../tmp/embedding_model.pt
2023-02-07 19:07:45.068 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:07:46.585 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:07:47.133 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:07:48.360 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.7552413345928737, 'test_mae': 1.2576614180136976, 'test_r2': -4.968187799110124}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.0
wandb: percentage 0.0
wandb:   test_mae 1.25766
wandb:   test_mse 2.75524
wandb:    test_r2 -4.96819
wandb: 
wandb: üöÄ View run copper-sweep-33 at: https://wandb.ai/xiaoqiz/mof2vec/runs/f65zorqb
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_190644-f65zorqb/logs
wandb: Agent Starting Run: 467ru3b9 with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 383
wandb: 	model.gensim.alpha: 0.1184855571404746
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.3756901291832539
wandb: 	model.gensim.vector_size: 463
wandb: 	model.gensim.window: 19
wandb: 	model.sklearn.learning_rate: 0.24704205605369703
wandb: 	model.sklearn.max_depth: 46
wandb: 	model.sklearn.min_child_weight: 0.03828037118704895
wandb: 	model.sklearn.n_estimators: 4369
wandb: 	model.sklearn.num_leaves: 364
wandb: 	model.sklearn.reg_alpha: 0.22817078318959447
wandb: 	model.sklearn.reg_lambda: 0.5385597611110368
wandb: 	model.sklearn.subsample: 0.9997366853560276
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190758-467ru3b9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-34
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/467ru3b9
2023-02-07 19:08:06.756 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 19:08:06.757 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 383 for sweep.
2023-02-07 19:08:06.757 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.1184855571404746 for sweep.
2023-02-07 19:08:06.757 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 19:08:06.758 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 19:08:06.758 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3756901291832539 for sweep.
2023-02-07 19:08:06.758 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 463 for sweep.
2023-02-07 19:08:06.758 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 19 for sweep.
2023-02-07 19:08:06.759 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.24704205605369703 for sweep.
2023-02-07 19:08:06.759 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 46 for sweep.
2023-02-07 19:08:06.759 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03828037118704895 for sweep.
2023-02-07 19:08:06.759 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4369 for sweep.
2023-02-07 19:08:06.759 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 364 for sweep.
2023-02-07 19:08:06.760 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.22817078318959447 for sweep.
2023-02-07 19:08:06.760 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.5385597611110368 for sweep.
2023-02-07 19:08:06.760 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.9997366853560276 for sweep.
2023-02-07 19:08:06.760 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:08:06.769 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190758-467ru3b9/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 383, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 463, 'window': 19, 'min_count': 6, 'dm': 1, 'sample': 0.3756901291832539, 'workers': 4, 'alpha': 0.1184855571404746, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4369, 'max_depth': 46, 'num_leaves': 364, 'reg_alpha': 0.22817078318959447, 'reg_lambda': 0.5385597611110368, 'subsample': 0.9997366853560276, 'min_child_weight': 0.03828037118704895, 'n_jobs': 4, 'learning_rate': 0.24704205605369703}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:19, 166.65it/s]  1%|          | 35/3257 [00:00<00:18, 174.03it/s]  2%|‚ñè         | 53/3257 [00:00<00:18, 173.50it/s]  2%|‚ñè         | 74/3257 [00:00<00:17, 185.35it/s]  3%|‚ñé         | 95/3257 [00:00<00:16, 191.24it/s]  4%|‚ñé         | 115/3257 [00:00<00:17, 182.00it/s]  4%|‚ñç         | 134/3257 [00:00<00:17, 182.00it/s]  5%|‚ñç         | 154/3257 [00:00<00:16, 182.83it/s]  5%|‚ñå         | 173/3257 [00:00<00:17, 171.51it/s]  6%|‚ñå         | 194/3257 [00:01<00:16, 180.81it/s]  7%|‚ñã         | 216/3257 [00:01<00:16, 189.97it/s]  7%|‚ñã         | 237/3257 [00:01<00:15, 194.95it/s]  8%|‚ñä         | 258/3257 [00:01<00:15, 198.31it/s]  9%|‚ñä         | 282/3257 [00:01<00:14, 207.34it/s]  9%|‚ñâ         | 303/3257 [00:01<00:14, 204.04it/s] 10%|‚ñà         | 326/3257 [00:01<00:13, 211.04it/s] 11%|‚ñà         | 348/3257 [00:01<00:14, 197.63it/s] 11%|‚ñà‚ñè        | 372/3257 [00:01<00:13, 207.75it/s] 12%|‚ñà‚ñè        | 393/3257 [00:02<00:15, 189.92it/s] 13%|‚ñà‚ñé        | 415/3257 [00:02<00:14, 197.76it/s] 13%|‚ñà‚ñé        | 436/3257 [00:02<00:16, 171.14it/s] 14%|‚ñà‚ñç        | 457/3257 [00:02<00:15, 180.13it/s] 15%|‚ñà‚ñç        | 476/3257 [00:02<00:15, 182.36it/s] 15%|‚ñà‚ñå        | 498/3257 [00:02<00:14, 191.11it/s] 16%|‚ñà‚ñå        | 520/3257 [00:02<00:13, 197.81it/s] 17%|‚ñà‚ñã        | 541/3257 [00:03<00:23, 117.26it/s] 17%|‚ñà‚ñã        | 558/3257 [00:03<00:21, 125.76it/s] 18%|‚ñà‚ñä        | 574/3257 [00:03<00:20, 130.25it/s] 18%|‚ñà‚ñä        | 597/3257 [00:03<00:17, 152.26it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:16, 161.64it/s] 20%|‚ñà‚ñâ        | 641/3257 [00:03<00:15, 173.36it/s] 20%|‚ñà‚ñà        | 660/3257 [00:03<00:15, 166.85it/s] 21%|‚ñà‚ñà        | 682/3257 [00:03<00:14, 179.84it/s] 22%|‚ñà‚ñà‚ñè       | 701/3257 [00:03<00:14, 173.94it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:04<00:14, 179.91it/s] 23%|‚ñà‚ñà‚ñé       | 741/3257 [00:04<00:14, 173.54it/s] 23%|‚ñà‚ñà‚ñé       | 764/3257 [00:04<00:13, 187.82it/s] 24%|‚ñà‚ñà‚ñç       | 784/3257 [00:04<00:13, 181.75it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:04<00:13, 187.25it/s] 25%|‚ñà‚ñà‚ñå       | 825/3257 [00:04<00:13, 185.97it/s] 26%|‚ñà‚ñà‚ñå       | 844/3257 [00:04<00:13, 182.08it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:04<00:12, 184.32it/s] 27%|‚ñà‚ñà‚ñã       | 884/3257 [00:04<00:13, 181.33it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:05<00:12, 189.08it/s] 28%|‚ñà‚ñà‚ñä       | 928/3257 [00:05<00:11, 195.59it/s] 29%|‚ñà‚ñà‚ñâ       | 948/3257 [00:05<00:12, 191.19it/s] 30%|‚ñà‚ñà‚ñâ       | 969/3257 [00:05<00:11, 196.36it/s] 30%|‚ñà‚ñà‚ñà       | 989/3257 [00:05<00:11, 189.58it/s] 31%|‚ñà‚ñà‚ñà       | 1009/3257 [00:05<00:12, 183.99it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1028/3257 [00:05<00:12, 182.23it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:05<00:12, 180.23it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1068/3257 [00:05<00:11, 187.58it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1088/3257 [00:06<00:11, 188.14it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1107/3257 [00:06<00:11, 186.87it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1126/3257 [00:06<00:11, 183.79it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1145/3257 [00:06<00:11, 180.58it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1169/3257 [00:06<00:10, 196.94it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1189/3257 [00:06<00:12, 169.99it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1207/3257 [00:06<00:11, 171.99it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1230/3257 [00:06<00:10, 187.52it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1250/3257 [00:06<00:10, 183.43it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:07<00:10, 190.11it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1291/3257 [00:07<00:11, 175.61it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1309/3257 [00:07<00:11, 176.74it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1331/3257 [00:07<00:10, 187.27it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1351/3257 [00:07<00:10, 183.75it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:07<00:10, 180.38it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1389/3257 [00:07<00:10, 173.48it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1414/3257 [00:07<00:09, 193.52it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1435/3257 [00:07<00:09, 196.71it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1459/3257 [00:08<00:08, 206.13it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1480/3257 [00:08<00:08, 202.59it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1504/3257 [00:08<00:08, 211.12it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1526/3257 [00:08<00:08, 198.74it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1547/3257 [00:08<00:09, 188.32it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1567/3257 [00:08<00:09, 186.24it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1586/3257 [00:08<00:09, 182.68it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1606/3257 [00:08<00:08, 184.75it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1627/3257 [00:08<00:08, 191.46it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1647/3257 [00:09<00:09, 178.33it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1666/3257 [00:09<00:09, 171.47it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1684/3257 [00:09<00:09, 168.17it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1703/3257 [00:09<00:09, 171.54it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1721/3257 [00:09<00:08, 171.75it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1739/3257 [00:09<00:09, 158.68it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1760/3257 [00:09<00:08, 169.88it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1779/3257 [00:09<00:08, 173.72it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1797/3257 [00:09<00:08, 173.74it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1815/3257 [00:10<00:08, 169.70it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1833/3257 [00:10<00:08, 166.96it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1850/3257 [00:10<00:15, 88.27it/s]  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1870/3257 [00:10<00:12, 107.53it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1889/3257 [00:10<00:11, 123.90it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1910/3257 [00:10<00:09, 142.05it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1928/3257 [00:10<00:08, 150.71it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1954/3257 [00:11<00:07, 178.03it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1976/3257 [00:11<00:06, 188.05it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1997/3257 [00:11<00:06, 187.80it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2017/3257 [00:11<00:06, 189.75it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2038/3257 [00:11<00:06, 192.99it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2058/3257 [00:11<00:06, 177.76it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2077/3257 [00:11<00:06, 180.67it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2096/3257 [00:11<00:06, 170.48it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2117/3257 [00:11<00:06, 179.01it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2136/3257 [00:12<00:06, 170.49it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2154/3257 [00:12<00:06, 169.47it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2176/3257 [00:12<00:05, 182.76it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2195/3257 [00:12<00:05, 181.81it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2214/3257 [00:12<00:05, 176.42it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:12<00:05, 183.84it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2255/3257 [00:12<00:05, 179.68it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2274/3257 [00:12<00:05, 169.30it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2297/3257 [00:12<00:05, 184.68it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2316/3257 [00:13<00:05, 183.93it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2341/3257 [00:13<00:04, 200.63it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2365/3257 [00:13<00:04, 211.21it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2387/3257 [00:13<00:04, 211.77it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2409/3257 [00:13<00:04, 198.66it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2430/3257 [00:13<00:04, 195.91it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2450/3257 [00:13<00:04, 182.97it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2475/3257 [00:13<00:03, 197.93it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2498/3257 [00:13<00:03, 205.28it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2520/3257 [00:14<00:03, 209.15it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2542/3257 [00:14<00:03, 207.94it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2563/3257 [00:14<00:03, 197.27it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2583/3257 [00:14<00:03, 186.64it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2605/3257 [00:14<00:03, 193.53it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:14<00:02, 209.04it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2653/3257 [00:14<00:03, 197.28it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2673/3257 [00:14<00:03, 194.34it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2694/3257 [00:14<00:02, 189.23it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2714/3257 [00:15<00:03, 178.36it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2737/3257 [00:15<00:02, 189.63it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2759/3257 [00:15<00:02, 197.09it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2779/3257 [00:15<00:02, 190.31it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2803/3257 [00:15<00:02, 202.89it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2824/3257 [00:15<00:02, 186.88it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:15<00:02, 184.54it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2872/3257 [00:15<00:01, 207.10it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2894/3257 [00:16<00:01, 198.07it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2916/3257 [00:16<00:01, 199.32it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2937/3257 [00:16<00:01, 199.56it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2958/3257 [00:16<00:01, 182.21it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2977/3257 [00:16<00:01, 183.52it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2996/3257 [00:16<00:01, 183.11it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3016/3257 [00:16<00:01, 187.64it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3038/3257 [00:16<00:01, 195.32it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3063/3257 [00:16<00:00, 207.61it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3085/3257 [00:16<00:00, 206.07it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3109/3257 [00:17<00:00, 212.12it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3131/3257 [00:17<00:00, 206.95it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3152/3257 [00:17<00:00, 191.59it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3172/3257 [00:17<00:00, 188.53it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3191/3257 [00:17<00:00, 183.53it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3210/3257 [00:17<00:00, 182.42it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3229/3257 [00:17<00:00, 183.94it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:17<00:00, 194.14it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:17<00:00, 182.06it/s]
2023-02-07 19:08:25.357 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:08:25,358][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d463,n5,w19,mc6,s0.37569,t4>', 'datetime': '2023-02-07T19:08:25.358798', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:08:25,360][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:08:25,360][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:08:25,821][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 19:08:25,821][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:08:25,866][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 15588 unique words (49.01% of original 31803, drops 16215)', 'datetime': '2023-02-07T19:08:25.866033', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:08:25,867][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 5053508 word corpus (99.18% of original 5095118, drops 41610)', 'datetime': '2023-02-07T19:08:25.867415', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:08:25,919][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 19:08:25,920][gensim.models.word2vec][INFO] - sample=0.37569 downsamples 0 most-common words
[2023-02-07 19:08:25,920][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5053508 word corpus (100.0%% of prior 5053508)', 'datetime': '2023-02-07T19:08:25.920951', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:08:26,008][gensim.models.word2vec][INFO] - estimated required memory for 15588 words and 463 dimensions: 72215316 bytes
[2023-02-07 19:08:26,008][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:08:26,049][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 15588 vocabulary and 463 features, using sg=0 hs=0 sample=0.3756901291832539 negative=5 window=19 shrink_windows=True', 'datetime': '2023-02-07T19:08:26.049026', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:08:27,054][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 13.42% examples, 655987 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:08:28,067][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 26.71% examples, 672455 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:08:29,071][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 39.27% examples, 674246 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:08:30,088][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 52.44% examples, 669959 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:08:31,096][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 65.06% examples, 662344 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:08:32,103][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 78.08% examples, 657270 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:08:33,118][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 91.80% examples, 660760 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:08:33,695][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5045550 effective words) took 7.6s, 660085 effective words/s
[2023-02-07 19:08:34,746][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 13.17% examples, 603945 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:08:35,770][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 25.97% examples, 636375 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:08:36,799][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 38.07% examples, 632978 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:08:37,813][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 50.75% examples, 633601 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:08:38,834][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 63.89% examples, 638973 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:08:39,874][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 76.48% examples, 633522 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:08:40,892][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 90.14% examples, 635740 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:08:41,603][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5045550 effective words) took 7.9s, 638260 effective words/s
[2023-02-07 19:08:42,610][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 13.05% examples, 634010 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:08:43,619][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 25.45% examples, 636966 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:08:44,621][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 37.86% examples, 647781 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:08:45,630][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 50.75% examples, 647907 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:08:46,629][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 64.05% examples, 655059 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:08:47,652][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 77.96% examples, 656357 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:08:48,671][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 91.13% examples, 654886 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:08:49,343][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5045550 effective words) took 7.7s, 652066 effective words/s
[2023-02-07 19:08:50,349][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 13.05% examples, 634306 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:08:51,369][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 25.76% examples, 643061 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:08:52,378][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 38.47% examples, 655550 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:08:53,385][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 51.12% examples, 652528 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:08:54,384][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 65.06% examples, 663008 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:08:55,423][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 78.69% examples, 660205 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:08:56,424][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 91.65% examples, 658433 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:08:57,003][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5045550 effective words) took 7.7s, 658956 effective words/s
[2023-02-07 19:08:58,013][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 13.05% examples, 631444 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:08:59,032][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 25.58% examples, 633876 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:00,038][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 39.12% examples, 666287 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:01,054][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 51.70% examples, 659381 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:02,075][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 65.06% examples, 658916 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:03,084][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 78.69% examples, 659994 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:09:04,093][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 91.80% examples, 658774 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:04,635][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5045550 effective words) took 7.6s, 661227 effective words/s
[2023-02-07 19:09:05,641][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 13.02% examples, 624926 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:06,657][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 26.37% examples, 663326 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:07,663][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 38.75% examples, 661496 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:08,666][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 51.55% examples, 660541 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:09,670][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 64.81% examples, 661030 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:10,677][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 77.65% examples, 655580 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:11,682][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 90.97% examples, 656402 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:09:12,313][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5045550 effective words) took 7.7s, 657385 effective words/s
[2023-02-07 19:09:13,320][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 13.05% examples, 633397 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:14,321][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 25.45% examples, 639560 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:15,323][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 37.95% examples, 652997 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:16,331][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 51.06% examples, 655629 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:17,358][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 63.99% examples, 652595 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:18,370][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 77.25% examples, 650903 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:19,375][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 90.48% examples, 650000 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:20,114][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5045550 effective words) took 7.8s, 646968 effective words/s
[2023-02-07 19:09:21,141][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 12.77% examples, 595647 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:22,160][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 25.08% examples, 619734 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:23,169][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 37.21% examples, 631615 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:24,185][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 50.38% examples, 637091 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:25,192][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 63.06% examples, 638116 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:26,197][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 75.96% examples, 638859 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:27,205][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 88.98% examples, 636648 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:27,979][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5045550 effective words) took 7.9s, 641649 effective words/s
[2023-02-07 19:09:29,009][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 14.12% examples, 672963 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:30,028][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 26.99% examples, 671745 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:31,031][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 39.09% examples, 662443 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:32,050][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 52.13% examples, 661230 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:33,052][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 65.21% examples, 660824 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:34,067][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 78.91% examples, 661889 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:35,071][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 92.02% examples, 659989 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:35,610][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5045550 effective words) took 7.6s, 661403 effective words/s
[2023-02-07 19:09:36,614][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 13.69% examples, 673945 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:37,618][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 26.10% examples, 656972 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:38,619][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 38.47% examples, 661383 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:39,635][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 50.38% examples, 644478 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:40,642][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 63.71% examples, 650737 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:41,680][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 76.73% examples, 646336 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:42,689][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 91.28% examples, 655041 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:43,274][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5045550 effective words) took 7.7s, 658604 effective words/s
[2023-02-07 19:09:44,277][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 17.10% examples, 833423 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:45,279][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 27.94% examples, 708877 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:46,282][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 39.58% examples, 683489 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:47,284][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 51.73% examples, 666482 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:48,297][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 64.54% examples, 660276 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:49,339][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 77.80% examples, 653223 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:09:50,342][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 90.51% examples, 650812 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:51,061][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5045550 effective words) took 7.8s, 648063 effective words/s
[2023-02-07 19:09:52,100][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 13.17% examples, 611086 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:53,107][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 24.44% examples, 600535 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:54,129][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 35.40% examples, 592859 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:55,130][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 48.82% examples, 618725 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:56,137][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 61.96% examples, 625068 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:57,155][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 74.79% examples, 629104 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:09:58,162][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 87.84% examples, 628916 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:09:59,055][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5045550 effective words) took 8.0s, 631409 effective words/s
[2023-02-07 19:10:00,066][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 13.69% examples, 669459 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:10:01,075][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 25.12% examples, 625394 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:10:02,098][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 37.58% examples, 639574 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:10:03,127][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 50.05% examples, 632060 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:10:04,131][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 62.60% examples, 631859 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:10:05,132][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 74.64% examples, 629802 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:10:06,134][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 88.30% examples, 632979 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:10:06,993][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5045550 effective words) took 7.9s, 635783 effective words/s
[2023-02-07 19:10:08,021][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 13.05% examples, 621472 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:10:09,032][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 24.96% examples, 618796 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:10:10,036][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 36.91% examples, 628857 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:10:11,048][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 48.82% examples, 620946 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:10:12,049][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 61.01% examples, 619627 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:10:13,073][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 73.44% examples, 617861 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:10:14,077][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 86.15% examples, 618499 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:10:15,082][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 99.36% examples, 620132 words/s, in_qsize 4, out_qsize 0
[2023-02-07 19:10:15,107][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5045550 effective words) took 8.1s, 622112 effective words/s
[2023-02-07 19:10:16,120][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 13.11% examples, 638008 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:10:17,171][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 25.21% examples, 615390 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:10:18,175][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 36.94% examples, 623550 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:10:19,204][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 48.14% examples, 607801 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:10:20,214][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 62.45% examples, 625876 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:10:21,232][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 74.58% examples, 623462 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:10:22,241][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 88.85% examples, 632926 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:10:23,002][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5045550 effective words) took 7.9s, 639244 effective words/s
[2023-02-07 19:10:23,002][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (75683250 effective words) took 117.0s, 647124 effective words/s', 'datetime': '2023-02-07T19:10:23.002728', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:10:23.003 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:10:28,567][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190758-467ru3b9/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:10:28.566956', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:10:28,569][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:10:28,681][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_190758-467ru3b9/files/../tmp/embedding_model.pt
2023-02-07 19:10:28.681 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:10:31.099 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:10:31.984 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:10:35.145 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.8258504016018637, 'test_mae': 1.2550770442638592, 'test_r2': -3.9646114792662033}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.0
wandb: percentage 0.50986
wandb:   test_mae 1.25508
wandb:   test_mse 2.82585
wandb:    test_r2 -3.96461
wandb: 
wandb: üöÄ View run jumping-sweep-34 at: https://wandb.ai/xiaoqiz/mof2vec/runs/467ru3b9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_190758-467ru3b9/logs
wandb: Agent Starting Run: 46gguypb with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 380
wandb: 	model.gensim.alpha: 0.013242068604607622
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.2928579665958461
wandb: 	model.gensim.vector_size: 164
wandb: 	model.gensim.window: 18
wandb: 	model.sklearn.learning_rate: 0.006652402600576928
wandb: 	model.sklearn.max_depth: 59
wandb: 	model.sklearn.min_child_weight: 0.028431896772131093
wandb: 	model.sklearn.n_estimators: 4928
wandb: 	model.sklearn.num_leaves: 468
wandb: 	model.sklearn.reg_alpha: 0.8157784169597266
wandb: 	model.sklearn.reg_lambda: 0.06842350491795213
wandb: 	model.sklearn.subsample: 0.25069809382353914
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191043-46gguypb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-35
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/46gguypb
2023-02-07 19:10:51.550 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 19:10:51.551 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 380 for sweep.
2023-02-07 19:10:51.551 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.013242068604607622 for sweep.
2023-02-07 19:10:51.551 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:10:51.552 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 19:10:51.552 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2928579665958461 for sweep.
2023-02-07 19:10:51.553 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 164 for sweep.
2023-02-07 19:10:51.553 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 18 for sweep.
2023-02-07 19:10:51.553 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.006652402600576928 for sweep.
2023-02-07 19:10:51.553 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 59 for sweep.
2023-02-07 19:10:51.554 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.028431896772131093 for sweep.
2023-02-07 19:10:51.554 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4928 for sweep.
2023-02-07 19:10:51.554 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 468 for sweep.
2023-02-07 19:10:51.554 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.8157784169597266 for sweep.
2023-02-07 19:10:51.555 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.06842350491795213 for sweep.
2023-02-07 19:10:51.555 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.25069809382353914 for sweep.
2023-02-07 19:10:51.555 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:10:51.561 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191043-46gguypb/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 380, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 164, 'window': 18, 'min_count': 3, 'dm': 0, 'sample': 0.2928579665958461, 'workers': 4, 'alpha': 0.013242068604607622, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4928, 'max_depth': 59, 'num_leaves': 468, 'reg_alpha': 0.8157784169597266, 'reg_lambda': 0.06842350491795213, 'subsample': 0.25069809382353914, 'min_child_weight': 0.028431896772131093, 'n_jobs': 4, 'learning_rate': 0.006652402600576928}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 4/3257 [00:00<03:21, 16.12it/s]  1%|          | 22/3257 [00:00<00:42, 75.47it/s]  1%|          | 38/3257 [00:00<00:31, 103.59it/s]  2%|‚ñè         | 53/3257 [00:00<00:27, 115.52it/s]  2%|‚ñè         | 71/3257 [00:00<00:23, 133.76it/s]  3%|‚ñé         | 90/3257 [00:00<00:21, 145.53it/s]  3%|‚ñé         | 106/3257 [00:00<00:22, 141.79it/s]  4%|‚ñç         | 123/3257 [00:00<00:21, 148.52it/s]  4%|‚ñç         | 141/3257 [00:01<00:19, 156.76it/s]  5%|‚ñç         | 158/3257 [00:01<00:19, 156.68it/s]  5%|‚ñå         | 174/3257 [00:01<00:20, 150.65it/s]  6%|‚ñå         | 192/3257 [00:01<00:19, 156.47it/s]  6%|‚ñã         | 208/3257 [00:01<00:19, 156.45it/s]  7%|‚ñã         | 230/3257 [00:01<00:17, 170.70it/s]  8%|‚ñä         | 248/3257 [00:01<00:17, 171.41it/s]  8%|‚ñä         | 266/3257 [00:01<00:18, 161.70it/s]  9%|‚ñä         | 284/3257 [00:01<00:17, 166.33it/s]  9%|‚ñâ         | 301/3257 [00:02<00:19, 155.49it/s] 10%|‚ñâ         | 317/3257 [00:02<00:18, 156.44it/s] 10%|‚ñà         | 335/3257 [00:02<00:18, 160.88it/s] 11%|‚ñà         | 352/3257 [00:02<00:18, 161.39it/s] 11%|‚ñà‚ñè        | 371/3257 [00:02<00:17, 168.77it/s] 12%|‚ñà‚ñè        | 388/3257 [00:02<00:18, 151.01it/s] 12%|‚ñà‚ñè        | 406/3257 [00:02<00:18, 156.17it/s] 13%|‚ñà‚ñé        | 422/3257 [00:02<00:18, 151.59it/s] 13%|‚ñà‚ñé        | 438/3257 [00:03<00:21, 129.86it/s] 14%|‚ñà‚ñç        | 453/3257 [00:03<00:20, 134.74it/s] 14%|‚ñà‚ñç        | 470/3257 [00:03<00:19, 143.71it/s] 15%|‚ñà‚ñç        | 485/3257 [00:03<00:19, 142.29it/s] 15%|‚ñà‚ñå        | 501/3257 [00:03<00:18, 146.72it/s] 16%|‚ñà‚ñå        | 517/3257 [00:03<00:18, 150.18it/s] 16%|‚ñà‚ñã        | 533/3257 [00:03<00:18, 144.48it/s] 17%|‚ñà‚ñã        | 549/3257 [00:03<00:18, 148.12it/s] 17%|‚ñà‚ñã        | 564/3257 [00:03<00:19, 137.66it/s] 18%|‚ñà‚ñä        | 578/3257 [00:04<00:19, 137.64it/s] 18%|‚ñà‚ñä        | 599/3257 [00:04<00:17, 155.55it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:04<00:16, 159.03it/s] 20%|‚ñà‚ñâ        | 640/3257 [00:04<00:15, 169.35it/s] 20%|‚ñà‚ñà        | 658/3257 [00:04<00:16, 160.49it/s] 21%|‚ñà‚ñà        | 678/3257 [00:04<00:15, 168.33it/s] 21%|‚ñà‚ñà‚ñè       | 696/3257 [00:04<00:15, 165.76it/s] 22%|‚ñà‚ñà‚ñè       | 718/3257 [00:04<00:14, 177.26it/s] 23%|‚ñà‚ñà‚ñé       | 736/3257 [00:04<00:15, 167.13it/s] 23%|‚ñà‚ñà‚ñé       | 754/3257 [00:05<00:14, 170.26it/s] 24%|‚ñà‚ñà‚ñé       | 773/3257 [00:05<00:14, 172.83it/s] 24%|‚ñà‚ñà‚ñç       | 791/3257 [00:05<00:14, 174.46it/s] 25%|‚ñà‚ñà‚ñç       | 809/3257 [00:05<00:13, 175.72it/s] 25%|‚ñà‚ñà‚ñå       | 827/3257 [00:05<00:14, 171.43it/s] 26%|‚ñà‚ñà‚ñå       | 845/3257 [00:05<00:14, 164.33it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:05<00:14, 170.30it/s] 27%|‚ñà‚ñà‚ñã       | 882/3257 [00:05<00:13, 170.57it/s] 28%|‚ñà‚ñà‚ñä       | 904/3257 [00:05<00:12, 184.66it/s] 28%|‚ñà‚ñà‚ñä       | 924/3257 [00:05<00:12, 186.23it/s] 29%|‚ñà‚ñà‚ñâ       | 943/3257 [00:06<00:12, 182.38it/s] 30%|‚ñà‚ñà‚ñâ       | 964/3257 [00:06<00:12, 188.97it/s] 30%|‚ñà‚ñà‚ñà       | 983/3257 [00:06<00:12, 182.86it/s] 31%|‚ñà‚ñà‚ñà       | 1002/3257 [00:06<00:12, 177.74it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1021/3257 [00:06<00:12, 179.70it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1040/3257 [00:06<00:13, 168.93it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1058/3257 [00:06<00:12, 169.35it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:06<00:12, 172.27it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1099/3257 [00:06<00:12, 175.42it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:07<00:12, 173.22it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1135/3257 [00:07<00:12, 171.66it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1153/3257 [00:07<00:12, 170.72it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:07<00:20, 103.74it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1186/3257 [00:07<00:18, 111.94it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1200/3257 [00:07<00:17, 117.78it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1217/3257 [00:07<00:15, 129.73it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1241/3257 [00:08<00:12, 155.95it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1259/3257 [00:08<00:12, 157.86it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1276/3257 [00:08<00:12, 158.65it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1293/3257 [00:08<00:12, 158.13it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1312/3257 [00:08<00:11, 166.67it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1330/3257 [00:08<00:11, 164.66it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:08<00:11, 167.87it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1368/3257 [00:08<00:11, 169.16it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1386/3257 [00:08<00:11, 164.48it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1409/3257 [00:09<00:10, 181.33it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:09<00:09, 188.73it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1454/3257 [00:09<00:09, 196.37it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1475/3257 [00:09<00:08, 199.96it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1496/3257 [00:09<00:08, 198.61it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1516/3257 [00:09<00:08, 195.82it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1536/3257 [00:09<00:09, 177.01it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1555/3257 [00:09<00:09, 170.75it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1575/3257 [00:09<00:09, 171.47it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1595/3257 [00:10<00:09, 177.60it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:10<00:09, 181.42it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:10<00:09, 171.67it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1652/3257 [00:10<00:09, 169.27it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1670/3257 [00:10<00:09, 169.25it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1688/3257 [00:10<00:09, 167.46it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1707/3257 [00:10<00:09, 172.06it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1727/3257 [00:10<00:08, 177.74it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1745/3257 [00:10<00:09, 159.70it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1764/3257 [00:11<00:08, 166.95it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1784/3257 [00:11<00:08, 175.98it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1803/3257 [00:11<00:08, 178.73it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1823/3257 [00:11<00:07, 180.65it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1842/3257 [00:11<00:07, 180.13it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1861/3257 [00:11<00:07, 182.85it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1881/3257 [00:11<00:07, 187.48it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1900/3257 [00:11<00:07, 184.14it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1919/3257 [00:11<00:07, 179.64it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1944/3257 [00:11<00:06, 198.19it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1966/3257 [00:12<00:06, 203.67it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1987/3257 [00:12<00:06, 191.95it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2007/3257 [00:12<00:06, 191.52it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2030/3257 [00:12<00:06, 200.33it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2051/3257 [00:12<00:06, 179.21it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2070/3257 [00:12<00:06, 172.52it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2089/3257 [00:12<00:06, 175.55it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2107/3257 [00:12<00:06, 173.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2125/3257 [00:13<00:06, 164.22it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2143/3257 [00:13<00:06, 161.82it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2163/3257 [00:13<00:06, 170.99it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:13<00:06, 172.66it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2202/3257 [00:13<00:05, 180.11it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2221/3257 [00:13<00:05, 177.24it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2239/3257 [00:13<00:06, 169.04it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2259/3257 [00:13<00:05, 177.18it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2277/3257 [00:13<00:05, 168.45it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2294/3257 [00:14<00:09, 99.49it/s]  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2312/3257 [00:14<00:08, 114.47it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2336/3257 [00:14<00:06, 140.96it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:14<00:05, 160.97it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2383/3257 [00:14<00:04, 176.49it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:14<00:04, 180.06it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2425/3257 [00:14<00:04, 183.75it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2445/3257 [00:14<00:04, 175.29it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2464/3257 [00:15<00:04, 175.19it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2483/3257 [00:15<00:04, 178.07it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2506/3257 [00:15<00:03, 191.54it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2526/3257 [00:15<00:03, 189.62it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2546/3257 [00:15<00:03, 190.87it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2566/3257 [00:15<00:04, 171.33it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2584/3257 [00:15<00:04, 164.07it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2605/3257 [00:15<00:03, 175.02it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:15<00:03, 194.54it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2651/3257 [00:16<00:03, 189.15it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2671/3257 [00:16<00:03, 191.99it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2691/3257 [00:16<00:02, 193.46it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2711/3257 [00:16<00:03, 170.40it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2731/3257 [00:16<00:02, 177.92it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2752/3257 [00:16<00:02, 186.40it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2772/3257 [00:16<00:02, 180.84it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2797/3257 [00:16<00:02, 198.77it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2818/3257 [00:16<00:02, 191.41it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2838/3257 [00:17<00:02, 185.37it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2862/3257 [00:17<00:01, 199.75it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:17<00:01, 210.55it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2908/3257 [00:17<00:01, 192.63it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:17<00:01, 188.38it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2948/3257 [00:17<00:01, 173.15it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2967/3257 [00:17<00:01, 173.55it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:17<00:01, 171.01it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3008/3257 [00:18<00:01, 186.21it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3027/3257 [00:18<00:01, 180.79it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3048/3257 [00:18<00:01, 188.81it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3070/3257 [00:18<00:00, 195.83it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3090/3257 [00:18<00:00, 188.82it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3115/3257 [00:18<00:00, 205.21it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3136/3257 [00:18<00:00, 198.12it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3156/3257 [00:18<00:00, 189.31it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3176/3257 [00:18<00:00, 185.34it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3197/3257 [00:19<00:00, 185.63it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3216/3257 [00:19<00:00, 178.51it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3239/3257 [00:19<00:00, 190.61it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 168.47it/s]
2023-02-07 19:11:11.717 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:11:11,719][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d164,n5,mc3,s0.292858,t4>', 'datetime': '2023-02-07T19:11:11.719134', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:11:11,720][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:11:11,720][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:11:12,256][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 19:11:12,256][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:11:12,328][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 30516 unique words (71.46% of original 42701, drops 12185)', 'datetime': '2023-02-07T19:11:12.328689', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:11:12,329][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 5804704 word corpus (99.69% of original 5822992, drops 18288)', 'datetime': '2023-02-07T19:11:12.329799', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:11:12,425][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 19:11:12,426][gensim.models.word2vec][INFO] - sample=0.292858 downsamples 0 most-common words
[2023-02-07 19:11:12,426][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5804704 word corpus (100.0%% of prior 5804704)', 'datetime': '2023-02-07T19:11:12.426376', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:11:12,590][gensim.models.word2vec][INFO] - estimated required memory for 30516 words and 164 dimensions: 58082984 bytes
[2023-02-07 19:11:12,591][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:11:12,621][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 30516 vocabulary and 164 features, using sg=1 hs=0 sample=0.2928579665958461 negative=5 window=18 shrink_windows=True', 'datetime': '2023-02-07T19:11:12.621071', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:11:13,623][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 40.47% examples, 2401549 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:14,626][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 83.94% examples, 2446818 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:11:14,954][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5778362 effective words) took 2.3s, 2478002 effective words/s
[2023-02-07 19:11:15,957][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 47.44% examples, 2797654 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:16,958][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 96.35% examples, 2782459 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:17,029][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5778362 effective words) took 2.1s, 2787073 effective words/s
[2023-02-07 19:11:18,033][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 61.01% examples, 3577555 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:18,639][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5778362 effective words) took 1.6s, 3595814 effective words/s
[2023-02-07 19:11:19,652][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 46.95% examples, 2759331 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:20,653][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 95.64% examples, 2760870 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:20,737][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5778362 effective words) took 2.1s, 2768484 effective words/s
[2023-02-07 19:11:21,741][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 61.01% examples, 3572912 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:22,341][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5778362 effective words) took 1.6s, 3605585 effective words/s
[2023-02-07 19:11:23,351][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 47.71% examples, 2814561 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:24,355][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 96.75% examples, 2794521 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:24,414][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5778362 effective words) took 2.1s, 2800511 effective words/s
[2023-02-07 19:11:25,418][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 61.53% examples, 3605226 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:26,025][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5778362 effective words) took 1.6s, 3590667 effective words/s
[2023-02-07 19:11:27,029][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 56.95% examples, 3360143 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:11:27,855][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5778362 effective words) took 1.8s, 3159805 effective words/s
[2023-02-07 19:11:28,857][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 59.69% examples, 3511203 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:29,466][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5778362 effective words) took 1.6s, 3590847 effective words/s
[2023-02-07 19:11:30,473][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 61.71% examples, 3601117 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:31,078][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5778362 effective words) took 1.6s, 3588827 effective words/s
[2023-02-07 19:11:32,085][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 48.20% examples, 2837826 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:33,089][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 97.54% examples, 2811884 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:33,129][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5778362 effective words) took 2.0s, 2820040 effective words/s
[2023-02-07 19:11:34,131][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 61.01% examples, 3578667 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:34,732][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5778362 effective words) took 1.6s, 3605312 effective words/s
[2023-02-07 19:11:35,737][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 49.65% examples, 2908981 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:36,692][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5778362 effective words) took 2.0s, 2950204 effective words/s
[2023-02-07 19:11:37,694][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 63.65% examples, 3742097 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:38,242][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5778362 effective words) took 1.5s, 3730498 effective words/s
[2023-02-07 19:11:39,246][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 63.37% examples, 3722899 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:11:39,806][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5778362 effective words) took 1.6s, 3701902 effective words/s
[2023-02-07 19:11:39,806][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86675430 effective words) took 27.2s, 3188314 effective words/s', 'datetime': '2023-02-07T19:11:39.806870', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:11:39.807 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:11:42,138][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191043-46gguypb/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:11:42.138497', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:11:42,140][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:11:42,223][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191043-46gguypb/files/../tmp/embedding_model.pt
2023-02-07 19:11:42.223 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:11:43.563 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:11:44.033 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:11:45.198 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1404547261836755, 'test_mae': 1.0802649530122181, 'test_r2': -1.6116281589124606}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.87
wandb: percentage 0.28536
wandb:   test_mae 1.08026
wandb:   test_mse 2.14045
wandb:    test_r2 -1.61163
wandb: 
wandb: üöÄ View run sparkling-sweep-35 at: https://wandb.ai/xiaoqiz/mof2vec/runs/46gguypb
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_191043-46gguypb/logs
wandb: Agent Starting Run: rzg2b5ir with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 890
wandb: 	model.gensim.alpha: 0.1507957756079844
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.2222063361311177
wandb: 	model.gensim.vector_size: 415
wandb: 	model.gensim.window: 16
wandb: 	model.sklearn.learning_rate: 0.7389540467522778
wandb: 	model.sklearn.max_depth: 65
wandb: 	model.sklearn.min_child_weight: 0.0323116606241563
wandb: 	model.sklearn.n_estimators: 1350
wandb: 	model.sklearn.num_leaves: 480
wandb: 	model.sklearn.reg_alpha: 0.06809558207024825
wandb: 	model.sklearn.reg_lambda: 0.0038815513032873926
wandb: 	model.sklearn.subsample: 0.46788899849362214
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191158-rzg2b5ir
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-sweep-36
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/rzg2b5ir
2023-02-07 19:12:06.008 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 19:12:06.009 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 890 for sweep.
2023-02-07 19:12:06.009 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.1507957756079844 for sweep.
2023-02-07 19:12:06.009 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:12:06.009 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 19:12:06.009 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2222063361311177 for sweep.
2023-02-07 19:12:06.010 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 415 for sweep.
2023-02-07 19:12:06.010 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 16 for sweep.
2023-02-07 19:12:06.010 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.7389540467522778 for sweep.
2023-02-07 19:12:06.010 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 65 for sweep.
2023-02-07 19:12:06.011 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0323116606241563 for sweep.
2023-02-07 19:12:06.011 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1350 for sweep.
2023-02-07 19:12:06.012 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 480 for sweep.
2023-02-07 19:12:06.012 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.06809558207024825 for sweep.
2023-02-07 19:12:06.012 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.0038815513032873926 for sweep.
2023-02-07 19:12:06.012 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.46788899849362214 for sweep.
2023-02-07 19:12:06.012 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:12:06.018 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191158-rzg2b5ir/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 890, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 415, 'window': 16, 'min_count': 2, 'dm': 0, 'sample': 0.2222063361311177, 'workers': 4, 'alpha': 0.1507957756079844, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1350, 'max_depth': 65, 'num_leaves': 480, 'reg_alpha': 0.06809558207024825, 'reg_lambda': 0.0038815513032873926, 'subsample': 0.46788899849362214, 'min_child_weight': 0.0323116606241563, 'n_jobs': 4, 'learning_rate': 0.7389540467522778}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 30/3257 [00:00<00:10, 299.76it/s]  2%|‚ñè         | 63/3257 [00:00<00:10, 316.34it/s]  3%|‚ñé         | 95/3257 [00:00<00:10, 299.14it/s]  4%|‚ñç         | 126/3257 [00:00<00:11, 263.73it/s]  5%|‚ñç         | 160/3257 [00:00<00:10, 288.13it/s]  6%|‚ñå         | 193/3257 [00:00<00:10, 299.40it/s]  7%|‚ñã         | 230/3257 [00:00<00:09, 316.87it/s]  8%|‚ñä         | 263/3257 [00:00<00:09, 314.21it/s]  9%|‚ñâ         | 296/3257 [00:00<00:09, 314.05it/s] 10%|‚ñà         | 328/3257 [00:01<00:09, 311.03it/s] 11%|‚ñà         | 363/3257 [00:01<00:09, 319.97it/s] 12%|‚ñà‚ñè        | 396/3257 [00:01<00:09, 312.33it/s] 13%|‚ñà‚ñé        | 428/3257 [00:01<00:09, 302.39it/s] 14%|‚ñà‚ñç        | 459/3257 [00:01<00:09, 300.92it/s] 15%|‚ñà‚ñå        | 493/3257 [00:01<00:08, 310.58it/s] 16%|‚ñà‚ñå        | 525/3257 [00:01<00:12, 224.54it/s] 17%|‚ñà‚ñã        | 558/3257 [00:01<00:10, 245.59it/s] 18%|‚ñà‚ñä        | 587/3257 [00:02<00:10, 255.76it/s] 19%|‚ñà‚ñâ        | 622/3257 [00:02<00:09, 276.35it/s] 20%|‚ñà‚ñà        | 656/3257 [00:02<00:08, 290.09it/s] 21%|‚ñà‚ñà        | 687/3257 [00:02<00:08, 292.09it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:02<00:08, 303.52it/s] 23%|‚ñà‚ñà‚ñé       | 756/3257 [00:02<00:08, 311.92it/s] 24%|‚ñà‚ñà‚ñç       | 789/3257 [00:02<00:07, 315.75it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:02<00:07, 320.77it/s] 26%|‚ñà‚ñà‚ñã       | 856/3257 [00:02<00:07, 314.39it/s] 27%|‚ñà‚ñà‚ñã       | 889/3257 [00:02<00:07, 318.80it/s] 28%|‚ñà‚ñà‚ñä       | 925/3257 [00:03<00:07, 328.17it/s] 29%|‚ñà‚ñà‚ñâ       | 958/3257 [00:03<00:06, 328.65it/s] 31%|‚ñà‚ñà‚ñà       | 994/3257 [00:03<00:06, 335.32it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1028/3257 [00:03<00:06, 334.33it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1063/3257 [00:03<00:06, 337.17it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1098/3257 [00:03<00:06, 338.79it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1135/3257 [00:03<00:06, 346.96it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:03<00:06, 346.73it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1205/3257 [00:03<00:06, 327.26it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1243/3257 [00:04<00:05, 339.88it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1278/3257 [00:04<00:05, 331.93it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1312/3257 [00:04<00:05, 330.88it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1349/3257 [00:04<00:05, 341.75it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:04<00:05, 334.01it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1422/3257 [00:04<00:05, 347.21it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1460/3257 [00:04<00:05, 355.19it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1500/3257 [00:04<00:04, 365.98it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1537/3257 [00:04<00:04, 350.86it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1573/3257 [00:04<00:04, 347.86it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1609/3257 [00:05<00:04, 351.03it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1645/3257 [00:05<00:04, 347.69it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1680/3257 [00:05<00:04, 342.33it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:05<00:04, 342.59it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1750/3257 [00:05<00:06, 229.49it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1788/3257 [00:05<00:05, 261.96it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1824/3257 [00:05<00:05, 284.15it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1858/3257 [00:05<00:04, 298.01it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1895/3257 [00:06<00:04, 316.21it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1931/3257 [00:06<00:04, 327.93it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1974/3257 [00:06<00:03, 355.68it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2011/3257 [00:06<00:03, 353.38it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2048/3257 [00:06<00:03, 348.66it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2084/3257 [00:06<00:03, 348.94it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:06<00:03, 342.77it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2155/3257 [00:06<00:03, 336.84it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2191/3257 [00:06<00:03, 343.18it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2226/3257 [00:07<00:03, 339.33it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2261/3257 [00:07<00:03, 330.21it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2295/3257 [00:07<00:02, 332.07it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2332/3257 [00:07<00:02, 341.84it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2371/3257 [00:07<00:02, 352.94it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2409/3257 [00:07<00:02, 358.29it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2445/3257 [00:07<00:02, 352.01it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2483/3257 [00:07<00:02, 355.65it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2525/3257 [00:07<00:01, 372.14it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2563/3257 [00:07<00:01, 363.71it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2600/3257 [00:08<00:01, 353.49it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2642/3257 [00:08<00:01, 370.97it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2680/3257 [00:08<00:01, 363.34it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2717/3257 [00:08<00:01, 340.75it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:08<00:01, 350.73it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2793/3257 [00:08<00:01, 356.03it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2829/3257 [00:08<00:01, 344.31it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2872/3257 [00:08<00:01, 367.12it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2909/3257 [00:08<00:00, 361.18it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2946/3257 [00:09<00:00, 360.36it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2983/3257 [00:09<00:00, 350.20it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3022/3257 [00:09<00:00, 357.16it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3066/3257 [00:09<00:00, 378.52it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3107/3257 [00:09<00:00, 385.66it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3146/3257 [00:09<00:00, 376.58it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3184/3257 [00:09<00:00, 368.03it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3221/3257 [00:09<00:00, 367.10it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 330.29it/s]
2023-02-07 19:12:16.129 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:12:16,130][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d415,n5,mc2,s0.222206,t4>', 'datetime': '2023-02-07T19:12:16.130023', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:12:16,130][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:12:16,130][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:12:16,312][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 19:12:16,313][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:12:16,319][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 2542 unique words (90.17% of original 2819, drops 277)', 'datetime': '2023-02-07T19:12:16.319401', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:12:16,319][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 2183345 word corpus (99.99% of original 2183622, drops 277)', 'datetime': '2023-02-07T19:12:16.319660', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:12:16,327][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 19:12:16,327][gensim.models.word2vec][INFO] - sample=0.222206 downsamples 0 most-common words
[2023-02-07 19:12:16,328][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2183345 word corpus (100.0%% of prior 2183345)', 'datetime': '2023-02-07T19:12:16.328007', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:12:16,341][gensim.models.word2vec][INFO] - estimated required memory for 2542 words and 415 dimensions: 15768460 bytes
[2023-02-07 19:12:16,341][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:12:16,352][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2542 vocabulary and 415 features, using sg=1 hs=0 sample=0.2222063361311177 negative=5 window=16 shrink_windows=True', 'datetime': '2023-02-07T19:12:16.352462', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:12:17,146][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2186602 effective words) took 0.8s, 2764935 effective words/s
[2023-02-07 19:12:18,085][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2186602 effective words) took 0.9s, 2332758 effective words/s
[2023-02-07 19:12:18,791][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2186602 effective words) took 0.7s, 3100609 effective words/s
[2023-02-07 19:12:19,743][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2186602 effective words) took 1.0s, 2301278 effective words/s
[2023-02-07 19:12:20,650][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2186602 effective words) took 0.9s, 2416481 effective words/s
[2023-02-07 19:12:21,571][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2186602 effective words) took 0.9s, 2380698 effective words/s
[2023-02-07 19:12:22,307][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2186602 effective words) took 0.7s, 2975249 effective words/s
[2023-02-07 19:12:23,251][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2186602 effective words) took 0.9s, 2321133 effective words/s
[2023-02-07 19:12:23,984][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2186602 effective words) took 0.7s, 2984947 effective words/s
[2023-02-07 19:12:24,715][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2186602 effective words) took 0.7s, 2998578 effective words/s
[2023-02-07 19:12:25,628][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2186602 effective words) took 0.9s, 2398353 effective words/s
[2023-02-07 19:12:26,594][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2186602 effective words) took 1.0s, 2266714 effective words/s
[2023-02-07 19:12:27,591][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2186602 effective words) took 1.0s, 2196349 effective words/s
[2023-02-07 19:12:28,597][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 98.74% examples, 2154407 words/s, in_qsize 3, out_qsize 1
[2023-02-07 19:12:28,606][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2186602 effective words) took 1.0s, 2157096 effective words/s
[2023-02-07 19:12:29,611][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 92.20% examples, 2022873 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:12:29,673][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2186602 effective words) took 1.1s, 2051612 effective words/s
[2023-02-07 19:12:29,674][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 32754330 raw words (32799030 effective words) took 13.3s, 2462211 effective words/s', 'datetime': '2023-02-07T19:12:29.674366', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:12:29.674 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:12:30,989][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191158-rzg2b5ir/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:12:30.989379', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:12:30,990][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:12:31,012][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191158-rzg2b5ir/files/../tmp/embedding_model.pt
2023-02-07 19:12:31.012 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:12:33.046 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:12:33.795 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:12:36.818 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.3072753770818397, 'test_mae': 1.1396179804649844, 'test_r2': -2.931363121887126}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.82
wandb: percentage 0.09826
wandb:   test_mae 1.13962
wandb:   test_mse 2.30728
wandb:    test_r2 -2.93136
wandb: 
wandb: üöÄ View run peach-sweep-36 at: https://wandb.ai/xiaoqiz/mof2vec/runs/rzg2b5ir
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_191158-rzg2b5ir/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: z4o3gh86 with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 1015
wandb: 	model.gensim.alpha: 0.0031177915737457557
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.7055818317320512
wandb: 	model.gensim.vector_size: 75
wandb: 	model.gensim.window: 6
wandb: 	model.sklearn.learning_rate: 0.01289981119365487
wandb: 	model.sklearn.max_depth: 72
wandb: 	model.sklearn.min_child_weight: 0.00227568131838108
wandb: 	model.sklearn.n_estimators: 2472
wandb: 	model.sklearn.num_leaves: 350
wandb: 	model.sklearn.reg_alpha: 0.04531821606660591
wandb: 	model.sklearn.reg_lambda: 0.029593529856746677
wandb: 	model.sklearn.subsample: 0.8151466591369703
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191256-z4o3gh86
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-37
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/z4o3gh86
2023-02-07 19:13:03.972 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 19:13:03.973 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 1015 for sweep.
2023-02-07 19:13:03.973 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0031177915737457557 for sweep.
2023-02-07 19:13:03.974 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:13:03.974 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 19:13:03.974 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7055818317320512 for sweep.
2023-02-07 19:13:03.974 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 75 for sweep.
2023-02-07 19:13:03.974 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 6 for sweep.
2023-02-07 19:13:03.975 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.01289981119365487 for sweep.
2023-02-07 19:13:03.975 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 72 for sweep.
2023-02-07 19:13:03.976 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.00227568131838108 for sweep.
2023-02-07 19:13:03.976 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2472 for sweep.
2023-02-07 19:13:03.976 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 350 for sweep.
2023-02-07 19:13:03.976 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.04531821606660591 for sweep.
2023-02-07 19:13:03.976 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.029593529856746677 for sweep.
2023-02-07 19:13:03.977 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8151466591369703 for sweep.
2023-02-07 19:13:03.977 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:13:03.982 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191256-z4o3gh86/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 1015, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 75, 'window': 6, 'min_count': 5, 'dm': 0, 'sample': 0.7055818317320512, 'workers': 4, 'alpha': 0.0031177915737457557, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2472, 'max_depth': 72, 'num_leaves': 350, 'reg_alpha': 0.04531821606660591, 'reg_lambda': 0.029593529856746677, 'subsample': 0.8151466591369703, 'min_child_weight': 0.00227568131838108, 'n_jobs': 4, 'learning_rate': 0.01289981119365487}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 21/3257 [00:00<00:15, 204.87it/s]  1%|‚ñè         | 42/3257 [00:00<00:16, 200.29it/s]  2%|‚ñè         | 65/3257 [00:00<00:14, 213.04it/s]  3%|‚ñé         | 87/3257 [00:00<00:14, 215.32it/s]  3%|‚ñé         | 109/3257 [00:00<00:16, 195.76it/s]  4%|‚ñç         | 133/3257 [00:00<00:15, 207.88it/s]  5%|‚ñç         | 158/3257 [00:00<00:14, 213.99it/s]  6%|‚ñå         | 180/3257 [00:00<00:14, 205.24it/s]  6%|‚ñå         | 201/3257 [00:00<00:15, 200.71it/s]  7%|‚ñã         | 227/3257 [00:01<00:13, 217.16it/s]  8%|‚ñä         | 249/3257 [00:01<00:14, 212.60it/s]  8%|‚ñä         | 271/3257 [00:01<00:14, 207.45it/s]  9%|‚ñâ         | 297/3257 [00:01<00:13, 220.07it/s] 10%|‚ñâ         | 320/3257 [00:01<00:13, 217.39it/s] 11%|‚ñà         | 342/3257 [00:01<00:13, 209.77it/s] 11%|‚ñà         | 365/3257 [00:01<00:13, 215.23it/s] 12%|‚ñà‚ñè        | 387/3257 [00:01<00:14, 202.03it/s] 13%|‚ñà‚ñé        | 412/3257 [00:01<00:13, 214.71it/s] 13%|‚ñà‚ñé        | 434/3257 [00:02<00:14, 190.75it/s] 14%|‚ñà‚ñç        | 458/3257 [00:02<00:13, 203.67it/s] 15%|‚ñà‚ñç        | 480/3257 [00:02<00:13, 202.33it/s] 16%|‚ñà‚ñå        | 505/3257 [00:02<00:12, 214.39it/s] 16%|‚ñà‚ñå        | 527/3257 [00:02<00:12, 213.85it/s] 17%|‚ñà‚ñã        | 551/3257 [00:02<00:12, 220.85it/s] 18%|‚ñà‚ñä        | 574/3257 [00:02<00:13, 199.07it/s] 18%|‚ñà‚ñä        | 599/3257 [00:02<00:12, 209.34it/s] 19%|‚ñà‚ñâ        | 621/3257 [00:02<00:12, 207.37it/s] 20%|‚ñà‚ñâ        | 644/3257 [00:03<00:12, 210.81it/s] 20%|‚ñà‚ñà        | 666/3257 [00:03<00:13, 196.81it/s] 21%|‚ñà‚ñà        | 687/3257 [00:03<00:12, 198.22it/s] 22%|‚ñà‚ñà‚ñè       | 711/3257 [00:03<00:12, 209.53it/s] 23%|‚ñà‚ñà‚ñé       | 733/3257 [00:03<00:12, 199.60it/s] 23%|‚ñà‚ñà‚ñé       | 754/3257 [00:03<00:13, 191.64it/s] 24%|‚ñà‚ñà‚ñç       | 775/3257 [00:03<00:12, 195.57it/s] 24%|‚ñà‚ñà‚ñç       | 795/3257 [00:03<00:12, 193.82it/s] 25%|‚ñà‚ñà‚ñå       | 815/3257 [00:03<00:12, 194.76it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:04<00:12, 193.00it/s] 26%|‚ñà‚ñà‚ñã       | 855/3257 [00:04<00:12, 184.87it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:04<00:12, 187.49it/s] 27%|‚ñà‚ñà‚ñã       | 895/3257 [00:04<00:12, 190.27it/s] 28%|‚ñà‚ñà‚ñä       | 915/3257 [00:04<00:12, 192.85it/s] 29%|‚ñà‚ñà‚ñâ       | 937/3257 [00:04<00:11, 198.23it/s] 29%|‚ñà‚ñà‚ñâ       | 960/3257 [00:04<00:11, 206.88it/s] 30%|‚ñà‚ñà‚ñà       | 981/3257 [00:04<00:11, 205.52it/s] 31%|‚ñà‚ñà‚ñà       | 1002/3257 [00:04<00:11, 204.80it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1024/3257 [00:05<00:10, 207.81it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1045/3257 [00:05<00:11, 189.34it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:05<00:11, 195.30it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1090/3257 [00:05<00:10, 201.47it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1112/3257 [00:05<00:10, 205.11it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1133/3257 [00:05<00:10, 193.50it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1153/3257 [00:05<00:10, 191.49it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:05<00:16, 128.83it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1189/3257 [00:06<00:16, 128.16it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1208/3257 [00:06<00:14, 139.45it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1232/3257 [00:06<00:12, 159.77it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1250/3257 [00:06<00:12, 161.52it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:06<00:11, 170.35it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1289/3257 [00:06<00:12, 162.53it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1308/3257 [00:06<00:11, 169.27it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1329/3257 [00:06<00:10, 180.07it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:06<00:10, 182.49it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1369/3257 [00:07<00:10, 184.13it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1388/3257 [00:07<00:10, 177.10it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1412/3257 [00:07<00:09, 192.82it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1434/3257 [00:07<00:09, 197.47it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1459/3257 [00:07<00:08, 209.75it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1481/3257 [00:07<00:08, 209.41it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1506/3257 [00:07<00:07, 219.98it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1529/3257 [00:07<00:08, 202.39it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1550/3257 [00:07<00:08, 193.93it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1570/3257 [00:08<00:08, 193.49it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1590/3257 [00:08<00:08, 195.27it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1611/3257 [00:08<00:08, 197.81it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:08<00:08, 192.68it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1654/3257 [00:08<00:08, 182.89it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1673/3257 [00:08<00:08, 177.62it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1691/3257 [00:08<00:08, 178.08it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1713/3257 [00:08<00:08, 189.28it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1733/3257 [00:08<00:08, 176.23it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1753/3257 [00:09<00:08, 181.50it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1776/3257 [00:09<00:07, 191.48it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1797/3257 [00:09<00:07, 195.39it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1817/3257 [00:09<00:07, 196.54it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1838/3257 [00:09<00:07, 199.49it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1859/3257 [00:09<00:07, 198.69it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1883/3257 [00:09<00:06, 206.39it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1906/3257 [00:09<00:06, 212.95it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1928/3257 [00:09<00:06, 209.13it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1958/3257 [00:10<00:05, 235.29it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1982/3257 [00:10<00:05, 226.35it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2006/3257 [00:10<00:05, 227.56it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2032/3257 [00:10<00:05, 233.97it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2056/3257 [00:10<00:05, 203.53it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2078/3257 [00:10<00:05, 202.43it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2099/3257 [00:10<00:05, 198.94it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:10<00:05, 194.95it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2141/3257 [00:10<00:05, 196.66it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2161/3257 [00:11<00:05, 195.92it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:11<00:05, 196.87it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2204/3257 [00:11<00:05, 203.03it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2225/3257 [00:11<00:05, 199.05it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2245/3257 [00:11<00:05, 197.94it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2267/3257 [00:11<00:04, 202.87it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2288/3257 [00:11<00:04, 201.21it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2309/3257 [00:11<00:04, 197.94it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2336/3257 [00:11<00:04, 215.72it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2361/3257 [00:11<00:04, 223.47it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2384/3257 [00:12<00:03, 225.22it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2407/3257 [00:12<00:03, 220.12it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2430/3257 [00:12<00:03, 215.13it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2452/3257 [00:12<00:03, 202.27it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2478/3257 [00:12<00:03, 217.17it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2504/3257 [00:12<00:03, 227.59it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2527/3257 [00:12<00:03, 226.45it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2550/3257 [00:13<00:05, 126.24it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2568/3257 [00:13<00:05, 133.36it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2588/3257 [00:13<00:04, 145.88it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2614/3257 [00:13<00:03, 171.20it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:13<00:03, 189.97it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2662/3257 [00:13<00:03, 187.34it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2687/3257 [00:13<00:02, 201.48it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2709/3257 [00:13<00:02, 186.77it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:13<00:02, 195.04it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:14<00:02, 205.07it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2778/3257 [00:14<00:02, 201.58it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2805/3257 [00:14<00:02, 218.43it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2828/3257 [00:14<00:02, 204.73it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2851/3257 [00:14<00:01, 208.32it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:14<00:01, 232.39it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2905/3257 [00:14<00:01, 214.64it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2929/3257 [00:14<00:01, 220.32it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2952/3257 [00:15<00:01, 204.60it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:15<00:01, 206.02it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2998/3257 [00:15<00:01, 209.41it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3020/3257 [00:15<00:01, 208.26it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3045/3257 [00:15<00:00, 219.68it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3069/3257 [00:15<00:00, 224.27it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3092/3257 [00:15<00:00, 221.99it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3120/3257 [00:15<00:00, 238.61it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3145/3257 [00:15<00:00, 220.43it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3168/3257 [00:16<00:00, 215.54it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3190/3257 [00:16<00:00, 206.88it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3211/3257 [00:16<00:00, 204.38it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3233/3257 [00:16<00:00, 208.69it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3254/3257 [00:16<00:00, 207.14it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 198.12it/s]
2023-02-07 19:13:21.053 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:13:21,054][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d75,n5,mc5,s0.705582,t4>', 'datetime': '2023-02-07T19:13:21.054774', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:13:21,055][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:13:21,055][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:13:21,503][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 19:13:21,504][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:13:21,547][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 16108 unique words (50.65% of original 31803, drops 15695)', 'datetime': '2023-02-07T19:13:21.547381', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:13:21,549][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5056108 word corpus (99.23% of original 5095118, drops 39010)', 'datetime': '2023-02-07T19:13:21.548996', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:13:21,603][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 19:13:21,603][gensim.models.word2vec][INFO] - sample=0.705582 downsamples 0 most-common words
[2023-02-07 19:13:21,604][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5056108 word corpus (100.0%% of prior 5056108)', 'datetime': '2023-02-07T19:13:21.604182', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:13:21,694][gensim.models.word2vec][INFO] - estimated required memory for 16108 words and 75 dimensions: 19347300 bytes
[2023-02-07 19:13:21,694][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:13:21,700][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 16108 vocabulary and 75 features, using sg=1 hs=0 sample=0.7055818317320512 negative=5 window=6 shrink_windows=True', 'datetime': '2023-02-07T19:13:21.700677', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:13:22,706][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 48.14% examples, 2479999 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:13:23,728][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 97.36% examples, 2428924 words/s, in_qsize 8, out_qsize 5
[2023-02-07 19:13:23,761][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5048130 effective words) took 2.1s, 2451437 effective words/s
[2023-02-07 19:13:24,765][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 53.85% examples, 2783187 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:13:25,563][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5048130 effective words) took 1.8s, 2803988 effective words/s
[2023-02-07 19:13:26,567][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 52.44% examples, 2700130 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:13:27,425][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5048130 effective words) took 1.9s, 2713288 effective words/s
[2023-02-07 19:13:28,436][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 51.70% examples, 2654814 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:13:29,328][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5048130 effective words) took 1.9s, 2659073 effective words/s
[2023-02-07 19:13:30,333][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 51.21% examples, 2638733 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:13:31,238][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5048130 effective words) took 1.9s, 2647872 effective words/s
[2023-02-07 19:13:32,241][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 69.20% examples, 3559894 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:13:32,653][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5048130 effective words) took 1.4s, 3570468 effective words/s
[2023-02-07 19:13:33,662][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 53.70% examples, 2778493 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:13:34,488][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5048130 effective words) took 1.8s, 2763847 effective words/s
[2023-02-07 19:13:35,495][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 54.44% examples, 2809885 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:13:36,307][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5048130 effective words) took 1.8s, 2782175 effective words/s
[2023-02-07 19:13:37,316][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 53.18% examples, 2736605 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:13:38,159][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5048130 effective words) took 1.8s, 2729250 effective words/s
[2023-02-07 19:13:39,164][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 51.73% examples, 2667062 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:13:39,877][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5048130 effective words) took 1.7s, 2941832 effective words/s
[2023-02-07 19:13:40,887][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 53.45% examples, 2757031 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:13:41,718][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5048130 effective words) took 1.8s, 2748762 effective words/s
[2023-02-07 19:13:42,724][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 52.44% examples, 2701234 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:13:43,568][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5048130 effective words) took 1.8s, 2735377 effective words/s
[2023-02-07 19:13:44,574][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 65.77% examples, 3373949 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:13:45,056][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5048130 effective words) took 1.5s, 3394902 effective words/s
[2023-02-07 19:13:46,059][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 65.24% examples, 3356315 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:13:46,541][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5048130 effective words) took 1.5s, 3401250 effective words/s
[2023-02-07 19:13:47,544][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 54.28% examples, 2802550 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:13:48,328][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5048130 effective words) took 1.8s, 2828618 effective words/s
[2023-02-07 19:13:48,330][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (75721950 effective words) took 26.6s, 2843650 effective words/s', 'datetime': '2023-02-07T19:13:48.329951', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:13:48.330 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:13:50,230][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191256-z4o3gh86/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:13:50.230725', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:13:50,231][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:13:50,297][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191256-z4o3gh86/files/../tmp/embedding_model.pt
2023-02-07 19:13:50.298 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:13:51.427 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:13:51.839 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:13:52.715 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1970949509339537, 'test_mae': 1.1082919876439714, 'test_r2': -1.6113520862530955}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.69
wandb: percentage 0.49351
wandb:   test_mae 1.10829
wandb:   test_mse 2.19709
wandb:    test_r2 -1.61135
wandb: 
wandb: üöÄ View run easy-sweep-37 at: https://wandb.ai/xiaoqiz/mof2vec/runs/z4o3gh86
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_191256-z4o3gh86/logs
wandb: Agent Starting Run: f9fngp8d with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 483
wandb: 	model.gensim.alpha: 0.005137708202712796
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 7
wandb: 	model.gensim.sample: 0.9833040015708878
wandb: 	model.gensim.vector_size: 124
wandb: 	model.gensim.window: 11
wandb: 	model.sklearn.learning_rate: 0.0017395120246829114
wandb: 	model.sklearn.max_depth: 32
wandb: 	model.sklearn.min_child_weight: 0.045920858562441
wandb: 	model.sklearn.n_estimators: 3131
wandb: 	model.sklearn.num_leaves: 11
wandb: 	model.sklearn.reg_alpha: 0.004905647484654816
wandb: 	model.sklearn.reg_lambda: 0.11535468680148409
wandb: 	model.sklearn.subsample: 0.6192793564388454
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191405-f9fngp8d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-38
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/f9fngp8d
2023-02-07 19:14:12.876 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 19:14:12.876 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 483 for sweep.
2023-02-07 19:14:12.876 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.005137708202712796 for sweep.
2023-02-07 19:14:12.877 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 19:14:12.877 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 7 for sweep.
2023-02-07 19:14:12.877 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.9833040015708878 for sweep.
2023-02-07 19:14:12.877 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 124 for sweep.
2023-02-07 19:14:12.878 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 11 for sweep.
2023-02-07 19:14:12.878 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0017395120246829114 for sweep.
2023-02-07 19:14:12.878 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 32 for sweep.
2023-02-07 19:14:12.878 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.045920858562441 for sweep.
2023-02-07 19:14:12.879 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3131 for sweep.
2023-02-07 19:14:12.879 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 11 for sweep.
2023-02-07 19:14:12.879 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.004905647484654816 for sweep.
2023-02-07 19:14:12.879 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.11535468680148409 for sweep.
2023-02-07 19:14:12.880 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6192793564388454 for sweep.
2023-02-07 19:14:12.880 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:14:12.887 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191405-f9fngp8d/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 483, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 124, 'window': 11, 'min_count': 7, 'dm': 1, 'sample': 0.9833040015708878, 'workers': 4, 'alpha': 0.005137708202712796, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3131, 'max_depth': 32, 'num_leaves': 11, 'reg_alpha': 0.004905647484654816, 'reg_lambda': 0.11535468680148409, 'subsample': 0.6192793564388454, 'min_child_weight': 0.045920858562441, 'n_jobs': 4, 'learning_rate': 0.0017395120246829114}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 27/3257 [00:00<00:12, 263.52it/s]  2%|‚ñè         | 54/3257 [00:00<00:12, 264.40it/s]  3%|‚ñé         | 84/3257 [00:00<00:11, 278.53it/s]  3%|‚ñé         | 112/3257 [00:00<00:11, 268.49it/s]  4%|‚ñç         | 139/3257 [00:00<00:11, 267.01it/s]  5%|‚ñå         | 166/3257 [00:00<00:11, 264.37it/s]  6%|‚ñå         | 196/3257 [00:00<00:11, 274.24it/s]  7%|‚ñã         | 230/3257 [00:00<00:10, 290.04it/s]  8%|‚ñä         | 260/3257 [00:00<00:10, 285.34it/s]  9%|‚ñâ         | 295/3257 [00:01<00:09, 303.59it/s] 10%|‚ñà         | 326/3257 [00:01<00:09, 294.79it/s] 11%|‚ñà         | 356/3257 [00:01<00:09, 292.84it/s] 12%|‚ñà‚ñè        | 386/3257 [00:01<00:10, 284.05it/s] 13%|‚ñà‚ñé        | 419/3257 [00:01<00:09, 294.37it/s] 14%|‚ñà‚ñç        | 449/3257 [00:01<00:10, 269.57it/s] 15%|‚ñà‚ñç        | 479/3257 [00:01<00:10, 276.01it/s] 16%|‚ñà‚ñå        | 512/3257 [00:01<00:09, 287.99it/s] 17%|‚ñà‚ñã        | 542/3257 [00:01<00:09, 289.85it/s] 18%|‚ñà‚ñä        | 572/3257 [00:02<00:14, 189.21it/s] 19%|‚ñà‚ñä        | 603/3257 [00:02<00:12, 213.86it/s] 19%|‚ñà‚ñâ        | 633/3257 [00:02<00:11, 232.73it/s] 20%|‚ñà‚ñà        | 660/3257 [00:02<00:11, 232.99it/s] 21%|‚ñà‚ñà        | 686/3257 [00:02<00:10, 234.81it/s] 22%|‚ñà‚ñà‚ñè       | 718/3257 [00:02<00:09, 254.69it/s] 23%|‚ñà‚ñà‚ñé       | 745/3257 [00:02<00:09, 252.29it/s] 24%|‚ñà‚ñà‚ñç       | 774/3257 [00:02<00:09, 261.96it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:03<00:09, 270.67it/s] 26%|‚ñà‚ñà‚ñå       | 832/3257 [00:03<00:09, 266.76it/s] 26%|‚ñà‚ñà‚ñã       | 860/3257 [00:03<00:09, 264.70it/s] 27%|‚ñà‚ñà‚ñã       | 887/3257 [00:03<00:08, 265.62it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:03<00:08, 260.40it/s] 29%|‚ñà‚ñà‚ñâ       | 945/3257 [00:03<00:08, 273.54it/s] 30%|‚ñà‚ñà‚ñâ       | 974/3257 [00:03<00:08, 277.05it/s] 31%|‚ñà‚ñà‚ñà       | 1002/3257 [00:03<00:08, 261.98it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1029/3257 [00:03<00:08, 258.53it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1056/3257 [00:04<00:08, 258.04it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1084/3257 [00:04<00:08, 263.76it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1113/3257 [00:04<00:07, 269.49it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1141/3257 [00:04<00:08, 259.63it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:04<00:07, 266.34it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1197/3257 [00:04<00:08, 255.41it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1226/3257 [00:04<00:07, 263.84it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1256/3257 [00:04<00:07, 272.32it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1284/3257 [00:04<00:07, 260.40it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1311/3257 [00:04<00:07, 258.39it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:05<00:07, 262.79it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1366/3257 [00:05<00:07, 253.60it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1392/3257 [00:05<00:07, 252.26it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1425/3257 [00:05<00:06, 272.50it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1458/3257 [00:05<00:06, 287.28it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1488/3257 [00:05<00:06, 290.81it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:05<00:05, 290.10it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1549/3257 [00:05<00:06, 283.89it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1578/3257 [00:05<00:05, 283.89it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1611/3257 [00:06<00:05, 295.30it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1641/3257 [00:06<00:05, 288.28it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1670/3257 [00:06<00:08, 184.90it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1696/3257 [00:06<00:07, 199.82it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1724/3257 [00:06<00:07, 217.66it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1750/3257 [00:06<00:06, 219.29it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1780/3257 [00:06<00:06, 239.04it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1808/3257 [00:06<00:05, 249.26it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1839/3257 [00:07<00:05, 264.79it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:07<00:05, 272.65it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1898/3257 [00:07<00:04, 276.39it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1927/3257 [00:07<00:04, 276.51it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:07<00:04, 300.58it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1996/3257 [00:07<00:04, 301.17it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2027/3257 [00:07<00:04, 299.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2058/3257 [00:07<00:04, 279.18it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2087/3257 [00:07<00:04, 275.50it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2117/3257 [00:08<00:04, 281.63it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2146/3257 [00:08<00:04, 265.50it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:08<00:03, 277.38it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2207/3257 [00:08<00:03, 280.75it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:08<00:03, 282.04it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2265/3257 [00:08<00:03, 280.35it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2294/3257 [00:08<00:03, 283.11it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2327/3257 [00:08<00:03, 296.18it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:08<00:02, 303.17it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2394/3257 [00:08<00:02, 313.71it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2426/3257 [00:09<00:02, 306.78it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2457/3257 [00:09<00:02, 293.35it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2489/3257 [00:09<00:02, 300.20it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2525/3257 [00:09<00:02, 314.76it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2557/3257 [00:09<00:02, 309.24it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2589/3257 [00:09<00:02, 294.62it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:09<00:01, 319.59it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2661/3257 [00:09<00:01, 304.42it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2694/3257 [00:09<00:01, 308.15it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2726/3257 [00:10<00:01, 293.64it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2760/3257 [00:10<00:01, 305.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2791/3257 [00:10<00:01, 303.20it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2822/3257 [00:10<00:01, 299.55it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2853/3257 [00:10<00:01, 295.69it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2890/3257 [00:10<00:01, 315.17it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2922/3257 [00:10<00:01, 313.34it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2954/3257 [00:11<00:01, 187.68it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2982/3257 [00:11<00:01, 205.61it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3016/3257 [00:11<00:01, 233.11it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3051/3257 [00:11<00:00, 260.42it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3086/3257 [00:11<00:00, 282.73it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3122/3257 [00:11<00:00, 302.63it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3155/3257 [00:11<00:00, 298.30it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3187/3257 [00:11<00:00, 298.91it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3219/3257 [00:11<00:00, 300.45it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3254/3257 [00:11<00:00, 313.97it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 272.15it/s]
2023-02-07 19:14:25.209 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:14:25,211][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d124,n5,w11,mc7,s0.983304,t4>', 'datetime': '2023-02-07T19:14:25.211684', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:14:25,212][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:14:25,212][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:14:25,456][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 19:14:25,456][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:14:25,465][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 retains 3228 unique words (48.45% of original 6662, drops 3434)', 'datetime': '2023-02-07T19:14:25.465715', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:14:25,465][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 leaves 2901608 word corpus (99.66% of original 2911496, drops 9888)', 'datetime': '2023-02-07T19:14:25.465977', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:14:25,476][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 19:14:25,477][gensim.models.word2vec][INFO] - sample=0.983304 downsamples 0 most-common words
[2023-02-07 19:14:25,477][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2901608 word corpus (100.0%% of prior 2901608)', 'datetime': '2023-02-07T19:14:25.477329', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:14:25,495][gensim.models.word2vec][INFO] - estimated required memory for 3228 words and 124 dimensions: 7083048 bytes
[2023-02-07 19:14:25,496][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:14:25,499][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 3228 vocabulary and 124 features, using sg=0 hs=0 sample=0.9833040015708878 negative=5 window=11 shrink_windows=True', 'datetime': '2023-02-07T19:14:25.499278', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:14:26,505][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 47.28% examples, 1397033 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:27,516][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 97.70% examples, 1409763 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:27,548][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2904865 effective words) took 2.0s, 1418501 effective words/s
[2023-02-07 19:14:28,562][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 47.56% examples, 1395384 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:29,489][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2904865 effective words) took 1.9s, 1497929 effective words/s
[2023-02-07 19:14:30,493][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 51.06% examples, 1512957 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:31,273][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2904865 effective words) took 1.8s, 1629814 effective words/s
[2023-02-07 19:14:32,284][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 46.30% examples, 1353461 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:33,287][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 94.23% examples, 1366092 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:33,402][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2904865 effective words) took 2.1s, 1365577 effective words/s
[2023-02-07 19:14:34,407][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 44.67% examples, 1322277 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:35,410][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 95.46% examples, 1383552 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:35,500][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2904865 effective words) took 2.1s, 1385692 effective words/s
[2023-02-07 19:14:36,502][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 45.07% examples, 1335323 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:14:37,511][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 95.73% examples, 1386203 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:37,584][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2904865 effective words) took 2.1s, 1394815 effective words/s
[2023-02-07 19:14:38,587][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 49.43% examples, 1460255 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:39,559][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2904865 effective words) took 2.0s, 1471768 effective words/s
[2023-02-07 19:14:40,567][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 59.72% examples, 1761321 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:41,173][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2904865 effective words) took 1.6s, 1801834 effective words/s
[2023-02-07 19:14:42,184][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 48.08% examples, 1417510 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:43,185][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 99.82% examples, 1441322 words/s, in_qsize 1, out_qsize 1
[2023-02-07 19:14:43,192][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2904865 effective words) took 2.0s, 1439610 effective words/s
[2023-02-07 19:14:44,200][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 49.43% examples, 1455934 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:14:45,184][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2904865 effective words) took 2.0s, 1459762 effective words/s
[2023-02-07 19:14:46,191][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 48.60% examples, 1433311 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:47,191][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 98.93% examples, 1435205 words/s, in_qsize 3, out_qsize 1
[2023-02-07 19:14:47,200][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2904865 effective words) took 2.0s, 1442268 effective words/s
[2023-02-07 19:14:48,206][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 49.43% examples, 1455613 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:49,185][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2904865 effective words) took 2.0s, 1464567 effective words/s
[2023-02-07 19:14:50,187][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 51.34% examples, 1524315 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:50,950][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2904865 effective words) took 1.8s, 1646681 effective words/s
[2023-02-07 19:14:51,965][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 51.06% examples, 1497373 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:14:52,897][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2904865 effective words) took 1.9s, 1493629 effective words/s
[2023-02-07 19:14:53,899][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 51.92% examples, 1543580 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:14:54,761][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2904865 effective words) took 1.9s, 1559578 effective words/s
[2023-02-07 19:14:54,762][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 43672440 raw words (43572975 effective words) took 29.3s, 1489023 effective words/s', 'datetime': '2023-02-07T19:14:54.762321', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:14:54.762 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:14:57,705][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191405-f9fngp8d/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:14:57.705464', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:14:57,711][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:14:57,737][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191405-f9fngp8d/files/../tmp/embedding_model.pt
2023-02-07 19:14:57.737 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:14:58.965 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:14:59.421 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:15:14.795 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.3491708785599315, 'test_mae': 1.1442661884375076, 'test_r2': -2.721513651090937}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.029 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.029 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.5
wandb: percentage 0.51546
wandb:   test_mae 1.14427
wandb:   test_mse 2.34917
wandb:    test_r2 -2.72151
wandb: 
wandb: üöÄ View run sweet-sweep-38 at: https://wandb.ai/xiaoqiz/mof2vec/runs/f9fngp8d
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_191405-f9fngp8d/logs
wandb: Agent Starting Run: 1jsvwe97 with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 748
wandb: 	model.gensim.alpha: 0.0069735783272226294
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.94327435942457
wandb: 	model.gensim.vector_size: 358
wandb: 	model.gensim.window: 7
wandb: 	model.sklearn.learning_rate: 0.7334703663072238
wandb: 	model.sklearn.max_depth: 67
wandb: 	model.sklearn.min_child_weight: 0.0703928550827573
wandb: 	model.sklearn.n_estimators: 212
wandb: 	model.sklearn.num_leaves: 235
wandb: 	model.sklearn.reg_alpha: 0.005897985318286081
wandb: 	model.sklearn.reg_lambda: 0.02521704903249925
wandb: 	model.sklearn.subsample: 0.6123102386754368
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191524-1jsvwe97
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-sweep-39
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/1jsvwe97
2023-02-07 19:15:32.140 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 19:15:32.140 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 748 for sweep.
2023-02-07 19:15:32.141 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0069735783272226294 for sweep.
2023-02-07 19:15:32.141 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:15:32.142 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 19:15:32.142 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.94327435942457 for sweep.
2023-02-07 19:15:32.142 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 358 for sweep.
2023-02-07 19:15:32.142 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 7 for sweep.
2023-02-07 19:15:32.142 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.7334703663072238 for sweep.
2023-02-07 19:15:32.143 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 67 for sweep.
2023-02-07 19:15:32.143 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0703928550827573 for sweep.
2023-02-07 19:15:32.143 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 212 for sweep.
2023-02-07 19:15:32.143 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 235 for sweep.
2023-02-07 19:15:32.144 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.005897985318286081 for sweep.
2023-02-07 19:15:32.144 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.02521704903249925 for sweep.
2023-02-07 19:15:32.144 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6123102386754368 for sweep.
2023-02-07 19:15:32.144 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:15:32.156 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191524-1jsvwe97/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 748, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 358, 'window': 7, 'min_count': 1, 'dm': 0, 'sample': 0.94327435942457, 'workers': 4, 'alpha': 0.0069735783272226294, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 212, 'max_depth': 67, 'num_leaves': 235, 'reg_alpha': 0.005897985318286081, 'reg_lambda': 0.02521704903249925, 'subsample': 0.6123102386754368, 'min_child_weight': 0.0703928550827573, 'n_jobs': 4, 'learning_rate': 0.7334703663072238}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 14/3257 [00:00<00:23, 137.53it/s]  1%|          | 31/3257 [00:00<00:21, 153.22it/s]  1%|‚ñè         | 47/3257 [00:00<00:20, 155.98it/s]  2%|‚ñè         | 63/3257 [00:00<00:20, 153.79it/s]  2%|‚ñè         | 81/3257 [00:00<00:19, 160.27it/s]  3%|‚ñé         | 98/3257 [00:00<00:20, 156.86it/s]  4%|‚ñé         | 114/3257 [00:00<00:20, 153.25it/s]  4%|‚ñç         | 131/3257 [00:00<00:19, 157.66it/s]  5%|‚ñç         | 151/3257 [00:00<00:18, 167.12it/s]  5%|‚ñå         | 168/3257 [00:01<00:18, 163.36it/s]  6%|‚ñå         | 185/3257 [00:01<00:18, 164.52it/s]  6%|‚ñå         | 202/3257 [00:01<00:19, 160.54it/s]  7%|‚ñã         | 224/3257 [00:01<00:17, 176.66it/s]  7%|‚ñã         | 243/3257 [00:01<00:16, 177.64it/s]  8%|‚ñä         | 261/3257 [00:01<00:17, 169.22it/s]  9%|‚ñä         | 283/3257 [00:01<00:16, 183.38it/s]  9%|‚ñâ         | 302/3257 [00:01<00:16, 174.93it/s] 10%|‚ñâ         | 320/3257 [00:01<00:17, 172.02it/s] 10%|‚ñà         | 338/3257 [00:02<00:17, 170.60it/s] 11%|‚ñà         | 358/3257 [00:02<00:16, 177.29it/s] 12%|‚ñà‚ñè        | 376/3257 [00:02<00:17, 167.39it/s] 12%|‚ñà‚ñè        | 393/3257 [00:02<00:17, 161.19it/s] 13%|‚ñà‚ñé        | 413/3257 [00:02<00:16, 170.88it/s] 13%|‚ñà‚ñé        | 431/3257 [00:02<00:18, 149.08it/s] 14%|‚ñà‚ñé        | 447/3257 [00:02<00:18, 150.49it/s] 14%|‚ñà‚ñç        | 466/3257 [00:02<00:17, 160.73it/s] 15%|‚ñà‚ñç        | 483/3257 [00:02<00:17, 155.41it/s] 15%|‚ñà‚ñå        | 504/3257 [00:03<00:16, 167.55it/s] 16%|‚ñà‚ñå        | 522/3257 [00:03<00:16, 170.83it/s] 17%|‚ñà‚ñã        | 542/3257 [00:03<00:15, 176.77it/s] 17%|‚ñà‚ñã        | 560/3257 [00:03<00:16, 164.51it/s] 18%|‚ñà‚ñä        | 577/3257 [00:03<00:17, 151.80it/s] 18%|‚ñà‚ñä        | 597/3257 [00:03<00:16, 162.90it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:03<00:15, 167.01it/s] 19%|‚ñà‚ñâ        | 632/3257 [00:03<00:16, 160.46it/s] 20%|‚ñà‚ñâ        | 649/3257 [00:03<00:17, 145.73it/s] 20%|‚ñà‚ñà        | 664/3257 [00:04<00:18, 143.12it/s] 21%|‚ñà‚ñà        | 683/3257 [00:04<00:17, 150.22it/s] 22%|‚ñà‚ñà‚ñè       | 702/3257 [00:04<00:16, 159.67it/s] 22%|‚ñà‚ñà‚ñè       | 721/3257 [00:04<00:15, 165.02it/s] 23%|‚ñà‚ñà‚ñé       | 738/3257 [00:04<00:15, 157.80it/s] 23%|‚ñà‚ñà‚ñé       | 759/3257 [00:04<00:14, 170.52it/s] 24%|‚ñà‚ñà‚ñç       | 777/3257 [00:04<00:14, 170.82it/s] 24%|‚ñà‚ñà‚ñç       | 796/3257 [00:04<00:14, 174.88it/s] 25%|‚ñà‚ñà‚ñç       | 814/3257 [00:04<00:13, 175.21it/s] 26%|‚ñà‚ñà‚ñå       | 832/3257 [00:05<00:14, 170.07it/s] 26%|‚ñà‚ñà‚ñå       | 850/3257 [00:05<00:14, 160.58it/s] 27%|‚ñà‚ñà‚ñã       | 867/3257 [00:05<00:20, 116.28it/s] 27%|‚ñà‚ñà‚ñã       | 882/3257 [00:05<00:19, 122.94it/s] 28%|‚ñà‚ñà‚ñä       | 902/3257 [00:05<00:16, 140.81it/s] 28%|‚ñà‚ñà‚ñä       | 920/3257 [00:05<00:15, 150.54it/s] 29%|‚ñà‚ñà‚ñâ       | 938/3257 [00:05<00:14, 156.70it/s] 29%|‚ñà‚ñà‚ñâ       | 958/3257 [00:05<00:13, 167.77it/s] 30%|‚ñà‚ñà‚ñâ       | 976/3257 [00:06<00:13, 170.65it/s] 31%|‚ñà‚ñà‚ñà       | 994/3257 [00:06<00:13, 164.26it/s] 31%|‚ñà‚ñà‚ñà       | 1011/3257 [00:06<00:13, 161.59it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1028/3257 [00:06<00:13, 160.59it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1045/3257 [00:06<00:13, 159.65it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1062/3257 [00:06<00:13, 158.53it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:06<00:13, 157.79it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1097/3257 [00:06<00:13, 158.36it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:06<00:13, 163.45it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1134/3257 [00:07<00:13, 162.47it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1151/3257 [00:07<00:13, 158.91it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:07<00:12, 166.50it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1188/3257 [00:07<00:13, 150.60it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1204/3257 [00:07<00:13, 152.68it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1222/3257 [00:07<00:12, 158.65it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1243/3257 [00:07<00:11, 170.12it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1261/3257 [00:07<00:11, 171.21it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1279/3257 [00:07<00:12, 159.25it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1296/3257 [00:08<00:12, 158.17it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1315/3257 [00:08<00:11, 165.90it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1334/3257 [00:08<00:11, 172.45it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1352/3257 [00:08<00:11, 169.47it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:08<00:11, 168.18it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1387/3257 [00:08<00:11, 163.50it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1408/3257 [00:08<00:10, 175.74it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1429/3257 [00:08<00:09, 183.22it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1448/3257 [00:08<00:09, 183.74it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1471/3257 [00:09<00:09, 194.83it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1491/3257 [00:09<00:09, 192.35it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1512/3257 [00:09<00:08, 197.35it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1532/3257 [00:09<00:09, 180.88it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1551/3257 [00:09<00:09, 172.27it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1569/3257 [00:09<00:09, 171.80it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1587/3257 [00:09<00:09, 169.60it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1607/3257 [00:09<00:09, 177.56it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1626/3257 [00:09<00:09, 180.76it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1645/3257 [00:10<00:09, 172.98it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1663/3257 [00:10<00:09, 167.05it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1680/3257 [00:10<00:09, 166.86it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1698/3257 [00:10<00:09, 170.10it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1716/3257 [00:10<00:09, 169.93it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1734/3257 [00:10<00:09, 156.65it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1752/3257 [00:10<00:09, 162.60it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1771/3257 [00:10<00:08, 168.97it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1792/3257 [00:10<00:08, 177.37it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1810/3257 [00:11<00:08, 167.45it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1827/3257 [00:11<00:08, 167.96it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1846/3257 [00:11<00:08, 170.31it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:11<00:07, 178.92it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1888/3257 [00:11<00:07, 181.14it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1908/3257 [00:11<00:07, 185.78it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1927/3257 [00:11<00:07, 180.32it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1951/3257 [00:11<00:06, 196.68it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1972/3257 [00:11<00:06, 198.11it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1992/3257 [00:11<00:06, 186.32it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2011/3257 [00:12<00:06, 183.76it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2033/3257 [00:12<00:06, 188.41it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2052/3257 [00:12<00:06, 173.10it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2070/3257 [00:12<00:07, 167.08it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2088/3257 [00:12<00:06, 169.14it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2106/3257 [00:12<00:06, 168.25it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2123/3257 [00:12<00:07, 152.88it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2141/3257 [00:12<00:07, 158.51it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2158/3257 [00:13<00:06, 158.23it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:13<00:09, 110.49it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2196/3257 [00:13<00:08, 123.64it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2211/3257 [00:13<00:08, 125.59it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2232/3257 [00:13<00:07, 144.94it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2249/3257 [00:13<00:06, 147.30it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2267/3257 [00:13<00:06, 153.77it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2286/3257 [00:13<00:06, 161.37it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2303/3257 [00:14<00:05, 160.38it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2326/3257 [00:14<00:05, 177.21it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2348/3257 [00:14<00:04, 187.46it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2368/3257 [00:14<00:04, 190.31it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2390/3257 [00:14<00:04, 198.16it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2411/3257 [00:14<00:04, 184.35it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2430/3257 [00:14<00:04, 182.92it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2449/3257 [00:14<00:04, 173.02it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2474/3257 [00:14<00:04, 192.12it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2494/3257 [00:15<00:03, 192.24it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2516/3257 [00:15<00:03, 199.58it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2539/3257 [00:15<00:03, 205.41it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2560/3257 [00:15<00:03, 184.62it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2579/3257 [00:15<00:03, 172.85it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2598/3257 [00:15<00:03, 176.12it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2625/3257 [00:15<00:03, 200.84it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2646/3257 [00:15<00:03, 189.30it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2666/3257 [00:15<00:03, 183.51it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2687/3257 [00:16<00:03, 189.54it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2707/3257 [00:16<00:03, 167.64it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2725/3257 [00:16<00:03, 170.16it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2747/3257 [00:16<00:02, 183.43it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2766/3257 [00:16<00:02, 183.59it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2785/3257 [00:16<00:02, 185.01it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:16<00:02, 189.26it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2826/3257 [00:16<00:02, 176.80it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:16<00:02, 174.17it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2870/3257 [00:17<00:01, 196.61it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2890/3257 [00:17<00:01, 191.49it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2910/3257 [00:17<00:01, 181.66it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2929/3257 [00:17<00:01, 182.10it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2948/3257 [00:17<00:01, 173.75it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2967/3257 [00:17<00:01, 177.95it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:17<00:01, 168.99it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3007/3257 [00:17<00:01, 181.74it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3026/3257 [00:17<00:01, 175.95it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:18<00:01, 186.15it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3075/3257 [00:18<00:00, 202.88it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3096/3257 [00:18<00:00, 192.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3121/3257 [00:18<00:00, 205.07it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3142/3257 [00:18<00:00, 191.83it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3162/3257 [00:18<00:00, 187.11it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3181/3257 [00:18<00:00, 176.45it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3203/3257 [00:18<00:00, 186.89it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3222/3257 [00:18<00:00, 180.43it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:19<00:00, 191.09it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 170.00it/s]
2023-02-07 19:15:52.108 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:15:52,109][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d358,n5,s0.943274,t4>', 'datetime': '2023-02-07T19:15:52.109107', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:15:52,109][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:15:52,110][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:15:52,669][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 19:15:52,671][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:15:52,804][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 54054 unique words (100.00% of original 54054, drops 0)', 'datetime': '2023-02-07T19:15:52.804030', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:15:52,804][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 6550866 word corpus (100.00% of original 6550866, drops 0)', 'datetime': '2023-02-07T19:15:52.804443', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:15:52,986][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 19:15:52,987][gensim.models.word2vec][INFO] - sample=0.943274 downsamples 0 most-common words
[2023-02-07 19:15:52,987][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6550866 word corpus (100.0%% of prior 6550866)', 'datetime': '2023-02-07T19:15:52.987785', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:15:53,288][gensim.models.word2vec][INFO] - estimated required memory for 54054 words and 358 dimensions: 187153080 bytes
[2023-02-07 19:15:53,288][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:15:53,365][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 54054 vocabulary and 358 features, using sg=1 hs=0 sample=0.94327435942457 negative=5 window=7 shrink_windows=True', 'datetime': '2023-02-07T19:15:53.365298', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:15:54,377][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 20.97% examples, 1330366 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:15:55,383][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 40.84% examples, 1349607 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:15:56,396][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 62.82% examples, 1366065 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:15:57,400][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 85.26% examples, 1385758 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:15:58,022][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6494971 effective words) took 4.7s, 1395276 effective words/s
[2023-02-07 19:15:59,025][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 24.99% examples, 1612911 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:00,025][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 48.33% examples, 1600895 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:16:01,027][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 73.10% examples, 1604146 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:16:02,031][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 99.02% examples, 1607605 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:16:02,059][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6494971 effective words) took 4.0s, 1609423 effective words/s
[2023-02-07 19:16:03,062][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 31.44% examples, 2045938 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:04,070][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 62.91% examples, 2061882 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:05,074][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 96.07% examples, 2071881 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:05,191][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6494971 effective words) took 3.1s, 2075282 effective words/s
[2023-02-07 19:16:06,194][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 26.22% examples, 1699852 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:07,200][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 51.73% examples, 1710978 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:08,200][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 78.32% examples, 1709322 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:08,993][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6494971 effective words) took 3.8s, 1708633 effective words/s
[2023-02-07 19:16:09,998][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 25.76% examples, 1672169 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:10,999][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 50.60% examples, 1672064 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:16:12,008][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 77.25% examples, 1684214 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:12,844][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6494971 effective words) took 3.8s, 1687959 effective words/s
[2023-02-07 19:16:13,849][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 25.94% examples, 1683060 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:14,853][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 51.40% examples, 1697705 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:15,857][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 78.29% examples, 1705941 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:16,633][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6494971 effective words) took 3.8s, 1714788 effective words/s
[2023-02-07 19:16:17,639][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 26.83% examples, 1743346 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:18,641][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 51.55% examples, 1706462 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:19,646][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 78.23% examples, 1706757 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:20,438][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6494971 effective words) took 3.8s, 1708733 effective words/s
[2023-02-07 19:16:21,442][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 25.97% examples, 1687314 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:22,443][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 51.00% examples, 1688343 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:23,446][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 77.62% examples, 1693600 words/s, in_qsize 4, out_qsize 2
[2023-02-07 19:16:24,266][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6494971 effective words) took 3.8s, 1697427 effective words/s
[2023-02-07 19:16:25,272][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 26.40% examples, 1712543 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:26,275][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 51.89% examples, 1716877 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:16:27,279][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 78.88% examples, 1722085 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:16:28,035][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6494971 effective words) took 3.8s, 1724242 effective words/s
[2023-02-07 19:16:29,040][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 27.05% examples, 1752672 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:30,041][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 53.61% examples, 1778819 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:31,043][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 81.76% examples, 1780216 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:16:31,666][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6494971 effective words) took 3.6s, 1789774 effective words/s
[2023-02-07 19:16:32,671][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 33.47% examples, 2186252 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:33,674][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 61.59% examples, 2023052 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:34,680][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 88.89% examples, 1930914 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:35,052][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6494971 effective words) took 3.4s, 1918981 effective words/s
[2023-02-07 19:16:36,055][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 27.63% examples, 1803137 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:16:37,055][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 54.53% examples, 1807834 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:16:38,060][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 82.90% examples, 1810988 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:38,632][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6494971 effective words) took 3.6s, 1815124 effective words/s
[2023-02-07 19:16:39,636][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 33.34% examples, 2179228 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:40,636][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 64.17% examples, 2117147 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:41,640][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 92.91% examples, 2021100 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:41,868][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6494971 effective words) took 3.2s, 2007675 effective words/s
[2023-02-07 19:16:42,878][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 28.12% examples, 1826179 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:16:43,882][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 55.08% examples, 1817813 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:16:44,882][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 82.19% examples, 1787398 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:45,509][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6494971 effective words) took 3.6s, 1784960 effective words/s
[2023-02-07 19:16:46,513][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 27.05% examples, 1767488 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:47,513][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 53.21% examples, 1768780 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:16:48,513][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 81.03% examples, 1768033 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:16:49,194][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6494971 effective words) took 3.7s, 1763950 effective words/s
[2023-02-07 19:16:49,194][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (97424565 effective words) took 55.8s, 1745056 effective words/s', 'datetime': '2023-02-07T19:16:49.194656', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:16:49.194 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:16:53,585][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191524-1jsvwe97/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:16:53.585306', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:16:53,587][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191524-1jsvwe97/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 19:16:53,666][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191524-1jsvwe97/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 19:16:53,745][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:16:53,781][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191524-1jsvwe97/files/../tmp/embedding_model.pt
2023-02-07 19:16:53.782 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:16:55.709 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:16:56.398 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:16:59.098 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1537577526465115, 'test_mae': 1.0724858274080569, 'test_r2': -1.4001175746223833}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.8
wandb: percentage 0.0
wandb:   test_mae 1.07249
wandb:   test_mse 2.15376
wandb:    test_r2 -1.40012
wandb: 
wandb: üöÄ View run rosy-sweep-39 at: https://wandb.ai/xiaoqiz/mof2vec/runs/1jsvwe97
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_191524-1jsvwe97/logs
wandb: Agent Starting Run: ofxi33bp with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 842
wandb: 	model.gensim.alpha: 0.25638999323447015
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 9
wandb: 	model.gensim.sample: 0.9554412469569102
wandb: 	model.gensim.vector_size: 392
wandb: 	model.gensim.window: 14
wandb: 	model.sklearn.learning_rate: 0.004754166447001291
wandb: 	model.sklearn.max_depth: 90
wandb: 	model.sklearn.min_child_weight: 0.03850543158674835
wandb: 	model.sklearn.n_estimators: 2945
wandb: 	model.sklearn.num_leaves: 88
wandb: 	model.sklearn.reg_alpha: 0.2867706675511494
wandb: 	model.sklearn.reg_lambda: 0.7680413053161431
wandb: 	model.sklearn.subsample: 0.3888548880791558
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191712-ofxi33bp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-sweep-40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/ofxi33bp
2023-02-07 19:17:21.276 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 19:17:21.276 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 842 for sweep.
2023-02-07 19:17:21.277 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.25638999323447015 for sweep.
2023-02-07 19:17:21.277 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:17:21.277 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 9 for sweep.
2023-02-07 19:17:21.277 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.9554412469569102 for sweep.
2023-02-07 19:17:21.278 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 392 for sweep.
2023-02-07 19:17:21.278 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 14 for sweep.
2023-02-07 19:17:21.278 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.004754166447001291 for sweep.
2023-02-07 19:17:21.278 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 90 for sweep.
2023-02-07 19:17:21.279 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03850543158674835 for sweep.
2023-02-07 19:17:21.279 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2945 for sweep.
2023-02-07 19:17:21.279 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 88 for sweep.
2023-02-07 19:17:21.279 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.2867706675511494 for sweep.
2023-02-07 19:17:21.279 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.7680413053161431 for sweep.
2023-02-07 19:17:21.280 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3888548880791558 for sweep.
2023-02-07 19:17:21.280 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:17:21.288 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191712-ofxi33bp/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 842, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 392, 'window': 14, 'min_count': 9, 'dm': 0, 'sample': 0.9554412469569102, 'workers': 4, 'alpha': 0.25638999323447015, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2945, 'max_depth': 90, 'num_leaves': 88, 'reg_alpha': 0.2867706675511494, 'reg_lambda': 0.7680413053161431, 'subsample': 0.3888548880791558, 'min_child_weight': 0.03850543158674835, 'n_jobs': 4, 'learning_rate': 0.004754166447001291}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 36/3257 [00:00<00:08, 359.82it/s]  2%|‚ñè         | 77/3257 [00:00<00:08, 387.70it/s]  4%|‚ñé         | 116/3257 [00:00<00:08, 381.28it/s]  5%|‚ñç         | 161/3257 [00:00<00:07, 405.51it/s]  6%|‚ñå         | 202/3257 [00:00<00:07, 404.86it/s]  8%|‚ñä         | 249/3257 [00:00<00:07, 425.63it/s]  9%|‚ñâ         | 295/3257 [00:00<00:06, 434.63it/s] 10%|‚ñà         | 339/3257 [00:00<00:06, 419.82it/s] 12%|‚ñà‚ñè        | 382/3257 [00:01<00:10, 282.87it/s] 13%|‚ñà‚ñé        | 420/3257 [00:01<00:09, 301.88it/s] 14%|‚ñà‚ñç        | 455/3257 [00:01<00:09, 303.92it/s] 15%|‚ñà‚ñå        | 493/3257 [00:01<00:08, 322.68it/s] 16%|‚ñà‚ñã        | 532/3257 [00:01<00:08, 339.01it/s] 17%|‚ñà‚ñã        | 568/3257 [00:01<00:07, 342.51it/s] 19%|‚ñà‚ñä        | 604/3257 [00:01<00:07, 346.12it/s] 20%|‚ñà‚ñâ        | 640/3257 [00:01<00:07, 346.61it/s] 21%|‚ñà‚ñà        | 676/3257 [00:01<00:07, 346.81it/s] 22%|‚ñà‚ñà‚ñè       | 712/3257 [00:02<00:07, 349.63it/s] 23%|‚ñà‚ñà‚ñé       | 748/3257 [00:02<00:07, 349.89it/s] 24%|‚ñà‚ñà‚ñç       | 787/3257 [00:02<00:06, 361.19it/s] 25%|‚ñà‚ñà‚ñå       | 824/3257 [00:02<00:06, 363.01it/s] 26%|‚ñà‚ñà‚ñã       | 861/3257 [00:02<00:06, 352.93it/s] 28%|‚ñà‚ñà‚ñä       | 898/3257 [00:02<00:06, 356.48it/s] 29%|‚ñà‚ñà‚ñâ       | 937/3257 [00:02<00:06, 366.02it/s] 30%|‚ñà‚ñà‚ñà       | 978/3257 [00:02<00:06, 377.61it/s] 31%|‚ñà‚ñà‚ñà       | 1016/3257 [00:02<00:05, 374.65it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1054/3257 [00:02<00:05, 369.82it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1092/3257 [00:03<00:05, 369.17it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1131/3257 [00:03<00:05, 371.33it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:03<00:05, 369.07it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1207/3257 [00:03<00:05, 355.84it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1247/3257 [00:03<00:05, 367.64it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1284/3257 [00:03<00:05, 366.84it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1324/3257 [00:03<00:05, 375.92it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1362/3257 [00:03<00:05, 375.96it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1401/3257 [00:03<00:04, 378.13it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1445/3257 [00:03<00:04, 395.25it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1492/3257 [00:04<00:04, 416.02it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1534/3257 [00:04<00:04, 403.60it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1575/3257 [00:04<00:04, 390.72it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1618/3257 [00:04<00:04, 400.50it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:04<00:05, 268.32it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1692/3257 [00:04<00:05, 279.73it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1730/3257 [00:04<00:05, 302.84it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1770/3257 [00:04<00:04, 326.26it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1809/3257 [00:05<00:04, 342.96it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1851/3257 [00:05<00:03, 363.67it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1893/3257 [00:05<00:03, 377.83it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1939/3257 [00:05<00:03, 400.14it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1985/3257 [00:05<00:03, 416.45it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2031/3257 [00:05<00:02, 428.37it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2075/3257 [00:05<00:02, 410.39it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2117/3257 [00:05<00:02, 409.22it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2159/3257 [00:05<00:02, 392.82it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2202/3257 [00:06<00:02, 400.93it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2243/3257 [00:06<00:02, 394.85it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2286/3257 [00:06<00:02, 403.62it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2331/3257 [00:06<00:02, 415.51it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2380/3257 [00:06<00:02, 435.13it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2424/3257 [00:06<00:01, 428.29it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2467/3257 [00:06<00:01, 426.53it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2514/3257 [00:06<00:01, 438.54it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2559/3257 [00:06<00:01, 438.05it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2603/3257 [00:06<00:01, 431.49it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2649/3257 [00:07<00:01, 436.63it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2693/3257 [00:07<00:01, 435.55it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2737/3257 [00:07<00:01, 416.67it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2780/3257 [00:07<00:01, 418.54it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2824/3257 [00:07<00:01, 416.73it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2872/3257 [00:07<00:00, 432.92it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2916/3257 [00:07<00:00, 429.49it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2960/3257 [00:07<00:00, 412.32it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3003/3257 [00:07<00:00, 416.54it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3048/3257 [00:08<00:00, 425.07it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3095/3257 [00:08<00:00, 436.51it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3140/3257 [00:08<00:00, 436.98it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3184/3257 [00:08<00:00, 271.02it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3231/3257 [00:08<00:00, 311.59it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 375.13it/s]
2023-02-07 19:17:30.149 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:17:30,150][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d392,n5,mc9,s0.955441,t4>', 'datetime': '2023-02-07T19:17:30.150429', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:17:30,150][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:17:30,150][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:17:30,278][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 19:17:30,278][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:17:30,280][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=9 retains 462 unique words (50.00% of original 924, drops 462)', 'datetime': '2023-02-07T19:17:30.280467', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:17:30,280][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=9 leaves 1453902 word corpus (99.87% of original 1455748, drops 1846)', 'datetime': '2023-02-07T19:17:30.280654', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:17:30,282][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 19:17:30,282][gensim.models.word2vec][INFO] - sample=0.955441 downsamples 0 most-common words
[2023-02-07 19:17:30,282][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1453902 word corpus (100.0%% of prior 1453902)', 'datetime': '2023-02-07T19:17:30.282611', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:17:30,285][gensim.models.word2vec][INFO] - estimated required memory for 462 words and 392 dimensions: 7438208 bytes
[2023-02-07 19:17:30,286][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:17:30,292][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 462 vocabulary and 392 features, using sg=1 hs=0 sample=0.9554412469569102 negative=5 window=14 shrink_windows=True', 'datetime': '2023-02-07T19:17:30.292619', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:17:31,021][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1457159 effective words) took 0.7s, 2008140 effective words/s
[2023-02-07 19:17:31,714][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1457159 effective words) took 0.7s, 2104674 effective words/s
[2023-02-07 19:17:32,319][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1457159 effective words) took 0.6s, 2416085 effective words/s
[2023-02-07 19:17:33,012][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1457159 effective words) took 0.7s, 2108538 effective words/s
[2023-02-07 19:17:33,709][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1457159 effective words) took 0.7s, 2093823 effective words/s
[2023-02-07 19:17:34,374][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1457159 effective words) took 0.7s, 2197636 effective words/s
[2023-02-07 19:17:35,121][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1457159 effective words) took 0.7s, 1956170 effective words/s
[2023-02-07 19:17:35,573][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1457159 effective words) took 0.5s, 3235206 effective words/s
[2023-02-07 19:17:36,357][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1457159 effective words) took 0.8s, 1863288 effective words/s
[2023-02-07 19:17:36,836][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1457159 effective words) took 0.5s, 3046204 effective words/s
[2023-02-07 19:17:37,317][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1457159 effective words) took 0.5s, 3039287 effective words/s
[2023-02-07 19:17:37,816][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1457159 effective words) took 0.5s, 2928371 effective words/s
[2023-02-07 19:17:38,345][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1457159 effective words) took 0.5s, 2761213 effective words/s
[2023-02-07 19:17:38,883][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1457159 effective words) took 0.5s, 2709768 effective words/s
[2023-02-07 19:17:39,453][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1457159 effective words) took 0.6s, 2561773 effective words/s
[2023-02-07 19:17:39,454][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 21836220 raw words (21857385 effective words) took 9.2s, 2385952 effective words/s', 'datetime': '2023-02-07T19:17:39.454396', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:17:39.454 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:17:40,076][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191712-ofxi33bp/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:17:40.076322', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:17:40,077][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:17:40,096][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191712-ofxi33bp/files/../tmp/embedding_model.pt
2023-02-07 19:17:40.097 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:17:42.132 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:17:42.845 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:17:46.324 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.439941177832944, 'test_mae': 1.1944175218331965, 'test_r2': -3.1231423588038814}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.026 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.026 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.0
wandb: percentage 0.5
wandb:   test_mae 1.19442
wandb:   test_mse 2.43994
wandb:    test_r2 -3.12314
wandb: 
wandb: üöÄ View run stilted-sweep-40 at: https://wandb.ai/xiaoqiz/mof2vec/runs/ofxi33bp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_191712-ofxi33bp/logs
wandb: Agent Starting Run: cm3nrm84 with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 113
wandb: 	model.gensim.alpha: 0.037915934565333057
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.6283528403730718
wandb: 	model.gensim.vector_size: 338
wandb: 	model.gensim.window: 6
wandb: 	model.sklearn.learning_rate: 0.0024181104101647236
wandb: 	model.sklearn.max_depth: 89
wandb: 	model.sklearn.min_child_weight: 0.06135716418578425
wandb: 	model.sklearn.n_estimators: 934
wandb: 	model.sklearn.num_leaves: 483
wandb: 	model.sklearn.reg_alpha: 0.14733092983507473
wandb: 	model.sklearn.reg_lambda: 0.9506367427520492
wandb: 	model.sklearn.subsample: 0.3475391116963913
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191756-cm3nrm84
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-41
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/cm3nrm84
2023-02-07 19:18:03.928 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 19:18:03.929 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 113 for sweep.
2023-02-07 19:18:03.929 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.037915934565333057 for sweep.
2023-02-07 19:18:03.929 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 19:18:03.929 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 19:18:03.930 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6283528403730718 for sweep.
2023-02-07 19:18:03.930 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 338 for sweep.
2023-02-07 19:18:03.930 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 6 for sweep.
2023-02-07 19:18:03.930 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0024181104101647236 for sweep.
2023-02-07 19:18:03.931 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 89 for sweep.
2023-02-07 19:18:03.931 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.06135716418578425 for sweep.
2023-02-07 19:18:03.931 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 934 for sweep.
2023-02-07 19:18:03.931 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 483 for sweep.
2023-02-07 19:18:03.932 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.14733092983507473 for sweep.
2023-02-07 19:18:03.932 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.9506367427520492 for sweep.
2023-02-07 19:18:03.932 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3475391116963913 for sweep.
2023-02-07 19:18:03.933 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:18:03.938 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191756-cm3nrm84/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 113, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 338, 'window': 6, 'min_count': 1, 'dm': 1, 'sample': 0.6283528403730718, 'workers': 4, 'alpha': 0.037915934565333057, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 934, 'max_depth': 89, 'num_leaves': 483, 'reg_alpha': 0.14733092983507473, 'reg_lambda': 0.9506367427520492, 'subsample': 0.3475391116963913, 'min_child_weight': 0.06135716418578425, 'n_jobs': 4, 'learning_rate': 0.0024181104101647236}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 16/3257 [00:00<00:21, 151.64it/s]  1%|          | 34/3257 [00:00<00:20, 155.33it/s]  2%|‚ñè         | 50/3257 [00:00<00:20, 156.90it/s]  2%|‚ñè         | 67/3257 [00:00<00:20, 153.69it/s]  3%|‚ñé         | 87/3257 [00:00<00:18, 169.21it/s]  3%|‚ñé         | 105/3257 [00:00<00:19, 158.21it/s]  4%|‚ñé         | 122/3257 [00:00<00:20, 155.90it/s]  4%|‚ñç         | 141/3257 [00:00<00:18, 165.81it/s]  5%|‚ñç         | 158/3257 [00:00<00:18, 164.83it/s]  5%|‚ñå         | 175/3257 [00:01<00:19, 158.38it/s]  6%|‚ñå         | 193/3257 [00:01<00:18, 161.95it/s]  6%|‚ñã         | 211/3257 [00:01<00:18, 165.77it/s]  7%|‚ñã         | 234/3257 [00:01<00:16, 184.14it/s]  8%|‚ñä         | 253/3257 [00:01<00:16, 181.92it/s]  8%|‚ñä         | 272/3257 [00:01<00:16, 181.91it/s]  9%|‚ñâ         | 296/3257 [00:01<00:15, 195.81it/s] 10%|‚ñâ         | 316/3257 [00:01<00:15, 185.06it/s] 10%|‚ñà         | 337/3257 [00:01<00:15, 190.36it/s] 11%|‚ñà         | 359/3257 [00:02<00:15, 192.46it/s] 12%|‚ñà‚ñè        | 379/3257 [00:02<00:16, 179.34it/s] 12%|‚ñà‚ñè        | 398/3257 [00:02<00:15, 178.72it/s] 13%|‚ñà‚ñé        | 420/3257 [00:02<00:15, 185.14it/s] 13%|‚ñà‚ñé        | 439/3257 [00:02<00:17, 159.18it/s] 14%|‚ñà‚ñç        | 459/3257 [00:02<00:16, 169.47it/s] 15%|‚ñà‚ñç        | 479/3257 [00:02<00:16, 173.35it/s] 15%|‚ñà‚ñå        | 501/3257 [00:02<00:14, 185.50it/s] 16%|‚ñà‚ñå        | 522/3257 [00:02<00:14, 187.28it/s] 17%|‚ñà‚ñã        | 542/3257 [00:03<00:14, 190.65it/s] 17%|‚ñà‚ñã        | 562/3257 [00:03<00:15, 176.29it/s] 18%|‚ñà‚ñä        | 581/3257 [00:03<00:15, 168.51it/s] 18%|‚ñà‚ñä        | 602/3257 [00:03<00:14, 178.32it/s] 19%|‚ñà‚ñâ        | 621/3257 [00:03<00:15, 175.20it/s] 20%|‚ñà‚ñâ        | 642/3257 [00:03<00:14, 183.98it/s] 20%|‚ñà‚ñà        | 661/3257 [00:03<00:15, 169.58it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:14, 172.97it/s] 22%|‚ñà‚ñà‚ñè       | 702/3257 [00:04<00:14, 176.31it/s] 22%|‚ñà‚ñà‚ñè       | 721/3257 [00:04<00:14, 178.86it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:04<00:14, 169.86it/s] 23%|‚ñà‚ñà‚ñé       | 762/3257 [00:04<00:13, 181.17it/s] 24%|‚ñà‚ñà‚ñç       | 781/3257 [00:04<00:14, 174.57it/s] 25%|‚ñà‚ñà‚ñç       | 802/3257 [00:04<00:13, 183.47it/s] 25%|‚ñà‚ñà‚ñå       | 821/3257 [00:04<00:14, 172.87it/s] 26%|‚ñà‚ñà‚ñå       | 839/3257 [00:04<00:14, 167.46it/s] 26%|‚ñà‚ñà‚ñã       | 856/3257 [00:04<00:14, 163.01it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:05<00:14, 167.47it/s] 27%|‚ñà‚ñà‚ñã       | 893/3257 [00:05<00:13, 170.02it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:05<00:13, 173.53it/s] 29%|‚ñà‚ñà‚ñä       | 935/3257 [00:05<00:12, 181.48it/s] 29%|‚ñà‚ñà‚ñâ       | 956/3257 [00:05<00:12, 188.91it/s] 30%|‚ñà‚ñà‚ñâ       | 975/3257 [00:05<00:12, 187.02it/s] 31%|‚ñà‚ñà‚ñà       | 994/3257 [00:05<00:12, 175.68it/s] 31%|‚ñà‚ñà‚ñà       | 1012/3257 [00:05<00:13, 169.70it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1030/3257 [00:05<00:13, 165.27it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:06<00:13, 159.43it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:06<00:13, 167.02it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1084/3257 [00:06<00:12, 167.18it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1101/3257 [00:06<00:13, 164.79it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1119/3257 [00:06<00:12, 164.49it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1137/3257 [00:06<00:12, 167.97it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1154/3257 [00:06<00:12, 162.81it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1174/3257 [00:06<00:12, 171.07it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1192/3257 [00:07<00:20, 99.07it/s]  37%|‚ñà‚ñà‚ñà‚ñã      | 1206/3257 [00:07<00:19, 106.35it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1224/3257 [00:07<00:16, 121.42it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1244/3257 [00:07<00:14, 138.05it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1262/3257 [00:07<00:13, 147.24it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1279/3257 [00:07<00:13, 144.70it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1295/3257 [00:07<00:13, 145.97it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1313/3257 [00:07<00:12, 154.41it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1332/3257 [00:07<00:11, 162.52it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:08<00:11, 162.49it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1367/3257 [00:08<00:11, 162.95it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:08<00:11, 159.33it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1403/3257 [00:08<00:11, 166.48it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1425/3257 [00:08<00:10, 179.06it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1444/3257 [00:08<00:10, 180.12it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1466/3257 [00:08<00:09, 189.91it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1486/3257 [00:08<00:09, 186.99it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1509/3257 [00:08<00:08, 198.63it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1529/3257 [00:09<00:09, 178.21it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1548/3257 [00:09<00:10, 168.62it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1567/3257 [00:09<00:09, 172.70it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1585/3257 [00:09<00:09, 172.17it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1605/3257 [00:09<00:09, 177.93it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1624/3257 [00:09<00:09, 180.33it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1643/3257 [00:09<00:09, 171.72it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1661/3257 [00:09<00:09, 164.35it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1678/3257 [00:09<00:09, 160.36it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1695/3257 [00:10<00:09, 160.89it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:10<00:09, 168.01it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1732/3257 [00:10<00:09, 160.71it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1750/3257 [00:10<00:09, 165.10it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1768/3257 [00:10<00:08, 168.26it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1788/3257 [00:10<00:08, 175.68it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1806/3257 [00:10<00:08, 173.40it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1825/3257 [00:10<00:08, 177.43it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1843/3257 [00:10<00:08, 176.26it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1861/3257 [00:11<00:07, 174.62it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1880/3257 [00:11<00:07, 178.50it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1898/3257 [00:11<00:07, 175.61it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1917/3257 [00:11<00:07, 177.17it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1938/3257 [00:11<00:07, 185.65it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1963/3257 [00:11<00:06, 202.49it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1984/3257 [00:11<00:06, 184.95it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:11<00:06, 188.54it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2024/3257 [00:11<00:06, 191.03it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2044/3257 [00:12<00:06, 184.13it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2063/3257 [00:12<00:07, 168.57it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2083/3257 [00:12<00:06, 175.47it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2101/3257 [00:12<00:06, 170.07it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:12<00:06, 171.11it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2138/3257 [00:12<00:06, 169.26it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2156/3257 [00:12<00:06, 166.47it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2177/3257 [00:12<00:06, 176.98it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2195/3257 [00:12<00:06, 175.10it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2213/3257 [00:13<00:06, 167.41it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2235/3257 [00:13<00:05, 180.37it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2254/3257 [00:13<00:05, 171.65it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2272/3257 [00:13<00:06, 159.96it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2294/3257 [00:13<00:05, 174.87it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2312/3257 [00:13<00:05, 174.24it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2337/3257 [00:13<00:04, 191.45it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:13<00:04, 201.63it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2381/3257 [00:13<00:04, 202.21it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2403/3257 [00:14<00:04, 207.30it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2424/3257 [00:14<00:04, 191.84it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2444/3257 [00:14<00:04, 178.42it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2465/3257 [00:14<00:04, 186.03it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2484/3257 [00:14<00:04, 185.96it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2508/3257 [00:14<00:03, 199.73it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2530/3257 [00:14<00:03, 204.92it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:14<00:03, 200.05it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2572/3257 [00:14<00:03, 180.79it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2591/3257 [00:15<00:06, 103.14it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2615/3257 [00:15<00:05, 127.28it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2638/3257 [00:15<00:04, 146.34it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2657/3257 [00:15<00:04, 149.65it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2676/3257 [00:15<00:03, 158.78it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2695/3257 [00:15<00:03, 159.63it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2713/3257 [00:15<00:03, 153.85it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2733/3257 [00:16<00:03, 165.37it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:16<00:02, 177.61it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2775/3257 [00:16<00:02, 171.64it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2799/3257 [00:16<00:02, 189.79it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2819/3257 [00:16<00:02, 181.43it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2838/3257 [00:16<00:02, 174.62it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2860/3257 [00:16<00:02, 186.55it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2885/3257 [00:16<00:01, 203.23it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2906/3257 [00:17<00:01, 177.00it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:17<00:01, 181.63it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2947/3257 [00:17<00:01, 174.92it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2965/3257 [00:17<00:01, 176.05it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2983/3257 [00:17<00:01, 165.52it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3005/3257 [00:17<00:01, 179.06it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3024/3257 [00:17<00:01, 172.42it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3048/3257 [00:17<00:01, 187.36it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3070/3257 [00:17<00:00, 196.24it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3090/3257 [00:18<00:00, 193.01it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3114/3257 [00:18<00:00, 205.82it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3135/3257 [00:18<00:00, 195.00it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3155/3257 [00:18<00:00, 189.12it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3175/3257 [00:18<00:00, 188.86it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3195/3257 [00:18<00:00, 187.28it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3214/3257 [00:18<00:00, 175.81it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3239/3257 [00:18<00:00, 194.24it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 172.42it/s]
2023-02-07 19:18:23.610 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:18:23,611][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d338,n5,w6,s0.628353,t4>', 'datetime': '2023-02-07T19:18:23.611276', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:18:23,611][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:18:23,611][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:18:24,159][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 19:18:24,160][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:18:24,298][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 54054 unique words (100.00% of original 54054, drops 0)', 'datetime': '2023-02-07T19:18:24.298116', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:18:24,298][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 6550866 word corpus (100.00% of original 6550866, drops 0)', 'datetime': '2023-02-07T19:18:24.298496', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:18:24,473][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 19:18:24,475][gensim.models.word2vec][INFO] - sample=0.628353 downsamples 0 most-common words
[2023-02-07 19:18:24,475][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6550866 word corpus (100.0%% of prior 6550866)', 'datetime': '2023-02-07T19:18:24.475399', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:18:24,776][gensim.models.word2vec][INFO] - estimated required memory for 54054 words and 338 dimensions: 178243880 bytes
[2023-02-07 19:18:24,777][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:18:24,849][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 54054 vocabulary and 338 features, using sg=0 hs=0 sample=0.6283528403730718 negative=5 window=6 shrink_windows=True', 'datetime': '2023-02-07T19:18:24.849187', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:18:25,855][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 18.51% examples, 1169400 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:26,858][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 37.40% examples, 1239182 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:27,863][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 57.45% examples, 1264949 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:28,863][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 78.29% examples, 1277510 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:29,864][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 98.62% examples, 1279644 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:29,922][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6494971 effective words) took 5.1s, 1280705 effective words/s
[2023-02-07 19:18:30,927][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 19.31% examples, 1213594 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:31,936][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 36.60% examples, 1204955 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:18:32,938][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 55.48% examples, 1224357 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:33,939][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 74.92% examples, 1229539 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:34,942][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 95.27% examples, 1234997 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:18:35,183][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6494971 effective words) took 5.3s, 1234958 effective words/s
[2023-02-07 19:18:36,189][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 21.55% examples, 1373036 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:37,195][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 41.45% examples, 1376096 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:38,202][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 63.13% examples, 1382598 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:39,205][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 84.99% examples, 1386633 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:39,879][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6494971 effective words) took 4.7s, 1383854 effective words/s
[2023-02-07 19:18:40,888][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 22.01% examples, 1396359 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:41,889][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 41.45% examples, 1377599 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:42,890][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 63.22% examples, 1388368 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:43,892][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 85.08% examples, 1391668 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:44,544][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6494971 effective words) took 4.7s, 1392857 effective words/s
[2023-02-07 19:18:45,552][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 25.79% examples, 1662432 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:18:46,556][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 50.72% examples, 1669009 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:47,563][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 76.30% examples, 1667065 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:48,561][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 99.85% examples, 1613956 words/s, in_qsize 2, out_qsize 1
[2023-02-07 19:18:48,570][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6494971 effective words) took 4.0s, 1614129 effective words/s
[2023-02-07 19:18:49,580][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 22.44% examples, 1424418 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:50,581][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 42.92% examples, 1427825 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:51,584][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 64.81% examples, 1421960 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:52,589][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 86.80% examples, 1413950 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:53,180][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6494971 effective words) took 4.6s, 1409340 effective words/s
[2023-02-07 19:18:54,182][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 21.95% examples, 1396110 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:55,183][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 41.85% examples, 1399069 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:56,185][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 63.89% examples, 1404266 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:18:57,187][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 85.66% examples, 1400306 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:57,827][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6494971 effective words) took 4.6s, 1398357 effective words/s
[2023-02-07 19:18:58,831][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 22.04% examples, 1402195 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:18:59,839][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 42.46% examples, 1409626 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:19:00,842][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 64.42% examples, 1411882 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:01,845][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 86.92% examples, 1418914 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:02,367][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6494971 effective words) took 4.5s, 1431368 effective words/s
[2023-02-07 19:19:03,370][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 23.21% examples, 1490329 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:04,373][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 45.16% examples, 1492005 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:19:05,376][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 67.70% examples, 1490749 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:19:06,386][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 91.16% examples, 1483972 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:06,784][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6494971 effective words) took 4.4s, 1471217 effective words/s
[2023-02-07 19:19:07,811][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 21.55% examples, 1344304 words/s, in_qsize 5, out_qsize 2
[2023-02-07 19:19:08,813][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 42.09% examples, 1385153 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:09,816][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 63.89% examples, 1391148 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:19:10,823][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 86.80% examples, 1407726 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:11,363][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6494971 effective words) took 4.6s, 1418646 effective words/s
[2023-02-07 19:19:12,368][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 22.72% examples, 1460411 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:13,369][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 44.00% examples, 1459904 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:14,370][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 66.13% examples, 1458768 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:15,378][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 88.89% examples, 1449028 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:15,840][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6494971 effective words) took 4.5s, 1451328 effective words/s
[2023-02-07 19:19:16,842][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 22.38% examples, 1436388 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:17,845][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 42.98% examples, 1430979 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:18,849][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 64.75% examples, 1422715 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:19:19,852][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 88.21% examples, 1436494 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:20,300][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6494971 effective words) took 4.5s, 1456851 effective words/s
[2023-02-07 19:19:21,307][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 23.00% examples, 1475406 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:22,308][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 44.12% examples, 1462818 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:23,324][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 64.42% examples, 1407642 words/s, in_qsize 7, out_qsize 1
[2023-02-07 19:19:24,325][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 84.74% examples, 1378937 words/s, in_qsize 7, out_qsize 3
[2023-02-07 19:19:25,037][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6494971 effective words) took 4.7s, 1371820 effective words/s
[2023-02-07 19:19:26,046][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 18.82% examples, 1182384 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:27,050][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 38.62% examples, 1271841 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:28,053][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 58.21% examples, 1281282 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:19:29,062][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 78.94% examples, 1287197 words/s, in_qsize 7, out_qsize 1
[2023-02-07 19:19:30,077][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 98.19% examples, 1266154 words/s, in_qsize 5, out_qsize 7
[2023-02-07 19:19:30,109][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6494971 effective words) took 5.1s, 1281030 effective words/s
[2023-02-07 19:19:31,113][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 17.53% examples, 1120305 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:32,124][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 38.19% examples, 1258972 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:19:33,134][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 57.84% examples, 1269565 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:19:34,139][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 78.42% examples, 1277771 words/s, in_qsize 7, out_qsize 1
[2023-02-07 19:19:35,155][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 98.28% examples, 1266881 words/s, in_qsize 8, out_qsize 2
[2023-02-07 19:19:35,209][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6494971 effective words) took 5.1s, 1274229 effective words/s
[2023-02-07 19:19:35,210][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (97424565 effective words) took 70.4s, 1384643 effective words/s', 'datetime': '2023-02-07T19:19:35.210398', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:19:35.211 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:19:40,898][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191756-cm3nrm84/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:19:40.897975', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:19:40,899][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191756-cm3nrm84/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 19:19:40,957][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191756-cm3nrm84/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 19:19:41,006][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:19:41,058][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_191756-cm3nrm84/files/../tmp/embedding_model.pt
2023-02-07 19:19:41.058 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:19:43.038 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:19:43.713 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:19:51.863 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.29836652201222, 'test_mae': 1.1503870131553613, 'test_r2': -2.710186192944958}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.86
wandb: percentage 0.0
wandb:   test_mae 1.15039
wandb:   test_mse 2.29837
wandb:    test_r2 -2.71019
wandb: 
wandb: üöÄ View run decent-sweep-41 at: https://wandb.ai/xiaoqiz/mof2vec/runs/cm3nrm84
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_191756-cm3nrm84/logs
wandb: Agent Starting Run: 5hmdmd9n with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 412
wandb: 	model.gensim.alpha: 0.04284063896229872
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.3901054435687856
wandb: 	model.gensim.vector_size: 176
wandb: 	model.gensim.window: 18
wandb: 	model.sklearn.learning_rate: 0.0008049896032893622
wandb: 	model.sklearn.max_depth: 30
wandb: 	model.sklearn.min_child_weight: 0.08054459734805676
wandb: 	model.sklearn.n_estimators: 1783
wandb: 	model.sklearn.num_leaves: 57
wandb: 	model.sklearn.reg_alpha: 0.3375702694917385
wandb: 	model.sklearn.reg_lambda: 0.08187284673544731
wandb: 	model.sklearn.subsample: 0.9923106275926254
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192005-5hmdmd9n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-42
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/5hmdmd9n
2023-02-07 19:20:13.411 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 19:20:13.412 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 412 for sweep.
2023-02-07 19:20:13.412 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.04284063896229872 for sweep.
2023-02-07 19:20:13.413 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 19:20:13.413 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 19:20:13.413 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3901054435687856 for sweep.
2023-02-07 19:20:13.413 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 176 for sweep.
2023-02-07 19:20:13.414 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 18 for sweep.
2023-02-07 19:20:13.414 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0008049896032893622 for sweep.
2023-02-07 19:20:13.414 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 30 for sweep.
2023-02-07 19:20:13.414 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.08054459734805676 for sweep.
2023-02-07 19:20:13.414 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1783 for sweep.
2023-02-07 19:20:13.415 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 57 for sweep.
2023-02-07 19:20:13.415 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.3375702694917385 for sweep.
2023-02-07 19:20:13.415 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.08187284673544731 for sweep.
2023-02-07 19:20:13.415 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.9923106275926254 for sweep.
2023-02-07 19:20:13.416 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:20:13.421 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192005-5hmdmd9n/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 412, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 176, 'window': 18, 'min_count': 4, 'dm': 1, 'sample': 0.3901054435687856, 'workers': 4, 'alpha': 0.04284063896229872, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1783, 'max_depth': 30, 'num_leaves': 57, 'reg_alpha': 0.3375702694917385, 'reg_lambda': 0.08187284673544731, 'subsample': 0.9923106275926254, 'min_child_weight': 0.08054459734805676, 'n_jobs': 4, 'learning_rate': 0.0008049896032893622}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 35/3257 [00:00<00:09, 346.36it/s]  2%|‚ñè         | 70/3257 [00:00<00:09, 348.26it/s]  3%|‚ñé         | 106/3257 [00:00<00:09, 346.23it/s]  4%|‚ñç         | 145/3257 [00:00<00:08, 360.67it/s]  6%|‚ñå         | 182/3257 [00:00<00:08, 353.35it/s]  7%|‚ñã         | 221/3257 [00:00<00:08, 362.76it/s]  8%|‚ñä         | 260/3257 [00:00<00:08, 369.65it/s]  9%|‚ñâ         | 301/3257 [00:00<00:07, 380.62it/s] 10%|‚ñà         | 340/3257 [00:00<00:07, 381.70it/s] 12%|‚ñà‚ñè        | 379/3257 [00:01<00:07, 373.40it/s] 13%|‚ñà‚ñé        | 419/3257 [00:01<00:07, 377.90it/s] 14%|‚ñà‚ñç        | 457/3257 [00:01<00:07, 357.56it/s] 15%|‚ñà‚ñå        | 499/3257 [00:01<00:07, 373.73it/s] 17%|‚ñà‚ñã        | 539/3257 [00:01<00:07, 381.08it/s] 18%|‚ñà‚ñä        | 578/3257 [00:01<00:07, 368.22it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:01<00:06, 379.26it/s] 20%|‚ñà‚ñà        | 659/3257 [00:01<00:09, 270.86it/s] 21%|‚ñà‚ñà‚ñè       | 698/3257 [00:02<00:08, 296.82it/s] 23%|‚ñà‚ñà‚ñé       | 737/3257 [00:02<00:07, 318.70it/s] 24%|‚ñà‚ñà‚ñç       | 776/3257 [00:02<00:07, 335.88it/s] 25%|‚ñà‚ñà‚ñå       | 815/3257 [00:02<00:06, 350.06it/s] 26%|‚ñà‚ñà‚ñå       | 852/3257 [00:02<00:06, 349.98it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:02<00:06, 360.77it/s] 29%|‚ñà‚ñà‚ñä       | 932/3257 [00:02<00:06, 373.14it/s] 30%|‚ñà‚ñà‚ñâ       | 975/3257 [00:02<00:05, 388.30it/s] 31%|‚ñà‚ñà‚ñà       | 1015/3257 [00:02<00:05, 380.41it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1054/3257 [00:02<00:05, 375.74it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1092/3257 [00:03<00:05, 373.70it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1132/3257 [00:03<00:05, 380.86it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:03<00:05, 388.18it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1212/3257 [00:03<00:05, 365.28it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:03<00:05, 362.10it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1287/3257 [00:03<00:05, 364.31it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1326/3257 [00:03<00:05, 371.53it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1365/3257 [00:03<00:05, 373.86it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1403/3257 [00:03<00:04, 375.35it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1448/3257 [00:03<00:04, 394.35it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1494/3257 [00:04<00:04, 411.80it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1536/3257 [00:04<00:04, 401.92it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1577/3257 [00:04<00:04, 394.18it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1618/3257 [00:04<00:04, 396.79it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1658/3257 [00:04<00:04, 387.90it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1697/3257 [00:04<00:04, 383.45it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1736/3257 [00:04<00:04, 374.02it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1777/3257 [00:04<00:03, 380.41it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1817/3257 [00:04<00:03, 385.85it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1856/3257 [00:05<00:04, 287.26it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1898/3257 [00:05<00:04, 318.15it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1941/3257 [00:05<00:03, 345.25it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1983/3257 [00:05<00:03, 362.93it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2023/3257 [00:05<00:03, 372.95it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2062/3257 [00:05<00:03, 358.14it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2100/3257 [00:05<00:03, 356.42it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2137/3257 [00:05<00:03, 357.83it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2174/3257 [00:05<00:03, 360.09it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2211/3257 [00:06<00:02, 357.82it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2248/3257 [00:06<00:02, 358.05it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2286/3257 [00:06<00:02, 362.59it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2326/3257 [00:06<00:02, 370.37it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2367/3257 [00:06<00:02, 379.33it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2407/3257 [00:06<00:02, 383.41it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2446/3257 [00:06<00:02, 377.88it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2489/3257 [00:06<00:01, 390.50it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2535/3257 [00:06<00:01, 409.47it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2577/3257 [00:07<00:01, 396.04it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2621/3257 [00:07<00:01, 406.55it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2662/3257 [00:07<00:01, 392.54it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2702/3257 [00:07<00:01, 391.73it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2748/3257 [00:07<00:01, 409.33it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2791/3257 [00:07<00:01, 412.21it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2833/3257 [00:07<00:01, 403.80it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2883/3257 [00:07<00:00, 429.74it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2927/3257 [00:07<00:00, 424.97it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2970/3257 [00:07<00:00, 408.41it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:08<00:00, 411.45it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3059/3257 [00:08<00:00, 427.02it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3105/3257 [00:08<00:00, 436.62it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3149/3257 [00:08<00:00, 430.37it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3193/3257 [00:08<00:00, 428.89it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3237/3257 [00:08<00:00, 430.90it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 371.16it/s]
2023-02-07 19:20:22.366 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:20:22,367][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d176,n5,w18,mc4,s0.390105,t4>', 'datetime': '2023-02-07T19:20:22.367395', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:20:22,367][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:20:22,367][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:20:22,491][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 19:20:22,491][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:20:22,493][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 742 unique words (80.30% of original 924, drops 182)', 'datetime': '2023-02-07T19:20:22.493733', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:20:22,493][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 1455401 word corpus (99.98% of original 1455748, drops 347)', 'datetime': '2023-02-07T19:20:22.493937', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:20:22,496][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 19:20:22,496][gensim.models.word2vec][INFO] - sample=0.390105 downsamples 0 most-common words
[2023-02-07 19:20:22,496][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455401 word corpus (100.0%% of prior 1455401)', 'datetime': '2023-02-07T19:20:22.496688', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:20:22,501][gensim.models.word2vec][INFO] - estimated required memory for 742 words and 176 dimensions: 4360064 bytes
[2023-02-07 19:20:22,501][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:20:22,504][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 742 vocabulary and 176 features, using sg=0 hs=0 sample=0.3901054435687856 negative=5 window=18 shrink_windows=True', 'datetime': '2023-02-07T19:20:22.504604', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:20:23,526][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 50.35% examples, 734308 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:20:24,189][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458658 effective words) took 1.7s, 866770 effective words/s
[2023-02-07 19:20:25,195][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 79.06% examples, 1161252 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:20:25,439][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458658 effective words) took 1.2s, 1168097 effective words/s
[2023-02-07 19:20:26,454][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 76.24% examples, 1111682 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:20:26,729][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458658 effective words) took 1.3s, 1131860 effective words/s
[2023-02-07 19:20:27,733][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 77.96% examples, 1143177 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:20:27,995][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458658 effective words) took 1.3s, 1153110 effective words/s
[2023-02-07 19:20:29,014][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 64.14% examples, 935983 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:20:29,478][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458658 effective words) took 1.5s, 984514 effective words/s
[2023-02-07 19:20:30,487][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 54.62% examples, 811229 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:20:31,034][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458658 effective words) took 1.6s, 938444 effective words/s
[2023-02-07 19:20:32,044][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 52.16% examples, 773969 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:20:32,958][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458658 effective words) took 1.9s, 759802 effective words/s
[2023-02-07 19:20:33,965][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 53.21% examples, 793707 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:20:34,829][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458658 effective words) took 1.9s, 780866 effective words/s
[2023-02-07 19:20:35,841][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 54.62% examples, 808552 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:20:36,387][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458658 effective words) took 1.6s, 937384 effective words/s
[2023-02-07 19:20:37,393][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 77.96% examples, 1142364 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:20:37,746][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458658 effective words) took 1.4s, 1074917 effective words/s
[2023-02-07 19:20:38,757][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 52.56% examples, 780198 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:20:39,602][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458658 effective words) took 1.9s, 786867 effective words/s
[2023-02-07 19:20:40,614][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 58.64% examples, 865832 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:20:41,323][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458658 effective words) took 1.7s, 848416 effective words/s
[2023-02-07 19:20:42,332][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 49.74% examples, 735074 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:20:43,264][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458658 effective words) took 1.9s, 752494 effective words/s
[2023-02-07 19:20:44,275][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 50.88% examples, 751756 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:20:45,209][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458658 effective words) took 1.9s, 750580 effective words/s
[2023-02-07 19:20:46,215][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 50.41% examples, 746016 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:20:47,112][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458658 effective words) took 1.9s, 767474 effective words/s
[2023-02-07 19:20:47,113][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 21836220 raw words (21879870 effective words) took 24.6s, 889128 effective words/s', 'datetime': '2023-02-07T19:20:47.113151', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:20:47.113 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:20:48,005][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192005-5hmdmd9n/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:20:48.005018', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:20:48,005][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:20:48,012][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192005-5hmdmd9n/files/../tmp/embedding_model.pt
2023-02-07 19:20:48.012 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:20:49.281 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:20:49.767 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:20:51.040 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.6923423952657175, 'test_mae': 1.2491167527494917, 'test_r2': -4.219397418215327}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.09
wandb: percentage 0.19697
wandb:   test_mae 1.24912
wandb:   test_mse 2.69234
wandb:    test_r2 -4.2194
wandb: 
wandb: üöÄ View run flowing-sweep-42 at: https://wandb.ai/xiaoqiz/mof2vec/runs/5hmdmd9n
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_192005-5hmdmd9n/logs
wandb: Agent Starting Run: 2725ofdq with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 675
wandb: 	model.gensim.alpha: 0.0024556093972191227
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.7925295810820965
wandb: 	model.gensim.vector_size: 259
wandb: 	model.gensim.window: 6
wandb: 	model.sklearn.learning_rate: 0.008248948177045997
wandb: 	model.sklearn.max_depth: 47
wandb: 	model.sklearn.min_child_weight: 0.012909410463403104
wandb: 	model.sklearn.n_estimators: 1747
wandb: 	model.sklearn.num_leaves: 380
wandb: 	model.sklearn.reg_alpha: 0.03173279867934339
wandb: 	model.sklearn.reg_lambda: 0.01652539423614149
wandb: 	model.sklearn.subsample: 0.6969995146243995
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192104-2725ofdq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-43
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/2725ofdq
2023-02-07 19:21:12.022 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 19:21:12.023 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 675 for sweep.
2023-02-07 19:21:12.023 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0024556093972191227 for sweep.
2023-02-07 19:21:12.024 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 19:21:12.024 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 19:21:12.024 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7925295810820965 for sweep.
2023-02-07 19:21:12.024 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 259 for sweep.
2023-02-07 19:21:12.024 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 6 for sweep.
2023-02-07 19:21:12.025 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.008248948177045997 for sweep.
2023-02-07 19:21:12.025 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 47 for sweep.
2023-02-07 19:21:12.025 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.012909410463403104 for sweep.
2023-02-07 19:21:12.025 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1747 for sweep.
2023-02-07 19:21:12.026 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 380 for sweep.
2023-02-07 19:21:12.026 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.03173279867934339 for sweep.
2023-02-07 19:21:12.026 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.01652539423614149 for sweep.
2023-02-07 19:21:12.026 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6969995146243995 for sweep.
2023-02-07 19:21:12.026 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:21:12.035 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192104-2725ofdq/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 675, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 259, 'window': 6, 'min_count': 4, 'dm': 1, 'sample': 0.7925295810820965, 'workers': 4, 'alpha': 0.0024556093972191227, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1747, 'max_depth': 47, 'num_leaves': 380, 'reg_alpha': 0.03173279867934339, 'reg_lambda': 0.01652539423614149, 'subsample': 0.6969995146243995, 'min_child_weight': 0.012909410463403104, 'n_jobs': 4, 'learning_rate': 0.008248948177045997}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:19, 168.74it/s]  1%|          | 35/3257 [00:00<00:18, 172.80it/s]  2%|‚ñè         | 53/3257 [00:00<00:18, 170.98it/s]  2%|‚ñè         | 72/3257 [00:00<00:17, 177.80it/s]  3%|‚ñé         | 92/3257 [00:00<00:17, 182.37it/s]  3%|‚ñé         | 111/3257 [00:00<00:18, 169.88it/s]  4%|‚ñç         | 131/3257 [00:00<00:17, 176.38it/s]  5%|‚ñç         | 152/3257 [00:00<00:16, 185.42it/s]  5%|‚ñå         | 171/3257 [00:00<00:17, 180.08it/s]  6%|‚ñå         | 190/3257 [00:01<00:16, 182.06it/s]  6%|‚ñã         | 209/3257 [00:01<00:16, 183.75it/s]  7%|‚ñã         | 232/3257 [00:01<00:15, 195.65it/s]  8%|‚ñä         | 252/3257 [00:01<00:15, 195.40it/s]  8%|‚ñä         | 272/3257 [00:01<00:15, 190.78it/s]  9%|‚ñâ         | 296/3257 [00:01<00:14, 204.96it/s] 10%|‚ñâ         | 317/3257 [00:01<00:15, 195.58it/s] 10%|‚ñà         | 337/3257 [00:01<00:14, 196.52it/s] 11%|‚ñà         | 359/3257 [00:01<00:14, 196.93it/s] 12%|‚ñà‚ñè        | 379/3257 [00:02<00:15, 182.33it/s] 12%|‚ñà‚ñè        | 398/3257 [00:02<00:15, 182.82it/s] 13%|‚ñà‚ñé        | 420/3257 [00:02<00:15, 186.37it/s] 13%|‚ñà‚ñé        | 439/3257 [00:02<00:17, 160.59it/s] 14%|‚ñà‚ñç        | 459/3257 [00:02<00:16, 168.55it/s] 15%|‚ñà‚ñç        | 479/3257 [00:02<00:16, 170.52it/s] 15%|‚ñà‚ñå        | 500/3257 [00:02<00:15, 179.78it/s] 16%|‚ñà‚ñå        | 520/3257 [00:02<00:14, 182.93it/s] 17%|‚ñà‚ñã        | 539/3257 [00:02<00:14, 182.45it/s] 17%|‚ñà‚ñã        | 558/3257 [00:03<00:15, 173.71it/s] 18%|‚ñà‚ñä        | 576/3257 [00:03<00:16, 162.72it/s] 18%|‚ñà‚ñä        | 599/3257 [00:03<00:15, 175.34it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:14, 176.06it/s] 20%|‚ñà‚ñâ        | 641/3257 [00:03<00:14, 183.64it/s] 20%|‚ñà‚ñà        | 660/3257 [00:03<00:15, 169.43it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:14, 173.87it/s] 22%|‚ñà‚ñà‚ñè       | 702/3257 [00:03<00:14, 176.63it/s] 22%|‚ñà‚ñà‚ñè       | 721/3257 [00:03<00:14, 179.37it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:04<00:15, 167.77it/s] 23%|‚ñà‚ñà‚ñé       | 762/3257 [00:04<00:13, 180.20it/s] 24%|‚ñà‚ñà‚ñç       | 781/3257 [00:04<00:14, 174.73it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:04<00:13, 183.33it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:04<00:13, 174.64it/s] 26%|‚ñà‚ñà‚ñå       | 841/3257 [00:04<00:14, 165.59it/s] 26%|‚ñà‚ñà‚ñã       | 860/3257 [00:04<00:13, 171.42it/s] 27%|‚ñà‚ñà‚ñã       | 878/3257 [00:04<00:14, 168.01it/s] 28%|‚ñà‚ñà‚ñä       | 899/3257 [00:05<00:13, 178.60it/s] 28%|‚ñà‚ñà‚ñä       | 918/3257 [00:05<00:13, 175.79it/s] 29%|‚ñà‚ñà‚ñâ       | 937/3257 [00:05<00:13, 177.79it/s] 29%|‚ñà‚ñà‚ñâ       | 958/3257 [00:05<00:12, 186.28it/s] 30%|‚ñà‚ñà‚ñâ       | 977/3257 [00:05<00:12, 185.92it/s] 31%|‚ñà‚ñà‚ñà       | 996/3257 [00:05<00:13, 168.60it/s] 31%|‚ñà‚ñà‚ñà       | 1014/3257 [00:05<00:13, 168.58it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1032/3257 [00:05<00:13, 166.63it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1049/3257 [00:05<00:13, 164.24it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1068/3257 [00:06<00:12, 169.79it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1086/3257 [00:06<00:12, 169.11it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1104/3257 [00:06<00:12, 171.84it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1122/3257 [00:06<00:12, 169.10it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1139/3257 [00:06<00:12, 163.70it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1156/3257 [00:06<00:21, 99.87it/s]  36%|‚ñà‚ñà‚ñà‚ñå      | 1175/3257 [00:06<00:18, 114.35it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1190/3257 [00:07<00:17, 115.73it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1206/3257 [00:07<00:16, 125.11it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1223/3257 [00:07<00:14, 135.61it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1243/3257 [00:07<00:13, 151.44it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1260/3257 [00:07<00:12, 155.36it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1277/3257 [00:07<00:13, 152.14it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1293/3257 [00:07<00:12, 151.38it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1311/3257 [00:07<00:12, 158.69it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1330/3257 [00:07<00:11, 165.48it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:07<00:11, 165.65it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1368/3257 [00:08<00:11, 169.15it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1386/3257 [00:08<00:11, 160.44it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1408/3257 [00:08<00:10, 175.85it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1430/3257 [00:08<00:09, 186.59it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1449/3257 [00:08<00:09, 186.46it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1471/3257 [00:08<00:09, 194.98it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1491/3257 [00:08<00:09, 190.90it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1512/3257 [00:08<00:08, 194.97it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1532/3257 [00:08<00:09, 178.84it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1551/3257 [00:09<00:09, 174.95it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1569/3257 [00:09<00:09, 172.41it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1587/3257 [00:09<00:09, 171.93it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1607/3257 [00:09<00:09, 179.15it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1627/3257 [00:09<00:08, 184.85it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1646/3257 [00:09<00:09, 174.22it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1664/3257 [00:09<00:09, 162.58it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1681/3257 [00:09<00:09, 163.20it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1700/3257 [00:09<00:09, 168.11it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1718/3257 [00:10<00:09, 169.72it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1736/3257 [00:10<00:09, 156.06it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1755/3257 [00:10<00:09, 164.46it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1774/3257 [00:10<00:08, 170.28it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1795/3257 [00:10<00:08, 178.25it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1814/3257 [00:10<00:08, 170.98it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1832/3257 [00:10<00:08, 167.16it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1850/3257 [00:10<00:08, 169.67it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:10<00:07, 174.32it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1887/3257 [00:11<00:07, 174.01it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1908/3257 [00:11<00:07, 183.54it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1927/3257 [00:11<00:07, 178.08it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1953/3257 [00:11<00:06, 200.42it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1974/3257 [00:11<00:06, 196.88it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1994/3257 [00:11<00:06, 192.83it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2014/3257 [00:11<00:06, 183.54it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2035/3257 [00:11<00:06, 189.52it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2055/3257 [00:11<00:06, 172.24it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2073/3257 [00:12<00:06, 172.12it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2092/3257 [00:12<00:06, 174.94it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2110/3257 [00:12<00:06, 170.95it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2128/3257 [00:12<00:06, 162.77it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2145/3257 [00:12<00:07, 153.89it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2164/3257 [00:12<00:06, 162.85it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:12<00:06, 161.19it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2200/3257 [00:12<00:06, 168.56it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2218/3257 [00:12<00:06, 159.30it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:13<00:06, 161.13it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2253/3257 [00:13<00:06, 158.15it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2269/3257 [00:13<00:06, 157.34it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2287/3257 [00:13<00:05, 163.22it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2304/3257 [00:13<00:05, 159.88it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2326/3257 [00:13<00:05, 176.26it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2348/3257 [00:13<00:04, 187.87it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2367/3257 [00:13<00:04, 186.97it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2387/3257 [00:13<00:04, 190.43it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2407/3257 [00:14<00:04, 176.95it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2426/3257 [00:14<00:04, 179.18it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2445/3257 [00:14<00:04, 166.77it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2464/3257 [00:14<00:04, 172.49it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2483/3257 [00:14<00:04, 176.13it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2501/3257 [00:14<00:07, 102.70it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2520/3257 [00:14<00:06, 119.24it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2540/3257 [00:15<00:05, 132.01it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2557/3257 [00:15<00:05, 138.42it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2573/3257 [00:15<00:04, 140.43it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2589/3257 [00:15<00:04, 143.82it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2614/3257 [00:15<00:03, 170.14it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2634/3257 [00:15<00:03, 177.45it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2653/3257 [00:15<00:03, 168.52it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2671/3257 [00:15<00:03, 170.51it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2690/3257 [00:15<00:03, 175.30it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2708/3257 [00:16<00:03, 152.92it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2726/3257 [00:16<00:03, 158.47it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2749/3257 [00:16<00:02, 176.32it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:16<00:02, 170.93it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2787/3257 [00:16<00:02, 173.89it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:16<00:02, 176.23it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2824/3257 [00:16<00:02, 163.41it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2841/3257 [00:16<00:02, 162.26it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2864/3257 [00:16<00:02, 179.81it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:17<00:02, 182.51it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2905/3257 [00:17<00:02, 166.33it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2926/3257 [00:17<00:01, 176.83it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2945/3257 [00:17<00:01, 159.19it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2963/3257 [00:17<00:01, 163.37it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2980/3257 [00:17<00:01, 157.13it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2997/3257 [00:17<00:01, 159.92it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3015/3257 [00:17<00:01, 164.38it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3032/3257 [00:17<00:01, 164.56it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:18<00:01, 165.35it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3069/3257 [00:18<00:01, 175.13it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3087/3257 [00:18<00:00, 171.04it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3109/3257 [00:18<00:00, 185.02it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3130/3257 [00:18<00:00, 190.01it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3150/3257 [00:18<00:00, 176.30it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3169/3257 [00:18<00:00, 178.22it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3188/3257 [00:18<00:00, 173.80it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:18<00:00, 176.36it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3225/3257 [00:19<00:00, 171.43it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:19<00:00, 180.89it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 169.53it/s]
2023-02-07 19:21:32.096 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:21:32,097][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d259,n5,w6,mc4,s0.79253,t4>', 'datetime': '2023-02-07T19:21:32.097905', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:21:32,099][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:21:32,099][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:21:32,662][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 19:21:32,662][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:21:32,758][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 36921 unique words (68.30% of original 54054, drops 17133)', 'datetime': '2023-02-07T19:21:32.758434', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:21:32,758][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 6522479 word corpus (99.57% of original 6550866, drops 28387)', 'datetime': '2023-02-07T19:21:32.758835', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:21:32,880][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 19:21:32,881][gensim.models.word2vec][INFO] - sample=0.79253 downsamples 0 most-common words
[2023-02-07 19:21:32,881][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6522479 word corpus (100.0%% of prior 6522479)', 'datetime': '2023-02-07T19:21:32.881970', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:21:33,083][gensim.models.word2vec][INFO] - estimated required memory for 36921 words and 259 dimensions: 98986464 bytes
[2023-02-07 19:21:33,084][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:21:33,122][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 36921 vocabulary and 259 features, using sg=0 hs=0 sample=0.7925295810820965 negative=5 window=6 shrink_windows=True', 'datetime': '2023-02-07T19:21:33.122562', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:21:34,127][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 20.11% examples, 1273735 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:35,127][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 40.16% examples, 1331742 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:36,128][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 63.49% examples, 1391420 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:37,129][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 86.58% examples, 1408806 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:37,698][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6467889 effective words) took 4.6s, 1414023 effective words/s
[2023-02-07 19:21:38,702][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 22.11% examples, 1405093 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:39,712][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 43.11% examples, 1425555 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:40,715][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 66.29% examples, 1450676 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:41,730][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 91.19% examples, 1474934 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:42,055][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6467889 effective words) took 4.4s, 1485218 effective words/s
[2023-02-07 19:21:43,058][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 29.54% examples, 1908086 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:44,063][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 56.19% examples, 1855956 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:45,063][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 86.95% examples, 1888004 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:21:45,477][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6467889 effective words) took 3.4s, 1890809 effective words/s
[2023-02-07 19:21:46,485][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 24.44% examples, 1556185 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:47,500][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 47.16% examples, 1540763 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:48,512][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 70.25% examples, 1534335 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:49,515][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 95.82% examples, 1536027 words/s, in_qsize 7, out_qsize 1
[2023-02-07 19:21:49,671][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6467889 effective words) took 4.2s, 1542622 effective words/s
[2023-02-07 19:21:50,679][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 24.50% examples, 1556990 words/s, in_qsize 6, out_qsize 0
[2023-02-07 19:21:51,679][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 47.62% examples, 1565982 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:52,680][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 71.45% examples, 1566715 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:53,684][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 96.81% examples, 1562980 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:53,810][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6467889 effective words) took 4.1s, 1563444 effective words/s
[2023-02-07 19:21:54,815][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 23.76% examples, 1519186 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:55,816][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 48.57% examples, 1602288 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:56,817][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 78.35% examples, 1702310 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:57,524][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6467889 effective words) took 3.7s, 1742736 effective words/s
[2023-02-07 19:21:58,526][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 29.75% examples, 1923448 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:21:59,529][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 58.80% examples, 1933914 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:00,530][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 89.62% examples, 1940654 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:00,858][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6467889 effective words) took 3.3s, 1940516 effective words/s
[2023-02-07 19:22:01,871][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 24.90% examples, 1575637 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:22:02,874][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 50.45% examples, 1651154 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:03,878][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 76.42% examples, 1658770 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:04,764][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6467889 effective words) took 3.9s, 1656950 effective words/s
[2023-02-07 19:22:05,772][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 25.24% examples, 1612296 words/s, in_qsize 2, out_qsize 2
[2023-02-07 19:22:06,774][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 48.91% examples, 1608028 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:22:07,774][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 74.42% examples, 1624470 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:08,723][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6467889 effective words) took 4.0s, 1634791 effective words/s
[2023-02-07 19:22:09,727][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 25.02% examples, 1602837 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:10,735][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 48.42% examples, 1592663 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:22:11,740][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 73.10% examples, 1593508 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:12,741][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 99.26% examples, 1600510 words/s, in_qsize 6, out_qsize 0
[2023-02-07 19:22:12,763][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6467889 effective words) took 4.0s, 1601943 effective words/s
[2023-02-07 19:22:13,772][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 24.90% examples, 1588700 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:22:14,779][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 50.94% examples, 1666990 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:15,781][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 82.38% examples, 1779867 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:16,323][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6467889 effective words) took 3.6s, 1817504 effective words/s
[2023-02-07 19:22:17,326][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 25.58% examples, 1645759 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:18,329][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 49.68% examples, 1633310 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:19,332][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 74.52% examples, 1624173 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:20,304][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6467889 effective words) took 4.0s, 1624947 effective words/s
[2023-02-07 19:22:21,310][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 24.84% examples, 1585606 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:22,312][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 48.57% examples, 1601251 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:22:23,315][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 78.02% examples, 1694842 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:22:24,173][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6467889 effective words) took 3.9s, 1672968 effective words/s
[2023-02-07 19:22:25,178][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 24.16% examples, 1545944 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:26,180][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 47.16% examples, 1552811 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:22:27,185][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 72.06% examples, 1575598 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:28,187][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 98.28% examples, 1586639 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:28,248][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6467889 effective words) took 4.1s, 1588584 effective words/s
[2023-02-07 19:22:29,250][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 26.68% examples, 1729621 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:30,256][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 51.49% examples, 1695478 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:31,258][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 76.70% examples, 1668668 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:22:32,135][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6467889 effective words) took 3.9s, 1664953 effective words/s
[2023-02-07 19:22:32,137][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (97018335 effective words) took 59.0s, 1644001 effective words/s', 'datetime': '2023-02-07T19:22:32.136975', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:22:32.138 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:22:36,394][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192104-2725ofdq/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:22:36.393972', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:22:36,395][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:22:36,590][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192104-2725ofdq/files/../tmp/embedding_model.pt
2023-02-07 19:22:36.590 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:22:38.293 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:22:38.832 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:22:44.334 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.456541250238518, 'test_mae': 1.1928132732178833, 'test_r2': -2.753598010051253}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.22
wandb: percentage 0.31696
wandb:   test_mae 1.19281
wandb:   test_mse 2.45654
wandb:    test_r2 -2.7536
wandb: 
wandb: üöÄ View run magic-sweep-43 at: https://wandb.ai/xiaoqiz/mof2vec/runs/2725ofdq
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_192104-2725ofdq/logs
wandb: Agent Starting Run: rkrnzkno with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 557
wandb: 	model.gensim.alpha: 0.022063504955343424
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.5439169914687934
wandb: 	model.gensim.vector_size: 341
wandb: 	model.gensim.window: 9
wandb: 	model.sklearn.learning_rate: 0.6578275152668939
wandb: 	model.sklearn.max_depth: 42
wandb: 	model.sklearn.min_child_weight: 0.09182132888371967
wandb: 	model.sklearn.n_estimators: 724
wandb: 	model.sklearn.num_leaves: 422
wandb: 	model.sklearn.reg_alpha: 0.06045478836711162
wandb: 	model.sklearn.reg_lambda: 0.07119347588645618
wandb: 	model.sklearn.subsample: 0.5228971814090493
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192256-rkrnzkno
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-sweep-44
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/rkrnzkno
2023-02-07 19:23:04.589 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 19:23:04.589 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 557 for sweep.
2023-02-07 19:23:04.590 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.022063504955343424 for sweep.
2023-02-07 19:23:04.590 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:23:04.590 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 19:23:04.590 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5439169914687934 for sweep.
2023-02-07 19:23:04.590 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 341 for sweep.
2023-02-07 19:23:04.591 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 9 for sweep.
2023-02-07 19:23:04.591 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.6578275152668939 for sweep.
2023-02-07 19:23:04.591 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 42 for sweep.
2023-02-07 19:23:04.591 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.09182132888371967 for sweep.
2023-02-07 19:23:04.592 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 724 for sweep.
2023-02-07 19:23:04.592 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 422 for sweep.
2023-02-07 19:23:04.592 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.06045478836711162 for sweep.
2023-02-07 19:23:04.592 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.07119347588645618 for sweep.
2023-02-07 19:23:04.592 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5228971814090493 for sweep.
2023-02-07 19:23:04.593 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:23:04.598 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192256-rkrnzkno/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 557, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 341, 'window': 9, 'min_count': 2, 'dm': 0, 'sample': 0.5439169914687934, 'workers': 4, 'alpha': 0.022063504955343424, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 724, 'max_depth': 42, 'num_leaves': 422, 'reg_alpha': 0.06045478836711162, 'reg_lambda': 0.07119347588645618, 'subsample': 0.5228971814090493, 'min_child_weight': 0.09182132888371967, 'n_jobs': 4, 'learning_rate': 0.6578275152668939}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 25/3257 [00:00<00:12, 248.68it/s]  2%|‚ñè         | 50/3257 [00:00<00:14, 218.02it/s]  2%|‚ñè         | 73/3257 [00:00<00:15, 208.59it/s]  3%|‚ñé         | 99/3257 [00:00<00:14, 225.01it/s]  4%|‚ñç         | 124/3257 [00:00<00:13, 231.11it/s]  5%|‚ñç         | 154/3257 [00:00<00:12, 248.11it/s]  5%|‚ñå         | 179/3257 [00:00<00:12, 242.13it/s]  6%|‚ñã         | 207/3257 [00:00<00:12, 251.76it/s]  7%|‚ñã         | 237/3257 [00:00<00:11, 264.20it/s]  8%|‚ñä         | 264/3257 [00:01<00:11, 259.02it/s]  9%|‚ñâ         | 295/3257 [00:01<00:10, 273.68it/s] 10%|‚ñâ         | 324/3257 [00:01<00:10, 276.30it/s] 11%|‚ñà         | 352/3257 [00:01<00:10, 271.29it/s] 12%|‚ñà‚ñè        | 380/3257 [00:01<00:11, 261.11it/s] 13%|‚ñà‚ñé        | 408/3257 [00:01<00:10, 264.08it/s] 13%|‚ñà‚ñé        | 435/3257 [00:01<00:11, 237.73it/s] 14%|‚ñà‚ñç        | 465/3257 [00:01<00:11, 251.87it/s] 15%|‚ñà‚ñå        | 491/3257 [00:01<00:11, 249.51it/s] 16%|‚ñà‚ñå        | 517/3257 [00:02<00:15, 177.05it/s] 17%|‚ñà‚ñã        | 542/3257 [00:02<00:14, 191.86it/s] 17%|‚ñà‚ñã        | 564/3257 [00:02<00:13, 192.96it/s] 18%|‚ñà‚ñä        | 587/3257 [00:02<00:13, 200.78it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:02<00:12, 218.87it/s] 20%|‚ñà‚ñâ        | 640/3257 [00:02<00:11, 225.85it/s] 20%|‚ñà‚ñà        | 664/3257 [00:02<00:12, 215.17it/s] 21%|‚ñà‚ñà        | 687/3257 [00:02<00:11, 216.73it/s] 22%|‚ñà‚ñà‚ñè       | 713/3257 [00:03<00:11, 227.59it/s] 23%|‚ñà‚ñà‚ñé       | 737/3257 [00:03<00:11, 221.43it/s] 23%|‚ñà‚ñà‚ñé       | 764/3257 [00:03<00:10, 234.37it/s] 24%|‚ñà‚ñà‚ñç       | 788/3257 [00:03<00:10, 232.17it/s] 25%|‚ñà‚ñà‚ñç       | 812/3257 [00:03<00:10, 234.12it/s] 26%|‚ñà‚ñà‚ñå       | 836/3257 [00:03<00:10, 225.63it/s] 26%|‚ñà‚ñà‚ñã       | 859/3257 [00:03<00:10, 221.87it/s] 27%|‚ñà‚ñà‚ñã       | 882/3257 [00:03<00:10, 222.37it/s] 28%|‚ñà‚ñà‚ñä       | 908/3257 [00:03<00:10, 230.72it/s] 29%|‚ñà‚ñà‚ñä       | 932/3257 [00:04<00:10, 227.15it/s] 29%|‚ñà‚ñà‚ñâ       | 957/3257 [00:04<00:09, 231.52it/s] 30%|‚ñà‚ñà‚ñà       | 981/3257 [00:04<00:09, 229.84it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:04<00:09, 227.99it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1031/3257 [00:04<00:09, 229.99it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1055/3257 [00:04<00:09, 228.43it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:04<00:09, 229.09it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1105/3257 [00:04<00:09, 234.21it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1129/3257 [00:04<00:09, 229.15it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1152/3257 [00:04<00:09, 228.02it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1177/3257 [00:05<00:08, 231.85it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1201/3257 [00:05<00:09, 213.58it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1230/3257 [00:05<00:08, 233.41it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1255/3257 [00:05<00:08, 235.15it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1279/3257 [00:05<00:08, 228.93it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1303/3257 [00:05<00:08, 224.75it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:05<00:08, 230.74it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1352/3257 [00:05<00:08, 232.06it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1376/3257 [00:05<00:08, 227.29it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1400/3257 [00:06<00:08, 230.27it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1426/3257 [00:06<00:07, 237.89it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1452/3257 [00:06<00:07, 241.16it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1479/3257 [00:06<00:07, 247.30it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1507/3257 [00:06<00:06, 255.12it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1533/3257 [00:06<00:07, 236.43it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1557/3257 [00:06<00:07, 226.49it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1581/3257 [00:06<00:07, 227.64it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1608/3257 [00:06<00:06, 237.91it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:07<00:07, 231.70it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1658/3257 [00:07<00:07, 223.31it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1681/3257 [00:07<00:07, 219.08it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1704/3257 [00:07<00:07, 220.82it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:07<00:07, 213.75it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1750/3257 [00:07<00:10, 140.57it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1775/3257 [00:07<00:09, 162.48it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1801/3257 [00:07<00:07, 183.95it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1824/3257 [00:08<00:07, 194.68it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1846/3257 [00:08<00:07, 199.88it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1874/3257 [00:08<00:06, 219.94it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1898/3257 [00:08<00:06, 221.50it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1923/3257 [00:08<00:05, 228.54it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1957/3257 [00:08<00:05, 259.92it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1984/3257 [00:08<00:05, 248.51it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2010/3257 [00:08<00:05, 247.73it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2038/3257 [00:08<00:04, 254.48it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2064/3257 [00:09<00:05, 237.18it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2090/3257 [00:09<00:04, 243.23it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2115/3257 [00:09<00:04, 244.67it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2140/3257 [00:09<00:04, 231.61it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2164/3257 [00:09<00:04, 233.30it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2189/3257 [00:09<00:04, 237.91it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2213/3257 [00:09<00:04, 232.82it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2238/3257 [00:09<00:04, 234.73it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2262/3257 [00:09<00:04, 235.09it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2289/3257 [00:10<00:03, 242.76it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2315/3257 [00:10<00:03, 244.95it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2348/3257 [00:10<00:03, 268.66it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2375/3257 [00:10<00:03, 265.76it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2402/3257 [00:10<00:03, 263.33it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2429/3257 [00:10<00:03, 245.13it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2454/3257 [00:10<00:03, 233.43it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2483/3257 [00:10<00:03, 248.06it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2514/3257 [00:10<00:02, 264.35it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2541/3257 [00:10<00:02, 263.09it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2568/3257 [00:11<00:02, 243.98it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2593/3257 [00:11<00:02, 243.78it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2626/3257 [00:11<00:02, 267.21it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2654/3257 [00:11<00:02, 256.47it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2681/3257 [00:11<00:02, 258.56it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2708/3257 [00:11<00:02, 244.16it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2736/3257 [00:11<00:02, 253.34it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2763/3257 [00:11<00:01, 256.21it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2791/3257 [00:11<00:01, 262.64it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2818/3257 [00:12<00:01, 257.32it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:12<00:01, 251.08it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:12<00:01, 279.94it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2910/3257 [00:12<00:01, 265.25it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2938/3257 [00:12<00:01, 267.66it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2966/3257 [00:12<00:01, 267.85it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2993/3257 [00:12<00:01, 262.24it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3024/3257 [00:12<00:00, 273.46it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3060/3257 [00:12<00:00, 296.86it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3091/3257 [00:13<00:00, 298.00it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3126/3257 [00:13<00:00, 312.55it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3158/3257 [00:13<00:00, 180.41it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3184/3257 [00:13<00:00, 195.65it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3213/3257 [00:13<00:00, 213.64it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:13<00:00, 239.63it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 235.34it/s]
2023-02-07 19:23:18.859 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:23:18,860][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d341,n5,mc2,s0.543917,t4>', 'datetime': '2023-02-07T19:23:18.860028', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:23:18,860][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:23:18,860][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:23:19,155][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 19:23:19,155][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:23:19,181][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 11155 unique words (85.41% of original 13061, drops 1906)', 'datetime': '2023-02-07T19:23:19.181614', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:23:19,181][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 3637464 word corpus (99.95% of original 3639370, drops 1906)', 'datetime': '2023-02-07T19:23:19.181982', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:23:19,217][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 19:23:19,218][gensim.models.word2vec][INFO] - sample=0.543917 downsamples 0 most-common words
[2023-02-07 19:23:19,218][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3637464 word corpus (100.0%% of prior 3637464)', 'datetime': '2023-02-07T19:23:19.218527', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:23:19,278][gensim.models.word2vec][INFO] - estimated required memory for 11155 words and 341 dimensions: 41102288 bytes
[2023-02-07 19:23:19,278][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:23:19,296][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 11155 vocabulary and 341 features, using sg=1 hs=0 sample=0.5439169914687934 negative=5 window=9 shrink_windows=True', 'datetime': '2023-02-07T19:23:19.296211', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:23:20,301][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 46.73% examples, 1727541 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:23:21,224][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3640721 effective words) took 1.9s, 1891126 effective words/s
[2023-02-07 19:23:22,226][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 51.86% examples, 1931911 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:23:22,967][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3640721 effective words) took 1.7s, 2090365 effective words/s
[2023-02-07 19:23:23,972][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 69.85% examples, 2600225 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:23:24,367][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3640721 effective words) took 1.4s, 2603263 effective words/s
[2023-02-07 19:23:25,369][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 54.04% examples, 2018096 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:23:26,172][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3640721 effective words) took 1.8s, 2018269 effective words/s
[2023-02-07 19:23:27,175][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 72.86% examples, 2691037 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:23:27,541][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3640721 effective words) took 1.4s, 2661930 effective words/s
[2023-02-07 19:23:28,543][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 54.93% examples, 2047207 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:23:29,154][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3640721 effective words) took 1.6s, 2259704 effective words/s
[2023-02-07 19:23:30,160][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 54.01% examples, 2009411 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:23:30,989][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3640721 effective words) took 1.8s, 1985143 effective words/s
[2023-02-07 19:23:31,996][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 52.13% examples, 1934434 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:23:32,857][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3640721 effective words) took 1.9s, 1952007 effective words/s
[2023-02-07 19:23:33,863][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 52.78% examples, 1954324 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:23:34,520][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3640721 effective words) took 1.7s, 2191154 effective words/s
[2023-02-07 19:23:35,523][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 53.21% examples, 1987690 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:23:36,356][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3640721 effective words) took 1.8s, 1985521 effective words/s
[2023-02-07 19:23:37,361][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 53.02% examples, 1970672 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:23:38,199][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3640721 effective words) took 1.8s, 1977921 effective words/s
[2023-02-07 19:23:39,208][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 53.73% examples, 1994932 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:23:40,029][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3640721 effective words) took 1.8s, 1992359 effective words/s
[2023-02-07 19:23:41,032][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 54.59% examples, 2035497 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:23:41,793][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3640721 effective words) took 1.8s, 2065354 effective words/s
[2023-02-07 19:23:42,798][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 54.28% examples, 2023118 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:23:43,581][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3640721 effective words) took 1.8s, 2039282 effective words/s
[2023-02-07 19:23:44,589][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 55.14% examples, 2045251 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:23:45,369][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3640721 effective words) took 1.8s, 2038418 effective words/s
[2023-02-07 19:23:45,369][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54610815 effective words) took 26.1s, 2094601 effective words/s', 'datetime': '2023-02-07T19:23:45.369664', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:23:45.369 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:23:47,189][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192256-rkrnzkno/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:23:47.189574', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:23:47,190][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:23:47,250][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192256-rkrnzkno/files/../tmp/embedding_model.pt
2023-02-07 19:23:47.250 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:23:49.083 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:23:49.741 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:23:52.418 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.10414083472258, 'test_mae': 1.079874401576417, 'test_r2': -1.8760859334055415}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.94
wandb: percentage 0.14593
wandb:   test_mae 1.07987
wandb:   test_mse 2.10414
wandb:    test_r2 -1.87609
wandb: 
wandb: üöÄ View run swept-sweep-44 at: https://wandb.ai/xiaoqiz/mof2vec/runs/rkrnzkno
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_192256-rkrnzkno/logs
wandb: Agent Starting Run: j0h7l9ay with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 869
wandb: 	model.gensim.alpha: 0.020563100794761095
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.7188123095670336
wandb: 	model.gensim.vector_size: 269
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.8274392676228589
wandb: 	model.sklearn.max_depth: 21
wandb: 	model.sklearn.min_child_weight: 0.0891287296050339
wandb: 	model.sklearn.n_estimators: 498
wandb: 	model.sklearn.num_leaves: 415
wandb: 	model.sklearn.reg_alpha: 0.18648477888181905
wandb: 	model.sklearn.reg_lambda: 0.9657780990636464
wandb: 	model.sklearn.subsample: 0.7383307300142359
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192402-j0h7l9ay
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-45
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/j0h7l9ay
2023-02-07 19:24:09.835 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 19:24:09.836 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 869 for sweep.
2023-02-07 19:24:09.836 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.020563100794761095 for sweep.
2023-02-07 19:24:09.836 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:24:09.836 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 19:24:09.836 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7188123095670336 for sweep.
2023-02-07 19:24:09.837 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 269 for sweep.
2023-02-07 19:24:09.837 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 19:24:09.837 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.8274392676228589 for sweep.
2023-02-07 19:24:09.837 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 21 for sweep.
2023-02-07 19:24:09.838 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0891287296050339 for sweep.
2023-02-07 19:24:09.838 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 498 for sweep.
2023-02-07 19:24:09.838 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 415 for sweep.
2023-02-07 19:24:09.838 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.18648477888181905 for sweep.
2023-02-07 19:24:09.838 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.9657780990636464 for sweep.
2023-02-07 19:24:09.839 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.7383307300142359 for sweep.
2023-02-07 19:24:09.839 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:24:09.847 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192402-j0h7l9ay/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 869, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 269, 'window': 3, 'min_count': 4, 'dm': 0, 'sample': 0.7188123095670336, 'workers': 4, 'alpha': 0.020563100794761095, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 498, 'max_depth': 21, 'num_leaves': 415, 'reg_alpha': 0.18648477888181905, 'reg_lambda': 0.9657780990636464, 'subsample': 0.7383307300142359, 'min_child_weight': 0.0891287296050339, 'n_jobs': 4, 'learning_rate': 0.8274392676228589}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:18, 175.01it/s]  1%|          | 39/3257 [00:00<00:16, 192.43it/s]  2%|‚ñè         | 59/3257 [00:00<00:17, 186.00it/s]  3%|‚ñé         | 82/3257 [00:00<00:15, 199.20it/s]  3%|‚ñé         | 102/3257 [00:00<00:15, 197.46it/s]  4%|‚ñé         | 122/3257 [00:00<00:16, 187.89it/s]  4%|‚ñç         | 145/3257 [00:00<00:15, 199.64it/s]  5%|‚ñå         | 166/3257 [00:00<00:16, 190.41it/s]  6%|‚ñå         | 187/3257 [00:00<00:15, 194.64it/s]  6%|‚ñã         | 208/3257 [00:01<00:15, 198.84it/s]  7%|‚ñã         | 233/3257 [00:01<00:14, 212.91it/s]  8%|‚ñä         | 255/3257 [00:01<00:14, 206.09it/s]  8%|‚ñä         | 276/3257 [00:01<00:14, 206.60it/s]  9%|‚ñâ         | 299/3257 [00:01<00:14, 210.40it/s] 10%|‚ñâ         | 321/3257 [00:01<00:13, 210.97it/s] 11%|‚ñà         | 343/3257 [00:01<00:14, 204.08it/s] 11%|‚ñà         | 365/3257 [00:01<00:13, 207.19it/s] 12%|‚ñà‚ñè        | 386/3257 [00:01<00:14, 192.92it/s] 13%|‚ñà‚ñé        | 410/3257 [00:02<00:13, 205.35it/s] 13%|‚ñà‚ñé        | 431/3257 [00:02<00:15, 178.90it/s] 14%|‚ñà‚ñç        | 450/3257 [00:02<00:15, 178.03it/s] 15%|‚ñà‚ñç        | 473/3257 [00:02<00:14, 191.20it/s] 15%|‚ñà‚ñå        | 493/3257 [00:02<00:14, 185.91it/s] 16%|‚ñà‚ñå        | 515/3257 [00:02<00:14, 194.87it/s] 16%|‚ñà‚ñã        | 535/3257 [00:02<00:13, 194.91it/s] 17%|‚ñà‚ñã        | 557/3257 [00:02<00:13, 199.35it/s] 18%|‚ñà‚ñä        | 578/3257 [00:02<00:15, 176.47it/s] 18%|‚ñà‚ñä        | 602/3257 [00:03<00:13, 191.34it/s] 19%|‚ñà‚ñâ        | 622/3257 [00:03<00:13, 189.35it/s] 20%|‚ñà‚ñâ        | 644/3257 [00:03<00:13, 197.52it/s] 20%|‚ñà‚ñà        | 665/3257 [00:03<00:13, 186.52it/s] 21%|‚ñà‚ñà        | 685/3257 [00:03<00:13, 184.19it/s] 22%|‚ñà‚ñà‚ñè       | 706/3257 [00:03<00:13, 190.18it/s] 22%|‚ñà‚ñà‚ñè       | 726/3257 [00:03<00:13, 184.01it/s] 23%|‚ñà‚ñà‚ñé       | 745/3257 [00:03<00:13, 183.50it/s] 24%|‚ñà‚ñà‚ñé       | 768/3257 [00:03<00:12, 195.20it/s] 24%|‚ñà‚ñà‚ñç       | 788/3257 [00:04<00:13, 185.69it/s] 25%|‚ñà‚ñà‚ñç       | 807/3257 [00:04<00:13, 186.58it/s] 25%|‚ñà‚ñà‚ñå       | 826/3257 [00:04<00:13, 185.45it/s] 26%|‚ñà‚ñà‚ñå       | 845/3257 [00:04<00:13, 176.43it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:04<00:13, 179.34it/s] 27%|‚ñà‚ñà‚ñã       | 884/3257 [00:04<00:13, 176.87it/s] 28%|‚ñà‚ñà‚ñä       | 908/3257 [00:04<00:12, 191.58it/s] 29%|‚ñà‚ñà‚ñä       | 930/3257 [00:04<00:11, 198.98it/s] 29%|‚ñà‚ñà‚ñâ       | 951/3257 [00:04<00:11, 198.18it/s] 30%|‚ñà‚ñà‚ñâ       | 972/3257 [00:05<00:11, 199.03it/s] 30%|‚ñà‚ñà‚ñà       | 992/3257 [00:05<00:11, 196.37it/s] 31%|‚ñà‚ñà‚ñà       | 1012/3257 [00:05<00:11, 189.54it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1032/3257 [00:05<00:11, 188.68it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1051/3257 [00:05<00:12, 182.65it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1075/3257 [00:05<00:11, 197.39it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1095/3257 [00:05<00:11, 188.93it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:05<00:10, 195.28it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1137/3257 [00:05<00:10, 193.84it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1157/3257 [00:06<00:10, 194.93it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1177/3257 [00:06<00:10, 193.57it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1197/3257 [00:06<00:11, 181.08it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1216/3257 [00:06<00:17, 115.45it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1241/3257 [00:06<00:14, 140.82it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1259/3257 [00:06<00:13, 149.27it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1277/3257 [00:06<00:12, 153.37it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1295/3257 [00:06<00:12, 156.18it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1317/3257 [00:07<00:11, 172.61it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:07<00:10, 183.92it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1359/3257 [00:07<00:10, 183.80it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1379/3257 [00:07<00:10, 185.18it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1398/3257 [00:07<00:10, 181.52it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:07<00:09, 196.96it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1445/3257 [00:07<00:08, 203.14it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1471/3257 [00:07<00:08, 216.51it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1493/3257 [00:07<00:08, 217.14it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1517/3257 [00:08<00:07, 219.87it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1540/3257 [00:08<00:08, 199.66it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1561/3257 [00:08<00:08, 192.03it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1581/3257 [00:08<00:08, 191.05it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1605/3257 [00:08<00:08, 201.37it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1628/3257 [00:08<00:07, 208.72it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1650/3257 [00:08<00:08, 192.75it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1670/3257 [00:08<00:08, 192.47it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1690/3257 [00:08<00:08, 188.67it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1713/3257 [00:09<00:07, 199.42it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1734/3257 [00:09<00:08, 180.40it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1757/3257 [00:09<00:07, 192.92it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1779/3257 [00:09<00:07, 197.74it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1801/3257 [00:09<00:07, 202.91it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1822/3257 [00:09<00:07, 201.78it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1843/3257 [00:09<00:07, 196.18it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1866/3257 [00:09<00:06, 205.41it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1887/3257 [00:09<00:06, 198.19it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1910/3257 [00:10<00:06, 203.99it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1931/3257 [00:10<00:06, 200.93it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1961/3257 [00:10<00:05, 226.00it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1984/3257 [00:10<00:06, 211.16it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2007/3257 [00:10<00:05, 214.00it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2032/3257 [00:10<00:05, 222.91it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2055/3257 [00:10<00:06, 198.75it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2076/3257 [00:10<00:05, 200.90it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2097/3257 [00:10<00:05, 194.84it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:11<00:05, 194.23it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2140/3257 [00:11<00:05, 195.35it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2160/3257 [00:11<00:05, 193.00it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:11<00:05, 195.93it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2204/3257 [00:11<00:05, 204.77it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2225/3257 [00:11<00:05, 197.54it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2245/3257 [00:11<00:05, 193.27it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2266/3257 [00:11<00:05, 197.72it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2288/3257 [00:11<00:04, 200.85it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2309/3257 [00:12<00:04, 198.45it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2337/3257 [00:12<00:04, 218.56it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2364/3257 [00:12<00:03, 231.33it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2388/3257 [00:12<00:03, 232.87it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2412/3257 [00:12<00:03, 220.31it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2435/3257 [00:12<00:03, 210.18it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2457/3257 [00:12<00:03, 209.84it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2481/3257 [00:12<00:03, 216.89it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2507/3257 [00:12<00:03, 227.08it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2531/3257 [00:12<00:03, 230.48it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2555/3257 [00:13<00:03, 217.36it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2577/3257 [00:13<00:03, 208.08it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2598/3257 [00:13<00:03, 206.15it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:13<00:02, 228.99it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2652/3257 [00:13<00:02, 217.36it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2674/3257 [00:13<00:02, 214.49it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2696/3257 [00:14<00:04, 117.85it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2713/3257 [00:14<00:04, 126.43it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2736/3257 [00:14<00:03, 147.34it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2759/3257 [00:14<00:03, 165.44it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2779/3257 [00:14<00:02, 170.44it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2805/3257 [00:14<00:02, 192.15it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2827/3257 [00:14<00:02, 185.25it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2849/3257 [00:14<00:02, 192.97it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2879/3257 [00:14<00:01, 220.20it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2903/3257 [00:15<00:01, 200.68it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:15<00:01, 204.47it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:15<00:01, 195.83it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2972/3257 [00:15<00:01, 201.76it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2993/3257 [00:15<00:01, 194.67it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3016/3257 [00:15<00:01, 202.44it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3040/3257 [00:15<00:01, 211.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3066/3257 [00:15<00:00, 223.14it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3089/3257 [00:15<00:00, 217.73it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3115/3257 [00:16<00:00, 229.27it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3139/3257 [00:16<00:00, 221.70it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3162/3257 [00:16<00:00, 209.16it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3184/3257 [00:16<00:00, 198.87it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:16<00:00, 204.75it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3228/3257 [00:16<00:00, 203.05it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:16<00:00, 209.79it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 194.47it/s]
2023-02-07 19:24:27.292 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:24:27,294][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d269,n5,mc4,s0.718812,t4>', 'datetime': '2023-02-07T19:24:27.293939', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:24:27,294][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:24:27,294][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:24:27,798][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 19:24:27,798][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:24:27,872][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 29175 unique words (68.32% of original 42701, drops 13526)', 'datetime': '2023-02-07T19:24:27.872767', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:24:27,873][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 5800681 word corpus (99.62% of original 5822992, drops 22311)', 'datetime': '2023-02-07T19:24:27.873805', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:24:27,967][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 19:24:27,969][gensim.models.word2vec][INFO] - sample=0.718812 downsamples 0 most-common words
[2023-02-07 19:24:27,969][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5800681 word corpus (100.0%% of prior 5800681)', 'datetime': '2023-02-07T19:24:27.969320', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:24:28,127][gensim.models.word2vec][INFO] - estimated required memory for 29175 words and 269 dimensions: 81528032 bytes
[2023-02-07 19:24:28,127][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:24:28,164][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 29175 vocabulary and 269 features, using sg=1 hs=0 sample=0.7188123095670336 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T19:24:28.164131', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:24:29,171][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 31.53% examples, 1834239 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:30,172][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 65.83% examples, 1937011 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:24:30,894][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5774537 effective words) took 2.7s, 2117446 effective words/s
[2023-02-07 19:24:31,900][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 37.09% examples, 2187933 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:32,905][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 74.03% examples, 2158559 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:33,589][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5774537 effective words) took 2.7s, 2143914 effective words/s
[2023-02-07 19:24:34,593][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 44.43% examples, 2625068 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:24:35,592][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 79.95% examples, 2332839 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:36,118][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5774537 effective words) took 2.5s, 2284966 effective words/s
[2023-02-07 19:24:37,120][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 35.22% examples, 2067279 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:38,125][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 71.23% examples, 2094738 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:24:38,838][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5774537 effective words) took 2.7s, 2125033 effective words/s
[2023-02-07 19:24:39,848][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 37.55% examples, 2196796 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:40,851][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 76.27% examples, 2219529 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:41,438][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5774537 effective words) took 2.6s, 2221418 effective words/s
[2023-02-07 19:24:42,453][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 38.16% examples, 2226440 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:43,453][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 77.56% examples, 2249159 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:44,000][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5774537 effective words) took 2.6s, 2256676 effective words/s
[2023-02-07 19:24:45,003][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 38.44% examples, 2267218 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:24:46,005][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 78.26% examples, 2276329 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:46,538][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5774537 effective words) took 2.5s, 2276471 effective words/s
[2023-02-07 19:24:47,541][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 48.97% examples, 2882074 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:48,544][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 89.71% examples, 2597717 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:48,805][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5774537 effective words) took 2.3s, 2548632 effective words/s
[2023-02-07 19:24:49,815][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 36.84% examples, 2163142 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:50,820][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 73.44% examples, 2136876 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:24:51,490][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5774537 effective words) took 2.7s, 2153578 effective words/s
[2023-02-07 19:24:52,493][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 48.97% examples, 2884188 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:53,494][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 99.11% examples, 2864693 words/s, in_qsize 6, out_qsize 0
[2023-02-07 19:24:53,506][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5774537 effective words) took 2.0s, 2867252 effective words/s
[2023-02-07 19:24:54,513][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 49.89% examples, 2920725 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:55,506][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5774537 effective words) took 2.0s, 2890737 effective words/s
[2023-02-07 19:24:56,509][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 47.93% examples, 2826736 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:57,510][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 99.97% examples, 2878618 words/s, in_qsize 1, out_qsize 1
[2023-02-07 19:24:57,512][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5774537 effective words) took 2.0s, 2881127 effective words/s
[2023-02-07 19:24:58,513][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 35.92% examples, 2104543 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:24:59,517][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 72.98% examples, 2135198 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:25:00,056][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5774537 effective words) took 2.5s, 2270367 effective words/s
[2023-02-07 19:25:01,060][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 39.09% examples, 2305565 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:25:02,078][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 77.31% examples, 2234188 words/s, in_qsize 0, out_qsize 6
[2023-02-07 19:25:02,613][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5774537 effective words) took 2.6s, 2260637 effective words/s
[2023-02-07 19:25:03,623][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 39.24% examples, 2312736 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:25:04,628][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 79.28% examples, 2305360 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:25:05,136][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5774537 effective words) took 2.5s, 2293162 effective words/s
[2023-02-07 19:25:05,136][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86618055 effective words) took 37.0s, 2342843 effective words/s', 'datetime': '2023-02-07T19:25:05.136835', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:25:05.137 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:25:08,511][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192402-j0h7l9ay/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:25:08.511679', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:25:08,512][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:25:08,655][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192402-j0h7l9ay/files/../tmp/embedding_model.pt
2023-02-07 19:25:08.655 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:25:10.292 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:25:10.854 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:25:31.175 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0101085109195345, 'test_mae': 1.0434072825077785, 'test_r2': -1.8914031112998155}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.89
wandb: percentage 0.31676
wandb:   test_mae 1.04341
wandb:   test_mse 2.01011
wandb:    test_r2 -1.8914
wandb: 
wandb: üöÄ View run wild-sweep-45 at: https://wandb.ai/xiaoqiz/mof2vec/runs/j0h7l9ay
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_192402-j0h7l9ay/logs
wandb: Agent Starting Run: 08tkvco2 with config:
wandb: 	data.data.wl_step: 5
wandb: 	data.nn.batch_size: 998
wandb: 	model.gensim.alpha: 0.01214629508403654
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.6571849083924348
wandb: 	model.gensim.vector_size: 254
wandb: 	model.gensim.window: 6
wandb: 	model.sklearn.learning_rate: 0.1932143048924612
wandb: 	model.sklearn.max_depth: 25
wandb: 	model.sklearn.min_child_weight: 0.08662511787698644
wandb: 	model.sklearn.n_estimators: 550
wandb: 	model.sklearn.num_leaves: 213
wandb: 	model.sklearn.reg_alpha: 0.05586194635907099
wandb: 	model.sklearn.reg_lambda: 0.1331114851509733
wandb: 	model.sklearn.subsample: 0.6345549670116483
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192543-08tkvco2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-46
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/08tkvco2
2023-02-07 19:25:52.283 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 5 for sweep.
2023-02-07 19:25:52.283 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 998 for sweep.
2023-02-07 19:25:52.283 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.01214629508403654 for sweep.
2023-02-07 19:25:52.284 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:25:52.284 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 19:25:52.285 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6571849083924348 for sweep.
2023-02-07 19:25:52.285 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 254 for sweep.
2023-02-07 19:25:52.285 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 6 for sweep.
2023-02-07 19:25:52.285 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.1932143048924612 for sweep.
2023-02-07 19:25:52.286 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 25 for sweep.
2023-02-07 19:25:52.286 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.08662511787698644 for sweep.
2023-02-07 19:25:52.286 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 550 for sweep.
2023-02-07 19:25:52.286 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 213 for sweep.
2023-02-07 19:25:52.286 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.05586194635907099 for sweep.
2023-02-07 19:25:52.287 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.1331114851509733 for sweep.
2023-02-07 19:25:52.287 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6345549670116483 for sweep.
2023-02-07 19:25:52.287 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:25:52.294 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 5}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192543-08tkvco2/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 998, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 254, 'window': 6, 'min_count': 3, 'dm': 0, 'sample': 0.6571849083924348, 'workers': 4, 'alpha': 0.01214629508403654, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 550, 'max_depth': 25, 'num_leaves': 213, 'reg_alpha': 0.05586194635907099, 'reg_lambda': 0.1331114851509733, 'subsample': 0.6345549670116483, 'min_child_weight': 0.08662511787698644, 'n_jobs': 4, 'learning_rate': 0.1932143048924612}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 21/3257 [00:00<00:15, 207.89it/s]  1%|‚ñè         | 42/3257 [00:00<00:15, 204.28it/s]  2%|‚ñè         | 66/3257 [00:00<00:14, 219.70it/s]  3%|‚ñé         | 90/3257 [00:00<00:14, 222.82it/s]  3%|‚ñé         | 113/3257 [00:00<00:14, 220.35it/s]  4%|‚ñç         | 137/3257 [00:00<00:13, 225.93it/s]  5%|‚ñç         | 161/3257 [00:00<00:13, 230.34it/s]  6%|‚ñå         | 185/3257 [00:00<00:13, 225.29it/s]  6%|‚ñã         | 208/3257 [00:00<00:13, 224.91it/s]  7%|‚ñã         | 236/3257 [00:01<00:12, 239.54it/s]  8%|‚ñä         | 260/3257 [00:01<00:12, 235.66it/s]  9%|‚ñâ         | 291/3257 [00:01<00:11, 256.39it/s] 10%|‚ñâ         | 317/3257 [00:01<00:11, 253.93it/s] 11%|‚ñà         | 343/3257 [00:01<00:11, 255.60it/s] 11%|‚ñà‚ñè        | 371/3257 [00:01<00:11, 260.72it/s] 12%|‚ñà‚ñè        | 398/3257 [00:01<00:11, 244.19it/s] 13%|‚ñà‚ñé        | 425/3257 [00:01<00:11, 239.50it/s] 14%|‚ñà‚ñç        | 450/3257 [00:01<00:12, 230.42it/s] 15%|‚ñà‚ñç        | 478/3257 [00:02<00:11, 243.20it/s] 16%|‚ñà‚ñå        | 505/3257 [00:02<00:11, 247.94it/s] 16%|‚ñà‚ñã        | 531/3257 [00:02<00:11, 247.36it/s] 17%|‚ñà‚ñã        | 558/3257 [00:02<00:11, 243.81it/s] 18%|‚ñà‚ñä        | 583/3257 [00:02<00:11, 232.81it/s] 19%|‚ñà‚ñä        | 610/3257 [00:02<00:10, 240.77it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:02<00:10, 243.82it/s] 20%|‚ñà‚ñà        | 662/3257 [00:02<00:11, 225.39it/s] 21%|‚ñà‚ñà        | 687/3257 [00:02<00:11, 231.00it/s] 22%|‚ñà‚ñà‚ñè       | 716/3257 [00:03<00:10, 246.06it/s] 23%|‚ñà‚ñà‚ñé       | 741/3257 [00:03<00:16, 155.08it/s] 24%|‚ñà‚ñà‚ñé       | 770/3257 [00:03<00:13, 180.01it/s] 24%|‚ñà‚ñà‚ñç       | 793/3257 [00:03<00:13, 189.31it/s] 25%|‚ñà‚ñà‚ñå       | 816/3257 [00:03<00:12, 198.68it/s] 26%|‚ñà‚ñà‚ñå       | 839/3257 [00:03<00:12, 196.75it/s] 26%|‚ñà‚ñà‚ñã       | 861/3257 [00:03<00:11, 200.97it/s] 27%|‚ñà‚ñà‚ñã       | 884/3257 [00:03<00:11, 203.11it/s] 28%|‚ñà‚ñà‚ñä       | 912/3257 [00:04<00:10, 221.48it/s] 29%|‚ñà‚ñà‚ñä       | 935/3257 [00:04<00:10, 221.58it/s] 30%|‚ñà‚ñà‚ñâ       | 961/3257 [00:04<00:09, 229.88it/s] 30%|‚ñà‚ñà‚ñà       | 985/3257 [00:04<00:09, 231.72it/s] 31%|‚ñà‚ñà‚ñà       | 1009/3257 [00:04<00:10, 222.04it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1032/3257 [00:04<00:10, 221.68it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1055/3257 [00:04<00:10, 214.86it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:04<00:10, 215.07it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1103/3257 [00:04<00:09, 216.45it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1125/3257 [00:05<00:09, 214.40it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:05<00:10, 209.60it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:05<00:09, 217.33it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1193/3257 [00:05<00:10, 201.64it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1214/3257 [00:05<00:10, 197.43it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1241/3257 [00:05<00:09, 216.10it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1263/3257 [00:05<00:09, 214.63it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1285/3257 [00:05<00:09, 200.66it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1308/3257 [00:05<00:09, 205.84it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1331/3257 [00:06<00:09, 211.62it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1354/3257 [00:06<00:08, 213.31it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1376/3257 [00:06<00:08, 213.46it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1398/3257 [00:06<00:08, 212.48it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:06<00:08, 222.35it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1448/3257 [00:06<00:07, 227.89it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1475/3257 [00:06<00:07, 237.98it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1499/3257 [00:06<00:07, 235.74it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1523/3257 [00:06<00:07, 216.80it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1545/3257 [00:07<00:08, 205.03it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1570/3257 [00:07<00:07, 214.64it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1592/3257 [00:07<00:07, 214.25it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:07<00:07, 216.78it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1637/3257 [00:07<00:07, 210.35it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:07<00:07, 209.62it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1681/3257 [00:07<00:07, 204.43it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1704/3257 [00:07<00:07, 209.72it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:07<00:07, 204.45it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1749/3257 [00:07<00:07, 199.97it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1772/3257 [00:08<00:07, 207.26it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1796/3257 [00:08<00:06, 215.89it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1819/3257 [00:08<00:06, 219.37it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1842/3257 [00:08<00:06, 215.18it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1868/3257 [00:08<00:06, 225.34it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1891/3257 [00:08<00:06, 221.02it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:08<00:05, 225.78it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1939/3257 [00:08<00:08, 159.23it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1970/3257 [00:09<00:06, 192.46it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1993/3257 [00:09<00:06, 200.46it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2018/3257 [00:09<00:05, 212.89it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2042/3257 [00:09<00:05, 218.36it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2066/3257 [00:09<00:05, 213.69it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2093/3257 [00:09<00:05, 226.90it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2117/3257 [00:09<00:04, 229.07it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2141/3257 [00:09<00:05, 219.84it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2165/3257 [00:09<00:04, 224.93it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2188/3257 [00:10<00:04, 225.07it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2211/3257 [00:10<00:04, 221.68it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2237/3257 [00:10<00:04, 229.73it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2261/3257 [00:10<00:04, 225.56it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2286/3257 [00:10<00:04, 230.07it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2310/3257 [00:10<00:04, 231.33it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2340/3257 [00:10<00:03, 249.22it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2365/3257 [00:10<00:03, 243.69it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2390/3257 [00:10<00:03, 243.93it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2415/3257 [00:10<00:03, 237.96it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2439/3257 [00:11<00:03, 234.48it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2465/3257 [00:11<00:03, 239.85it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2492/3257 [00:11<00:03, 247.88it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2519/3257 [00:11<00:02, 254.01it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2545/3257 [00:11<00:02, 253.75it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2571/3257 [00:11<00:02, 232.37it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2595/3257 [00:11<00:02, 232.66it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:11<00:02, 258.64it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2655/3257 [00:11<00:02, 249.48it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2681/3257 [00:12<00:02, 244.00it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2706/3257 [00:12<00:02, 224.57it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2729/3257 [00:12<00:02, 224.16it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2757/3257 [00:12<00:02, 236.94it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2781/3257 [00:12<00:02, 235.97it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2808/3257 [00:12<00:01, 245.32it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2833/3257 [00:12<00:01, 234.77it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2862/3257 [00:12<00:01, 247.17it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2890/3257 [00:12<00:01, 254.29it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2916/3257 [00:13<00:01, 246.38it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2943/3257 [00:13<00:01, 239.02it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2968/3257 [00:13<00:01, 240.24it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2993/3257 [00:13<00:01, 229.95it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3018/3257 [00:13<00:01, 234.51it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3048/3257 [00:13<00:00, 250.36it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:13<00:00, 263.82it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3107/3257 [00:13<00:00, 266.64it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3134/3257 [00:13<00:00, 264.60it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3161/3257 [00:14<00:00, 251.18it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3187/3257 [00:14<00:00, 242.54it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3212/3257 [00:14<00:00, 244.58it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3239/3257 [00:14<00:00, 250.96it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:14<00:00, 225.97it/s]
2023-02-07 19:26:07.231 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:26:07,232][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d254,n5,mc3,s0.657185,t4>', 'datetime': '2023-02-07T19:26:07.232035', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:26:07,232][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:26:07,232][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:26:07,600][gensim.models.doc2vec][INFO] - collected 21699 word types and 3257 unique tags from a corpus of 3257 examples and 4367244 words
[2023-02-07 19:26:07,600][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:26:07,638][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 15507 unique words (71.46% of original 21699, drops 6192)', 'datetime': '2023-02-07T19:26:07.638875', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:26:07,639][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 4358064 word corpus (99.79% of original 4367244, drops 9180)', 'datetime': '2023-02-07T19:26:07.639257', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:26:07,689][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 21699 items
[2023-02-07 19:26:07,690][gensim.models.word2vec][INFO] - sample=0.657185 downsamples 0 most-common words
[2023-02-07 19:26:07,690][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4358064 word corpus (100.0%% of prior 4358064)', 'datetime': '2023-02-07T19:26:07.690180', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:26:07,773][gensim.models.word2vec][INFO] - estimated required memory for 15507 words and 254 dimensions: 43224236 bytes
[2023-02-07 19:26:07,773][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:26:07,790][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 15507 vocabulary and 254 features, using sg=1 hs=0 sample=0.6571849083924348 negative=5 window=6 shrink_windows=True', 'datetime': '2023-02-07T19:26:07.790441', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:26:08,795][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 38.62% examples, 1725352 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:09,796][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 81.82% examples, 1797094 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:10,209][gensim.models.word2vec][INFO] - EPOCH 0: training on 4367244 raw words (4359560 effective words) took 2.4s, 1804949 effective words/s
[2023-02-07 19:26:11,213][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 46.82% examples, 2078209 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:12,214][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 94.69% examples, 2067012 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:12,324][gensim.models.word2vec][INFO] - EPOCH 1: training on 4367244 raw words (4359560 effective words) took 2.1s, 2063782 effective words/s
[2023-02-07 19:26:13,339][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 45.81% examples, 2022460 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:14,339][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 93.00% examples, 2033857 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:14,471][gensim.models.word2vec][INFO] - EPOCH 2: training on 4367244 raw words (4359560 effective words) took 2.1s, 2039102 effective words/s
[2023-02-07 19:26:15,476][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 59.35% examples, 2635804 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:16,112][gensim.models.word2vec][INFO] - EPOCH 3: training on 4367244 raw words (4359560 effective words) took 1.6s, 2658645 effective words/s
[2023-02-07 19:26:17,119][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 60.82% examples, 2682637 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:17,726][gensim.models.word2vec][INFO] - EPOCH 4: training on 4367244 raw words (4359560 effective words) took 1.6s, 2702937 effective words/s
[2023-02-07 19:26:18,730][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 47.68% examples, 2121688 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:19,550][gensim.models.word2vec][INFO] - EPOCH 5: training on 4367244 raw words (4359560 effective words) took 1.8s, 2392062 effective words/s
[2023-02-07 19:26:20,558][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 62.57% examples, 2755436 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:21,135][gensim.models.word2vec][INFO] - EPOCH 6: training on 4367244 raw words (4359560 effective words) took 1.6s, 2756566 effective words/s
[2023-02-07 19:26:22,140][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 63.19% examples, 2792058 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:22,703][gensim.models.word2vec][INFO] - EPOCH 7: training on 4367244 raw words (4359560 effective words) took 1.6s, 2782025 effective words/s
[2023-02-07 19:26:23,706][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 47.93% examples, 2133044 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:24,709][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 97.08% examples, 2113350 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:24,766][gensim.models.word2vec][INFO] - EPOCH 8: training on 4367244 raw words (4359560 effective words) took 2.1s, 2115267 effective words/s
[2023-02-07 19:26:25,769][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 46.95% examples, 2083706 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:26,772][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 94.69% examples, 2064284 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:26,876][gensim.models.word2vec][INFO] - EPOCH 9: training on 4367244 raw words (4359560 effective words) took 2.1s, 2066669 effective words/s
[2023-02-07 19:26:27,878][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 46.09% examples, 2042835 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:28,882][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 92.82% examples, 2032436 words/s, in_qsize 8, out_qsize 3
[2023-02-07 19:26:29,005][gensim.models.word2vec][INFO] - EPOCH 10: training on 4367244 raw words (4359560 effective words) took 2.1s, 2049984 effective words/s
[2023-02-07 19:26:30,010][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 46.64% examples, 2067966 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:31,010][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 93.28% examples, 2043339 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:31,145][gensim.models.word2vec][INFO] - EPOCH 11: training on 4367244 raw words (4359560 effective words) took 2.1s, 2040922 effective words/s
[2023-02-07 19:26:32,149][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 44.03% examples, 1965744 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:33,150][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 91.56% examples, 2009865 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:33,325][gensim.models.word2vec][INFO] - EPOCH 12: training on 4367244 raw words (4359560 effective words) took 2.2s, 2001252 effective words/s
[2023-02-07 19:26:34,332][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 45.81% examples, 2023565 words/s, in_qsize 4, out_qsize 5
[2023-02-07 19:26:35,334][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 93.92% examples, 2047873 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:26:35,444][gensim.models.word2vec][INFO] - EPOCH 13: training on 4367244 raw words (4359560 effective words) took 2.1s, 2058980 effective words/s
[2023-02-07 19:26:36,448][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 60.82% examples, 2692258 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:26:37,056][gensim.models.word2vec][INFO] - EPOCH 14: training on 4367244 raw words (4359560 effective words) took 1.6s, 2707474 effective words/s
[2023-02-07 19:26:37,057][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 65508660 raw words (65393400 effective words) took 29.3s, 2234426 effective words/s', 'datetime': '2023-02-07T19:26:37.057156', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:26:37.057 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:26:39,467][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192543-08tkvco2/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:26:39.467042', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:26:39,468][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:26:39,532][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192543-08tkvco2/files/../tmp/embedding_model.pt
2023-02-07 19:26:39.533 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:26:41.465 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:26:42.036 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:26:43.983 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2359709158930787, 'test_mae': 1.0967917417499602, 'test_r2': -1.5425351568314887}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.78
wandb: percentage 0.28536
wandb:   test_mae 1.09679
wandb:   test_mse 2.23597
wandb:    test_r2 -1.54254
wandb: 
wandb: üöÄ View run ethereal-sweep-46 at: https://wandb.ai/xiaoqiz/mof2vec/runs/08tkvco2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_192543-08tkvco2/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 01xzcrtq with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 861
wandb: 	model.gensim.alpha: 0.010386357358672651
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.5924367427400302
wandb: 	model.gensim.vector_size: 193
wandb: 	model.gensim.window: 5
wandb: 	model.sklearn.learning_rate: 0.21110786728087583
wandb: 	model.sklearn.max_depth: 22
wandb: 	model.sklearn.min_child_weight: 0.0280135061524851
wandb: 	model.sklearn.n_estimators: 2077
wandb: 	model.sklearn.num_leaves: 258
wandb: 	model.sklearn.reg_alpha: 0.041088256216128945
wandb: 	model.sklearn.reg_lambda: 0.08563361374087283
wandb: 	model.sklearn.subsample: 0.463511336307024
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192702-01xzcrtq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-47
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/01xzcrtq
2023-02-07 19:27:10.063 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 19:27:10.063 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 861 for sweep.
2023-02-07 19:27:10.064 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.010386357358672651 for sweep.
2023-02-07 19:27:10.064 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:27:10.064 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 19:27:10.064 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5924367427400302 for sweep.
2023-02-07 19:27:10.065 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 193 for sweep.
2023-02-07 19:27:10.065 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 5 for sweep.
2023-02-07 19:27:10.065 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.21110786728087583 for sweep.
2023-02-07 19:27:10.065 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 22 for sweep.
2023-02-07 19:27:10.066 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0280135061524851 for sweep.
2023-02-07 19:27:10.066 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2077 for sweep.
2023-02-07 19:27:10.066 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 258 for sweep.
2023-02-07 19:27:10.066 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.041088256216128945 for sweep.
2023-02-07 19:27:10.066 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.08563361374087283 for sweep.
2023-02-07 19:27:10.067 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.463511336307024 for sweep.
2023-02-07 19:27:10.067 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:27:10.073 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192702-01xzcrtq/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 861, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 193, 'window': 5, 'min_count': 4, 'dm': 0, 'sample': 0.5924367427400302, 'workers': 4, 'alpha': 0.010386357358672651, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2077, 'max_depth': 22, 'num_leaves': 258, 'reg_alpha': 0.041088256216128945, 'reg_lambda': 0.08563361374087283, 'subsample': 0.463511336307024, 'min_child_weight': 0.0280135061524851, 'n_jobs': 4, 'learning_rate': 0.21110786728087583}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 16/3257 [00:00<00:20, 157.70it/s]  1%|          | 34/3257 [00:00<00:19, 161.62it/s]  2%|‚ñè         | 52/3257 [00:00<00:19, 164.72it/s]  2%|‚ñè         | 69/3257 [00:00<00:19, 162.41it/s]  3%|‚ñé         | 90/3257 [00:00<00:18, 174.09it/s]  3%|‚ñé         | 108/3257 [00:00<00:18, 169.57it/s]  4%|‚ñç         | 126/3257 [00:00<00:18, 172.13it/s]  5%|‚ñç         | 149/3257 [00:00<00:16, 186.21it/s]  5%|‚ñå         | 168/3257 [00:00<00:17, 179.10it/s]  6%|‚ñå         | 186/3257 [00:01<00:17, 179.21it/s]  6%|‚ñã         | 204/3257 [00:01<00:17, 176.30it/s]  7%|‚ñã         | 230/3257 [00:01<00:15, 197.35it/s]  8%|‚ñä         | 250/3257 [00:01<00:15, 193.62it/s]  8%|‚ñä         | 270/3257 [00:01<00:16, 183.37it/s]  9%|‚ñâ         | 296/3257 [00:01<00:14, 200.62it/s] 10%|‚ñâ         | 317/3257 [00:01<00:15, 190.59it/s] 10%|‚ñà         | 337/3257 [00:01<00:15, 190.11it/s] 11%|‚ñà         | 358/3257 [00:01<00:15, 192.19it/s] 12%|‚ñà‚ñè        | 378/3257 [00:02<00:16, 176.50it/s] 12%|‚ñà‚ñè        | 396/3257 [00:02<00:16, 176.19it/s] 13%|‚ñà‚ñé        | 416/3257 [00:02<00:15, 181.40it/s] 13%|‚ñà‚ñé        | 435/3257 [00:02<00:18, 152.49it/s] 14%|‚ñà‚ñç        | 456/3257 [00:02<00:16, 166.29it/s] 15%|‚ñà‚ñç        | 475/3257 [00:02<00:16, 170.44it/s] 15%|‚ñà‚ñå        | 493/3257 [00:02<00:16, 172.37it/s] 16%|‚ñà‚ñå        | 514/3257 [00:02<00:15, 181.20it/s] 16%|‚ñà‚ñã        | 533/3257 [00:02<00:15, 175.55it/s] 17%|‚ñà‚ñã        | 551/3257 [00:03<00:15, 176.59it/s] 17%|‚ñà‚ñã        | 569/3257 [00:03<00:16, 166.92it/s] 18%|‚ñà‚ñä        | 586/3257 [00:03<00:16, 164.15it/s] 19%|‚ñà‚ñä        | 606/3257 [00:03<00:15, 174.01it/s] 19%|‚ñà‚ñâ        | 624/3257 [00:03<00:15, 174.06it/s] 20%|‚ñà‚ñâ        | 644/3257 [00:03<00:14, 179.96it/s] 20%|‚ñà‚ñà        | 663/3257 [00:03<00:16, 161.68it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:15, 166.11it/s] 22%|‚ñà‚ñà‚ñè       | 702/3257 [00:03<00:14, 172.46it/s] 22%|‚ñà‚ñà‚ñè       | 720/3257 [00:04<00:14, 174.48it/s] 23%|‚ñà‚ñà‚ñé       | 738/3257 [00:04<00:15, 164.55it/s] 23%|‚ñà‚ñà‚ñé       | 758/3257 [00:04<00:14, 173.74it/s] 24%|‚ñà‚ñà‚ñç       | 776/3257 [00:04<00:14, 169.65it/s] 24%|‚ñà‚ñà‚ñç       | 796/3257 [00:04<00:13, 176.08it/s] 25%|‚ñà‚ñà‚ñç       | 814/3257 [00:04<00:13, 174.83it/s] 26%|‚ñà‚ñà‚ñå       | 832/3257 [00:04<00:14, 165.83it/s] 26%|‚ñà‚ñà‚ñå       | 849/3257 [00:04<00:15, 154.92it/s] 27%|‚ñà‚ñà‚ñã       | 866/3257 [00:04<00:15, 157.92it/s] 27%|‚ñà‚ñà‚ñã       | 883/3257 [00:05<00:14, 160.00it/s] 28%|‚ñà‚ñà‚ñä       | 902/3257 [00:05<00:14, 168.05it/s] 28%|‚ñà‚ñà‚ñä       | 919/3257 [00:05<00:13, 167.63it/s] 29%|‚ñà‚ñà‚ñä       | 936/3257 [00:05<00:13, 166.78it/s] 29%|‚ñà‚ñà‚ñâ       | 956/3257 [00:05<00:13, 173.43it/s] 30%|‚ñà‚ñà‚ñâ       | 974/3257 [00:05<00:13, 169.21it/s] 30%|‚ñà‚ñà‚ñà       | 991/3257 [00:05<00:13, 162.30it/s] 31%|‚ñà‚ñà‚ñà       | 1008/3257 [00:05<00:14, 159.57it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:05<00:14, 157.72it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1043/3257 [00:06<00:14, 155.92it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1061/3257 [00:06<00:13, 162.08it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:06<00:13, 163.40it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1097/3257 [00:06<00:13, 164.24it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:06<00:12, 168.85it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1134/3257 [00:06<00:12, 166.12it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1151/3257 [00:06<00:12, 162.23it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:06<00:12, 168.57it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1188/3257 [00:06<00:13, 149.86it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1204/3257 [00:07<00:13, 150.08it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1220/3257 [00:07<00:13, 151.82it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:07<00:12, 167.19it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1259/3257 [00:07<00:19, 103.71it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:07<00:17, 112.83it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1289/3257 [00:07<00:16, 116.08it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1306/3257 [00:07<00:15, 127.84it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1324/3257 [00:08<00:13, 140.26it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1342/3257 [00:08<00:12, 150.61it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1359/3257 [00:08<00:12, 150.39it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1375/3257 [00:08<00:12, 146.98it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1391/3257 [00:08<00:12, 149.61it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1413/3257 [00:08<00:11, 166.69it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1433/3257 [00:08<00:10, 174.12it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1453/3257 [00:08<00:10, 179.98it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1475/3257 [00:08<00:09, 188.12it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1496/3257 [00:08<00:09, 191.93it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1516/3257 [00:09<00:09, 193.29it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1536/3257 [00:09<00:10, 170.13it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1554/3257 [00:09<00:10, 169.12it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1574/3257 [00:09<00:09, 174.17it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1592/3257 [00:09<00:09, 172.20it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1612/3257 [00:09<00:09, 177.74it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1631/3257 [00:09<00:08, 180.93it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1650/3257 [00:09<00:09, 164.18it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1667/3257 [00:10<00:09, 160.91it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1684/3257 [00:10<00:09, 158.50it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1703/3257 [00:10<00:09, 164.73it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1721/3257 [00:10<00:09, 168.63it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1739/3257 [00:10<00:10, 150.13it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1759/3257 [00:10<00:09, 161.89it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1776/3257 [00:10<00:09, 161.87it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1795/3257 [00:10<00:08, 168.79it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1813/3257 [00:10<00:08, 162.21it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1830/3257 [00:11<00:08, 162.97it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1850/3257 [00:11<00:08, 171.10it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1871/3257 [00:11<00:07, 181.14it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1890/3257 [00:11<00:07, 178.19it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1910/3257 [00:11<00:07, 183.26it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1929/3257 [00:11<00:07, 176.10it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1957/3257 [00:11<00:06, 203.53it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1978/3257 [00:11<00:06, 193.93it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1998/3257 [00:11<00:06, 192.07it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2018/3257 [00:11<00:06, 191.07it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2038/3257 [00:12<00:06, 190.45it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2058/3257 [00:12<00:06, 174.16it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2076/3257 [00:12<00:06, 175.34it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2094/3257 [00:12<00:06, 175.96it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2112/3257 [00:12<00:06, 172.78it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2130/3257 [00:12<00:07, 159.77it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2147/3257 [00:12<00:07, 156.87it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:12<00:06, 169.64it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2187/3257 [00:13<00:06, 170.32it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2207/3257 [00:13<00:05, 175.72it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2225/3257 [00:13<00:05, 173.58it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2243/3257 [00:13<00:06, 167.29it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2262/3257 [00:13<00:05, 171.98it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2280/3257 [00:13<00:05, 169.35it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:13<00:05, 174.29it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2321/3257 [00:13<00:05, 185.08it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2343/3257 [00:13<00:04, 194.12it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2366/3257 [00:13<00:04, 203.61it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2387/3257 [00:14<00:04, 201.37it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2408/3257 [00:14<00:04, 189.28it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2428/3257 [00:14<00:04, 184.17it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2447/3257 [00:14<00:04, 173.21it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2466/3257 [00:14<00:04, 176.79it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2484/3257 [00:14<00:04, 173.57it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2507/3257 [00:14<00:04, 187.12it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2527/3257 [00:14<00:03, 189.68it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2547/3257 [00:14<00:03, 189.24it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2567/3257 [00:15<00:03, 176.37it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2585/3257 [00:15<00:04, 165.55it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2605/3257 [00:15<00:03, 171.59it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:15<00:03, 184.31it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2647/3257 [00:15<00:03, 172.66it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2665/3257 [00:15<00:03, 172.20it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2685/3257 [00:15<00:03, 178.52it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2704/3257 [00:15<00:03, 158.92it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2721/3257 [00:16<00:03, 155.56it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2744/3257 [00:16<00:02, 171.81it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2762/3257 [00:16<00:05, 95.01it/s]  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2778/3257 [00:16<00:04, 106.19it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2800/3257 [00:16<00:03, 128.49it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2817/3257 [00:16<00:03, 134.09it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2834/3257 [00:16<00:02, 141.20it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2855/3257 [00:17<00:02, 156.52it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2882/3257 [00:17<00:02, 184.39it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2902/3257 [00:17<00:02, 172.52it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2925/3257 [00:17<00:01, 187.12it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2945/3257 [00:17<00:01, 173.86it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2966/3257 [00:17<00:01, 179.81it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:17<00:01, 168.13it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3007/3257 [00:17<00:01, 181.16it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3026/3257 [00:18<00:01, 177.24it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3048/3257 [00:18<00:01, 187.56it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3071/3257 [00:18<00:00, 198.12it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3092/3257 [00:18<00:00, 190.44it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3116/3257 [00:18<00:00, 202.13it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3137/3257 [00:18<00:00, 191.10it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3157/3257 [00:18<00:00, 180.00it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3176/3257 [00:18<00:00, 178.08it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3197/3257 [00:18<00:00, 179.28it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3216/3257 [00:19<00:00, 172.86it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3239/3257 [00:19<00:00, 187.59it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 169.36it/s]
2023-02-07 19:27:30.148 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:27:30,149][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d193,n5,mc4,s0.592437,t4>', 'datetime': '2023-02-07T19:27:30.149462', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:27:30,149][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:27:30,149][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:27:30,727][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 19:27:30,728][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:27:30,817][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 36921 unique words (68.30% of original 54054, drops 17133)', 'datetime': '2023-02-07T19:27:30.817864', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:27:30,818][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 6522479 word corpus (99.57% of original 6550866, drops 28387)', 'datetime': '2023-02-07T19:27:30.818270', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:27:30,938][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 19:27:30,939][gensim.models.word2vec][INFO] - sample=0.592437 downsamples 0 most-common words
[2023-02-07 19:27:30,939][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6522479 word corpus (100.0%% of prior 6522479)', 'datetime': '2023-02-07T19:27:30.939823', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:27:31,146][gensim.models.word2vec][INFO] - estimated required memory for 36921 words and 193 dimensions: 78632328 bytes
[2023-02-07 19:27:31,146][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:27:31,182][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 36921 vocabulary and 193 features, using sg=1 hs=0 sample=0.5924367427400302 negative=5 window=5 shrink_windows=True', 'datetime': '2023-02-07T19:27:31.182687', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:27:32,190][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 32.27% examples, 2099105 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:33,190][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 65.09% examples, 2139048 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:34,167][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6467889 effective words) took 3.0s, 2168302 effective words/s
[2023-02-07 19:27:35,170][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 37.06% examples, 2451304 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:27:36,173][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 75.59% examples, 2476663 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:36,779][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6467889 effective words) took 2.6s, 2478637 effective words/s
[2023-02-07 19:27:37,782][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 37.58% examples, 2475763 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:38,783][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 75.35% examples, 2473366 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:39,423][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6467889 effective words) took 2.6s, 2447751 effective words/s
[2023-02-07 19:27:40,427][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 46.05% examples, 3018783 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:41,433][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 95.43% examples, 3078858 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:41,524][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6467889 effective words) took 2.1s, 3081461 effective words/s
[2023-02-07 19:27:42,537][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 47.62% examples, 3130826 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:43,537][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 98.43% examples, 3179504 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:43,562][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6467889 effective words) took 2.0s, 3187777 effective words/s
[2023-02-07 19:27:44,570][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 37.64% examples, 2485729 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:45,573][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 76.17% examples, 2492293 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:46,164][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6467889 effective words) took 2.6s, 2491143 effective words/s
[2023-02-07 19:27:47,168][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 38.01% examples, 2504604 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:48,168][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 76.70% examples, 2509656 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:27:48,722][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6467889 effective words) took 2.6s, 2532215 effective words/s
[2023-02-07 19:27:49,731][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 36.97% examples, 2431003 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:50,731][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 77.80% examples, 2536947 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:51,257][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6467889 effective words) took 2.5s, 2555532 effective words/s
[2023-02-07 19:27:52,263][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 37.40% examples, 2470257 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:53,264][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 74.39% examples, 2437913 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:53,911][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6467889 effective words) took 2.7s, 2439419 effective words/s
[2023-02-07 19:27:54,917][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 35.92% examples, 2353594 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:55,922][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 72.89% examples, 2387798 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:56,600][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6467889 effective words) took 2.7s, 2409804 effective words/s
[2023-02-07 19:27:57,605][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 37.83% examples, 2495358 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:58,606][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 75.71% examples, 2483439 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:27:59,207][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6467889 effective words) took 2.6s, 2485730 effective words/s
[2023-02-07 19:28:00,209][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 37.83% examples, 2495752 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:28:01,212][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 89.16% examples, 2902209 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:28:01,400][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6467889 effective words) took 2.2s, 2952741 effective words/s
[2023-02-07 19:28:02,404][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 45.32% examples, 2986528 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:28:03,403][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 85.20% examples, 2780473 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:28:03,745][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6467889 effective words) took 2.3s, 2759114 effective words/s
[2023-02-07 19:28:04,757][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 38.47% examples, 2521557 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:28:05,761][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 79.24% examples, 2577730 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:28:06,256][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6467889 effective words) took 2.5s, 2579730 effective words/s
[2023-02-07 19:28:07,265][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 39.42% examples, 2593077 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:28:08,266][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 79.12% examples, 2578843 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:28:08,759][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6467889 effective words) took 2.5s, 2584878 effective words/s
[2023-02-07 19:28:08,760][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (97018335 effective words) took 37.6s, 2581829 effective words/s', 'datetime': '2023-02-07T19:28:08.760523', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:28:08.760 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:28:11,980][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192702-01xzcrtq/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:28:11.980120', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:28:11,981][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:28:12,104][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192702-01xzcrtq/files/../tmp/embedding_model.pt
2023-02-07 19:28:12.105 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:28:13.590 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:28:14.075 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:28:15.641 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0520369706283628, 'test_mae': 1.0899827955190686, 'test_r2': -1.799579329094529}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.82
wandb: percentage 0.31696
wandb:   test_mae 1.08998
wandb:   test_mse 2.05204
wandb:    test_r2 -1.79958
wandb: 
wandb: üöÄ View run copper-sweep-47 at: https://wandb.ai/xiaoqiz/mof2vec/runs/01xzcrtq
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_192702-01xzcrtq/logs
wandb: Agent Starting Run: l0vj1it8 with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 633
wandb: 	model.gensim.alpha: 0.046952018078403435
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.5939092758979172
wandb: 	model.gensim.vector_size: 228
wandb: 	model.gensim.window: 17
wandb: 	model.sklearn.learning_rate: 0.013322788551013064
wandb: 	model.sklearn.max_depth: 29
wandb: 	model.sklearn.min_child_weight: 0.07007052307363432
wandb: 	model.sklearn.n_estimators: 2330
wandb: 	model.sklearn.num_leaves: 480
wandb: 	model.sklearn.reg_alpha: 0.5488319834971149
wandb: 	model.sklearn.reg_lambda: 0.07058372210726135
wandb: 	model.sklearn.subsample: 0.9062079101959492
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192826-l0vj1it8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-48
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/l0vj1it8
2023-02-07 19:28:34.913 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 19:28:34.914 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 633 for sweep.
2023-02-07 19:28:34.914 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.046952018078403435 for sweep.
2023-02-07 19:28:34.914 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:28:34.914 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 19:28:34.915 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5939092758979172 for sweep.
2023-02-07 19:28:34.915 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 228 for sweep.
2023-02-07 19:28:34.915 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 17 for sweep.
2023-02-07 19:28:34.915 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.013322788551013064 for sweep.
2023-02-07 19:28:34.916 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 29 for sweep.
2023-02-07 19:28:34.916 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.07007052307363432 for sweep.
2023-02-07 19:28:34.916 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2330 for sweep.
2023-02-07 19:28:34.916 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 480 for sweep.
2023-02-07 19:28:34.916 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.5488319834971149 for sweep.
2023-02-07 19:28:34.917 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.07058372210726135 for sweep.
2023-02-07 19:28:34.917 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.9062079101959492 for sweep.
2023-02-07 19:28:34.918 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:28:34.925 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192826-l0vj1it8/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 633, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 228, 'window': 17, 'min_count': 1, 'dm': 0, 'sample': 0.5939092758979172, 'workers': 4, 'alpha': 0.046952018078403435, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2330, 'max_depth': 29, 'num_leaves': 480, 'reg_alpha': 0.5488319834971149, 'reg_lambda': 0.07058372210726135, 'subsample': 0.9062079101959492, 'min_child_weight': 0.07007052307363432, 'n_jobs': 4, 'learning_rate': 0.013322788551013064}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:19, 162.74it/s]  1%|          | 36/3257 [00:00<00:18, 177.47it/s]  2%|‚ñè         | 54/3257 [00:00<00:18, 170.05it/s]  2%|‚ñè         | 72/3257 [00:00<00:18, 170.13it/s]  3%|‚ñé         | 92/3257 [00:00<00:17, 178.85it/s]  3%|‚ñé         | 110/3257 [00:00<00:18, 169.39it/s]  4%|‚ñç         | 130/3257 [00:00<00:17, 177.96it/s]  5%|‚ñç         | 150/3257 [00:00<00:16, 184.39it/s]  5%|‚ñå         | 169/3257 [00:00<00:17, 179.64it/s]  6%|‚ñå         | 188/3257 [00:01<00:16, 181.44it/s]  6%|‚ñã         | 207/3257 [00:01<00:16, 182.53it/s]  7%|‚ñã         | 230/3257 [00:01<00:15, 192.80it/s]  8%|‚ñä         | 250/3257 [00:01<00:15, 190.43it/s]  8%|‚ñä         | 270/3257 [00:01<00:16, 185.86it/s]  9%|‚ñâ         | 296/3257 [00:01<00:14, 202.60it/s] 10%|‚ñâ         | 317/3257 [00:01<00:15, 190.88it/s] 10%|‚ñà         | 338/3257 [00:01<00:15, 193.79it/s] 11%|‚ñà         | 359/3257 [00:01<00:15, 192.91it/s] 12%|‚ñà‚ñè        | 379/3257 [00:02<00:15, 181.52it/s] 12%|‚ñà‚ñè        | 398/3257 [00:02<00:15, 181.50it/s] 13%|‚ñà‚ñé        | 420/3257 [00:02<00:15, 187.13it/s] 13%|‚ñà‚ñé        | 439/3257 [00:02<00:17, 163.12it/s] 14%|‚ñà‚ñç        | 460/3257 [00:02<00:16, 174.55it/s] 15%|‚ñà‚ñç        | 479/3257 [00:02<00:15, 176.62it/s] 15%|‚ñà‚ñå        | 501/3257 [00:02<00:14, 187.95it/s] 16%|‚ñà‚ñå        | 522/3257 [00:02<00:14, 189.43it/s] 17%|‚ñà‚ñã        | 542/3257 [00:02<00:14, 189.87it/s] 17%|‚ñà‚ñã        | 562/3257 [00:03<00:15, 176.70it/s] 18%|‚ñà‚ñä        | 580/3257 [00:03<00:15, 170.05it/s] 18%|‚ñà‚ñä        | 601/3257 [00:03<00:14, 178.65it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:15, 173.51it/s] 20%|‚ñà‚ñâ        | 641/3257 [00:03<00:14, 179.57it/s] 20%|‚ñà‚ñà        | 660/3257 [00:03<00:15, 166.62it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:15, 171.22it/s] 22%|‚ñà‚ñà‚ñè       | 701/3257 [00:03<00:14, 172.76it/s] 22%|‚ñà‚ñà‚ñè       | 721/3257 [00:04<00:14, 177.12it/s] 23%|‚ñà‚ñà‚ñé       | 739/3257 [00:04<00:15, 166.75it/s] 23%|‚ñà‚ñà‚ñé       | 761/3257 [00:04<00:13, 180.66it/s] 24%|‚ñà‚ñà‚ñç       | 780/3257 [00:04<00:14, 173.88it/s] 25%|‚ñà‚ñà‚ñç       | 800/3257 [00:04<00:13, 178.98it/s] 25%|‚ñà‚ñà‚ñå       | 819/3257 [00:04<00:13, 175.48it/s] 26%|‚ñà‚ñà‚ñå       | 837/3257 [00:04<00:14, 166.45it/s] 26%|‚ñà‚ñà‚ñå       | 854/3257 [00:04<00:14, 163.57it/s] 27%|‚ñà‚ñà‚ñã       | 873/3257 [00:04<00:14, 170.10it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:05<00:14, 166.62it/s] 28%|‚ñà‚ñà‚ñä       | 912/3257 [00:05<00:13, 177.41it/s] 29%|‚ñà‚ñà‚ñä       | 930/3257 [00:05<00:19, 116.50it/s] 29%|‚ñà‚ñà‚ñâ       | 948/3257 [00:05<00:17, 128.53it/s] 30%|‚ñà‚ñà‚ñâ       | 968/3257 [00:05<00:15, 144.20it/s] 30%|‚ñà‚ñà‚ñà       | 985/3257 [00:05<00:15, 149.47it/s] 31%|‚ñà‚ñà‚ñà       | 1002/3257 [00:05<00:14, 152.68it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1021/3257 [00:05<00:13, 162.07it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1039/3257 [00:06<00:14, 155.36it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1057/3257 [00:06<00:13, 159.59it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:06<00:13, 166.95it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1099/3257 [00:06<00:12, 172.16it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1118/3257 [00:06<00:12, 176.20it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1136/3257 [00:06<00:12, 171.37it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1154/3257 [00:06<00:12, 169.18it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:06<00:11, 174.37it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:06<00:13, 155.22it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1207/3257 [00:07<00:13, 152.64it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1227/3257 [00:07<00:12, 165.32it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1246/3257 [00:07<00:11, 170.38it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1265/3257 [00:07<00:11, 175.45it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1283/3257 [00:07<00:12, 158.25it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1300/3257 [00:07<00:12, 155.68it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1318/3257 [00:07<00:12, 161.27it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:07<00:11, 169.24it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1357/3257 [00:07<00:11, 165.44it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1374/3257 [00:08<00:11, 164.23it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1391/3257 [00:08<00:11, 162.13it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1413/3257 [00:08<00:10, 176.69it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:08<00:10, 177.90it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1452/3257 [00:08<00:09, 183.31it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1473/3257 [00:08<00:09, 190.24it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1493/3257 [00:08<00:09, 192.31it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:08<00:09, 192.88it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1533/3257 [00:08<00:09, 172.82it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1551/3257 [00:09<00:10, 168.48it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1569/3257 [00:09<00:10, 167.14it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1586/3257 [00:09<00:10, 165.71it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1606/3257 [00:09<00:09, 172.79it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1626/3257 [00:09<00:09, 179.81it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1645/3257 [00:09<00:09, 167.59it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1663/3257 [00:09<00:10, 159.06it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1680/3257 [00:09<00:09, 158.66it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1697/3257 [00:09<00:09, 161.54it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:10<00:09, 164.56it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1732/3257 [00:10<00:09, 155.74it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1750/3257 [00:10<00:09, 160.77it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1768/3257 [00:10<00:09, 165.18it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1787/3257 [00:10<00:08, 172.21it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:10<00:08, 164.06it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1823/3257 [00:10<00:08, 167.21it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1840/3257 [00:10<00:08, 167.30it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1858/3257 [00:10<00:08, 166.18it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1879/3257 [00:10<00:07, 174.61it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1897/3257 [00:11<00:07, 171.07it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:11<00:07, 176.13it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1937/3257 [00:11<00:07, 183.30it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1961/3257 [00:11<00:06, 198.45it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1981/3257 [00:11<00:06, 187.21it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2000/3257 [00:11<00:06, 187.24it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2019/3257 [00:11<00:06, 180.81it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2038/3257 [00:11<00:06, 179.37it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2057/3257 [00:12<00:07, 162.82it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2074/3257 [00:12<00:07, 163.15it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2091/3257 [00:12<00:07, 163.91it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2108/3257 [00:12<00:07, 161.34it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2125/3257 [00:12<00:07, 155.60it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2143/3257 [00:12<00:07, 153.84it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2162/3257 [00:12<00:06, 163.05it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:12<00:06, 165.54it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2202/3257 [00:12<00:06, 175.00it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2220/3257 [00:13<00:11, 93.76it/s]  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2238/3257 [00:13<00:09, 106.44it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2256/3257 [00:13<00:08, 119.39it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2272/3257 [00:13<00:07, 123.15it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2294/3257 [00:13<00:06, 144.44it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2312/3257 [00:13<00:06, 152.71it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2336/3257 [00:13<00:05, 175.17it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:14<00:04, 186.15it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2380/3257 [00:14<00:04, 189.90it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2402/3257 [00:14<00:04, 196.85it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2423/3257 [00:14<00:04, 182.96it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2442/3257 [00:14<00:04, 169.15it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2462/3257 [00:14<00:04, 175.87it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2481/3257 [00:14<00:04, 178.46it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2504/3257 [00:14<00:03, 190.09it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2524/3257 [00:14<00:03, 189.64it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2544/3257 [00:15<00:03, 187.82it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2563/3257 [00:15<00:03, 178.17it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2581/3257 [00:15<00:03, 170.37it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2599/3257 [00:15<00:03, 172.77it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2624/3257 [00:15<00:03, 194.26it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2644/3257 [00:15<00:03, 181.78it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2663/3257 [00:15<00:03, 174.12it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2684/3257 [00:15<00:03, 182.97it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2703/3257 [00:15<00:03, 164.96it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2720/3257 [00:16<00:03, 162.68it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2742/3257 [00:16<00:02, 176.12it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:16<00:02, 176.97it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2779/3257 [00:16<00:02, 174.94it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2801/3257 [00:16<00:02, 186.99it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2820/3257 [00:16<00:02, 175.90it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2838/3257 [00:16<00:02, 170.49it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2861/3257 [00:16<00:02, 185.81it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2884/3257 [00:16<00:01, 197.20it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2904/3257 [00:17<00:02, 174.08it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2927/3257 [00:17<00:01, 188.74it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2947/3257 [00:17<00:01, 173.60it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2966/3257 [00:17<00:01, 176.62it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:17<00:01, 165.50it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3007/3257 [00:17<00:01, 178.78it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3026/3257 [00:17<00:01, 172.25it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3048/3257 [00:17<00:01, 182.89it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3070/3257 [00:17<00:00, 192.58it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3090/3257 [00:18<00:00, 188.37it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3112/3257 [00:18<00:00, 196.39it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3132/3257 [00:18<00:00, 197.09it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3152/3257 [00:18<00:00, 187.99it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3172/3257 [00:18<00:00, 189.68it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3192/3257 [00:18<00:00, 188.34it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3211/3257 [00:18<00:00, 184.17it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3232/3257 [00:18<00:00, 190.97it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:18<00:00, 190.87it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 171.66it/s]
2023-02-07 19:28:54.725 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:28:54,727][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d228,n5,s0.593909,t4>', 'datetime': '2023-02-07T19:28:54.727130', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:28:54,727][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:28:54,727][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:28:55,291][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 19:28:55,291][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:28:55,428][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 54054 unique words (100.00% of original 54054, drops 0)', 'datetime': '2023-02-07T19:28:55.427963', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:28:55,428][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 6550866 word corpus (100.00% of original 6550866, drops 0)', 'datetime': '2023-02-07T19:28:55.428335', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:28:55,608][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 19:28:55,609][gensim.models.word2vec][INFO] - sample=0.593909 downsamples 0 most-common words
[2023-02-07 19:28:55,610][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6550866 word corpus (100.0%% of prior 6550866)', 'datetime': '2023-02-07T19:28:55.610141', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:28:55,902][gensim.models.word2vec][INFO] - estimated required memory for 54054 words and 228 dimensions: 129243280 bytes
[2023-02-07 19:28:55,902][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:28:55,946][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 54054 vocabulary and 228 features, using sg=1 hs=0 sample=0.5939092758979172 negative=5 window=17 shrink_windows=True', 'datetime': '2023-02-07T19:28:55.946595', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:28:56,952][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 26.04% examples, 1689056 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:28:57,953][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 53.76% examples, 1784208 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:28:58,955][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 83.05% examples, 1814636 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:28:59,496][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6494971 effective words) took 3.5s, 1831324 effective words/s
[2023-02-07 19:29:00,502][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 40.59% examples, 2691739 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:01,505][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 81.76% examples, 2667100 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:01,918][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6494971 effective words) took 2.4s, 2683539 effective words/s
[2023-02-07 19:29:02,920][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 42.37% examples, 2825139 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:03,921][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 86.12% examples, 2817301 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:04,226][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6494971 effective words) took 2.3s, 2816732 effective words/s
[2023-02-07 19:29:05,228][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 33.74% examples, 2213958 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:06,229][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 67.58% examples, 2234911 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:07,182][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6494971 effective words) took 3.0s, 2198567 effective words/s
[2023-02-07 19:29:08,190][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 32.55% examples, 2126951 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:09,192][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 64.32% examples, 2117098 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:10,118][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6494971 effective words) took 2.9s, 2215070 effective words/s
[2023-02-07 19:29:11,120][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 41.63% examples, 2780544 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:12,127][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 80.47% examples, 2625567 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:12,704][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6494971 effective words) took 2.6s, 2512601 effective words/s
[2023-02-07 19:29:13,708][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 41.45% examples, 2760010 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:14,711][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 75.13% examples, 2474051 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:15,434][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6494971 effective words) took 2.7s, 2380807 effective words/s
[2023-02-07 19:29:16,442][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 36.51% examples, 2411132 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:17,444][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 69.17% examples, 2286948 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:18,263][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6494971 effective words) took 2.8s, 2297840 effective words/s
[2023-02-07 19:29:19,266][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 36.48% examples, 2413549 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:20,267][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 72.37% examples, 2386902 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:20,996][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6494971 effective words) took 2.7s, 2377459 effective words/s
[2023-02-07 19:29:22,000][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 36.17% examples, 2376192 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:23,010][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 72.61% examples, 2375043 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:29:23,736][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6494971 effective words) took 2.7s, 2372174 effective words/s
[2023-02-07 19:29:24,740][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 36.08% examples, 2368348 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:25,743][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 72.43% examples, 2380186 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:26,460][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6494971 effective words) took 2.7s, 2386246 effective words/s
[2023-02-07 19:29:27,464][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 46.64% examples, 3077519 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:29:28,464][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 93.58% examples, 3050700 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:28,585][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6494971 effective words) took 2.1s, 3060481 effective words/s
[2023-02-07 19:29:29,593][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 36.66% examples, 2424280 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:30,595][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 73.81% examples, 2420028 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:31,320][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6494971 effective words) took 2.7s, 2378181 effective words/s
[2023-02-07 19:29:32,327][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 41.57% examples, 2768554 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:29:33,329][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 86.95% examples, 2843166 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:33,598][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6494971 effective words) took 2.3s, 2856939 effective words/s
[2023-02-07 19:29:34,603][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 45.47% examples, 2999345 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:35,608][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 91.86% examples, 2994214 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:29:35,766][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6494971 effective words) took 2.2s, 2998085 effective words/s
[2023-02-07 19:29:35,766][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (97424565 effective words) took 39.8s, 2446699 effective words/s', 'datetime': '2023-02-07T19:29:35.766725', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:29:35.766 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:29:40,017][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192826-l0vj1it8/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:29:40.017217', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:29:40,017][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192826-l0vj1it8/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 19:29:40,073][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192826-l0vj1it8/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 19:29:40,123][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:29:40,155][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_192826-l0vj1it8/files/../tmp/embedding_model.pt
2023-02-07 19:29:40.155 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:29:41.626 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:29:42.154 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:29:43.686 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1313011600177285, 'test_mae': 1.0640959903280724, 'test_r2': -2.706367035363541}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.95
wandb: percentage 0.0
wandb:   test_mae 1.0641
wandb:   test_mse 2.1313
wandb:    test_r2 -2.70637
wandb: 
wandb: üöÄ View run eager-sweep-48 at: https://wandb.ai/xiaoqiz/mof2vec/runs/l0vj1it8
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_192826-l0vj1it8/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: z0xawavg with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 695
wandb: 	model.gensim.alpha: 0.001307684364236681
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.5071679181864567
wandb: 	model.gensim.vector_size: 373
wandb: 	model.gensim.window: 7
wandb: 	model.sklearn.learning_rate: 0.11666421500564886
wandb: 	model.sklearn.max_depth: 12
wandb: 	model.sklearn.min_child_weight: 0.04348456344334391
wandb: 	model.sklearn.n_estimators: 644
wandb: 	model.sklearn.num_leaves: 382
wandb: 	model.sklearn.reg_alpha: 0.4600559562658294
wandb: 	model.sklearn.reg_lambda: 0.21181155260541423
wandb: 	model.sklearn.subsample: 0.7113997162599144
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193002-z0xawavg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-49
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/z0xawavg
2023-02-07 19:30:10.516 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 19:30:10.516 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 695 for sweep.
2023-02-07 19:30:10.516 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.001307684364236681 for sweep.
2023-02-07 19:30:10.517 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:30:10.517 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 19:30:10.517 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5071679181864567 for sweep.
2023-02-07 19:30:10.517 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 373 for sweep.
2023-02-07 19:30:10.518 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 7 for sweep.
2023-02-07 19:30:10.518 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.11666421500564886 for sweep.
2023-02-07 19:30:10.518 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 12 for sweep.
2023-02-07 19:30:10.518 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.04348456344334391 for sweep.
2023-02-07 19:30:10.519 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 644 for sweep.
2023-02-07 19:30:10.519 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 382 for sweep.
2023-02-07 19:30:10.519 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.4600559562658294 for sweep.
2023-02-07 19:30:10.519 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.21181155260541423 for sweep.
2023-02-07 19:30:10.519 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.7113997162599144 for sweep.
2023-02-07 19:30:10.520 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:30:10.527 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193002-z0xawavg/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 695, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 373, 'window': 7, 'min_count': 2, 'dm': 0, 'sample': 0.5071679181864567, 'workers': 4, 'alpha': 0.001307684364236681, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 644, 'max_depth': 12, 'num_leaves': 382, 'reg_alpha': 0.4600559562658294, 'reg_lambda': 0.21181155260541423, 'subsample': 0.7113997162599144, 'min_child_weight': 0.04348456344334391, 'n_jobs': 4, 'learning_rate': 0.11666421500564886}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 19/3257 [00:00<00:17, 185.68it/s]  1%|‚ñè         | 41/3257 [00:00<00:16, 199.92it/s]  2%|‚ñè         | 61/3257 [00:00<00:16, 195.86it/s]  3%|‚ñé         | 84/3257 [00:00<00:15, 205.30it/s]  3%|‚ñé         | 105/3257 [00:00<00:15, 197.08it/s]  4%|‚ñç         | 125/3257 [00:00<00:16, 192.66it/s]  5%|‚ñç         | 150/3257 [00:00<00:14, 209.19it/s]  5%|‚ñå         | 172/3257 [00:00<00:15, 200.19it/s]  6%|‚ñå         | 193/3257 [00:00<00:15, 200.02it/s]  7%|‚ñã         | 216/3257 [00:01<00:14, 204.63it/s]  7%|‚ñã         | 239/3257 [00:01<00:14, 211.31it/s]  8%|‚ñä         | 261/3257 [00:01<00:14, 206.80it/s]  9%|‚ñâ         | 287/3257 [00:01<00:13, 221.77it/s] 10%|‚ñâ         | 310/3257 [00:01<00:13, 217.32it/s] 10%|‚ñà         | 334/3257 [00:01<00:13, 219.79it/s] 11%|‚ñà         | 357/3257 [00:01<00:13, 221.04it/s] 12%|‚ñà‚ñè        | 380/3257 [00:01<00:14, 203.83it/s] 12%|‚ñà‚ñè        | 401/3257 [00:02<00:19, 143.29it/s] 13%|‚ñà‚ñé        | 421/3257 [00:02<00:18, 153.79it/s] 13%|‚ñà‚ñé        | 439/3257 [00:02<00:18, 149.17it/s] 14%|‚ñà‚ñç        | 461/3257 [00:02<00:16, 164.67it/s] 15%|‚ñà‚ñç        | 481/3257 [00:02<00:16, 172.81it/s] 16%|‚ñà‚ñå        | 505/3257 [00:02<00:14, 189.28it/s] 16%|‚ñà‚ñå        | 526/3257 [00:02<00:14, 194.42it/s] 17%|‚ñà‚ñã        | 547/3257 [00:02<00:13, 196.56it/s] 17%|‚ñà‚ñã        | 568/3257 [00:02<00:14, 190.94it/s] 18%|‚ñà‚ñä        | 588/3257 [00:03<00:14, 185.99it/s] 19%|‚ñà‚ñä        | 610/3257 [00:03<00:13, 194.26it/s] 19%|‚ñà‚ñâ        | 630/3257 [00:03<00:13, 194.92it/s] 20%|‚ñà‚ñâ        | 650/3257 [00:03<00:13, 190.05it/s] 21%|‚ñà‚ñà        | 670/3257 [00:03<00:13, 186.63it/s] 21%|‚ñà‚ñà        | 689/3257 [00:03<00:13, 185.68it/s] 22%|‚ñà‚ñà‚ñè       | 711/3257 [00:03<00:13, 194.26it/s] 22%|‚ñà‚ñà‚ñè       | 731/3257 [00:03<00:13, 187.21it/s] 23%|‚ñà‚ñà‚ñé       | 750/3257 [00:03<00:13, 186.06it/s] 24%|‚ñà‚ñà‚ñé       | 773/3257 [00:04<00:12, 195.26it/s] 24%|‚ñà‚ñà‚ñç       | 793/3257 [00:04<00:12, 196.50it/s] 25%|‚ñà‚ñà‚ñç       | 814/3257 [00:04<00:12, 197.88it/s] 26%|‚ñà‚ñà‚ñå       | 834/3257 [00:04<00:12, 193.27it/s] 26%|‚ñà‚ñà‚ñå       | 854/3257 [00:04<00:13, 184.04it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:04<00:12, 187.71it/s] 27%|‚ñà‚ñà‚ñã       | 895/3257 [00:04<00:12, 189.23it/s] 28%|‚ñà‚ñà‚ñä       | 915/3257 [00:04<00:12, 192.17it/s] 29%|‚ñà‚ñà‚ñä       | 936/3257 [00:04<00:11, 194.09it/s] 29%|‚ñà‚ñà‚ñâ       | 959/3257 [00:04<00:11, 200.18it/s] 30%|‚ñà‚ñà‚ñà       | 980/3257 [00:05<00:11, 194.36it/s] 31%|‚ñà‚ñà‚ñà       | 1000/3257 [00:05<00:11, 193.68it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1020/3257 [00:05<00:11, 194.86it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1040/3257 [00:05<00:12, 183.15it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1059/3257 [00:05<00:12, 180.63it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:05<00:12, 178.84it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1100/3257 [00:05<00:11, 183.63it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1119/3257 [00:05<00:11, 185.37it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1139/3257 [00:05<00:11, 183.69it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1160/3257 [00:06<00:11, 189.43it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1180/3257 [00:06<00:11, 181.46it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1199/3257 [00:06<00:11, 177.61it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1217/3257 [00:06<00:11, 174.80it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:06<00:10, 192.48it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1262/3257 [00:06<00:10, 190.39it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1282/3257 [00:06<00:11, 177.14it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1302/3257 [00:06<00:10, 180.78it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1325/3257 [00:06<00:09, 194.33it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1346/3257 [00:07<00:09, 196.45it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1366/3257 [00:07<00:09, 190.85it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1386/3257 [00:07<00:10, 185.26it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1410/3257 [00:07<00:09, 199.09it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:07<00:08, 203.46it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1458/3257 [00:07<00:08, 216.42it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1480/3257 [00:07<00:08, 209.91it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1505/3257 [00:07<00:07, 220.61it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1528/3257 [00:07<00:08, 205.28it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1549/3257 [00:08<00:08, 198.21it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1570/3257 [00:08<00:08, 199.57it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1591/3257 [00:08<00:08, 198.34it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:08<00:07, 207.90it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1636/3257 [00:08<00:08, 194.83it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1657/3257 [00:08<00:08, 192.18it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1677/3257 [00:08<00:08, 186.15it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1698/3257 [00:08<00:08, 192.33it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1718/3257 [00:09<00:13, 115.25it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1734/3257 [00:09<00:12, 120.20it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1757/3257 [00:09<00:10, 141.79it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1777/3257 [00:09<00:09, 154.42it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1799/3257 [00:09<00:08, 169.43it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1820/3257 [00:09<00:08, 178.31it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1840/3257 [00:09<00:07, 180.79it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1860/3257 [00:09<00:07, 185.18it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1881/3257 [00:10<00:07, 192.03it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1901/3257 [00:10<00:06, 194.26it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1921/3257 [00:10<00:06, 191.49it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1948/3257 [00:10<00:06, 212.03it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1974/3257 [00:10<00:05, 219.76it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1997/3257 [00:10<00:05, 211.74it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2019/3257 [00:10<00:05, 210.79it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2041/3257 [00:10<00:06, 199.13it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2062/3257 [00:10<00:06, 189.93it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2084/3257 [00:11<00:05, 196.25it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2104/3257 [00:11<00:06, 191.34it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2124/3257 [00:11<00:06, 181.23it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2143/3257 [00:11<00:06, 176.10it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2166/3257 [00:11<00:05, 188.34it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2186/3257 [00:11<00:05, 187.86it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2207/3257 [00:11<00:05, 193.16it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2227/3257 [00:11<00:05, 190.62it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2247/3257 [00:11<00:05, 188.11it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2267/3257 [00:12<00:05, 190.45it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2287/3257 [00:12<00:05, 189.92it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2307/3257 [00:12<00:05, 184.99it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2334/3257 [00:12<00:04, 208.20it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:12<00:04, 213.58it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2384/3257 [00:12<00:03, 220.25it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2407/3257 [00:12<00:03, 213.83it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2429/3257 [00:12<00:03, 211.34it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2451/3257 [00:12<00:04, 196.67it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2478/3257 [00:13<00:03, 212.74it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2504/3257 [00:13<00:03, 225.24it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2527/3257 [00:13<00:03, 224.08it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2550/3257 [00:13<00:03, 220.73it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2573/3257 [00:13<00:03, 201.98it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2594/3257 [00:13<00:03, 197.48it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2622/3257 [00:13<00:02, 219.25it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2645/3257 [00:13<00:02, 214.35it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2667/3257 [00:13<00:02, 207.82it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2690/3257 [00:14<00:02, 211.81it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2712/3257 [00:14<00:02, 187.79it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2735/3257 [00:14<00:02, 198.77it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2758/3257 [00:14<00:02, 205.85it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2780/3257 [00:14<00:02, 201.99it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:14<00:02, 213.50it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2828/3257 [00:14<00:02, 200.58it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2849/3257 [00:14<00:02, 201.49it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2878/3257 [00:14<00:01, 225.04it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2901/3257 [00:15<00:01, 203.03it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2927/3257 [00:15<00:01, 217.59it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:15<00:01, 197.08it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2972/3257 [00:15<00:01, 202.06it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2993/3257 [00:15<00:01, 195.93it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3017/3257 [00:15<00:01, 206.08it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3040/3257 [00:15<00:01, 211.87it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3066/3257 [00:15<00:00, 223.46it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3089/3257 [00:15<00:00, 221.16it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3116/3257 [00:16<00:00, 232.81it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3140/3257 [00:16<00:00, 117.52it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3160/3257 [00:16<00:00, 131.08it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3179/3257 [00:16<00:00, 138.40it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3203/3257 [00:16<00:00, 159.23it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3223/3257 [00:16<00:00, 161.45it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3250/3257 [00:17<00:00, 186.31it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:17<00:00, 190.79it/s]
2023-02-07 19:30:28.459 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:30:28,461][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d373,n5,mc2,s0.507168,t4>', 'datetime': '2023-02-07T19:30:28.461659', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:30:28,462][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:30:28,462][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:30:29,041][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 19:30:29,042][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:30:29,142][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 36619 unique words (85.76% of original 42701, drops 6082)', 'datetime': '2023-02-07T19:30:29.142642', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:30:29,143][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 5816910 word corpus (99.90% of original 5822992, drops 6082)', 'datetime': '2023-02-07T19:30:29.143405', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:30:29,263][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 19:30:29,265][gensim.models.word2vec][INFO] - sample=0.507168 downsamples 0 most-common words
[2023-02-07 19:30:29,266][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5816910 word corpus (100.0%% of prior 5816910)', 'datetime': '2023-02-07T19:30:29.266170', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:30:29,473][gensim.models.word2vec][INFO] - estimated required memory for 36619 words and 373 dimensions: 133091440 bytes
[2023-02-07 19:30:29,474][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:30:29,530][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 36619 vocabulary and 373 features, using sg=1 hs=0 sample=0.5071679181864567 negative=5 window=7 shrink_windows=True', 'datetime': '2023-02-07T19:30:29.530247', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:30:30,539][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 21.58% examples, 1229366 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:31,540][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 42.43% examples, 1262896 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:30:32,540][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 65.21% examples, 1276738 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:33,541][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 88.58% examples, 1286366 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:34,032][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5790320 effective words) took 4.5s, 1287527 effective words/s
[2023-02-07 19:30:35,043][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 22.35% examples, 1269819 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:30:36,053][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 44.03% examples, 1294847 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:37,063][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 66.84% examples, 1303541 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:38,067][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 91.77% examples, 1328203 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:38,362][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5790320 effective words) took 4.3s, 1338088 effective words/s
[2023-02-07 19:30:39,364][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 31.69% examples, 1850512 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:40,370][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 63.37% examples, 1862864 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:41,375][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 96.56% examples, 1859561 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:30:41,469][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5790320 effective words) took 3.1s, 1864412 effective words/s
[2023-02-07 19:30:42,475][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 25.79% examples, 1486997 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:43,486][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 56.22% examples, 1657583 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:44,485][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 85.78% examples, 1661494 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:44,908][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5790320 effective words) took 3.4s, 1684914 effective words/s
[2023-02-07 19:30:45,920][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 24.04% examples, 1375222 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:46,921][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 47.62% examples, 1400812 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:47,922][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 72.77% examples, 1421181 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:30:48,925][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 99.60% examples, 1437004 words/s, in_qsize 3, out_qsize 1
[2023-02-07 19:30:48,937][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5790320 effective words) took 4.0s, 1438405 effective words/s
[2023-02-07 19:30:49,949][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 31.65% examples, 1821900 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:50,950][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 62.70% examples, 1829540 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:51,955][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 94.38% examples, 1816375 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:52,119][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5790320 effective words) took 3.2s, 1820781 effective words/s
[2023-02-07 19:30:53,128][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 24.99% examples, 1428415 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:54,139][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 48.51% examples, 1424312 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:55,147][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 73.20% examples, 1423507 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:56,150][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 98.50% examples, 1418785 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:56,197][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5790320 effective words) took 4.1s, 1422283 effective words/s
[2023-02-07 19:30:57,207][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 24.72% examples, 1409461 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:58,210][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 47.90% examples, 1410234 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:30:59,220][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 73.84% examples, 1437201 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:00,222][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 99.88% examples, 1438854 words/s, in_qsize 1, out_qsize 1
[2023-02-07 19:31:00,225][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5790320 effective words) took 4.0s, 1438835 effective words/s
[2023-02-07 19:31:01,229][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 24.65% examples, 1412319 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:02,230][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 47.93% examples, 1417751 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:03,231][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 72.64% examples, 1422175 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:31:04,232][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 97.88% examples, 1417941 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:04,307][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5790320 effective words) took 4.1s, 1419739 effective words/s
[2023-02-07 19:31:05,310][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 24.72% examples, 1417182 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:06,313][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 48.30% examples, 1423847 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:07,313][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 77.46% examples, 1508827 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:31:07,988][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5790320 effective words) took 3.7s, 1573950 effective words/s
[2023-02-07 19:31:08,992][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 25.67% examples, 1483081 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:09,994][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 50.02% examples, 1470202 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:11,008][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 74.76% examples, 1457807 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:31:11,978][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5790320 effective words) took 4.0s, 1452566 effective words/s
[2023-02-07 19:31:12,984][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 24.13% examples, 1379345 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:13,997][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 47.80% examples, 1399911 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:14,999][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 72.43% examples, 1409656 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:16,003][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 97.54% examples, 1407088 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:16,080][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5790320 effective words) took 4.1s, 1412136 effective words/s
[2023-02-07 19:31:17,089][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 25.36% examples, 1452640 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:18,096][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 49.65% examples, 1452979 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:19,098][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 74.76% examples, 1458642 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:31:20,032][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5790320 effective words) took 3.9s, 1466458 effective words/s
[2023-02-07 19:31:21,043][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 25.30% examples, 1447521 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:31:22,045][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 49.55% examples, 1452507 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:23,059][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 74.24% examples, 1442376 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:31:24,053][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5790320 effective words) took 4.0s, 1440528 effective words/s
[2023-02-07 19:31:25,063][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 25.73% examples, 1481608 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:26,065][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 50.38% examples, 1479737 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:27,074][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 75.19% examples, 1466579 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:31:28,018][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5790320 effective words) took 4.0s, 1461604 effective words/s
[2023-02-07 19:31:28,018][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86854800 effective words) took 58.5s, 1485028 effective words/s', 'datetime': '2023-02-07T19:31:28.018827', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:31:28.019 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:31:32,290][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193002-z0xawavg/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:31:32.290427', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:31:32,291][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193002-z0xawavg/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 19:31:32,349][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193002-z0xawavg/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 19:31:32,399][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:31:32,424][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193002-z0xawavg/files/../tmp/embedding_model.pt
2023-02-07 19:31:32.425 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:31:34.453 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:31:35.121 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:31:38.086 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.227773480074624, 'test_mae': 1.109167717484346, 'test_r2': -1.713302529835711}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.43
wandb: percentage 0.14243
wandb:   test_mae 1.10917
wandb:   test_mse 2.22777
wandb:    test_r2 -1.7133
wandb: 
wandb: üöÄ View run rich-sweep-49 at: https://wandb.ai/xiaoqiz/mof2vec/runs/z0xawavg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_193002-z0xawavg/logs
wandb: Agent Starting Run: n9dbwr5z with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 612
wandb: 	model.gensim.alpha: 0.013116484725214385
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.9474260860976798
wandb: 	model.gensim.vector_size: 315
wandb: 	model.gensim.window: 6
wandb: 	model.sklearn.learning_rate: 0.07956562088975525
wandb: 	model.sklearn.max_depth: 45
wandb: 	model.sklearn.min_child_weight: 0.029157136246906468
wandb: 	model.sklearn.n_estimators: 647
wandb: 	model.sklearn.num_leaves: 495
wandb: 	model.sklearn.reg_alpha: 0.07547862450617032
wandb: 	model.sklearn.reg_lambda: 0.2533451254340283
wandb: 	model.sklearn.subsample: 0.6899624150069245
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193151-n9dbwr5z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-50
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/n9dbwr5z
2023-02-07 19:31:59.873 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 19:31:59.874 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 612 for sweep.
2023-02-07 19:31:59.874 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.013116484725214385 for sweep.
2023-02-07 19:31:59.874 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:31:59.875 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 19:31:59.875 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.9474260860976798 for sweep.
2023-02-07 19:31:59.875 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 315 for sweep.
2023-02-07 19:31:59.875 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 6 for sweep.
2023-02-07 19:31:59.875 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.07956562088975525 for sweep.
2023-02-07 19:31:59.876 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 45 for sweep.
2023-02-07 19:31:59.876 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.029157136246906468 for sweep.
2023-02-07 19:31:59.876 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 647 for sweep.
2023-02-07 19:31:59.876 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 495 for sweep.
2023-02-07 19:31:59.877 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.07547862450617032 for sweep.
2023-02-07 19:31:59.877 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.2533451254340283 for sweep.
2023-02-07 19:31:59.877 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6899624150069245 for sweep.
2023-02-07 19:31:59.877 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:31:59.886 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193151-n9dbwr5z/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 612, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 315, 'window': 6, 'min_count': 4, 'dm': 0, 'sample': 0.9474260860976798, 'workers': 4, 'alpha': 0.013116484725214385, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 647, 'max_depth': 45, 'num_leaves': 495, 'reg_alpha': 0.07547862450617032, 'reg_lambda': 0.2533451254340283, 'subsample': 0.6899624150069245, 'min_child_weight': 0.029157136246906468, 'n_jobs': 4, 'learning_rate': 0.07956562088975525}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:20, 158.85it/s]  1%|          | 34/3257 [00:00<00:19, 163.27it/s]  2%|‚ñè         | 53/3257 [00:00<00:18, 173.38it/s]  2%|‚ñè         | 74/3257 [00:00<00:17, 184.57it/s]  3%|‚ñé         | 94/3257 [00:00<00:16, 186.42it/s]  3%|‚ñé         | 113/3257 [00:00<00:17, 178.69it/s]  4%|‚ñç         | 131/3257 [00:00<00:17, 178.32it/s]  5%|‚ñç         | 151/3257 [00:00<00:16, 183.31it/s]  5%|‚ñå         | 170/3257 [00:00<00:17, 180.80it/s]  6%|‚ñå         | 190/3257 [00:01<00:16, 184.14it/s]  6%|‚ñã         | 210/3257 [00:01<00:16, 186.66it/s]  7%|‚ñã         | 232/3257 [00:01<00:15, 195.53it/s]  8%|‚ñä         | 252/3257 [00:01<00:15, 194.77it/s]  8%|‚ñä         | 272/3257 [00:01<00:15, 189.02it/s]  9%|‚ñâ         | 297/3257 [00:01<00:14, 203.19it/s] 10%|‚ñâ         | 318/3257 [00:01<00:15, 194.27it/s] 10%|‚ñà         | 338/3257 [00:01<00:14, 194.61it/s] 11%|‚ñà         | 359/3257 [00:01<00:14, 193.92it/s] 12%|‚ñà‚ñè        | 379/3257 [00:02<00:16, 178.54it/s] 12%|‚ñà‚ñè        | 398/3257 [00:02<00:15, 180.42it/s] 13%|‚ñà‚ñé        | 419/3257 [00:02<00:15, 188.45it/s] 13%|‚ñà‚ñé        | 439/3257 [00:02<00:17, 158.14it/s] 14%|‚ñà‚ñç        | 459/3257 [00:02<00:16, 168.33it/s] 15%|‚ñà‚ñç        | 479/3257 [00:02<00:16, 171.52it/s] 15%|‚ñà‚ñå        | 501/3257 [00:02<00:15, 182.48it/s] 16%|‚ñà‚ñå        | 521/3257 [00:02<00:14, 187.01it/s] 17%|‚ñà‚ñã        | 541/3257 [00:02<00:14, 185.19it/s] 17%|‚ñà‚ñã        | 560/3257 [00:03<00:15, 171.26it/s] 18%|‚ñà‚ñä        | 578/3257 [00:03<00:16, 163.56it/s] 18%|‚ñà‚ñä        | 600/3257 [00:03<00:14, 178.37it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:14, 177.72it/s] 20%|‚ñà‚ñâ        | 641/3257 [00:03<00:14, 186.45it/s] 20%|‚ñà‚ñà        | 660/3257 [00:03<00:15, 170.05it/s] 21%|‚ñà‚ñà        | 682/3257 [00:03<00:14, 182.92it/s] 22%|‚ñà‚ñà‚ñè       | 701/3257 [00:03<00:14, 172.62it/s] 22%|‚ñà‚ñà‚ñè       | 720/3257 [00:03<00:14, 176.55it/s] 23%|‚ñà‚ñà‚ñé       | 738/3257 [00:04<00:15, 166.19it/s] 23%|‚ñà‚ñà‚ñé       | 759/3257 [00:04<00:14, 177.84it/s] 24%|‚ñà‚ñà‚ñç       | 778/3257 [00:04<00:14, 177.00it/s] 24%|‚ñà‚ñà‚ñç       | 797/3257 [00:04<00:13, 180.61it/s] 25%|‚ñà‚ñà‚ñå       | 816/3257 [00:04<00:13, 182.20it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:04<00:13, 178.14it/s] 26%|‚ñà‚ñà‚ñå       | 853/3257 [00:04<00:14, 163.46it/s] 27%|‚ñà‚ñà‚ñã       | 873/3257 [00:04<00:13, 170.59it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:04<00:14, 165.23it/s] 28%|‚ñà‚ñà‚ñä       | 912/3257 [00:05<00:13, 175.74it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:05<00:13, 176.62it/s] 29%|‚ñà‚ñà‚ñâ       | 951/3257 [00:05<00:12, 183.13it/s] 30%|‚ñà‚ñà‚ñâ       | 970/3257 [00:05<00:12, 185.09it/s] 30%|‚ñà‚ñà‚ñà       | 989/3257 [00:05<00:12, 182.48it/s] 31%|‚ñà‚ñà‚ñà       | 1008/3257 [00:05<00:12, 179.45it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:05<00:12, 179.36it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1045/3257 [00:05<00:12, 178.33it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:05<00:11, 186.56it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1086/3257 [00:06<00:18, 120.20it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1105/3257 [00:06<00:16, 134.31it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1123/3257 [00:06<00:14, 143.92it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1140/3257 [00:06<00:14, 150.21it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1161/3257 [00:06<00:12, 164.20it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1179/3257 [00:06<00:12, 162.45it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1197/3257 [00:06<00:12, 161.10it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1214/3257 [00:06<00:12, 158.94it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1238/3257 [00:07<00:11, 179.50it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1257/3257 [00:07<00:11, 178.46it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1276/3257 [00:07<00:11, 176.90it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1294/3257 [00:07<00:11, 172.80it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1313/3257 [00:07<00:10, 177.44it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1334/3257 [00:07<00:10, 186.62it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1353/3257 [00:07<00:10, 184.45it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1372/3257 [00:07<00:10, 178.55it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1390/3257 [00:07<00:10, 178.30it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1416/3257 [00:08<00:09, 200.72it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1437/3257 [00:08<00:09, 193.69it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1460/3257 [00:08<00:08, 203.11it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1481/3257 [00:08<00:08, 200.49it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1504/3257 [00:08<00:08, 208.67it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1525/3257 [00:08<00:08, 192.53it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1545/3257 [00:08<00:09, 181.01it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1565/3257 [00:08<00:09, 183.30it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1584/3257 [00:08<00:09, 183.26it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1604/3257 [00:09<00:08, 186.89it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1623/3257 [00:09<00:08, 186.60it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1642/3257 [00:09<00:08, 180.75it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1661/3257 [00:09<00:09, 176.90it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1679/3257 [00:09<00:08, 177.57it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1699/3257 [00:09<00:08, 183.66it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1718/3257 [00:09<00:08, 180.20it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1737/3257 [00:09<00:09, 166.73it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1758/3257 [00:09<00:08, 178.15it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1777/3257 [00:10<00:08, 179.91it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1797/3257 [00:10<00:07, 185.49it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1816/3257 [00:10<00:07, 181.04it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1835/3257 [00:10<00:07, 181.03it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1855/3257 [00:10<00:07, 186.02it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1876/3257 [00:10<00:07, 191.26it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1896/3257 [00:10<00:07, 185.14it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:10<00:07, 187.19it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1938/3257 [00:10<00:06, 194.61it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:10<00:06, 210.80it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1987/3257 [00:11<00:06, 200.45it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2008/3257 [00:11<00:06, 199.31it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2032/3257 [00:11<00:05, 209.62it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2054/3257 [00:11<00:06, 186.29it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2074/3257 [00:11<00:06, 183.96it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2093/3257 [00:11<00:06, 181.71it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2112/3257 [00:11<00:06, 175.02it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2130/3257 [00:11<00:06, 165.20it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2147/3257 [00:12<00:06, 162.00it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:12<00:06, 174.92it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2187/3257 [00:12<00:10, 106.83it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2208/3257 [00:12<00:08, 126.24it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2226/3257 [00:12<00:07, 137.68it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2243/3257 [00:12<00:07, 144.74it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2263/3257 [00:12<00:06, 158.42it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2281/3257 [00:12<00:06, 160.16it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2301/3257 [00:13<00:05, 167.94it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2326/3257 [00:13<00:04, 187.61it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2352/3257 [00:13<00:04, 206.73it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2374/3257 [00:13<00:04, 201.76it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2397/3257 [00:13<00:04, 209.19it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2419/3257 [00:13<00:04, 197.36it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2440/3257 [00:13<00:04, 191.33it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2460/3257 [00:13<00:04, 191.28it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2482/3257 [00:13<00:03, 198.84it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2507/3257 [00:14<00:03, 210.58it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2529/3257 [00:14<00:03, 212.04it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:14<00:03, 207.48it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2572/3257 [00:14<00:03, 185.58it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2592/3257 [00:14<00:03, 181.24it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2620/3257 [00:14<00:03, 206.04it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2642/3257 [00:14<00:02, 206.49it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2663/3257 [00:14<00:03, 195.28it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2684/3257 [00:14<00:02, 199.23it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2705/3257 [00:15<00:03, 183.01it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2724/3257 [00:15<00:02, 179.11it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2748/3257 [00:15<00:02, 195.24it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:15<00:02, 189.32it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2790/3257 [00:15<00:02, 197.70it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2811/3257 [00:15<00:02, 197.38it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2831/3257 [00:15<00:02, 185.29it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2852/3257 [00:15<00:02, 191.83it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:15<00:01, 213.32it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2903/3257 [00:16<00:01, 196.53it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2927/3257 [00:16<00:01, 207.94it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2949/3257 [00:16<00:01, 188.95it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2970/3257 [00:16<00:01, 193.85it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2990/3257 [00:16<00:01, 181.85it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3013/3257 [00:16<00:01, 192.77it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3035/3257 [00:16<00:01, 199.99it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3058/3257 [00:16<00:00, 207.87it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3081/3257 [00:16<00:00, 210.14it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3104/3257 [00:17<00:00, 213.68it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3127/3257 [00:17<00:00, 216.30it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3149/3257 [00:17<00:00, 200.72it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3170/3257 [00:17<00:00, 201.62it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3191/3257 [00:17<00:00, 197.79it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3211/3257 [00:17<00:00, 195.23it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3234/3257 [00:17<00:00, 202.93it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3255/3257 [00:17<00:00, 200.99it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:17<00:00, 182.46it/s]
2023-02-07 19:32:18.454 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:32:18,455][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d315,n5,mc4,s0.947426,t4>', 'datetime': '2023-02-07T19:32:18.455105', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:32:18,455][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:32:18,455][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:32:18,973][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 19:32:18,973][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:32:19,049][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 29175 unique words (68.32% of original 42701, drops 13526)', 'datetime': '2023-02-07T19:32:19.049331', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:32:19,049][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 5800681 word corpus (99.62% of original 5822992, drops 22311)', 'datetime': '2023-02-07T19:32:19.049732', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:32:19,143][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 19:32:19,144][gensim.models.word2vec][INFO] - sample=0.947426 downsamples 0 most-common words
[2023-02-07 19:32:19,144][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5800681 word corpus (100.0%% of prior 5800681)', 'datetime': '2023-02-07T19:32:19.144497', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:32:19,299][gensim.models.word2vec][INFO] - estimated required memory for 29175 words and 315 dimensions: 92863720 bytes
[2023-02-07 19:32:19,299][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:32:19,333][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 29175 vocabulary and 315 features, using sg=1 hs=0 sample=0.9474260860976798 negative=5 window=6 shrink_windows=True', 'datetime': '2023-02-07T19:32:19.332999', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:32:20,341][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 23.40% examples, 1330461 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:21,346][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 47.37% examples, 1387255 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:32:22,355][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 72.98% examples, 1415787 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:23,344][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5774537 effective words) took 4.0s, 1440422 effective words/s
[2023-02-07 19:32:24,350][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 29.97% examples, 1725731 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:25,357][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 58.21% examples, 1709498 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:26,356][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 86.00% examples, 1662414 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:26,904][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5774537 effective words) took 3.6s, 1623052 effective words/s
[2023-02-07 19:32:27,908][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 24.81% examples, 1423092 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:32:28,920][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 48.33% examples, 1416069 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:29,930][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 73.20% examples, 1418938 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:30,932][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 98.62% examples, 1416870 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:30,981][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5774537 effective words) took 4.1s, 1417474 effective words/s
[2023-02-07 19:32:31,983][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 32.51% examples, 1899509 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:32:32,984][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 65.21% examples, 1911565 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:33,989][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 98.68% examples, 1900116 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:34,019][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5774537 effective words) took 3.0s, 1901780 effective words/s
[2023-02-07 19:32:35,029][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 25.79% examples, 1477754 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:36,031][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 50.32% examples, 1474659 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:37,032][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 76.51% examples, 1486198 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:37,757][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5774537 effective words) took 3.7s, 1545444 effective words/s
[2023-02-07 19:32:38,761][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 37.12% examples, 2195809 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:39,764][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 74.33% examples, 2172123 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:40,424][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5774537 effective words) took 2.7s, 2168457 effective words/s
[2023-02-07 19:32:41,426][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 36.51% examples, 2157550 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:42,427][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 72.92% examples, 2136997 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:43,139][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5774537 effective words) took 2.7s, 2127889 effective words/s
[2023-02-07 19:32:44,141][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 36.48% examples, 2148413 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:45,144][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 65.06% examples, 1908434 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:46,153][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 97.11% examples, 1865096 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:32:46,234][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5774537 effective words) took 3.1s, 1866912 effective words/s
[2023-02-07 19:32:47,244][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 31.65% examples, 1832323 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:32:48,240][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 63.22% examples, 1855050 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:49,248][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 96.56% examples, 1856066 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:49,343][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5774537 effective words) took 3.1s, 1859305 effective words/s
[2023-02-07 19:32:50,345][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 36.32% examples, 2134939 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:32:51,347][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 67.36% examples, 1983486 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:52,308][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5774537 effective words) took 3.0s, 1948557 effective words/s
[2023-02-07 19:32:53,312][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 37.76% examples, 2228159 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:32:54,313][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 70.43% examples, 2077454 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:55,173][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5774537 effective words) took 2.9s, 2016939 effective words/s
[2023-02-07 19:32:56,175][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 31.90% examples, 1863490 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:32:57,180][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 63.89% examples, 1871153 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:58,183][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 97.11% examples, 1867395 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:32:58,262][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5774537 effective words) took 3.1s, 1870663 effective words/s
[2023-02-07 19:32:59,266][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 32.36% examples, 1888470 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:33:00,272][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 64.54% examples, 1890668 words/s, in_qsize 7, out_qsize 1
[2023-02-07 19:33:01,275][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 98.68% examples, 1897196 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:33:01,301][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5774537 effective words) took 3.0s, 1901290 effective words/s
[2023-02-07 19:33:02,304][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 32.36% examples, 1888873 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:33:03,304][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 64.91% examples, 1904856 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:33:04,307][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 98.34% examples, 1892580 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:33:04,342][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5774537 effective words) took 3.0s, 1899575 effective words/s
[2023-02-07 19:33:05,348][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 39.94% examples, 2361247 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:33:06,349][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 82.68% examples, 2399302 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:33:06,754][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5774537 effective words) took 2.4s, 2396108 effective words/s
[2023-02-07 19:33:06,754][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86618055 effective words) took 47.4s, 1826559 effective words/s', 'datetime': '2023-02-07T19:33:06.754868', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:33:06.756 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:33:10,324][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193151-n9dbwr5z/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:33:10.324622', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:33:10,325][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:33:10,448][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193151-n9dbwr5z/files/../tmp/embedding_model.pt
2023-02-07 19:33:10.449 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:33:12.295 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:33:12.923 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:33:15.656 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1191342936957653, 'test_mae': 1.064346287052427, 'test_r2': -2.0677295893091054}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.032 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.032 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.87
wandb: percentage 0.31676
wandb:   test_mae 1.06435
wandb:   test_mse 2.11913
wandb:    test_r2 -2.06773
wandb: 
wandb: üöÄ View run revived-sweep-50 at: https://wandb.ai/xiaoqiz/mof2vec/runs/n9dbwr5z
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_193151-n9dbwr5z/logs
wandb: Agent Starting Run: xzs6mc2f with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 784
wandb: 	model.gensim.alpha: 0.013967489580110385
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.6747864598986598
wandb: 	model.gensim.vector_size: 53
wandb: 	model.gensim.window: 8
wandb: 	model.sklearn.learning_rate: 0.7314819455144044
wandb: 	model.sklearn.max_depth: 91
wandb: 	model.sklearn.min_child_weight: 0.04180666712477868
wandb: 	model.sklearn.n_estimators: 1393
wandb: 	model.sklearn.num_leaves: 350
wandb: 	model.sklearn.reg_alpha: 0.1390827515283619
wandb: 	model.sklearn.reg_lambda: 0.338242280987561
wandb: 	model.sklearn.subsample: 0.3620846583449283
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193327-xzs6mc2f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-sweep-51
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/xzs6mc2f
2023-02-07 19:33:34.880 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 19:33:34.881 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 784 for sweep.
2023-02-07 19:33:34.881 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.013967489580110385 for sweep.
2023-02-07 19:33:34.881 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:33:34.882 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 19:33:34.882 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6747864598986598 for sweep.
2023-02-07 19:33:34.882 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 53 for sweep.
2023-02-07 19:33:34.883 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 8 for sweep.
2023-02-07 19:33:34.883 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.7314819455144044 for sweep.
2023-02-07 19:33:34.883 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 91 for sweep.
2023-02-07 19:33:34.883 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.04180666712477868 for sweep.
2023-02-07 19:33:34.884 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1393 for sweep.
2023-02-07 19:33:34.884 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 350 for sweep.
2023-02-07 19:33:34.884 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.1390827515283619 for sweep.
2023-02-07 19:33:34.884 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.338242280987561 for sweep.
2023-02-07 19:33:34.885 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3620846583449283 for sweep.
2023-02-07 19:33:34.885 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:33:34.892 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193327-xzs6mc2f/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 784, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 53, 'window': 8, 'min_count': 3, 'dm': 0, 'sample': 0.6747864598986598, 'workers': 4, 'alpha': 0.013967489580110385, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1393, 'max_depth': 91, 'num_leaves': 350, 'reg_alpha': 0.1390827515283619, 'reg_lambda': 0.338242280987561, 'subsample': 0.3620846583449283, 'min_child_weight': 0.04180666712477868, 'n_jobs': 4, 'learning_rate': 0.7314819455144044}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 16/3257 [00:00<00:21, 153.31it/s]  1%|          | 34/3257 [00:00<00:20, 156.17it/s]  2%|‚ñè         | 50/3257 [00:00<00:21, 148.93it/s]  2%|‚ñè         | 66/3257 [00:00<00:21, 151.26it/s]  3%|‚ñé         | 82/3257 [00:00<00:21, 148.61it/s]  3%|‚ñé         | 97/3257 [00:00<00:22, 141.84it/s]  3%|‚ñé         | 112/3257 [00:00<00:23, 131.17it/s]  4%|‚ñç         | 127/3257 [00:00<00:23, 135.32it/s]  4%|‚ñç         | 145/3257 [00:01<00:21, 146.91it/s]  5%|‚ñç         | 160/3257 [00:01<00:21, 142.12it/s]  5%|‚ñå         | 175/3257 [00:01<00:21, 142.39it/s]  6%|‚ñå         | 193/3257 [00:01<00:20, 152.51it/s]  6%|‚ñã         | 211/3257 [00:01<00:18, 160.33it/s]  7%|‚ñã         | 232/3257 [00:01<00:17, 173.55it/s]  8%|‚ñä         | 250/3257 [00:01<00:17, 172.58it/s]  8%|‚ñä         | 268/3257 [00:01<00:18, 161.68it/s]  9%|‚ñâ         | 290/3257 [00:01<00:16, 176.31it/s]  9%|‚ñâ         | 308/3257 [00:01<00:17, 172.02it/s] 10%|‚ñà         | 326/3257 [00:02<00:23, 122.14it/s] 10%|‚ñà         | 341/3257 [00:02<00:23, 123.74it/s] 11%|‚ñà         | 360/3257 [00:02<00:21, 137.92it/s] 12%|‚ñà‚ñè        | 376/3257 [00:02<00:20, 139.58it/s] 12%|‚ñà‚ñè        | 391/3257 [00:02<00:20, 140.68it/s] 13%|‚ñà‚ñé        | 408/3257 [00:02<00:19, 147.69it/s] 13%|‚ñà‚ñé        | 424/3257 [00:02<00:19, 148.79it/s] 14%|‚ñà‚ñé        | 440/3257 [00:03<00:20, 135.44it/s] 14%|‚ñà‚ñç        | 458/3257 [00:03<00:19, 145.58it/s] 15%|‚ñà‚ñç        | 475/3257 [00:03<00:18, 150.35it/s] 15%|‚ñà‚ñå        | 491/3257 [00:03<00:18, 152.71it/s] 16%|‚ñà‚ñå        | 511/3257 [00:03<00:16, 163.85it/s] 16%|‚ñà‚ñå        | 528/3257 [00:03<00:17, 155.51it/s] 17%|‚ñà‚ñã        | 546/3257 [00:03<00:17, 158.15it/s] 17%|‚ñà‚ñã        | 562/3257 [00:03<00:18, 149.01it/s] 18%|‚ñà‚ñä        | 578/3257 [00:03<00:18, 141.59it/s] 18%|‚ñà‚ñä        | 597/3257 [00:04<00:17, 152.97it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:04<00:16, 156.90it/s] 19%|‚ñà‚ñâ        | 631/3257 [00:04<00:16, 155.71it/s] 20%|‚ñà‚ñâ        | 647/3257 [00:04<00:17, 151.15it/s] 20%|‚ñà‚ñà        | 663/3257 [00:04<00:18, 140.24it/s] 21%|‚ñà‚ñà        | 680/3257 [00:04<00:17, 147.92it/s] 21%|‚ñà‚ñà‚ñè       | 696/3257 [00:04<00:17, 142.45it/s] 22%|‚ñà‚ñà‚ñè       | 717/3257 [00:04<00:16, 156.62it/s] 23%|‚ñà‚ñà‚ñé       | 733/3257 [00:04<00:16, 151.11it/s] 23%|‚ñà‚ñà‚ñé       | 749/3257 [00:05<00:16, 149.05it/s] 24%|‚ñà‚ñà‚ñé       | 769/3257 [00:05<00:15, 161.97it/s] 24%|‚ñà‚ñà‚ñç       | 786/3257 [00:05<00:16, 154.36it/s] 25%|‚ñà‚ñà‚ñç       | 805/3257 [00:05<00:15, 163.40it/s] 25%|‚ñà‚ñà‚ñå       | 822/3257 [00:05<00:15, 161.33it/s] 26%|‚ñà‚ñà‚ñå       | 839/3257 [00:05<00:15, 157.56it/s] 26%|‚ñà‚ñà‚ñã       | 855/3257 [00:05<00:15, 153.53it/s] 27%|‚ñà‚ñà‚ñã       | 873/3257 [00:05<00:15, 157.96it/s] 27%|‚ñà‚ñà‚ñã       | 889/3257 [00:05<00:15, 157.77it/s] 28%|‚ñà‚ñà‚ñä       | 907/3257 [00:06<00:14, 163.52it/s] 28%|‚ñà‚ñà‚ñä       | 925/3257 [00:06<00:13, 167.75it/s] 29%|‚ñà‚ñà‚ñâ       | 942/3257 [00:06<00:14, 161.14it/s] 29%|‚ñà‚ñà‚ñâ       | 960/3257 [00:06<00:13, 164.51it/s] 30%|‚ñà‚ñà‚ñà       | 978/3257 [00:06<00:13, 167.03it/s] 31%|‚ñà‚ñà‚ñà       | 995/3257 [00:06<00:14, 158.85it/s] 31%|‚ñà‚ñà‚ñà       | 1012/3257 [00:06<00:13, 160.86it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1029/3257 [00:06<00:13, 159.26it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1045/3257 [00:06<00:14, 153.90it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1061/3257 [00:06<00:14, 151.77it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:07<00:14, 153.06it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1098/3257 [00:07<00:13, 158.39it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:07<00:13, 162.54it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1134/3257 [00:07<00:13, 159.11it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1150/3257 [00:07<00:13, 153.99it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:07<00:12, 165.81it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1187/3257 [00:07<00:13, 156.43it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1203/3257 [00:07<00:14, 145.44it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1218/3257 [00:07<00:13, 145.84it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1240/3257 [00:08<00:12, 165.61it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1257/3257 [00:08<00:12, 157.74it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1274/3257 [00:08<00:12, 158.90it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1291/3257 [00:08<00:13, 143.32it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1307/3257 [00:08<00:13, 146.65it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1325/3257 [00:08<00:12, 153.66it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1343/3257 [00:08<00:11, 160.65it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1360/3257 [00:08<00:12, 155.33it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1376/3257 [00:09<00:12, 152.42it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1392/3257 [00:09<00:12, 151.18it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1413/3257 [00:09<00:11, 165.83it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:09<00:10, 172.24it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1452/3257 [00:09<00:10, 178.94it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1473/3257 [00:09<00:09, 185.33it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1493/3257 [00:09<00:09, 187.46it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1512/3257 [00:09<00:09, 186.60it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1531/3257 [00:09<00:10, 169.25it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1549/3257 [00:09<00:10, 162.50it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1566/3257 [00:10<00:16, 100.24it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1581/3257 [00:10<00:15, 109.39it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1601/3257 [00:10<00:12, 128.52it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:10<00:11, 138.61it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1635/3257 [00:10<00:11, 141.19it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1651/3257 [00:10<00:11, 144.78it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1667/3257 [00:10<00:10, 147.19it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1683/3257 [00:11<00:10, 150.52it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1701/3257 [00:11<00:09, 158.27it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1718/3257 [00:11<00:09, 160.65it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1735/3257 [00:11<00:10, 146.65it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1754/3257 [00:11<00:09, 156.41it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1774/3257 [00:11<00:09, 163.18it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1794/3257 [00:11<00:08, 170.20it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1812/3257 [00:11<00:08, 162.64it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1829/3257 [00:11<00:08, 161.70it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1849/3257 [00:12<00:08, 169.45it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:12<00:07, 176.52it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1887/3257 [00:12<00:08, 171.11it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1905/3257 [00:12<00:07, 170.74it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1923/3257 [00:12<00:08, 163.35it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1945/3257 [00:12<00:07, 176.51it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:12<00:07, 180.13it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1984/3257 [00:12<00:07, 167.13it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2002/3257 [00:12<00:07, 169.97it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2020/3257 [00:13<00:07, 167.19it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2037/3257 [00:13<00:07, 163.83it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2054/3257 [00:13<00:08, 149.58it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2070/3257 [00:13<00:08, 141.99it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2085/3257 [00:13<00:08, 141.99it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2100/3257 [00:13<00:08, 140.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2118/3257 [00:13<00:07, 149.88it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2134/3257 [00:13<00:08, 136.31it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2148/3257 [00:14<00:08, 132.73it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2166/3257 [00:14<00:07, 143.85it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:14<00:07, 142.30it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2198/3257 [00:14<00:07, 147.84it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2213/3257 [00:14<00:07, 137.71it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2230/3257 [00:14<00:07, 145.58it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2245/3257 [00:14<00:07, 138.97it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2260/3257 [00:14<00:07, 140.96it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2275/3257 [00:14<00:07, 133.72it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2295/3257 [00:15<00:06, 150.97it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2311/3257 [00:15<00:06, 147.61it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2332/3257 [00:15<00:05, 163.60it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2353/3257 [00:15<00:05, 174.92it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2371/3257 [00:15<00:05, 167.58it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2391/3257 [00:15<00:04, 175.33it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2409/3257 [00:15<00:05, 156.95it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2426/3257 [00:15<00:05, 152.98it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2442/3257 [00:15<00:05, 142.27it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2458/3257 [00:16<00:05, 144.89it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2475/3257 [00:16<00:05, 151.30it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2491/3257 [00:16<00:05, 152.92it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2509/3257 [00:16<00:04, 159.66it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2526/3257 [00:16<00:04, 156.41it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2542/3257 [00:16<00:04, 155.50it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2558/3257 [00:16<00:04, 150.28it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2574/3257 [00:16<00:04, 140.15it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2589/3257 [00:16<00:04, 135.14it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2609/3257 [00:17<00:04, 152.10it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:17<00:03, 167.58it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2649/3257 [00:17<00:03, 163.51it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2666/3257 [00:17<00:03, 160.66it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2685/3257 [00:17<00:03, 167.34it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2702/3257 [00:17<00:03, 149.17it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2718/3257 [00:17<00:03, 149.72it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2741/3257 [00:17<00:03, 169.98it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2760/3257 [00:17<00:02, 174.84it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2778/3257 [00:18<00:02, 166.56it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2801/3257 [00:18<00:02, 183.36it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2820/3257 [00:18<00:02, 172.05it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2838/3257 [00:18<00:02, 163.63it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2860/3257 [00:18<00:02, 178.18it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2882/3257 [00:18<00:01, 189.34it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2902/3257 [00:18<00:02, 170.79it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2924/3257 [00:18<00:01, 181.89it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2943/3257 [00:18<00:01, 167.66it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2961/3257 [00:19<00:01, 169.03it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2979/3257 [00:19<00:01, 164.58it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2999/3257 [00:19<00:01, 173.14it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3017/3257 [00:19<00:01, 169.95it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3038/3257 [00:19<00:01, 178.92it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3060/3257 [00:19<00:01, 189.37it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3080/3257 [00:20<00:01, 93.51it/s]  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3097/3257 [00:20<00:01, 105.70it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3120/3257 [00:20<00:01, 128.74it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3138/3257 [00:20<00:00, 135.50it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3155/3257 [00:20<00:00, 139.60it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3173/3257 [00:20<00:00, 148.65it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3190/3257 [00:20<00:00, 151.53it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:20<00:00, 156.20it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3224/3257 [00:20<00:00, 156.92it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:21<00:00, 173.31it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:21<00:00, 154.20it/s]
2023-02-07 19:33:56.843 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:33:56,844][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d53,n5,mc3,s0.674786,t4>', 'datetime': '2023-02-07T19:33:56.844496', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:33:56,844][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:33:56,844][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:33:57,409][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 19:33:57,410][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:33:57,508][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 38627 unique words (71.46% of original 54054, drops 15427)', 'datetime': '2023-02-07T19:33:57.508864', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:33:57,509][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 6527597 word corpus (99.64% of original 6550866, drops 23269)', 'datetime': '2023-02-07T19:33:57.509277', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:33:57,633][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 19:33:57,635][gensim.models.word2vec][INFO] - sample=0.674786 downsamples 0 most-common words
[2023-02-07 19:33:57,635][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6527597 word corpus (100.0%% of prior 6527597)', 'datetime': '2023-02-07T19:33:57.635475', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:33:57,847][gensim.models.word2vec][INFO] - estimated required memory for 38627 words and 53 dimensions: 37033232 bytes
[2023-02-07 19:33:57,847][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:33:57,856][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 38627 vocabulary and 53 features, using sg=1 hs=0 sample=0.6747864598986598 negative=5 window=8 shrink_windows=True', 'datetime': '2023-02-07T19:33:57.856153', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:33:58,865][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 40.16% examples, 2657422 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:33:59,865][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 82.81% examples, 2696180 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:00,267][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6472763 effective words) took 2.4s, 2688516 effective words/s
[2023-02-07 19:34:01,274][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 43.32% examples, 2865228 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:02,276][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 88.03% examples, 2857618 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:02,537][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6472763 effective words) took 2.3s, 2853353 effective words/s
[2023-02-07 19:34:03,545][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 42.37% examples, 2808656 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:04,546][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 85.51% examples, 2784317 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:04,866][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6472763 effective words) took 2.3s, 2782424 effective words/s
[2023-02-07 19:34:05,875][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 43.08% examples, 2849617 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:06,876][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 88.36% examples, 2866778 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:07,115][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6472763 effective words) took 2.2s, 2883515 effective words/s
[2023-02-07 19:34:08,122][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 46.18% examples, 3016627 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:09,123][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 93.28% examples, 3024572 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:09,255][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6472763 effective words) took 2.1s, 3025822 effective words/s
[2023-02-07 19:34:10,259][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 56.83% examples, 3753500 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:10,987][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6472763 effective words) took 1.7s, 3740391 effective words/s
[2023-02-07 19:34:11,995][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 46.18% examples, 3018870 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:34:12,999][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 98.34% examples, 3172928 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:13,020][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6472763 effective words) took 2.0s, 3187516 effective words/s
[2023-02-07 19:34:14,023][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 46.18% examples, 3028212 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:34:14,952][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6472763 effective words) took 1.9s, 3352851 effective words/s
[2023-02-07 19:34:15,955][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 55.66% examples, 3680047 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:16,704][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6472763 effective words) took 1.8s, 3697737 effective words/s
[2023-02-07 19:34:17,706][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 55.20% examples, 3645927 words/s, in_qsize 5, out_qsize 2
[2023-02-07 19:34:18,612][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6472763 effective words) took 1.9s, 3394077 effective words/s
[2023-02-07 19:34:19,614][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 41.63% examples, 2771022 words/s, in_qsize 3, out_qsize 0
[2023-02-07 19:34:20,616][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 86.34% examples, 2811815 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:20,902][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6472763 effective words) took 2.3s, 2827956 effective words/s
[2023-02-07 19:34:21,906][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 43.94% examples, 2907894 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:22,906][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 89.16% examples, 2906422 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:23,127][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6472763 effective words) took 2.2s, 2913888 effective words/s
[2023-02-07 19:34:24,129][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 48.57% examples, 3211165 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:24,992][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6472763 effective words) took 1.9s, 3472705 effective words/s
[2023-02-07 19:34:25,997][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 46.61% examples, 3056276 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:26,963][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6472763 effective words) took 2.0s, 3288228 effective words/s
[2023-02-07 19:34:27,972][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 46.45% examples, 3039591 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:28,975][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 92.75% examples, 3011498 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:34:29,111][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6472763 effective words) took 2.1s, 3019749 effective words/s
[2023-02-07 19:34:29,112][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (97091445 effective words) took 31.3s, 3106340 effective words/s', 'datetime': '2023-02-07T19:34:29.112453', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:34:29.112 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:34:31,796][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193327-xzs6mc2f/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:34:31.796069', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:34:31,801][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:34:31,886][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193327-xzs6mc2f/files/../tmp/embedding_model.pt
2023-02-07 19:34:31.886 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:34:32.914 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:34:33.258 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:34:33.821 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.183284078648368, 'test_mae': 1.116028375142294, 'test_r2': -1.848474366078821}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.82
wandb: percentage 0.2854
wandb:   test_mae 1.11603
wandb:   test_mse 2.18328
wandb:    test_r2 -1.84847
wandb: 
wandb: üöÄ View run wise-sweep-51 at: https://wandb.ai/xiaoqiz/mof2vec/runs/xzs6mc2f
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_193327-xzs6mc2f/logs
wandb: Agent Starting Run: bhr5hxos with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 641
wandb: 	model.gensim.alpha: 0.05143611768775147
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.6069485699471158
wandb: 	model.gensim.vector_size: 326
wandb: 	model.gensim.window: 2
wandb: 	model.sklearn.learning_rate: 0.5154088071806932
wandb: 	model.sklearn.max_depth: 74
wandb: 	model.sklearn.min_child_weight: 0.08387785883816792
wandb: 	model.sklearn.n_estimators: 1887
wandb: 	model.sklearn.num_leaves: 331
wandb: 	model.sklearn.reg_alpha: 0.025443423864246053
wandb: 	model.sklearn.reg_lambda: 0.14419527109700883
wandb: 	model.sklearn.subsample: 0.7832916765096563
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193445-bhr5hxos
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sweep-52
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/bhr5hxos
2023-02-07 19:34:53.295 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 19:34:53.296 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 641 for sweep.
2023-02-07 19:34:53.296 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.05143611768775147 for sweep.
2023-02-07 19:34:53.296 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:34:53.296 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 19:34:53.297 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6069485699471158 for sweep.
2023-02-07 19:34:53.297 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 326 for sweep.
2023-02-07 19:34:53.297 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 2 for sweep.
2023-02-07 19:34:53.297 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.5154088071806932 for sweep.
2023-02-07 19:34:53.298 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 74 for sweep.
2023-02-07 19:34:53.298 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.08387785883816792 for sweep.
2023-02-07 19:34:53.298 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1887 for sweep.
2023-02-07 19:34:53.298 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 331 for sweep.
2023-02-07 19:34:53.298 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.025443423864246053 for sweep.
2023-02-07 19:34:53.299 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.14419527109700883 for sweep.
2023-02-07 19:34:53.299 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.7832916765096563 for sweep.
2023-02-07 19:34:53.299 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:34:53.308 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193445-bhr5hxos/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 641, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 326, 'window': 2, 'min_count': 5, 'dm': 0, 'sample': 0.6069485699471158, 'workers': 4, 'alpha': 0.05143611768775147, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1887, 'max_depth': 74, 'num_leaves': 331, 'reg_alpha': 0.025443423864246053, 'reg_lambda': 0.14419527109700883, 'subsample': 0.7832916765096563, 'min_child_weight': 0.08387785883816792, 'n_jobs': 4, 'learning_rate': 0.5154088071806932}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 15/3257 [00:00<00:22, 143.49it/s]  1%|          | 31/3257 [00:00<00:21, 152.91it/s]  1%|‚ñè         | 47/3257 [00:00<00:21, 146.61it/s]  2%|‚ñè         | 62/3257 [00:00<00:23, 134.73it/s]  2%|‚ñè         | 80/3257 [00:00<00:21, 147.89it/s]  3%|‚ñé         | 96/3257 [00:00<00:21, 148.14it/s]  3%|‚ñé         | 111/3257 [00:00<00:22, 142.05it/s]  4%|‚ñç         | 128/3257 [00:00<00:21, 148.17it/s]  5%|‚ñç         | 147/3257 [00:00<00:19, 159.62it/s]  5%|‚ñå         | 164/3257 [00:01<00:20, 149.96it/s]  6%|‚ñå         | 181/3257 [00:01<00:19, 155.50it/s]  6%|‚ñå         | 201/3257 [00:01<00:18, 164.72it/s]  7%|‚ñã         | 227/3257 [00:01<00:15, 190.36it/s]  8%|‚ñä         | 247/3257 [00:01<00:15, 188.85it/s]  8%|‚ñä         | 267/3257 [00:01<00:17, 172.10it/s]  9%|‚ñâ         | 290/3257 [00:01<00:15, 186.07it/s] 10%|‚ñâ         | 310/3257 [00:01<00:15, 184.99it/s] 10%|‚ñà         | 331/3257 [00:01<00:15, 189.56it/s] 11%|‚ñà         | 351/3257 [00:02<00:15, 184.53it/s] 11%|‚ñà‚ñè        | 371/3257 [00:02<00:15, 188.39it/s] 12%|‚ñà‚ñè        | 390/3257 [00:02<00:17, 168.30it/s] 13%|‚ñà‚ñé        | 411/3257 [00:02<00:16, 177.09it/s] 13%|‚ñà‚ñé        | 430/3257 [00:02<00:17, 161.60it/s] 14%|‚ñà‚ñé        | 447/3257 [00:02<00:17, 163.12it/s] 14%|‚ñà‚ñç        | 467/3257 [00:02<00:16, 172.98it/s] 15%|‚ñà‚ñç        | 485/3257 [00:02<00:15, 174.04it/s] 16%|‚ñà‚ñå        | 507/3257 [00:02<00:14, 186.84it/s] 16%|‚ñà‚ñå        | 526/3257 [00:03<00:14, 184.78it/s] 17%|‚ñà‚ñã        | 546/3257 [00:03<00:14, 185.74it/s] 17%|‚ñà‚ñã        | 565/3257 [00:03<00:15, 177.63it/s] 18%|‚ñà‚ñä        | 583/3257 [00:03<00:15, 175.38it/s] 19%|‚ñà‚ñä        | 604/3257 [00:03<00:14, 184.29it/s] 19%|‚ñà‚ñâ        | 623/3257 [00:03<00:14, 183.95it/s] 20%|‚ñà‚ñâ        | 645/3257 [00:03<00:13, 187.04it/s] 20%|‚ñà‚ñà        | 664/3257 [00:03<00:14, 179.63it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:14, 180.06it/s] 22%|‚ñà‚ñà‚ñè       | 704/3257 [00:04<00:13, 187.31it/s] 22%|‚ñà‚ñà‚ñè       | 723/3257 [00:04<00:13, 184.55it/s] 23%|‚ñà‚ñà‚ñé       | 742/3257 [00:04<00:14, 178.80it/s] 23%|‚ñà‚ñà‚ñé       | 763/3257 [00:04<00:13, 185.39it/s] 24%|‚ñà‚ñà‚ñç       | 782/3257 [00:04<00:13, 178.90it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:04<00:13, 186.02it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:04<00:13, 177.72it/s] 26%|‚ñà‚ñà‚ñå       | 841/3257 [00:04<00:14, 170.76it/s] 26%|‚ñà‚ñà‚ñã       | 860/3257 [00:04<00:13, 175.99it/s] 27%|‚ñà‚ñà‚ñã       | 878/3257 [00:05<00:13, 172.48it/s] 28%|‚ñà‚ñà‚ñä       | 901/3257 [00:05<00:12, 187.34it/s] 28%|‚ñà‚ñà‚ñä       | 920/3257 [00:05<00:12, 187.56it/s] 29%|‚ñà‚ñà‚ñâ       | 939/3257 [00:05<00:12, 187.73it/s] 30%|‚ñà‚ñà‚ñâ       | 961/3257 [00:05<00:11, 195.45it/s] 30%|‚ñà‚ñà‚ñà       | 981/3257 [00:05<00:11, 190.08it/s] 31%|‚ñà‚ñà‚ñà       | 1001/3257 [00:05<00:11, 190.71it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1021/3257 [00:05<00:17, 125.73it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1038/3257 [00:06<00:16, 132.72it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1056/3257 [00:06<00:15, 142.06it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:06<00:14, 154.39it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1101/3257 [00:06<00:12, 166.48it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1119/3257 [00:06<00:12, 168.49it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1139/3257 [00:06<00:12, 171.84it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1158/3257 [00:06<00:11, 175.09it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1177/3257 [00:06<00:11, 177.40it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1196/3257 [00:06<00:12, 166.33it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1213/3257 [00:07<00:12, 162.11it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1238/3257 [00:07<00:10, 183.93it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1257/3257 [00:07<00:11, 178.34it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1276/3257 [00:07<00:11, 170.51it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1294/3257 [00:07<00:12, 158.10it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1311/3257 [00:07<00:12, 161.08it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1329/3257 [00:07<00:11, 164.10it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1349/3257 [00:07<00:11, 171.21it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1367/3257 [00:08<00:11, 161.95it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:08<00:11, 159.75it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1403/3257 [00:08<00:11, 165.59it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:08<00:10, 174.95it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1441/3257 [00:08<00:10, 173.91it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1464/3257 [00:08<00:09, 189.85it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1484/3257 [00:08<00:09, 188.76it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1506/3257 [00:08<00:08, 196.91it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1526/3257 [00:08<00:09, 179.57it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1545/3257 [00:08<00:10, 169.88it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1565/3257 [00:09<00:09, 172.76it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1583/3257 [00:09<00:09, 169.57it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1604/3257 [00:09<00:09, 178.00it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1622/3257 [00:09<00:09, 174.48it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1640/3257 [00:09<00:09, 173.18it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1658/3257 [00:09<00:09, 165.73it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1675/3257 [00:09<00:09, 161.91it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1692/3257 [00:09<00:09, 160.07it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1711/3257 [00:09<00:09, 168.23it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:10<00:09, 158.29it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1744/3257 [00:10<00:09, 155.43it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1764/3257 [00:10<00:08, 166.81it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1786/3257 [00:10<00:08, 179.67it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:10<00:08, 175.06it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1824/3257 [00:10<00:08, 178.94it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1842/3257 [00:10<00:08, 176.05it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1860/3257 [00:10<00:07, 174.83it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1880/3257 [00:10<00:07, 180.74it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1899/3257 [00:11<00:07, 176.73it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1918/3257 [00:11<00:07, 176.23it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1942/3257 [00:11<00:06, 193.67it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:11<00:06, 201.91it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1986/3257 [00:11<00:06, 190.98it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2006/3257 [00:11<00:06, 191.83it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2027/3257 [00:11<00:06, 196.86it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2047/3257 [00:11<00:06, 178.07it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2066/3257 [00:11<00:07, 169.52it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2084/3257 [00:12<00:06, 171.98it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2102/3257 [00:12<00:06, 166.54it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:12<00:06, 167.56it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2137/3257 [00:12<00:06, 167.77it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2154/3257 [00:12<00:06, 166.64it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2176/3257 [00:12<00:05, 180.37it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2195/3257 [00:12<00:06, 174.91it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2213/3257 [00:12<00:06, 165.53it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2234/3257 [00:12<00:05, 176.40it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2252/3257 [00:13<00:05, 172.19it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2270/3257 [00:13<00:05, 170.38it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2290/3257 [00:13<00:05, 177.02it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2308/3257 [00:13<00:05, 172.18it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2329/3257 [00:13<00:08, 109.23it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2350/3257 [00:13<00:07, 128.32it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2370/3257 [00:13<00:06, 142.52it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2393/3257 [00:14<00:05, 161.76it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2412/3257 [00:14<00:05, 162.75it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2430/3257 [00:14<00:05, 163.42it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2448/3257 [00:14<00:05, 157.53it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2473/3257 [00:14<00:04, 180.35it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2493/3257 [00:14<00:04, 185.18it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2517/3257 [00:14<00:03, 198.31it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2539/3257 [00:14<00:03, 203.44it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2560/3257 [00:14<00:03, 188.56it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2580/3257 [00:15<00:03, 176.72it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2599/3257 [00:15<00:03, 179.38it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2624/3257 [00:15<00:03, 196.48it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2644/3257 [00:15<00:03, 188.26it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2664/3257 [00:15<00:03, 178.40it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2686/3257 [00:15<00:03, 188.41it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2706/3257 [00:15<00:03, 172.11it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2724/3257 [00:15<00:03, 167.15it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2747/3257 [00:15<00:02, 181.81it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2766/3257 [00:16<00:02, 183.14it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2785/3257 [00:16<00:02, 180.34it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:16<00:02, 185.64it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2825/3257 [00:16<00:02, 170.85it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2843/3257 [00:16<00:02, 171.48it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2868/3257 [00:16<00:02, 191.95it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2888/3257 [00:16<00:01, 189.16it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2908/3257 [00:16<00:01, 179.17it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2929/3257 [00:16<00:01, 184.16it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2948/3257 [00:17<00:01, 176.56it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2967/3257 [00:17<00:01, 177.75it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:17<00:01, 169.94it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3008/3257 [00:17<00:01, 184.52it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3027/3257 [00:17<00:01, 177.77it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:17<00:01, 187.40it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3073/3257 [00:17<00:00, 200.08it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3094/3257 [00:17<00:00, 195.00it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3118/3257 [00:17<00:00, 206.34it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3139/3257 [00:18<00:00, 200.23it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3160/3257 [00:18<00:00, 195.32it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3180/3257 [00:18<00:00, 186.02it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3203/3257 [00:18<00:00, 196.95it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3223/3257 [00:18<00:00, 189.37it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3247/3257 [00:18<00:00, 203.17it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 174.58it/s]
2023-02-07 19:35:12.709 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:35:12,711][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d326,n5,mc5,s0.606949,t4>', 'datetime': '2023-02-07T19:35:12.710950', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:35:12,711][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:35:12,712][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:35:13,217][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 19:35:13,217][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:35:13,276][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 21312 unique words (49.91% of original 42701, drops 21389)', 'datetime': '2023-02-07T19:35:13.276660', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:35:13,277][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5769229 word corpus (99.08% of original 5822992, drops 53763)', 'datetime': '2023-02-07T19:35:13.276999', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:35:13,347][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 19:35:13,348][gensim.models.word2vec][INFO] - sample=0.606949 downsamples 0 most-common words
[2023-02-07 19:35:13,348][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5769229 word corpus (100.0%% of prior 5769229)', 'datetime': '2023-02-07T19:35:13.348299', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:35:13,464][gensim.models.word2vec][INFO] - estimated required memory for 21312 words and 326 dimensions: 71136224 bytes
[2023-02-07 19:35:13,465][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:35:13,497][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 21312 vocabulary and 326 features, using sg=1 hs=0 sample=0.6069485699471158 negative=5 window=2 shrink_windows=True', 'datetime': '2023-02-07T19:35:13.497715', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:35:14,503][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 32.02% examples, 1859612 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:15,506][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 64.17% examples, 1868785 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:16,512][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 97.39% examples, 1859663 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:16,587][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5743174 effective words) took 3.1s, 1860536 effective words/s
[2023-02-07 19:35:17,591][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 33.71% examples, 1956874 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:18,593][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 68.04% examples, 1990142 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:19,432][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5743174 effective words) took 2.8s, 2019977 effective words/s
[2023-02-07 19:35:20,436][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 45.69% examples, 2668556 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:21,439][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 82.38% examples, 2376773 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:35:21,916][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5743174 effective words) took 2.5s, 2314121 effective words/s
[2023-02-07 19:35:22,919][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 44.18% examples, 2599195 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:23,920][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 90.36% examples, 2610300 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:24,113][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5743174 effective words) took 2.2s, 2616399 effective words/s
[2023-02-07 19:35:25,125][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 33.96% examples, 1952963 words/s, in_qsize 6, out_qsize 11
[2023-02-07 19:35:26,127][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 68.53% examples, 1995118 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:26,953][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5743174 effective words) took 2.8s, 2023148 effective words/s
[2023-02-07 19:35:27,958][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 45.69% examples, 2665098 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:28,959][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 93.28% examples, 2688571 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:35:29,087][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5743174 effective words) took 2.1s, 2692851 effective words/s
[2023-02-07 19:35:30,090][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 36.51% examples, 2145493 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:31,096][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 73.96% examples, 2143049 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:31,742][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5743174 effective words) took 2.7s, 2164865 effective words/s
[2023-02-07 19:35:32,748][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 36.84% examples, 2158905 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:33,750][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 74.58% examples, 2166720 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:34,393][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5743174 effective words) took 2.6s, 2168234 effective words/s
[2023-02-07 19:35:35,395][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 46.76% examples, 2733927 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:36,398][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 85.32% examples, 2470280 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:36,770][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5743174 effective words) took 2.4s, 2417548 effective words/s
[2023-02-07 19:35:37,776][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 36.44% examples, 2124581 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:38,783][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 72.77% examples, 2109813 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:39,495][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5743174 effective words) took 2.7s, 2109616 effective words/s
[2023-02-07 19:35:40,506][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 36.44% examples, 2119017 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:41,507][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 72.92% examples, 2118916 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:42,211][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5743174 effective words) took 2.7s, 2117864 effective words/s
[2023-02-07 19:35:43,216][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 36.14% examples, 2107569 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:35:44,221][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 72.77% examples, 2115576 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:44,938][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5743174 effective words) took 2.7s, 2109821 effective words/s
[2023-02-07 19:35:45,946][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 36.20% examples, 2112696 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:35:46,946][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 72.98% examples, 2119448 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:47,646][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5743174 effective words) took 2.7s, 2121603 effective words/s
[2023-02-07 19:35:48,653][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 35.31% examples, 2051772 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:35:49,657][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 70.43% examples, 2060867 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:35:50,430][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5743174 effective words) took 2.8s, 2065399 effective words/s
[2023-02-07 19:35:51,438][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 43.41% examples, 2550725 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:35:52,440][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 89.65% examples, 2579693 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:35:52,662][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5743174 effective words) took 2.2s, 2575315 effective words/s
[2023-02-07 19:35:52,663][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86147610 effective words) took 39.2s, 2199669 effective words/s', 'datetime': '2023-02-07T19:35:52.663110', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:35:52.663 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:35:55,422][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193445-bhr5hxos/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:35:55.421988', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:35:55,423][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:35:55,523][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193445-bhr5hxos/files/../tmp/embedding_model.pt
2023-02-07 19:35:55.523 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:35:57.373 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:35:58.009 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:36:00.147 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.369204009553056, 'test_mae': 1.1087815734326405, 'test_r2': -2.828594673223167}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.93
wandb: percentage 0.5009
wandb:   test_mae 1.10878
wandb:   test_mse 2.3692
wandb:    test_r2 -2.82859
wandb: 
wandb: üöÄ View run northern-sweep-52 at: https://wandb.ai/xiaoqiz/mof2vec/runs/bhr5hxos
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_193445-bhr5hxos/logs
wandb: Agent Starting Run: d3emsb5c with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 918
wandb: 	model.gensim.alpha: 0.01021682939488766
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.8108034444960768
wandb: 	model.gensim.vector_size: 210
wandb: 	model.gensim.window: 19
wandb: 	model.sklearn.learning_rate: 0.35910488256266127
wandb: 	model.sklearn.max_depth: 66
wandb: 	model.sklearn.min_child_weight: 0.06363938273522493
wandb: 	model.sklearn.n_estimators: 1703
wandb: 	model.sklearn.num_leaves: 365
wandb: 	model.sklearn.reg_alpha: 0.8554275485275763
wandb: 	model.sklearn.reg_lambda: 0.4660150980920541
wandb: 	model.sklearn.subsample: 0.437528414337372
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193611-d3emsb5c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-53
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/d3emsb5c
2023-02-07 19:36:19.804 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 19:36:19.805 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 918 for sweep.
2023-02-07 19:36:19.805 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.01021682939488766 for sweep.
2023-02-07 19:36:19.805 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:36:19.805 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 19:36:19.806 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.8108034444960768 for sweep.
2023-02-07 19:36:19.806 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 210 for sweep.
2023-02-07 19:36:19.806 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 19 for sweep.
2023-02-07 19:36:19.806 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.35910488256266127 for sweep.
2023-02-07 19:36:19.806 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 66 for sweep.
2023-02-07 19:36:19.807 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.06363938273522493 for sweep.
2023-02-07 19:36:19.807 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1703 for sweep.
2023-02-07 19:36:19.807 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 365 for sweep.
2023-02-07 19:36:19.807 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.8554275485275763 for sweep.
2023-02-07 19:36:19.808 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.4660150980920541 for sweep.
2023-02-07 19:36:19.808 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.437528414337372 for sweep.
2023-02-07 19:36:19.808 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:36:19.816 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193611-d3emsb5c/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 918, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 210, 'window': 19, 'min_count': 3, 'dm': 0, 'sample': 0.8108034444960768, 'workers': 4, 'alpha': 0.01021682939488766, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1703, 'max_depth': 66, 'num_leaves': 365, 'reg_alpha': 0.8554275485275763, 'reg_lambda': 0.4660150980920541, 'subsample': 0.437528414337372, 'min_child_weight': 0.06363938273522493, 'n_jobs': 4, 'learning_rate': 0.35910488256266127}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 15/3257 [00:00<00:22, 145.05it/s]  1%|          | 34/3257 [00:00<00:19, 161.18it/s]  2%|‚ñè         | 52/3257 [00:00<00:18, 168.97it/s]  2%|‚ñè         | 69/3257 [00:00<00:19, 162.06it/s]  3%|‚ñé         | 90/3257 [00:00<00:18, 169.72it/s]  3%|‚ñé         | 107/3257 [00:00<00:19, 159.51it/s]  4%|‚ñç         | 125/3257 [00:00<00:19, 162.07it/s]  4%|‚ñç         | 146/3257 [00:00<00:17, 175.15it/s]  5%|‚ñå         | 164/3257 [00:00<00:18, 167.77it/s]  6%|‚ñå         | 183/3257 [00:01<00:17, 171.95it/s]  6%|‚ñå         | 201/3257 [00:01<00:17, 171.70it/s]  7%|‚ñã         | 225/3257 [00:01<00:15, 190.96it/s]  8%|‚ñä         | 245/3257 [00:01<00:15, 190.39it/s]  8%|‚ñä         | 265/3257 [00:01<00:16, 180.51it/s]  9%|‚ñâ         | 288/3257 [00:01<00:15, 192.92it/s]  9%|‚ñâ         | 308/3257 [00:01<00:15, 187.56it/s] 10%|‚ñà         | 328/3257 [00:01<00:15, 188.50it/s] 11%|‚ñà         | 347/3257 [00:01<00:16, 179.63it/s] 11%|‚ñà         | 366/3257 [00:02<00:16, 177.66it/s] 12%|‚ñà‚ñè        | 384/3257 [00:02<00:17, 167.16it/s] 12%|‚ñà‚ñè        | 401/3257 [00:02<00:17, 165.57it/s] 13%|‚ñà‚ñé        | 420/3257 [00:02<00:16, 170.25it/s] 13%|‚ñà‚ñé        | 438/3257 [00:02<00:18, 149.39it/s] 14%|‚ñà‚ñç        | 457/3257 [00:02<00:17, 159.51it/s] 15%|‚ñà‚ñç        | 476/3257 [00:02<00:16, 165.04it/s] 15%|‚ñà‚ñå        | 496/3257 [00:02<00:15, 173.67it/s] 16%|‚ñà‚ñå        | 516/3257 [00:02<00:15, 179.67it/s] 16%|‚ñà‚ñã        | 535/3257 [00:03<00:15, 176.91it/s] 17%|‚ñà‚ñã        | 553/3257 [00:03<00:15, 174.40it/s] 18%|‚ñà‚ñä        | 571/3257 [00:03<00:24, 109.22it/s] 18%|‚ñà‚ñä        | 589/3257 [00:03<00:21, 123.38it/s] 19%|‚ñà‚ñä        | 609/3257 [00:03<00:18, 140.07it/s] 19%|‚ñà‚ñâ        | 626/3257 [00:03<00:18, 143.86it/s] 20%|‚ñà‚ñâ        | 643/3257 [00:03<00:17, 146.26it/s] 20%|‚ñà‚ñà        | 659/3257 [00:04<00:19, 136.62it/s] 21%|‚ñà‚ñà        | 677/3257 [00:04<00:17, 145.35it/s] 21%|‚ñà‚ñà‚ñè       | 693/3257 [00:04<00:17, 147.73it/s] 22%|‚ñà‚ñà‚ñè       | 713/3257 [00:04<00:15, 160.49it/s] 22%|‚ñà‚ñà‚ñè       | 730/3257 [00:04<00:16, 150.05it/s] 23%|‚ñà‚ñà‚ñé       | 747/3257 [00:04<00:16, 153.01it/s] 24%|‚ñà‚ñà‚ñé       | 767/3257 [00:04<00:15, 165.67it/s] 24%|‚ñà‚ñà‚ñç       | 784/3257 [00:04<00:16, 151.03it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:04<00:15, 160.31it/s] 25%|‚ñà‚ñà‚ñå       | 821/3257 [00:05<00:15, 159.40it/s] 26%|‚ñà‚ñà‚ñå       | 838/3257 [00:05<00:15, 153.60it/s] 26%|‚ñà‚ñà‚ñå       | 854/3257 [00:05<00:16, 147.56it/s] 27%|‚ñà‚ñà‚ñã       | 871/3257 [00:05<00:15, 151.41it/s] 27%|‚ñà‚ñà‚ñã       | 887/3257 [00:05<00:15, 151.74it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:05<00:14, 158.42it/s] 28%|‚ñà‚ñà‚ñä       | 924/3257 [00:05<00:14, 163.98it/s] 29%|‚ñà‚ñà‚ñâ       | 941/3257 [00:05<00:14, 155.99it/s] 29%|‚ñà‚ñà‚ñâ       | 959/3257 [00:05<00:14, 161.25it/s] 30%|‚ñà‚ñà‚ñâ       | 976/3257 [00:06<00:14, 162.25it/s] 30%|‚ñà‚ñà‚ñà       | 993/3257 [00:06<00:14, 154.21it/s] 31%|‚ñà‚ñà‚ñà       | 1009/3257 [00:06<00:15, 148.94it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1026/3257 [00:06<00:14, 152.80it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1042/3257 [00:06<00:15, 139.04it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1059/3257 [00:06<00:15, 146.19it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1078/3257 [00:06<00:13, 157.66it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1095/3257 [00:06<00:14, 150.78it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1114/3257 [00:06<00:13, 160.54it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1131/3257 [00:07<00:14, 147.88it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:07<00:14, 143.48it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1164/3257 [00:07<00:13, 149.71it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1180/3257 [00:07<00:14, 140.69it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1195/3257 [00:07<00:14, 139.49it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1210/3257 [00:07<00:15, 135.76it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1230/3257 [00:07<00:13, 152.47it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1246/3257 [00:07<00:13, 148.65it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1265/3257 [00:08<00:12, 159.24it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1282/3257 [00:08<00:13, 151.27it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1300/3257 [00:08<00:12, 158.00it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1318/3257 [00:08<00:11, 162.00it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1336/3257 [00:08<00:11, 166.30it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1353/3257 [00:08<00:11, 160.06it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:08<00:12, 155.29it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1386/3257 [00:08<00:12, 153.21it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1407/3257 [00:08<00:11, 167.95it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1426/3257 [00:08<00:10, 174.14it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1444/3257 [00:09<00:10, 172.23it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1465/3257 [00:09<00:09, 181.08it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1484/3257 [00:09<00:10, 175.62it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1506/3257 [00:09<00:09, 185.98it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1525/3257 [00:09<00:10, 167.65it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1543/3257 [00:09<00:10, 161.96it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1560/3257 [00:09<00:11, 153.46it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1576/3257 [00:09<00:11, 149.37it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1594/3257 [00:10<00:10, 155.14it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1611/3257 [00:10<00:10, 158.17it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1627/3257 [00:10<00:10, 157.20it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1643/3257 [00:10<00:10, 151.01it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:10<00:10, 151.82it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1675/3257 [00:10<00:10, 149.11it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1691/3257 [00:10<00:10, 151.01it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1712/3257 [00:10<00:09, 167.51it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1729/3257 [00:10<00:09, 154.27it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1745/3257 [00:11<00:10, 146.58it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:11<00:09, 153.34it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1782/3257 [00:11<00:09, 163.28it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1800/3257 [00:11<00:08, 166.21it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1817/3257 [00:11<00:08, 165.33it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1834/3257 [00:11<00:08, 163.54it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1852/3257 [00:11<00:08, 167.56it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1872/3257 [00:11<00:07, 175.11it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1890/3257 [00:12<00:13, 100.79it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1908/3257 [00:12<00:11, 115.12it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1923/3257 [00:12<00:10, 122.52it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1945/3257 [00:12<00:09, 144.48it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:12<00:08, 158.14it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1983/3257 [00:12<00:08, 153.97it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2002/3257 [00:12<00:07, 161.15it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2020/3257 [00:12<00:07, 160.51it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2037/3257 [00:12<00:07, 162.57it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2054/3257 [00:13<00:08, 148.35it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2070/3257 [00:13<00:08, 144.93it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2086/3257 [00:13<00:07, 147.41it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2102/3257 [00:13<00:07, 148.18it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:13<00:07, 149.43it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2136/3257 [00:13<00:07, 151.69it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2152/3257 [00:13<00:07, 150.72it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:13<00:06, 155.80it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2185/3257 [00:13<00:06, 153.89it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2204/3257 [00:14<00:06, 162.99it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2221/3257 [00:14<00:06, 158.80it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2237/3257 [00:14<00:06, 156.79it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2254/3257 [00:14<00:06, 159.33it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2270/3257 [00:14<00:06, 152.95it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2288/3257 [00:14<00:06, 160.50it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2305/3257 [00:14<00:05, 158.86it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2328/3257 [00:14<00:05, 178.26it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2348/3257 [00:14<00:04, 183.56it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2367/3257 [00:15<00:04, 180.22it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2387/3257 [00:15<00:04, 185.60it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2406/3257 [00:15<00:04, 175.94it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2424/3257 [00:15<00:04, 172.13it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2442/3257 [00:15<00:05, 155.98it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2461/3257 [00:15<00:04, 163.87it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2481/3257 [00:15<00:04, 171.44it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2502/3257 [00:15<00:04, 180.75it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2521/3257 [00:15<00:04, 177.27it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2540/3257 [00:16<00:03, 180.27it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2559/3257 [00:16<00:04, 173.18it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2577/3257 [00:16<00:04, 160.48it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2594/3257 [00:16<00:04, 153.34it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2615/3257 [00:16<00:03, 165.63it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2635/3257 [00:16<00:03, 173.21it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2653/3257 [00:16<00:03, 165.57it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:16<00:03, 165.30it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2687/3257 [00:16<00:03, 162.57it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2704/3257 [00:17<00:03, 143.58it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2719/3257 [00:17<00:03, 141.44it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2742/3257 [00:17<00:03, 164.17it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:17<00:02, 166.81it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2779/3257 [00:17<00:02, 165.12it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2800/3257 [00:17<00:02, 176.83it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2818/3257 [00:17<00:02, 169.00it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2836/3257 [00:17<00:02, 164.76it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2859/3257 [00:17<00:02, 181.29it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2884/3257 [00:18<00:01, 199.47it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2905/3257 [00:18<00:01, 176.01it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:18<00:01, 180.31it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2947/3257 [00:18<00:01, 173.53it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2965/3257 [00:18<00:01, 174.65it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2983/3257 [00:18<00:01, 158.12it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3003/3257 [00:18<00:01, 167.89it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3021/3257 [00:18<00:01, 166.57it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3042/3257 [00:19<00:01, 177.58it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3065/3257 [00:19<00:01, 191.95it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3085/3257 [00:19<00:00, 190.26it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3107/3257 [00:19<00:00, 197.79it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3128/3257 [00:19<00:00, 197.57it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3148/3257 [00:19<00:00, 177.84it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3167/3257 [00:19<00:00, 173.81it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3185/3257 [00:19<00:00, 170.78it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:19<00:00, 177.84it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3225/3257 [00:20<00:00, 174.34it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3247/3257 [00:20<00:00, 186.19it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:20<00:00, 161.29it/s]
2023-02-07 19:36:40.863 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:36:40,864][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d210,n5,mc3,s0.810803,t4>', 'datetime': '2023-02-07T19:36:40.864496', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:36:40,864][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:36:40,864][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:36:41,436][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 19:36:41,438][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:36:41,538][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 38627 unique words (71.46% of original 54054, drops 15427)', 'datetime': '2023-02-07T19:36:41.538597', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:36:41,539][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 6527597 word corpus (99.64% of original 6550866, drops 23269)', 'datetime': '2023-02-07T19:36:41.538999', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:36:41,665][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 19:36:41,667][gensim.models.word2vec][INFO] - sample=0.810803 downsamples 0 most-common words
[2023-02-07 19:36:41,667][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6527597 word corpus (100.0%% of prior 6527597)', 'datetime': '2023-02-07T19:36:41.667349', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:36:41,878][gensim.models.word2vec][INFO] - estimated required memory for 38627 words and 210 dimensions: 87594140 bytes
[2023-02-07 19:36:41,878][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:36:41,907][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 38627 vocabulary and 210 features, using sg=1 hs=0 sample=0.8108034444960768 negative=5 window=19 shrink_windows=True', 'datetime': '2023-02-07T19:36:41.907257', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:36:42,928][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 22.44% examples, 1408870 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:36:43,934][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 46.18% examples, 1498135 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:36:44,940][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 72.89% examples, 1581235 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:36:45,889][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6472763 effective words) took 4.0s, 1626612 effective words/s
[2023-02-07 19:36:46,895][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 32.55% examples, 2124776 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:36:47,897][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 63.19% examples, 2074997 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:36:48,902][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 95.27% examples, 2053024 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:36:49,046][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6472763 effective words) took 3.2s, 2053408 effective words/s
[2023-02-07 19:36:50,052][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 30.89% examples, 2000802 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:36:51,054][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 60.67% examples, 1988376 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:36:52,060][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 91.40% examples, 1981316 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:36:52,304][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6472763 effective words) took 3.3s, 1989119 effective words/s
[2023-02-07 19:36:53,310][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 38.19% examples, 2508359 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:36:54,311][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 77.96% examples, 2537524 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:36:54,835][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6472763 effective words) took 2.5s, 2558473 effective words/s
[2023-02-07 19:36:55,845][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 31.01% examples, 1997311 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:36:56,845][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 62.08% examples, 2028984 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:36:57,847][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 94.50% examples, 2036763 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:36:58,019][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6472763 effective words) took 3.2s, 2033622 effective words/s
[2023-02-07 19:36:59,024][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 31.53% examples, 2054262 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:00,030][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 63.22% examples, 2074385 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:01,031][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 96.93% examples, 2088567 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:01,117][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6472763 effective words) took 3.1s, 2091252 effective words/s
[2023-02-07 19:37:02,123][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 33.83% examples, 2206087 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:03,127][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 68.84% examples, 2267581 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:03,971][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6472763 effective words) took 2.9s, 2270582 effective words/s
[2023-02-07 19:37:04,978][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 42.22% examples, 2792425 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:37:05,978][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 86.34% examples, 2807050 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:06,277][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6472763 effective words) took 2.3s, 2808617 effective words/s
[2023-02-07 19:37:07,278][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 42.62% examples, 2839584 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:08,279][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 86.40% examples, 2817374 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:08,577][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6472763 effective words) took 2.3s, 2815841 effective words/s
[2023-02-07 19:37:09,587][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 33.62% examples, 2179888 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:10,589][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 66.38% examples, 2182948 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:11,541][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6472763 effective words) took 3.0s, 2186618 effective words/s
[2023-02-07 19:37:12,548][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 33.71% examples, 2192729 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:13,554][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 66.99% examples, 2199406 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:14,492][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6472763 effective words) took 2.9s, 2194246 effective words/s
[2023-02-07 19:37:15,495][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 36.08% examples, 2362171 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:16,503][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 68.84% examples, 2265832 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:17,372][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6472763 effective words) took 2.9s, 2248479 effective words/s
[2023-02-07 19:37:18,377][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 32.12% examples, 2097186 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:19,378][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 66.84% examples, 2199928 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:20,384][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 99.79% examples, 2144501 words/s, in_qsize 2, out_qsize 1
[2023-02-07 19:37:20,386][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6472763 effective words) took 3.0s, 2148852 effective words/s
[2023-02-07 19:37:21,393][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 32.70% examples, 2130763 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:22,397][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 65.49% examples, 2151503 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:23,374][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6472763 effective words) took 3.0s, 2167811 effective words/s
[2023-02-07 19:37:24,382][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 33.68% examples, 2192421 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:25,382][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 74.61% examples, 2443323 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:37:26,030][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6472763 effective words) took 2.7s, 2439657 effective words/s
[2023-02-07 19:37:26,031][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (97091445 effective words) took 44.1s, 2200435 effective words/s', 'datetime': '2023-02-07T19:37:26.031424', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:37:26.031 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:37:29,903][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193611-d3emsb5c/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:37:29.903377', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:37:29,905][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:37:30,045][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193611-d3emsb5c/files/../tmp/embedding_model.pt
2023-02-07 19:37:30.045 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:37:31.578 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:37:32.096 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:37:33.746 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1257154688153763, 'test_mae': 1.080274742503759, 'test_r2': -1.634196316984875}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.82
wandb: percentage 0.2854
wandb:   test_mae 1.08027
wandb:   test_mse 2.12572
wandb:    test_r2 -1.6342
wandb: 
wandb: üöÄ View run dazzling-sweep-53 at: https://wandb.ai/xiaoqiz/mof2vec/runs/d3emsb5c
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_193611-d3emsb5c/logs
wandb: Agent Starting Run: 65lqcy7n with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 674
wandb: 	model.gensim.alpha: 0.13276137045107175
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.8137783630720357
wandb: 	model.gensim.vector_size: 367
wandb: 	model.gensim.window: 10
wandb: 	model.sklearn.learning_rate: 0.09798334282681326
wandb: 	model.sklearn.max_depth: 88
wandb: 	model.sklearn.min_child_weight: 0.005292630192909891
wandb: 	model.sklearn.n_estimators: 195
wandb: 	model.sklearn.num_leaves: 426
wandb: 	model.sklearn.reg_alpha: 0.935579869283828
wandb: 	model.sklearn.reg_lambda: 0.5695330315137985
wandb: 	model.sklearn.subsample: 0.31958336855913283
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193745-65lqcy7n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-sweep-54
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/65lqcy7n
2023-02-07 19:37:53.744 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 19:37:53.744 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 674 for sweep.
2023-02-07 19:37:53.745 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.13276137045107175 for sweep.
2023-02-07 19:37:53.745 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:37:53.745 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 19:37:53.745 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.8137783630720357 for sweep.
2023-02-07 19:37:53.746 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 367 for sweep.
2023-02-07 19:37:53.746 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 10 for sweep.
2023-02-07 19:37:53.746 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.09798334282681326 for sweep.
2023-02-07 19:37:53.747 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 88 for sweep.
2023-02-07 19:37:53.747 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.005292630192909891 for sweep.
2023-02-07 19:37:53.747 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 195 for sweep.
2023-02-07 19:37:53.747 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 426 for sweep.
2023-02-07 19:37:53.748 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.935579869283828 for sweep.
2023-02-07 19:37:53.748 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.5695330315137985 for sweep.
2023-02-07 19:37:53.748 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.31958336855913283 for sweep.
2023-02-07 19:37:53.748 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:37:53.756 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193745-65lqcy7n/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 674, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 367, 'window': 10, 'min_count': 2, 'dm': 0, 'sample': 0.8137783630720357, 'workers': 4, 'alpha': 0.13276137045107175, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 195, 'max_depth': 88, 'num_leaves': 426, 'reg_alpha': 0.935579869283828, 'reg_lambda': 0.5695330315137985, 'subsample': 0.31958336855913283, 'min_child_weight': 0.005292630192909891, 'n_jobs': 4, 'learning_rate': 0.09798334282681326}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:19, 169.75it/s]  1%|          | 36/3257 [00:00<00:31, 103.15it/s]  2%|‚ñè         | 53/3257 [00:00<00:26, 121.07it/s]  2%|‚ñè         | 74/3257 [00:00<00:21, 146.47it/s]  3%|‚ñé         | 94/3257 [00:00<00:19, 161.10it/s]  3%|‚ñé         | 112/3257 [00:00<00:19, 164.25it/s]  4%|‚ñç         | 134/3257 [00:00<00:17, 176.19it/s]  5%|‚ñç         | 158/3257 [00:00<00:16, 187.63it/s]  5%|‚ñå         | 178/3257 [00:01<00:16, 187.56it/s]  6%|‚ñå         | 199/3257 [00:01<00:15, 193.52it/s]  7%|‚ñã         | 222/3257 [00:01<00:14, 203.86it/s]  7%|‚ñã         | 244/3257 [00:01<00:14, 208.03it/s]  8%|‚ñä         | 265/3257 [00:01<00:15, 195.10it/s]  9%|‚ñâ         | 290/3257 [00:01<00:14, 210.11it/s] 10%|‚ñâ         | 312/3257 [00:01<00:14, 197.48it/s] 10%|‚ñà         | 334/3257 [00:01<00:14, 202.31it/s] 11%|‚ñà         | 355/3257 [00:01<00:14, 204.01it/s] 12%|‚ñà‚ñè        | 376/3257 [00:02<00:14, 193.92it/s] 12%|‚ñà‚ñè        | 396/3257 [00:02<00:15, 189.13it/s] 13%|‚ñà‚ñé        | 418/3257 [00:02<00:14, 197.22it/s] 13%|‚ñà‚ñé        | 438/3257 [00:02<00:16, 170.47it/s] 14%|‚ñà‚ñç        | 459/3257 [00:02<00:15, 178.63it/s] 15%|‚ñà‚ñç        | 479/3257 [00:02<00:15, 183.54it/s] 15%|‚ñà‚ñå        | 502/3257 [00:02<00:14, 196.02it/s] 16%|‚ñà‚ñå        | 523/3257 [00:02<00:14, 193.77it/s] 17%|‚ñà‚ñã        | 543/3257 [00:02<00:14, 190.41it/s] 17%|‚ñà‚ñã        | 563/3257 [00:03<00:15, 171.58it/s] 18%|‚ñà‚ñä        | 581/3257 [00:03<00:16, 165.02it/s] 18%|‚ñà‚ñä        | 602/3257 [00:03<00:15, 175.52it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:15, 175.09it/s] 20%|‚ñà‚ñâ        | 641/3257 [00:03<00:14, 181.10it/s] 20%|‚ñà‚ñà        | 660/3257 [00:03<00:15, 166.09it/s] 21%|‚ñà‚ñà        | 681/3257 [00:03<00:14, 176.84it/s] 21%|‚ñà‚ñà‚ñè       | 700/3257 [00:03<00:15, 163.90it/s] 22%|‚ñà‚ñà‚ñè       | 721/3257 [00:04<00:14, 172.92it/s] 23%|‚ñà‚ñà‚ñé       | 739/3257 [00:04<00:15, 164.55it/s] 23%|‚ñà‚ñà‚ñé       | 760/3257 [00:04<00:14, 174.89it/s] 24%|‚ñà‚ñà‚ñç       | 778/3257 [00:04<00:14, 171.46it/s] 25%|‚ñà‚ñà‚ñç       | 799/3257 [00:04<00:13, 177.93it/s] 25%|‚ñà‚ñà‚ñå       | 818/3257 [00:04<00:13, 178.70it/s] 26%|‚ñà‚ñà‚ñå       | 836/3257 [00:04<00:14, 172.63it/s] 26%|‚ñà‚ñà‚ñå       | 854/3257 [00:04<00:14, 165.31it/s] 27%|‚ñà‚ñà‚ñã       | 873/3257 [00:04<00:14, 169.46it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:05<00:13, 169.54it/s] 28%|‚ñà‚ñà‚ñä       | 912/3257 [00:05<00:13, 178.84it/s] 29%|‚ñà‚ñà‚ñä       | 930/3257 [00:05<00:12, 179.10it/s] 29%|‚ñà‚ñà‚ñâ       | 948/3257 [00:05<00:12, 179.11it/s] 30%|‚ñà‚ñà‚ñâ       | 968/3257 [00:05<00:12, 184.82it/s] 30%|‚ñà‚ñà‚ñà       | 987/3257 [00:05<00:13, 170.96it/s] 31%|‚ñà‚ñà‚ñà       | 1006/3257 [00:05<00:12, 174.81it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1024/3257 [00:05<00:12, 175.86it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1042/3257 [00:05<00:14, 154.59it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1061/3257 [00:06<00:13, 162.33it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:06<00:13, 163.35it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1098/3257 [00:06<00:12, 167.71it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:06<00:12, 168.53it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1135/3257 [00:06<00:12, 165.24it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1152/3257 [00:06<00:12, 163.29it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1172/3257 [00:06<00:12, 172.84it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1190/3257 [00:06<00:13, 151.34it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1206/3257 [00:06<00:13, 150.53it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1226/3257 [00:07<00:12, 163.68it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1245/3257 [00:07<00:11, 169.72it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1265/3257 [00:07<00:11, 177.31it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1284/3257 [00:07<00:18, 105.50it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1302/3257 [00:07<00:16, 117.38it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1321/3257 [00:07<00:14, 131.34it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1342/3257 [00:07<00:12, 147.63it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1360/3257 [00:08<00:12, 150.72it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1377/3257 [00:08<00:12, 150.32it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1394/3257 [00:08<00:12, 154.95it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1417/3257 [00:08<00:10, 173.30it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1436/3257 [00:08<00:10, 171.64it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1459/3257 [00:08<00:09, 185.20it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1478/3257 [00:08<00:09, 184.88it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1500/3257 [00:08<00:09, 193.14it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1520/3257 [00:08<00:09, 182.86it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1539/3257 [00:08<00:09, 177.35it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1557/3257 [00:09<00:09, 174.46it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1575/3257 [00:09<00:09, 173.21it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1596/3257 [00:09<00:09, 180.98it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1617/3257 [00:09<00:08, 187.35it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1636/3257 [00:09<00:09, 179.45it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1655/3257 [00:09<00:09, 177.14it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1673/3257 [00:09<00:09, 168.80it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1691/3257 [00:09<00:09, 169.40it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1710/3257 [00:09<00:08, 174.82it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:10<00:09, 164.03it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1745/3257 [00:10<00:09, 161.80it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1765/3257 [00:10<00:08, 171.76it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1787/3257 [00:10<00:08, 183.41it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1806/3257 [00:10<00:08, 180.05it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1825/3257 [00:10<00:07, 182.77it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1845/3257 [00:10<00:07, 184.00it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1868/3257 [00:10<00:07, 195.31it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1888/3257 [00:10<00:07, 188.83it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1909/3257 [00:11<00:06, 194.10it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1929/3257 [00:11<00:07, 186.23it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1958/3257 [00:11<00:06, 213.47it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1980/3257 [00:11<00:06, 201.53it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2001/3257 [00:11<00:06, 203.42it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2022/3257 [00:11<00:06, 201.39it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2043/3257 [00:11<00:06, 192.63it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2063/3257 [00:11<00:06, 175.67it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2083/3257 [00:11<00:06, 181.18it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2102/3257 [00:12<00:06, 174.96it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:12<00:06, 173.66it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2138/3257 [00:12<00:06, 172.54it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2156/3257 [00:12<00:06, 169.65it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2177/3257 [00:12<00:06, 179.02it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2195/3257 [00:12<00:05, 177.21it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2213/3257 [00:12<00:06, 170.68it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2234/3257 [00:12<00:05, 181.69it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2253/3257 [00:12<00:05, 171.34it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2271/3257 [00:13<00:06, 159.23it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2293/3257 [00:13<00:05, 174.36it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2312/3257 [00:13<00:05, 176.87it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2337/3257 [00:13<00:04, 194.90it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2361/3257 [00:13<00:04, 206.60it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2385/3257 [00:13<00:04, 214.76it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2407/3257 [00:13<00:04, 201.44it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2428/3257 [00:13<00:04, 202.97it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2449/3257 [00:13<00:04, 189.11it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2475/3257 [00:14<00:03, 204.95it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2498/3257 [00:14<00:03, 211.70it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2523/3257 [00:14<00:03, 221.00it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2546/3257 [00:14<00:03, 222.44it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2569/3257 [00:14<00:03, 203.13it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2590/3257 [00:14<00:03, 197.79it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2619/3257 [00:14<00:02, 220.46it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2642/3257 [00:14<00:02, 218.99it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2665/3257 [00:15<00:04, 119.10it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2686/3257 [00:15<00:04, 135.01it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2705/3257 [00:15<00:03, 138.68it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2723/3257 [00:15<00:03, 146.55it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2748/3257 [00:15<00:02, 170.11it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:15<00:02, 174.43it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2792/3257 [00:15<00:02, 189.80it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2813/3257 [00:15<00:02, 194.81it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2834/3257 [00:16<00:02, 187.53it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2860/3257 [00:16<00:01, 206.25it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:16<00:01, 217.58it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2909/3257 [00:16<00:01, 205.31it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2931/3257 [00:16<00:01, 201.41it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2952/3257 [00:16<00:01, 192.58it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2975/3257 [00:16<00:01, 201.90it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2996/3257 [00:16<00:01, 196.23it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3018/3257 [00:16<00:01, 199.71it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3043/3257 [00:17<00:01, 213.07it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3068/3257 [00:17<00:00, 220.96it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3091/3257 [00:17<00:00, 212.89it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3118/3257 [00:17<00:00, 223.68it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3141/3257 [00:17<00:00, 205.63it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3162/3257 [00:17<00:00, 199.69it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3183/3257 [00:17<00:00, 185.77it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:17<00:00, 194.09it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3228/3257 [00:18<00:00, 196.27it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:18<00:00, 204.38it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 179.52it/s]
2023-02-07 19:38:12.644 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:38:12,645][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d367,n5,mc2,s0.813778,t4>', 'datetime': '2023-02-07T19:38:12.645424', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:38:12,645][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:38:12,645][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:38:13,175][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 19:38:13,176][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:38:13,273][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 36619 unique words (85.76% of original 42701, drops 6082)', 'datetime': '2023-02-07T19:38:13.273324', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:38:13,273][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 5816910 word corpus (99.90% of original 5822992, drops 6082)', 'datetime': '2023-02-07T19:38:13.273721', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:38:13,393][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 19:38:13,394][gensim.models.word2vec][INFO] - sample=0.813778 downsamples 0 most-common words
[2023-02-07 19:38:13,394][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5816910 word corpus (100.0%% of prior 5816910)', 'datetime': '2023-02-07T19:38:13.394625', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:38:13,594][gensim.models.word2vec][INFO] - estimated required memory for 36619 words and 367 dimensions: 131255560 bytes
[2023-02-07 19:38:13,594][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:38:13,644][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 36619 vocabulary and 367 features, using sg=1 hs=0 sample=0.8137783630720357 negative=5 window=10 shrink_windows=True', 'datetime': '2023-02-07T19:38:13.644442', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:38:14,647][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 30.92% examples, 1803693 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:15,649][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 63.52% examples, 1868521 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:16,660][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 97.91% examples, 1883578 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:16,708][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5790320 effective words) took 3.1s, 1890625 effective words/s
[2023-02-07 19:38:17,713][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 35.22% examples, 2066413 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:18,714][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 70.40% examples, 2076815 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:19,499][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5790320 effective words) took 2.8s, 2075986 effective words/s
[2023-02-07 19:38:20,503][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 35.49% examples, 2084626 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:21,503][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 70.34% examples, 2081176 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:38:22,292][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5790320 effective words) took 2.8s, 2075723 effective words/s
[2023-02-07 19:38:23,299][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 35.31% examples, 2065335 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:24,307][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 70.10% examples, 2058147 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:38:25,128][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5790320 effective words) took 2.8s, 2043172 effective words/s
[2023-02-07 19:38:26,130][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 33.71% examples, 1973619 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:27,133][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 66.60% examples, 1964756 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:28,069][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5790320 effective words) took 2.9s, 1969831 effective words/s
[2023-02-07 19:38:29,075][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 34.33% examples, 2001732 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:30,076][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 67.45% examples, 1988101 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:38:30,972][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5790320 effective words) took 2.9s, 1995675 effective words/s
[2023-02-07 19:38:31,974][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 33.71% examples, 1973244 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:32,977][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 67.36% examples, 1988472 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:33,891][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5790320 effective words) took 2.9s, 1985091 effective words/s
[2023-02-07 19:38:34,897][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 33.53% examples, 1959896 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:38:35,898][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 66.84% examples, 1968034 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:36,765][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5790320 effective words) took 2.9s, 2016047 effective words/s
[2023-02-07 19:38:37,770][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 43.35% examples, 2570398 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:38,772][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 88.79% examples, 2579279 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:39,011][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5790320 effective words) took 2.2s, 2579941 effective words/s
[2023-02-07 19:38:40,019][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 35.31% examples, 2066022 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:41,022][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 69.76% examples, 2059607 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:41,834][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5790320 effective words) took 2.8s, 2053478 effective words/s
[2023-02-07 19:38:42,839][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 42.43% examples, 2522293 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:43,844][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 87.81% examples, 2551709 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:44,103][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5790320 effective words) took 2.3s, 2554346 effective words/s
[2023-02-07 19:38:45,105][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 34.69% examples, 2034999 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:46,113][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 68.47% examples, 2015208 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:46,963][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5790320 effective words) took 2.9s, 2025220 effective words/s
[2023-02-07 19:38:47,970][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 34.82% examples, 2039441 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:38:48,973][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 69.67% examples, 2056372 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:49,797][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5790320 effective words) took 2.8s, 2046773 effective words/s
[2023-02-07 19:38:50,818][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 33.28% examples, 1924354 words/s, in_qsize 7, out_qsize 2
[2023-02-07 19:38:51,826][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 67.24% examples, 1961502 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:52,729][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5790320 effective words) took 2.9s, 1976831 effective words/s
[2023-02-07 19:38:53,737][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 34.45% examples, 2009046 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:38:54,738][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 67.36% examples, 1986190 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:38:55,668][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5790320 effective words) took 2.9s, 1972955 effective words/s
[2023-02-07 19:38:55,669][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86854800 effective words) took 42.0s, 2066739 effective words/s', 'datetime': '2023-02-07T19:38:55.669921', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:38:55.670 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:38:59,034][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193745-65lqcy7n/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:38:59.034719', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:38:59,036][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193745-65lqcy7n/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 19:38:59,100][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193745-65lqcy7n/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 19:38:59,153][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:38:59,194][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193745-65lqcy7n/files/../tmp/embedding_model.pt
2023-02-07 19:38:59.194 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:39:01.211 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:39:01.883 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:39:04.768 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.3920563633813416, 'test_mae': 1.1803511944470892, 'test_r2': -2.7578800951042224}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.92
wandb: percentage 0.14243
wandb:   test_mae 1.18035
wandb:   test_mse 2.39206
wandb:    test_r2 -2.75788
wandb: 
wandb: üöÄ View run helpful-sweep-54 at: https://wandb.ai/xiaoqiz/mof2vec/runs/65lqcy7n
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_193745-65lqcy7n/logs
wandb: Agent Starting Run: n32dfjb7 with config:
wandb: 	data.data.wl_step: 5
wandb: 	data.nn.batch_size: 926
wandb: 	model.gensim.alpha: 0.06048119952992349
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.9247993530142038
wandb: 	model.gensim.vector_size: 15
wandb: 	model.gensim.window: 8
wandb: 	model.sklearn.learning_rate: 0.7977608393281455
wandb: 	model.sklearn.max_depth: 21
wandb: 	model.sklearn.min_child_weight: 0.04173987519981564
wandb: 	model.sklearn.n_estimators: 2442
wandb: 	model.sklearn.num_leaves: 344
wandb: 	model.sklearn.reg_alpha: 0.0267968894269733
wandb: 	model.sklearn.reg_lambda: 0.4145924583096241
wandb: 	model.sklearn.subsample: 0.5645485776356274
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193914-n32dfjb7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-55
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/n32dfjb7
2023-02-07 19:39:22.750 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 5 for sweep.
2023-02-07 19:39:22.750 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 926 for sweep.
2023-02-07 19:39:22.750 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.06048119952992349 for sweep.
2023-02-07 19:39:22.751 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:39:22.751 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 19:39:22.751 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.9247993530142038 for sweep.
2023-02-07 19:39:22.752 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 15 for sweep.
2023-02-07 19:39:22.752 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 8 for sweep.
2023-02-07 19:39:22.752 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.7977608393281455 for sweep.
2023-02-07 19:39:22.752 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 21 for sweep.
2023-02-07 19:39:22.753 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.04173987519981564 for sweep.
2023-02-07 19:39:22.753 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2442 for sweep.
2023-02-07 19:39:22.753 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 344 for sweep.
2023-02-07 19:39:22.753 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.0267968894269733 for sweep.
2023-02-07 19:39:22.753 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.4145924583096241 for sweep.
2023-02-07 19:39:22.754 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5645485776356274 for sweep.
2023-02-07 19:39:22.754 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:39:22.761 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 5}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193914-n32dfjb7/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 926, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 15, 'window': 8, 'min_count': 6, 'dm': 0, 'sample': 0.9247993530142038, 'workers': 4, 'alpha': 0.06048119952992349, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2442, 'max_depth': 21, 'num_leaves': 344, 'reg_alpha': 0.0267968894269733, 'reg_lambda': 0.4145924583096241, 'subsample': 0.5645485776356274, 'min_child_weight': 0.04173987519981564, 'n_jobs': 4, 'learning_rate': 0.7977608393281455}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 21/3257 [00:00<00:15, 206.67it/s]  1%|‚ñè         | 43/3257 [00:00<00:15, 211.30it/s]  2%|‚ñè         | 67/3257 [00:00<00:14, 217.67it/s]  3%|‚ñé         | 92/3257 [00:00<00:13, 226.20it/s]  4%|‚ñé         | 115/3257 [00:00<00:14, 217.88it/s]  4%|‚ñç         | 138/3257 [00:00<00:14, 221.77it/s]  5%|‚ñç         | 161/3257 [00:00<00:13, 222.02it/s]  6%|‚ñå         | 184/3257 [00:00<00:14, 218.03it/s]  6%|‚ñã         | 208/3257 [00:00<00:13, 223.87it/s]  7%|‚ñã         | 233/3257 [00:01<00:13, 231.15it/s]  8%|‚ñä         | 257/3257 [00:01<00:13, 221.05it/s]  9%|‚ñä         | 284/3257 [00:01<00:12, 232.72it/s]  9%|‚ñâ         | 308/3257 [00:01<00:12, 227.19it/s] 10%|‚ñà         | 332/3257 [00:01<00:12, 228.05it/s] 11%|‚ñà         | 355/3257 [00:01<00:12, 224.08it/s] 12%|‚ñà‚ñè        | 378/3257 [00:01<00:13, 209.81it/s] 12%|‚ñà‚ñè        | 400/3257 [00:01<00:13, 210.17it/s] 13%|‚ñà‚ñé        | 422/3257 [00:01<00:13, 210.85it/s] 14%|‚ñà‚ñé        | 444/3257 [00:02<00:14, 188.65it/s] 14%|‚ñà‚ñç        | 467/3257 [00:02<00:14, 197.18it/s] 15%|‚ñà‚ñç        | 488/3257 [00:02<00:14, 197.46it/s] 16%|‚ñà‚ñå        | 513/3257 [00:02<00:13, 209.73it/s] 16%|‚ñà‚ñã        | 535/3257 [00:02<00:12, 210.59it/s] 17%|‚ñà‚ñã        | 558/3257 [00:02<00:18, 148.40it/s] 18%|‚ñà‚ñä        | 576/3257 [00:02<00:17, 151.15it/s] 18%|‚ñà‚ñä        | 600/3257 [00:02<00:15, 171.60it/s] 19%|‚ñà‚ñâ        | 622/3257 [00:03<00:14, 180.81it/s] 20%|‚ñà‚ñâ        | 646/3257 [00:03<00:13, 193.88it/s] 20%|‚ñà‚ñà        | 667/3257 [00:03<00:13, 192.48it/s] 21%|‚ñà‚ñà        | 688/3257 [00:03<00:13, 188.35it/s] 22%|‚ñà‚ñà‚ñè       | 712/3257 [00:03<00:12, 200.98it/s] 23%|‚ñà‚ñà‚ñé       | 733/3257 [00:03<00:12, 194.17it/s] 23%|‚ñà‚ñà‚ñé       | 753/3257 [00:03<00:12, 194.91it/s] 24%|‚ñà‚ñà‚ñç       | 776/3257 [00:03<00:12, 201.75it/s] 25%|‚ñà‚ñà‚ñç       | 799/3257 [00:03<00:11, 208.33it/s] 25%|‚ñà‚ñà‚ñå       | 821/3257 [00:04<00:11, 208.21it/s] 26%|‚ñà‚ñà‚ñå       | 842/3257 [00:04<00:12, 197.34it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:04<00:11, 202.08it/s] 27%|‚ñà‚ñà‚ñã       | 885/3257 [00:04<00:12, 191.65it/s] 28%|‚ñà‚ñà‚ñä       | 911/3257 [00:04<00:11, 210.43it/s] 29%|‚ñà‚ñà‚ñä       | 933/3257 [00:04<00:11, 209.60it/s] 29%|‚ñà‚ñà‚ñâ       | 958/3257 [00:04<00:10, 220.12it/s] 30%|‚ñà‚ñà‚ñà       | 981/3257 [00:04<00:10, 218.04it/s] 31%|‚ñà‚ñà‚ñà       | 1003/3257 [00:04<00:10, 216.86it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1025/3257 [00:05<00:10, 210.66it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:05<00:11, 189.40it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:05<00:11, 190.80it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1089/3257 [00:05<00:10, 198.57it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1112/3257 [00:05<00:10, 205.04it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1133/3257 [00:05<00:10, 195.25it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1153/3257 [00:05<00:10, 193.81it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1174/3257 [00:05<00:10, 197.57it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1194/3257 [00:05<00:11, 185.42it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1213/3257 [00:06<00:10, 185.82it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1241/3257 [00:06<00:09, 209.98it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1265/3257 [00:06<00:09, 216.33it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1287/3257 [00:06<00:09, 198.33it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1309/3257 [00:06<00:09, 202.74it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1336/3257 [00:06<00:08, 219.92it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1359/3257 [00:06<00:08, 216.16it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1381/3257 [00:06<00:08, 211.87it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1407/3257 [00:06<00:08, 223.62it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:06<00:08, 226.61it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1455/3257 [00:07<00:07, 227.14it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1481/3257 [00:07<00:07, 234.41it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1509/3257 [00:07<00:07, 244.85it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1534/3257 [00:07<00:07, 220.64it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1557/3257 [00:07<00:07, 220.43it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1580/3257 [00:07<00:07, 217.25it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1604/3257 [00:07<00:07, 223.35it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1628/3257 [00:07<00:07, 228.07it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1651/3257 [00:08<00:08, 194.26it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1672/3257 [00:08<00:08, 192.58it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1694/3257 [00:08<00:07, 197.99it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1719/3257 [00:08<00:07, 210.77it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1741/3257 [00:08<00:07, 198.47it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1766/3257 [00:08<00:07, 212.04it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1792/3257 [00:08<00:06, 223.52it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1815/3257 [00:08<00:06, 216.56it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1839/3257 [00:08<00:06, 221.98it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1864/3257 [00:08<00:06, 229.01it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1888/3257 [00:09<00:08, 159.48it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1915/3257 [00:09<00:07, 183.75it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1940/3257 [00:09<00:06, 198.64it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1971/3257 [00:09<00:05, 224.38it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1996/3257 [00:09<00:05, 225.53it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2021/3257 [00:09<00:05, 231.83it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2046/3257 [00:09<00:05, 228.41it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2070/3257 [00:10<00:05, 213.39it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2095/3257 [00:10<00:05, 222.99it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2119/3257 [00:10<00:05, 226.64it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2143/3257 [00:10<00:05, 209.66it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:10<00:04, 222.22it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2193/3257 [00:10<00:04, 225.74it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2216/3257 [00:10<00:04, 219.62it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2239/3257 [00:10<00:04, 212.95it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2264/3257 [00:10<00:04, 221.32it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2287/3257 [00:10<00:04, 222.57it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2310/3257 [00:11<00:04, 217.27it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2339/3257 [00:11<00:03, 235.26it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2366/3257 [00:11<00:03, 244.93it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2393/3257 [00:11<00:03, 250.06it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2419/3257 [00:11<00:03, 229.86it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2443/3257 [00:11<00:03, 218.83it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2470/3257 [00:11<00:03, 231.33it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2495/3257 [00:11<00:03, 234.00it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2522/3257 [00:11<00:03, 243.68it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2548/3257 [00:12<00:02, 247.02it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2573/3257 [00:12<00:02, 231.17it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2597/3257 [00:12<00:02, 230.28it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2629/3257 [00:12<00:02, 254.38it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2655/3257 [00:12<00:02, 246.56it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2680/3257 [00:12<00:02, 243.20it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2705/3257 [00:12<00:02, 228.24it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2729/3257 [00:12<00:02, 229.57it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2754/3257 [00:12<00:02, 235.11it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2778/3257 [00:13<00:02, 225.71it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:13<00:01, 239.00it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2831/3257 [00:13<00:01, 227.43it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2854/3257 [00:13<00:01, 224.20it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2885/3257 [00:13<00:01, 248.07it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2911/3257 [00:13<00:01, 222.01it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2934/3257 [00:13<00:01, 221.48it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2957/3257 [00:13<00:01, 211.73it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2979/3257 [00:13<00:01, 213.57it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3003/3257 [00:14<00:01, 220.41it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3026/3257 [00:14<00:01, 219.26it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3053/3257 [00:14<00:00, 231.34it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3080/3257 [00:14<00:00, 239.63it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3105/3257 [00:14<00:00, 240.56it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3130/3257 [00:14<00:00, 239.66it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3155/3257 [00:14<00:00, 222.29it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3178/3257 [00:14<00:00, 214.13it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3205/3257 [00:14<00:00, 228.74it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3229/3257 [00:15<00:00, 221.30it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3254/3257 [00:15<00:00, 226.70it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:15<00:00, 214.78it/s]
2023-02-07 19:39:38.565 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:39:38,567][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d15,n5,mc6,s0.924799,t4>', 'datetime': '2023-02-07T19:39:38.566977', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:39:38,568][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:39:38,568][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:39:38,942][gensim.models.doc2vec][INFO] - collected 21699 word types and 3257 unique tags from a corpus of 3257 examples and 4367244 words
[2023-02-07 19:39:38,942][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:39:38,970][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 10832 unique words (49.92% of original 21699, drops 10867)', 'datetime': '2023-02-07T19:39:38.970574', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:39:38,970][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 4339647 word corpus (99.37% of original 4367244, drops 27597)', 'datetime': '2023-02-07T19:39:38.970943', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:39:39,005][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 21699 items
[2023-02-07 19:39:39,006][gensim.models.word2vec][INFO] - sample=0.924799 downsamples 0 most-common words
[2023-02-07 19:39:39,007][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4339647 word corpus (100.0%% of prior 4339647)', 'datetime': '2023-02-07T19:39:39.007649', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:39:39,066][gensim.models.word2vec][INFO] - estimated required memory for 10832 words and 15 dimensions: 7562660 bytes
[2023-02-07 19:39:39,066][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:39:39,067][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 10832 vocabulary and 15 features, using sg=1 hs=0 sample=0.9247993530142038 negative=5 window=8 shrink_windows=True', 'datetime': '2023-02-07T19:39:39.067781', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:39:40,045][gensim.models.word2vec][INFO] - EPOCH 0: training on 4367244 raw words (4341209 effective words) took 1.0s, 4448902 effective words/s
[2023-02-07 19:39:41,051][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 92.75% examples, 4049264 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:39:41,122][gensim.models.word2vec][INFO] - EPOCH 1: training on 4367244 raw words (4341209 effective words) took 1.1s, 4046867 effective words/s
[2023-02-07 19:39:42,029][gensim.models.word2vec][INFO] - EPOCH 2: training on 4367244 raw words (4341209 effective words) took 0.9s, 4795904 effective words/s
[2023-02-07 19:39:43,035][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 96.41% examples, 4168481 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:39:43,066][gensim.models.word2vec][INFO] - EPOCH 3: training on 4367244 raw words (4341209 effective words) took 1.0s, 4191784 effective words/s
[2023-02-07 19:39:44,068][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 94.69% examples, 4116873 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:39:44,120][gensim.models.word2vec][INFO] - EPOCH 4: training on 4367244 raw words (4341209 effective words) took 1.1s, 4124630 effective words/s
[2023-02-07 19:39:44,980][gensim.models.word2vec][INFO] - EPOCH 5: training on 4367244 raw words (4341209 effective words) took 0.9s, 5058950 effective words/s
[2023-02-07 19:39:45,986][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 95.15% examples, 4121071 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:39:46,031][gensim.models.word2vec][INFO] - EPOCH 6: training on 4367244 raw words (4341209 effective words) took 1.0s, 4136899 effective words/s
[2023-02-07 19:39:47,035][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 95.98% examples, 4160200 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:39:47,077][gensim.models.word2vec][INFO] - EPOCH 7: training on 4367244 raw words (4341209 effective words) took 1.0s, 4152984 effective words/s
[2023-02-07 19:39:48,083][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 91.07% examples, 3969265 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:39:48,173][gensim.models.word2vec][INFO] - EPOCH 8: training on 4367244 raw words (4341209 effective words) took 1.1s, 3972335 effective words/s
[2023-02-07 19:39:49,176][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 92.75% examples, 4045838 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:39:49,245][gensim.models.word2vec][INFO] - EPOCH 9: training on 4367244 raw words (4341209 effective words) took 1.1s, 4053177 effective words/s
[2023-02-07 19:39:50,112][gensim.models.word2vec][INFO] - EPOCH 10: training on 4367244 raw words (4341209 effective words) took 0.9s, 5019140 effective words/s
[2023-02-07 19:39:51,114][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 92.42% examples, 4029641 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:39:51,189][gensim.models.word2vec][INFO] - EPOCH 11: training on 4367244 raw words (4341209 effective words) took 1.1s, 4033703 effective words/s
[2023-02-07 19:39:52,192][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 90.36% examples, 3948197 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:39:52,297][gensim.models.word2vec][INFO] - EPOCH 12: training on 4367244 raw words (4341209 effective words) took 1.1s, 3925840 effective words/s
[2023-02-07 19:39:53,301][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 86.12% examples, 3755052 words/s, in_qsize 5, out_qsize 3
[2023-02-07 19:39:53,444][gensim.models.word2vec][INFO] - EPOCH 13: training on 4367244 raw words (4341209 effective words) took 1.1s, 3789367 effective words/s
[2023-02-07 19:39:54,448][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 83.85% examples, 3667450 words/s, in_qsize 8, out_qsize 3
[2023-02-07 19:39:54,610][gensim.models.word2vec][INFO] - EPOCH 14: training on 4367244 raw words (4341209 effective words) took 1.2s, 3727898 effective words/s
[2023-02-07 19:39:54,610][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 65508660 raw words (65118135 effective words) took 15.5s, 4189631 effective words/s', 'datetime': '2023-02-07T19:39:54.610770', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:39:54.611 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:39:57,190][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193914-n32dfjb7/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:39:57.190293', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:39:57,192][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:39:57,205][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_193914-n32dfjb7/files/../tmp/embedding_model.pt
2023-02-07 19:39:57.205 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:39:58.076 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:39:58.401 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:40:07.504 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.5083060074539585, 'test_mae': 1.1667031640275467, 'test_r2': -3.3305697821170597}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.53
wandb: percentage 0.50081
wandb:   test_mae 1.1667
wandb:   test_mse 2.50831
wandb:    test_r2 -3.33057
wandb: 
wandb: üöÄ View run playful-sweep-55 at: https://wandb.ai/xiaoqiz/mof2vec/runs/n32dfjb7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_193914-n32dfjb7/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: jiyc1v1m with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 670
wandb: 	model.gensim.alpha: 0.006744147863300421
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.6076696232624921
wandb: 	model.gensim.vector_size: 395
wandb: 	model.gensim.window: 14
wandb: 	model.sklearn.learning_rate: 0.004225156330287428
wandb: 	model.sklearn.max_depth: 48
wandb: 	model.sklearn.min_child_weight: 0.052630567814734186
wandb: 	model.sklearn.n_estimators: 206
wandb: 	model.sklearn.num_leaves: 352
wandb: 	model.sklearn.reg_alpha: 0.011906800620372528
wandb: 	model.sklearn.reg_lambda: 0.04855833229580711
wandb: 	model.sklearn.subsample: 0.9703888845996198
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194027-jiyc1v1m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-56
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/jiyc1v1m
2023-02-07 19:40:35.218 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 19:40:35.219 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 670 for sweep.
2023-02-07 19:40:35.219 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.006744147863300421 for sweep.
2023-02-07 19:40:35.219 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:40:35.220 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 19:40:35.220 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6076696232624921 for sweep.
2023-02-07 19:40:35.220 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 395 for sweep.
2023-02-07 19:40:35.220 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 14 for sweep.
2023-02-07 19:40:35.221 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.004225156330287428 for sweep.
2023-02-07 19:40:35.221 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 48 for sweep.
2023-02-07 19:40:35.221 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.052630567814734186 for sweep.
2023-02-07 19:40:35.221 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 206 for sweep.
2023-02-07 19:40:35.222 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 352 for sweep.
2023-02-07 19:40:35.222 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.011906800620372528 for sweep.
2023-02-07 19:40:35.222 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.04855833229580711 for sweep.
2023-02-07 19:40:35.222 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.9703888845996198 for sweep.
2023-02-07 19:40:35.222 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:40:35.245 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194027-jiyc1v1m/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 670, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 395, 'window': 14, 'min_count': 1, 'dm': 0, 'sample': 0.6076696232624921, 'workers': 4, 'alpha': 0.006744147863300421, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 206, 'max_depth': 48, 'num_leaves': 352, 'reg_alpha': 0.011906800620372528, 'reg_lambda': 0.04855833229580711, 'subsample': 0.9703888845996198, 'min_child_weight': 0.052630567814734186, 'n_jobs': 4, 'learning_rate': 0.004225156330287428}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:18, 173.70it/s]  1%|          | 39/3257 [00:00<00:16, 191.21it/s]  2%|‚ñè         | 59/3257 [00:00<00:17, 184.43it/s]  2%|‚ñè         | 78/3257 [00:00<00:25, 126.30it/s]  3%|‚ñé         | 97/3257 [00:00<00:22, 142.71it/s]  4%|‚ñé         | 115/3257 [00:00<00:20, 151.94it/s]  4%|‚ñç         | 136/3257 [00:00<00:18, 167.28it/s]  5%|‚ñç         | 159/3257 [00:00<00:16, 182.72it/s]  5%|‚ñå         | 179/3257 [00:01<00:16, 183.78it/s]  6%|‚ñå         | 201/3257 [00:01<00:16, 190.57it/s]  7%|‚ñã         | 228/3257 [00:01<00:14, 212.52it/s]  8%|‚ñä         | 250/3257 [00:01<00:14, 205.27it/s]  8%|‚ñä         | 271/3257 [00:01<00:14, 203.16it/s]  9%|‚ñâ         | 297/3257 [00:01<00:13, 218.67it/s] 10%|‚ñâ         | 320/3257 [00:01<00:13, 218.99it/s] 11%|‚ñà         | 343/3257 [00:01<00:13, 213.15it/s] 11%|‚ñà         | 366/3257 [00:01<00:13, 214.97it/s] 12%|‚ñà‚ñè        | 388/3257 [00:02<00:14, 197.68it/s] 13%|‚ñà‚ñé        | 411/3257 [00:02<00:13, 204.14it/s] 13%|‚ñà‚ñé        | 432/3257 [00:02<00:15, 182.89it/s] 14%|‚ñà‚ñç        | 451/3257 [00:02<00:15, 179.63it/s] 15%|‚ñà‚ñç        | 474/3257 [00:02<00:14, 192.12it/s] 15%|‚ñà‚ñå        | 495/3257 [00:02<00:14, 196.42it/s] 16%|‚ñà‚ñå        | 517/3257 [00:02<00:13, 202.86it/s] 17%|‚ñà‚ñã        | 538/3257 [00:02<00:13, 200.94it/s] 17%|‚ñà‚ñã        | 559/3257 [00:02<00:14, 187.96it/s] 18%|‚ñà‚ñä        | 579/3257 [00:03<00:14, 182.52it/s] 18%|‚ñà‚ñä        | 602/3257 [00:03<00:13, 194.98it/s] 19%|‚ñà‚ñâ        | 622/3257 [00:03<00:13, 192.35it/s] 20%|‚ñà‚ñâ        | 644/3257 [00:03<00:13, 198.46it/s] 20%|‚ñà‚ñà        | 665/3257 [00:03<00:13, 185.51it/s] 21%|‚ñà‚ñà        | 684/3257 [00:03<00:13, 185.10it/s] 22%|‚ñà‚ñà‚ñè       | 704/3257 [00:03<00:13, 188.97it/s] 22%|‚ñà‚ñà‚ñè       | 724/3257 [00:03<00:13, 187.34it/s] 23%|‚ñà‚ñà‚ñé       | 743/3257 [00:03<00:13, 187.21it/s] 24%|‚ñà‚ñà‚ñé       | 766/3257 [00:04<00:12, 198.12it/s] 24%|‚ñà‚ñà‚ñç       | 786/3257 [00:04<00:13, 185.11it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:04<00:13, 187.46it/s] 25%|‚ñà‚ñà‚ñå       | 825/3257 [00:04<00:13, 184.83it/s] 26%|‚ñà‚ñà‚ñå       | 844/3257 [00:04<00:13, 180.31it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:04<00:12, 184.85it/s] 27%|‚ñà‚ñà‚ñã       | 883/3257 [00:04<00:12, 185.46it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:04<00:12, 193.30it/s] 28%|‚ñà‚ñà‚ñä       | 927/3257 [00:04<00:11, 200.43it/s] 29%|‚ñà‚ñà‚ñâ       | 948/3257 [00:04<00:11, 195.42it/s] 30%|‚ñà‚ñà‚ñâ       | 968/3257 [00:05<00:11, 196.30it/s] 30%|‚ñà‚ñà‚ñà       | 988/3257 [00:05<00:12, 188.17it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:05<00:12, 184.29it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:05<00:12, 185.36it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1046/3257 [00:05<00:12, 182.94it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1066/3257 [00:05<00:11, 187.26it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1086/3257 [00:05<00:11, 188.59it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1107/3257 [00:05<00:11, 192.50it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1127/3257 [00:05<00:11, 186.93it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1146/3257 [00:06<00:11, 181.86it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:06<00:10, 192.01it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1190/3257 [00:06<00:11, 175.49it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1208/3257 [00:06<00:18, 112.70it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1232/3257 [00:06<00:14, 136.11it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1251/3257 [00:06<00:13, 147.33it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1274/3257 [00:06<00:11, 166.29it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1293/3257 [00:07<00:11, 164.01it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1315/3257 [00:07<00:10, 177.77it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:07<00:09, 194.22it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1360/3257 [00:07<00:09, 193.18it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1381/3257 [00:07<00:10, 186.66it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1405/3257 [00:07<00:09, 200.14it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:07<00:08, 213.52it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1459/3257 [00:07<00:07, 228.57it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1483/3257 [00:07<00:07, 229.38it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1510/3257 [00:08<00:07, 239.29it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1535/3257 [00:08<00:08, 210.50it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1557/3257 [00:08<00:08, 208.98it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1579/3257 [00:08<00:08, 209.09it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1604/3257 [00:08<00:07, 219.40it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1629/3257 [00:08<00:07, 226.93it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1652/3257 [00:08<00:07, 207.57it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1674/3257 [00:08<00:07, 205.40it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1695/3257 [00:08<00:07, 204.55it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1718/3257 [00:09<00:07, 211.49it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1740/3257 [00:09<00:07, 197.84it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1764/3257 [00:09<00:07, 209.04it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1790/3257 [00:09<00:06, 221.55it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1813/3257 [00:09<00:06, 211.46it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1835/3257 [00:09<00:06, 211.52it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1858/3257 [00:09<00:06, 214.00it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1883/3257 [00:09<00:06, 220.95it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1906/3257 [00:09<00:06, 221.53it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1929/3257 [00:10<00:06, 214.63it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1962/3257 [00:10<00:05, 246.37it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1987/3257 [00:10<00:05, 230.67it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2011/3257 [00:10<00:05, 230.69it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2036/3257 [00:10<00:05, 234.13it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2060/3257 [00:10<00:05, 207.42it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2084/3257 [00:10<00:05, 214.11it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2106/3257 [00:10<00:05, 211.88it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2128/3257 [00:10<00:05, 199.71it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2149/3257 [00:11<00:05, 193.03it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2173/3257 [00:11<00:05, 203.74it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2194/3257 [00:11<00:05, 204.06it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2215/3257 [00:11<00:05, 198.75it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2237/3257 [00:11<00:04, 204.60it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2258/3257 [00:11<00:04, 201.78it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2279/3257 [00:11<00:05, 191.40it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2301/3257 [00:11<00:04, 197.31it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2329/3257 [00:11<00:04, 220.07it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2357/3257 [00:12<00:03, 234.64it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2381/3257 [00:12<00:03, 227.78it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:12<00:03, 224.17it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2428/3257 [00:12<00:03, 223.95it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2451/3257 [00:12<00:03, 209.84it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2478/3257 [00:12<00:03, 224.05it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2504/3257 [00:12<00:03, 233.54it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2529/3257 [00:12<00:03, 238.13it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2553/3257 [00:12<00:03, 230.37it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2577/3257 [00:13<00:04, 143.14it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2596/3257 [00:13<00:04, 152.31it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2626/3257 [00:13<00:03, 184.60it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2648/3257 [00:13<00:03, 189.03it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:13<00:03, 194.36it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2693/3257 [00:13<00:02, 202.04it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2715/3257 [00:13<00:02, 181.21it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2742/3257 [00:13<00:02, 202.11it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2764/3257 [00:14<00:02, 204.20it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2787/3257 [00:14<00:02, 208.97it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2809/3257 [00:14<00:02, 211.93it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2831/3257 [00:14<00:02, 201.30it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2854/3257 [00:14<00:01, 208.17it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2885/3257 [00:14<00:01, 234.50it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2909/3257 [00:14<00:01, 215.34it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2932/3257 [00:14<00:01, 217.14it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2955/3257 [00:14<00:01, 205.89it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:15<00:01, 206.44it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3000/3257 [00:15<00:01, 214.34it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3022/3257 [00:15<00:01, 206.74it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3048/3257 [00:15<00:00, 219.24it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3075/3257 [00:15<00:00, 232.75it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3099/3257 [00:15<00:00, 229.38it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3126/3257 [00:15<00:00, 240.80it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3151/3257 [00:15<00:00, 220.00it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3175/3257 [00:15<00:00, 222.66it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3198/3257 [00:16<00:00, 219.31it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3221/3257 [00:16<00:00, 212.81it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3248/3257 [00:16<00:00, 228.08it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 199.53it/s]
2023-02-07 19:40:52.282 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:40:52,283][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d395,n5,s0.60767,t4>', 'datetime': '2023-02-07T19:40:52.283248', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:40:52,283][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:40:52,283][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:40:52,757][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 19:40:52,758][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:40:52,832][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 31803 unique words (100.00% of original 31803, drops 0)', 'datetime': '2023-02-07T19:40:52.832154', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:40:52,832][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 5095118 word corpus (100.00% of original 5095118, drops 0)', 'datetime': '2023-02-07T19:40:52.832500', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:40:52,934][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 19:40:52,935][gensim.models.word2vec][INFO] - sample=0.60767 downsamples 0 most-common words
[2023-02-07 19:40:52,935][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5095118 word corpus (100.0%% of prior 5095118)', 'datetime': '2023-02-07T19:40:52.935481', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:40:53,107][gensim.models.word2vec][INFO] - estimated required memory for 31803 words and 395 dimensions: 122196440 bytes
[2023-02-07 19:40:53,108][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:40:53,155][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 31803 vocabulary and 395 features, using sg=1 hs=0 sample=0.6076696232624921 negative=5 window=14 shrink_windows=True', 'datetime': '2023-02-07T19:40:53.155587', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:40:54,162][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 25.88% examples, 1320789 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:40:55,171][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 53.45% examples, 1387685 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:40:56,178][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 83.24% examples, 1419186 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:40:56,702][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5086629 effective words) took 3.5s, 1435696 effective words/s
[2023-02-07 19:40:57,713][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 42.40% examples, 2203961 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:40:58,714][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 86.67% examples, 2210957 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:40:58,999][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5086629 effective words) took 2.3s, 2215758 effective words/s
[2023-02-07 19:41:00,003][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 42.40% examples, 2222205 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:01,006][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 86.67% examples, 2218072 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:01,292][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5086629 effective words) took 2.3s, 2221461 effective words/s
[2023-02-07 19:41:02,299][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 33.47% examples, 1716589 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:03,302][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 66.66% examples, 1723824 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:04,248][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5086629 effective words) took 3.0s, 1722573 effective words/s
[2023-02-07 19:41:05,253][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 42.55% examples, 2223371 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:06,256][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 88.15% examples, 2247063 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:06,513][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5086629 effective words) took 2.3s, 2246916 effective words/s
[2023-02-07 19:41:07,527][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 33.59% examples, 1711461 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:41:08,528][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 66.75% examples, 1721754 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:09,458][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5086629 effective words) took 2.9s, 1728104 effective words/s
[2023-02-07 19:41:10,464][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 33.47% examples, 1717849 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:41:11,466][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 67.18% examples, 1737366 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:12,360][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5086629 effective words) took 2.9s, 1754299 effective words/s
[2023-02-07 19:41:13,363][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 41.66% examples, 2177331 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:14,366][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 85.82% examples, 2195591 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:14,659][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5086629 effective words) took 2.3s, 2213745 effective words/s
[2023-02-07 19:41:15,663][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 34.48% examples, 1775849 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:16,663][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 68.35% examples, 1775391 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:17,527][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5086629 effective words) took 2.9s, 1774898 effective words/s
[2023-02-07 19:41:18,534][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 33.59% examples, 1723268 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:19,538][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 67.30% examples, 1739735 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:41:20,438][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5086629 effective words) took 2.9s, 1748088 effective words/s
[2023-02-07 19:41:21,447][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 41.66% examples, 2167562 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:41:22,448][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 87.10% examples, 2229426 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:41:22,711][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5086629 effective words) took 2.3s, 2241214 effective words/s
[2023-02-07 19:41:23,718][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 42.40% examples, 2210161 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:24,720][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 86.77% examples, 2219044 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:25,006][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5086629 effective words) took 2.3s, 2217470 effective words/s
[2023-02-07 19:41:26,009][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 34.94% examples, 1801336 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:27,016][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 68.84% examples, 1789141 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:27,697][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5086629 effective words) took 2.7s, 1892363 effective words/s
[2023-02-07 19:41:28,706][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 33.59% examples, 1720111 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:41:29,708][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 67.45% examples, 1743661 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:30,627][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5086629 effective words) took 2.9s, 1737467 effective words/s
[2023-02-07 19:41:31,641][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 36.97% examples, 1906363 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:32,639][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 79.52% examples, 2038095 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:41:33,136][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5086629 effective words) took 2.5s, 2028428 effective words/s
[2023-02-07 19:41:33,137][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (76299435 effective words) took 40.0s, 1908374 effective words/s', 'datetime': '2023-02-07T19:41:33.137388', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:41:33.137 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:41:36,506][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194027-jiyc1v1m/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:41:36.506139', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:41:36,509][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194027-jiyc1v1m/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 19:41:36,569][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194027-jiyc1v1m/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 19:41:36,625][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:41:36,666][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194027-jiyc1v1m/files/../tmp/embedding_model.pt
2023-02-07 19:41:36.666 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:41:38.704 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:41:39.395 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:41:43.442 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.165676637840166, 'test_mae': 1.0910942186701449, 'test_r2': -1.791700961439156}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.9
wandb: percentage 0.0
wandb:   test_mae 1.09109
wandb:   test_mse 2.16568
wandb:    test_r2 -1.7917
wandb: 
wandb: üöÄ View run exalted-sweep-56 at: https://wandb.ai/xiaoqiz/mof2vec/runs/jiyc1v1m
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_194027-jiyc1v1m/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: qpq0erg4 with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 342
wandb: 	model.gensim.alpha: 0.6839464669698534
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.5352249067561289
wandb: 	model.gensim.vector_size: 155
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.9902580017085476
wandb: 	model.sklearn.max_depth: 17
wandb: 	model.sklearn.min_child_weight: 0.09108676542376232
wandb: 	model.sklearn.n_estimators: 142
wandb: 	model.sklearn.num_leaves: 396
wandb: 	model.sklearn.reg_alpha: 0.17575175865907075
wandb: 	model.sklearn.reg_lambda: 0.10401864292444672
wandb: 	model.sklearn.subsample: 0.7600574880307032
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194203-qpq0erg4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-57
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/qpq0erg4
2023-02-07 19:42:12.132 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 19:42:12.133 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 342 for sweep.
2023-02-07 19:42:12.133 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.6839464669698534 for sweep.
2023-02-07 19:42:12.133 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:42:12.133 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 19:42:12.134 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5352249067561289 for sweep.
2023-02-07 19:42:12.134 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 155 for sweep.
2023-02-07 19:42:12.135 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 19:42:12.135 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.9902580017085476 for sweep.
2023-02-07 19:42:12.135 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 17 for sweep.
2023-02-07 19:42:12.135 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.09108676542376232 for sweep.
2023-02-07 19:42:12.136 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 142 for sweep.
2023-02-07 19:42:12.136 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 396 for sweep.
2023-02-07 19:42:12.136 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.17575175865907075 for sweep.
2023-02-07 19:42:12.136 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.10401864292444672 for sweep.
2023-02-07 19:42:12.136 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.7600574880307032 for sweep.
2023-02-07 19:42:12.137 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:42:12.144 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194203-qpq0erg4/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 342, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 155, 'window': 3, 'min_count': 1, 'dm': 0, 'sample': 0.5352249067561289, 'workers': 4, 'alpha': 0.6839464669698534, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 142, 'max_depth': 17, 'num_leaves': 396, 'reg_alpha': 0.17575175865907075, 'reg_lambda': 0.10401864292444672, 'subsample': 0.7600574880307032, 'min_child_weight': 0.09108676542376232, 'n_jobs': 4, 'learning_rate': 0.9902580017085476}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 16/3257 [00:00<00:20, 158.77it/s]  1%|          | 34/3257 [00:00<00:19, 168.41it/s]  2%|‚ñè         | 53/3257 [00:00<00:18, 173.13it/s]  2%|‚ñè         | 73/3257 [00:00<00:17, 182.13it/s]  3%|‚ñé         | 94/3257 [00:00<00:16, 187.87it/s]  3%|‚ñé         | 113/3257 [00:00<00:17, 178.46it/s]  4%|‚ñç         | 133/3257 [00:00<00:17, 183.02it/s]  5%|‚ñç         | 154/3257 [00:00<00:16, 188.75it/s]  5%|‚ñå         | 173/3257 [00:00<00:17, 180.00it/s]  6%|‚ñå         | 193/3257 [00:01<00:16, 182.54it/s]  7%|‚ñã         | 215/3257 [00:01<00:15, 191.69it/s]  7%|‚ñã         | 237/3257 [00:01<00:15, 194.06it/s]  8%|‚ñä         | 257/3257 [00:01<00:15, 195.60it/s]  9%|‚ñä         | 278/3257 [00:01<00:14, 198.84it/s]  9%|‚ñâ         | 300/3257 [00:01<00:14, 198.65it/s] 10%|‚ñâ         | 323/3257 [00:01<00:14, 206.41it/s] 11%|‚ñà         | 344/3257 [00:01<00:15, 194.15it/s] 11%|‚ñà         | 365/3257 [00:01<00:14, 198.49it/s] 12%|‚ñà‚ñè        | 385/3257 [00:02<00:15, 189.25it/s] 12%|‚ñà‚ñè        | 405/3257 [00:02<00:15, 188.30it/s] 13%|‚ñà‚ñé        | 424/3257 [00:02<00:15, 185.91it/s] 14%|‚ñà‚ñé        | 443/3257 [00:02<00:16, 167.39it/s] 14%|‚ñà‚ñç        | 465/3257 [00:02<00:15, 180.02it/s] 15%|‚ñà‚ñç        | 484/3257 [00:02<00:15, 180.05it/s] 16%|‚ñà‚ñå        | 509/3257 [00:02<00:13, 198.84it/s] 16%|‚ñà‚ñã        | 530/3257 [00:02<00:13, 196.10it/s] 17%|‚ñà‚ñã        | 550/3257 [00:02<00:13, 195.86it/s] 18%|‚ñà‚ñä        | 570/3257 [00:03<00:14, 190.27it/s] 18%|‚ñà‚ñä        | 590/3257 [00:03<00:14, 186.48it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:03<00:12, 203.34it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:03<00:12, 203.70it/s] 20%|‚ñà‚ñà        | 658/3257 [00:03<00:19, 130.14it/s] 21%|‚ñà‚ñà        | 682/3257 [00:03<00:17, 151.47it/s] 22%|‚ñà‚ñà‚ñè       | 701/3257 [00:03<00:16, 156.68it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:03<00:15, 166.47it/s] 23%|‚ñà‚ñà‚ñé       | 741/3257 [00:04<00:15, 166.64it/s] 23%|‚ñà‚ñà‚ñé       | 765/3257 [00:04<00:13, 183.89it/s] 24%|‚ñà‚ñà‚ñç       | 785/3257 [00:04<00:13, 180.17it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:04<00:13, 183.58it/s] 25%|‚ñà‚ñà‚ñå       | 825/3257 [00:04<00:13, 184.03it/s] 26%|‚ñà‚ñà‚ñå       | 844/3257 [00:04<00:13, 179.60it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:04<00:13, 182.87it/s] 27%|‚ñà‚ñà‚ñã       | 884/3257 [00:04<00:13, 180.17it/s] 28%|‚ñà‚ñà‚ñä       | 907/3257 [00:04<00:12, 192.64it/s] 29%|‚ñà‚ñà‚ñä       | 930/3257 [00:05<00:11, 200.08it/s] 29%|‚ñà‚ñà‚ñâ       | 951/3257 [00:05<00:11, 198.71it/s] 30%|‚ñà‚ñà‚ñâ       | 971/3257 [00:05<00:11, 198.51it/s] 30%|‚ñà‚ñà‚ñà       | 991/3257 [00:05<00:11, 193.69it/s] 31%|‚ñà‚ñà‚ñà       | 1011/3257 [00:05<00:11, 190.62it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1031/3257 [00:05<00:11, 186.45it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1050/3257 [00:05<00:12, 177.29it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1072/3257 [00:05<00:11, 188.55it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1092/3257 [00:05<00:11, 180.46it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1114/3257 [00:06<00:11, 191.24it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1134/3257 [00:06<00:11, 180.17it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1153/3257 [00:06<00:11, 180.23it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1175/3257 [00:06<00:11, 186.72it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1194/3257 [00:06<00:11, 173.48it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1212/3257 [00:06<00:12, 167.80it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1235/3257 [00:06<00:10, 183.96it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1254/3257 [00:06<00:10, 184.80it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1273/3257 [00:06<00:10, 184.25it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1292/3257 [00:07<00:11, 166.92it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1310/3257 [00:07<00:11, 168.59it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1329/3257 [00:07<00:11, 174.15it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1349/3257 [00:07<00:10, 181.08it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1368/3257 [00:07<00:10, 174.79it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1386/3257 [00:07<00:10, 171.92it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1410/3257 [00:07<00:09, 188.79it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:07<00:09, 192.56it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1454/3257 [00:07<00:09, 200.19it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1476/3257 [00:08<00:08, 204.86it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1500/3257 [00:08<00:08, 212.98it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1522/3257 [00:08<00:08, 197.26it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1543/3257 [00:08<00:08, 193.66it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1563/3257 [00:08<00:08, 188.92it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1583/3257 [00:08<00:09, 185.42it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1603/3257 [00:08<00:08, 187.39it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1622/3257 [00:08<00:08, 184.94it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1641/3257 [00:08<00:08, 179.96it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1660/3257 [00:09<00:09, 176.40it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1678/3257 [00:09<00:09, 169.77it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1699/3257 [00:09<00:08, 178.52it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1717/3257 [00:09<00:08, 178.56it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1735/3257 [00:09<00:09, 161.27it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1756/3257 [00:09<00:08, 173.15it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1776/3257 [00:09<00:08, 178.75it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1796/3257 [00:09<00:07, 184.31it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1815/3257 [00:09<00:08, 178.53it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1834/3257 [00:10<00:07, 178.91it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1855/3257 [00:10<00:07, 186.09it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1877/3257 [00:10<00:07, 195.56it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1897/3257 [00:10<00:07, 189.29it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1917/3257 [00:10<00:10, 122.04it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1936/3257 [00:10<00:09, 135.89it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1962/3257 [00:10<00:07, 163.30it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1982/3257 [00:10<00:07, 166.52it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:11<00:07, 176.74it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2029/3257 [00:11<00:06, 193.77it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2050/3257 [00:11<00:06, 181.58it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2070/3257 [00:11<00:06, 179.98it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2092/3257 [00:11<00:06, 189.84it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2112/3257 [00:11<00:06, 189.18it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2132/3257 [00:11<00:06, 181.52it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2151/3257 [00:11<00:06, 181.43it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2172/3257 [00:11<00:05, 189.11it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2192/3257 [00:12<00:05, 188.51it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2212/3257 [00:12<00:05, 183.39it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2233/3257 [00:12<00:05, 188.90it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2253/3257 [00:12<00:05, 183.95it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2272/3257 [00:12<00:05, 175.66it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2295/3257 [00:12<00:05, 189.73it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2315/3257 [00:12<00:05, 188.33it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2341/3257 [00:12<00:04, 207.86it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2366/3257 [00:12<00:04, 218.51it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2389/3257 [00:13<00:03, 220.62it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2412/3257 [00:13<00:04, 207.27it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2433/3257 [00:13<00:04, 194.56it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2453/3257 [00:13<00:04, 191.25it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2478/3257 [00:13<00:03, 203.95it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2503/3257 [00:13<00:03, 215.20it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2526/3257 [00:13<00:03, 216.20it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2549/3257 [00:13<00:03, 218.75it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2571/3257 [00:13<00:03, 198.06it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2592/3257 [00:14<00:03, 192.74it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2621/3257 [00:14<00:02, 216.17it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2643/3257 [00:14<00:02, 210.90it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2665/3257 [00:14<00:02, 200.36it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2688/3257 [00:14<00:02, 206.09it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2709/3257 [00:14<00:02, 182.67it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2730/3257 [00:14<00:02, 189.17it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2751/3257 [00:14<00:02, 193.32it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2771/3257 [00:14<00:02, 185.21it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2795/3257 [00:15<00:02, 197.41it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2816/3257 [00:15<00:02, 191.38it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2836/3257 [00:15<00:02, 185.07it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2861/3257 [00:15<00:01, 202.38it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:15<00:01, 210.40it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2908/3257 [00:15<00:01, 195.40it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2929/3257 [00:15<00:01, 196.43it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2949/3257 [00:15<00:01, 186.89it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2969/3257 [00:15<00:01, 189.65it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2989/3257 [00:16<00:01, 183.76it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:16<00:01, 195.66it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3032/3257 [00:16<00:01, 196.39it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3054/3257 [00:16<00:01, 200.49it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:16<00:00, 209.80it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3101/3257 [00:16<00:00, 206.17it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3124/3257 [00:16<00:00, 210.36it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3146/3257 [00:16<00:00, 194.14it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3166/3257 [00:16<00:00, 193.30it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3186/3257 [00:17<00:00, 186.12it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3205/3257 [00:17<00:00, 116.44it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3221/3257 [00:17<00:00, 124.60it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:17<00:00, 146.22it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:17<00:00, 184.28it/s]
2023-02-07 19:42:30.627 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:42:30,628][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d155,n5,s0.535225,t4>', 'datetime': '2023-02-07T19:42:30.628088', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:42:30,628][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:42:30,628][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:42:31,159][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 19:42:31,160][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:42:31,261][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 42701 unique words (100.00% of original 42701, drops 0)', 'datetime': '2023-02-07T19:42:31.261359', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:42:31,261][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 5822992 word corpus (100.00% of original 5822992, drops 0)', 'datetime': '2023-02-07T19:42:31.261884', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:42:31,400][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 19:42:31,401][gensim.models.word2vec][INFO] - sample=0.535225 downsamples 0 most-common words
[2023-02-07 19:42:31,402][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5822992 word corpus (100.0%% of prior 5822992)', 'datetime': '2023-02-07T19:42:31.402234', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:42:31,637][gensim.models.word2vec][INFO] - estimated required memory for 42701 words and 155 dimensions: 76970480 bytes
[2023-02-07 19:42:31,637][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:42:31,666][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 42701 vocabulary and 155 features, using sg=1 hs=0 sample=0.5352249067561289 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T19:42:31.665987', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:42:32,669][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 67.24% examples, 3969293 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:42:33,148][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5795785 effective words) took 1.5s, 3914908 effective words/s
[2023-02-07 19:42:34,152][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 66.41% examples, 3928370 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:42:34,609][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5795785 effective words) took 1.5s, 3977653 effective words/s
[2023-02-07 19:42:35,614][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 64.17% examples, 3767259 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:42:36,077][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5795785 effective words) took 1.5s, 3951082 effective words/s
[2023-02-07 19:42:37,081][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 72.92% examples, 4284442 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:42:37,425][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5795785 effective words) took 1.3s, 4309328 effective words/s
[2023-02-07 19:42:38,428][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 73.38% examples, 4300814 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:42:38,769][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5795785 effective words) took 1.3s, 4313796 effective words/s
[2023-02-07 19:42:39,770][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 75.19% examples, 4429753 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:42:40,076][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5795785 effective words) took 1.3s, 4440071 effective words/s
[2023-02-07 19:42:41,080][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 69.02% examples, 4070582 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:42:41,481][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5795785 effective words) took 1.4s, 4126069 effective words/s
[2023-02-07 19:42:42,482][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 69.33% examples, 4100912 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:42:42,881][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5795785 effective words) took 1.4s, 4143610 effective words/s
[2023-02-07 19:42:43,884][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 74.33% examples, 4357582 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:42:44,199][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5795785 effective words) took 1.3s, 4400621 effective words/s
[2023-02-07 19:42:45,201][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 73.13% examples, 4297422 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:42:45,575][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5795785 effective words) took 1.4s, 4215902 effective words/s
[2023-02-07 19:42:46,579][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 68.84% examples, 4067779 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:42:47,016][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5795785 effective words) took 1.4s, 4027710 effective words/s
[2023-02-07 19:42:48,020][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 65.83% examples, 3890412 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:42:48,493][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5795785 effective words) took 1.5s, 3927875 effective words/s
[2023-02-07 19:42:49,512][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 65.24% examples, 3833681 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:42:50,004][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5795785 effective words) took 1.5s, 3869511 effective words/s
[2023-02-07 19:42:51,010][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 68.41% examples, 4028420 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:42:51,460][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5795785 effective words) took 1.5s, 3988460 effective words/s
[2023-02-07 19:42:52,465][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 65.77% examples, 3878512 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:42:52,953][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5795785 effective words) took 1.5s, 3890655 effective words/s
[2023-02-07 19:42:52,954][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86936775 effective words) took 21.3s, 4083866 effective words/s', 'datetime': '2023-02-07T19:42:52.954249', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:42:52.954 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:42:55,338][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194203-qpq0erg4/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:42:55.338642', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:42:55,340][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:42:55,447][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194203-qpq0erg4/files/../tmp/embedding_model.pt
2023-02-07 19:42:55.447 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:42:56.747 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:42:57.187 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:42:58.256 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.900497362571898, 'test_mae': 1.2828994870280126, 'test_r2': -4.785585934456504}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.0
wandb: percentage 0.0
wandb:   test_mae 1.2829
wandb:   test_mse 2.9005
wandb:    test_r2 -4.78559
wandb: 
wandb: üöÄ View run leafy-sweep-57 at: https://wandb.ai/xiaoqiz/mof2vec/runs/qpq0erg4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_194203-qpq0erg4/logs
wandb: Agent Starting Run: 72fxz6y0 with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 988
wandb: 	model.gensim.alpha: 0.003811896997730256
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.5796386806168747
wandb: 	model.gensim.vector_size: 157
wandb: 	model.gensim.window: 9
wandb: 	model.sklearn.learning_rate: 0.008061233860150958
wandb: 	model.sklearn.max_depth: 61
wandb: 	model.sklearn.min_child_weight: 0.001613596714826522
wandb: 	model.sklearn.n_estimators: 2350
wandb: 	model.sklearn.num_leaves: 246
wandb: 	model.sklearn.reg_alpha: 0.10364356677439016
wandb: 	model.sklearn.reg_lambda: 0.5948741182217996
wandb: 	model.sklearn.subsample: 0.6473732192647537
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194308-72fxz6y0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-58
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/72fxz6y0
2023-02-07 19:43:16.061 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 19:43:16.061 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 988 for sweep.
2023-02-07 19:43:16.062 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.003811896997730256 for sweep.
2023-02-07 19:43:16.062 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:43:16.062 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 19:43:16.062 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5796386806168747 for sweep.
2023-02-07 19:43:16.063 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 157 for sweep.
2023-02-07 19:43:16.063 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 9 for sweep.
2023-02-07 19:43:16.063 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.008061233860150958 for sweep.
2023-02-07 19:43:16.063 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 61 for sweep.
2023-02-07 19:43:16.064 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.001613596714826522 for sweep.
2023-02-07 19:43:16.064 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2350 for sweep.
2023-02-07 19:43:16.064 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 246 for sweep.
2023-02-07 19:43:16.064 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.10364356677439016 for sweep.
2023-02-07 19:43:16.064 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.5948741182217996 for sweep.
2023-02-07 19:43:16.065 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6473732192647537 for sweep.
2023-02-07 19:43:16.065 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:43:16.072 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194308-72fxz6y0/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 988, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 157, 'window': 9, 'min_count': 3, 'dm': 0, 'sample': 0.5796386806168747, 'workers': 4, 'alpha': 0.003811896997730256, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2350, 'max_depth': 61, 'num_leaves': 246, 'reg_alpha': 0.10364356677439016, 'reg_lambda': 0.5948741182217996, 'subsample': 0.6473732192647537, 'min_child_weight': 0.001613596714826522, 'n_jobs': 4, 'learning_rate': 0.008061233860150958}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 19/3257 [00:00<00:17, 182.79it/s]  1%|‚ñè         | 41/3257 [00:00<00:16, 194.16it/s]  2%|‚ñè         | 61/3257 [00:00<00:16, 196.31it/s]  3%|‚ñé         | 84/3257 [00:00<00:15, 208.49it/s]  3%|‚ñé         | 105/3257 [00:00<00:15, 201.18it/s]  4%|‚ñç         | 126/3257 [00:00<00:16, 195.03it/s]  5%|‚ñç         | 149/3257 [00:00<00:15, 205.73it/s]  5%|‚ñå         | 170/3257 [00:00<00:16, 187.93it/s]  6%|‚ñå         | 190/3257 [00:00<00:16, 187.01it/s]  6%|‚ñã         | 210/3257 [00:01<00:16, 190.13it/s]  7%|‚ñã         | 236/3257 [00:01<00:14, 208.11it/s]  8%|‚ñä         | 258/3257 [00:01<00:14, 210.14it/s]  9%|‚ñä         | 283/3257 [00:01<00:13, 221.25it/s]  9%|‚ñâ         | 306/3257 [00:01<00:13, 215.92it/s] 10%|‚ñà         | 330/3257 [00:01<00:13, 222.17it/s] 11%|‚ñà         | 353/3257 [00:01<00:13, 217.41it/s] 12%|‚ñà‚ñè        | 375/3257 [00:01<00:13, 213.19it/s] 12%|‚ñà‚ñè        | 397/3257 [00:01<00:13, 205.52it/s] 13%|‚ñà‚ñé        | 420/3257 [00:02<00:13, 206.53it/s] 14%|‚ñà‚ñé        | 441/3257 [00:02<00:15, 181.13it/s] 14%|‚ñà‚ñç        | 462/3257 [00:02<00:14, 188.43it/s] 15%|‚ñà‚ñç        | 482/3257 [00:02<00:14, 189.18it/s] 16%|‚ñà‚ñå        | 505/3257 [00:02<00:13, 198.36it/s] 16%|‚ñà‚ñå        | 526/3257 [00:02<00:13, 200.22it/s] 17%|‚ñà‚ñã        | 548/3257 [00:02<00:13, 205.37it/s] 17%|‚ñà‚ñã        | 569/3257 [00:02<00:13, 197.94it/s] 18%|‚ñà‚ñä        | 589/3257 [00:02<00:14, 189.03it/s] 19%|‚ñà‚ñâ        | 614/3257 [00:03<00:12, 204.20it/s] 20%|‚ñà‚ñâ        | 636/3257 [00:03<00:12, 206.56it/s] 20%|‚ñà‚ñà        | 657/3257 [00:03<00:13, 189.74it/s] 21%|‚ñà‚ñà        | 680/3257 [00:03<00:12, 200.56it/s] 22%|‚ñà‚ñà‚ñè       | 701/3257 [00:03<00:13, 194.68it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:03<00:12, 196.01it/s] 23%|‚ñà‚ñà‚ñé       | 742/3257 [00:03<00:13, 191.01it/s] 24%|‚ñà‚ñà‚ñé       | 767/3257 [00:03<00:12, 207.03it/s] 24%|‚ñà‚ñà‚ñç       | 788/3257 [00:03<00:12, 193.90it/s] 25%|‚ñà‚ñà‚ñç       | 808/3257 [00:04<00:12, 190.10it/s] 25%|‚ñà‚ñà‚ñå       | 828/3257 [00:04<00:12, 190.28it/s] 26%|‚ñà‚ñà‚ñå       | 848/3257 [00:04<00:12, 185.55it/s] 27%|‚ñà‚ñà‚ñã       | 869/3257 [00:04<00:12, 190.27it/s] 27%|‚ñà‚ñà‚ñã       | 889/3257 [00:04<00:12, 189.99it/s] 28%|‚ñà‚ñà‚ñä       | 911/3257 [00:04<00:11, 196.62it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:04<00:11, 195.25it/s] 29%|‚ñà‚ñà‚ñâ       | 952/3257 [00:04<00:11, 198.89it/s] 30%|‚ñà‚ñà‚ñâ       | 972/3257 [00:04<00:11, 190.90it/s] 30%|‚ñà‚ñà‚ñà       | 992/3257 [00:05<00:12, 177.77it/s] 31%|‚ñà‚ñà‚ñà       | 1010/3257 [00:05<00:12, 175.57it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1029/3257 [00:05<00:12, 179.52it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1048/3257 [00:05<00:12, 178.30it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1070/3257 [00:05<00:11, 189.18it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1090/3257 [00:05<00:17, 122.86it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1107/3257 [00:05<00:16, 132.63it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1124/3257 [00:05<00:15, 140.42it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1142/3257 [00:06<00:14, 149.68it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1160/3257 [00:06<00:13, 156.81it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1178/3257 [00:06<00:13, 157.31it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1195/3257 [00:06<00:12, 160.53it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1212/3257 [00:06<00:12, 161.47it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1236/3257 [00:06<00:11, 182.48it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1255/3257 [00:06<00:10, 183.18it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:06<00:10, 184.28it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1294/3257 [00:06<00:10, 179.28it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1314/3257 [00:06<00:10, 184.52it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1335/3257 [00:07<00:10, 191.00it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1355/3257 [00:07<00:10, 183.38it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1374/3257 [00:07<00:10, 184.23it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1395/3257 [00:07<00:09, 190.63it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1420/3257 [00:07<00:08, 207.78it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1441/3257 [00:07<00:09, 200.14it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1466/3257 [00:07<00:08, 212.02it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1488/3257 [00:07<00:08, 214.10it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:07<00:07, 219.51it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1536/3257 [00:08<00:08, 202.76it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1557/3257 [00:08<00:08, 196.58it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1577/3257 [00:08<00:08, 189.13it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1599/3257 [00:08<00:08, 195.74it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1621/3257 [00:08<00:08, 199.10it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1642/3257 [00:08<00:08, 193.99it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1662/3257 [00:08<00:08, 189.68it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1682/3257 [00:08<00:08, 188.87it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1703/3257 [00:08<00:07, 194.47it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1724/3257 [00:09<00:07, 197.16it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1744/3257 [00:09<00:08, 183.95it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1767/3257 [00:09<00:07, 196.65it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1790/3257 [00:09<00:07, 204.75it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1811/3257 [00:09<00:07, 195.39it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1831/3257 [00:09<00:07, 194.10it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1854/3257 [00:09<00:06, 203.92it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1879/3257 [00:09<00:06, 212.10it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1901/3257 [00:09<00:06, 211.93it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1923/3257 [00:10<00:06, 205.69it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1950/3257 [00:10<00:05, 222.45it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1974/3257 [00:10<00:05, 222.51it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1997/3257 [00:10<00:05, 214.71it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2019/3257 [00:10<00:05, 212.83it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2041/3257 [00:10<00:05, 205.23it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2062/3257 [00:10<00:06, 190.89it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2085/3257 [00:10<00:05, 197.36it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2107/3257 [00:10<00:05, 202.69it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2128/3257 [00:11<00:05, 193.20it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2148/3257 [00:11<00:06, 179.73it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2171/3257 [00:11<00:05, 192.43it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2191/3257 [00:11<00:05, 193.80it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2211/3257 [00:11<00:05, 189.55it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:11<00:05, 201.98it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2257/3257 [00:11<00:04, 202.03it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2278/3257 [00:11<00:05, 194.29it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2300/3257 [00:11<00:04, 201.41it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2327/3257 [00:12<00:04, 219.48it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2357/3257 [00:12<00:03, 240.89it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2382/3257 [00:12<00:03, 234.93it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2406/3257 [00:12<00:03, 226.15it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2429/3257 [00:12<00:06, 133.92it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2448/3257 [00:12<00:05, 143.21it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2476/3257 [00:12<00:04, 170.62it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2503/3257 [00:13<00:03, 192.74it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2527/3257 [00:13<00:03, 204.34it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:13<00:03, 212.49it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2575/3257 [00:13<00:03, 203.57it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2597/3257 [00:13<00:03, 204.94it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:13<00:02, 229.90it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2652/3257 [00:13<00:02, 221.87it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2675/3257 [00:13<00:02, 220.46it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2698/3257 [00:13<00:02, 207.02it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2720/3257 [00:14<00:02, 202.28it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2748/3257 [00:14<00:02, 221.55it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2771/3257 [00:14<00:02, 216.73it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2798/3257 [00:14<00:01, 230.35it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2822/3257 [00:14<00:01, 222.73it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2845/3257 [00:14<00:01, 213.48it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2875/3257 [00:14<00:01, 236.61it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2900/3257 [00:14<00:01, 222.17it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2926/3257 [00:14<00:01, 232.06it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:15<00:01, 212.78it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2974/3257 [00:15<00:01, 219.31it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2997/3257 [00:15<00:01, 213.32it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3020/3257 [00:15<00:01, 216.00it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3047/3257 [00:15<00:00, 229.97it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3075/3257 [00:15<00:00, 241.37it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3100/3257 [00:15<00:00, 238.12it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3127/3257 [00:15<00:00, 246.68it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3152/3257 [00:15<00:00, 229.26it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3176/3257 [00:16<00:00, 226.99it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3200/3257 [00:16<00:00, 229.66it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3224/3257 [00:16<00:00, 222.50it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:16<00:00, 229.40it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 198.95it/s]
2023-02-07 19:43:33.067 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:43:33,068][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d157,n5,mc3,s0.579639,t4>', 'datetime': '2023-02-07T19:43:33.068805', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:43:33,069][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:43:33,070][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:43:33,503][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 19:43:33,503][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:43:33,564][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 22723 unique words (71.45% of original 31803, drops 9080)', 'datetime': '2023-02-07T19:43:33.563987', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:43:33,564][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 5081575 word corpus (99.73% of original 5095118, drops 13543)', 'datetime': '2023-02-07T19:43:33.564387', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:43:33,638][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 19:43:33,639][gensim.models.word2vec][INFO] - sample=0.579639 downsamples 0 most-common words
[2023-02-07 19:43:33,639][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5081575 word corpus (100.0%% of prior 5081575)', 'datetime': '2023-02-07T19:43:33.639679', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:43:33,762][gensim.models.word2vec][INFO] - estimated required memory for 22723 words and 157 dimensions: 42598384 bytes
[2023-02-07 19:43:33,762][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:43:33,777][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 22723 vocabulary and 157 features, using sg=1 hs=0 sample=0.5796386806168747 negative=5 window=9 shrink_windows=True', 'datetime': '2023-02-07T19:43:33.777151', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:43:34,785][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 44.15% examples, 2287807 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:35,787][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 95.67% examples, 2420221 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:35,868][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5073452 effective words) took 2.1s, 2430750 effective words/s
[2023-02-07 19:43:36,872][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 40.28% examples, 2095497 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:37,874][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 83.24% examples, 2131154 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:38,241][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5073452 effective words) took 2.4s, 2138930 effective words/s
[2023-02-07 19:43:39,247][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 41.23% examples, 2138823 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:40,250][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 82.68% examples, 2104609 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:40,668][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5073452 effective words) took 2.4s, 2091423 effective words/s
[2023-02-07 19:43:41,677][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 40.28% examples, 2089897 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:42,678][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 81.89% examples, 2089004 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:43:43,002][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5073452 effective words) took 2.3s, 2176384 effective words/s
[2023-02-07 19:43:44,008][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 39.70% examples, 2066983 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:45,012][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 83.57% examples, 2137617 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:45,369][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5073452 effective words) took 2.4s, 2146124 effective words/s
[2023-02-07 19:43:46,378][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 41.63% examples, 2160554 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:47,378][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 84.65% examples, 2160852 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:47,726][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5073452 effective words) took 2.4s, 2156063 effective words/s
[2023-02-07 19:43:48,727][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 52.47% examples, 2722390 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:49,587][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5073452 effective words) took 1.9s, 2727291 effective words/s
[2023-02-07 19:43:50,592][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 47.90% examples, 2477052 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:51,598][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 91.80% examples, 2336406 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:51,759][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5073452 effective words) took 2.2s, 2337876 effective words/s
[2023-02-07 19:43:52,766][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 54.59% examples, 2822799 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:53,542][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5073452 effective words) took 1.8s, 2848188 effective words/s
[2023-02-07 19:43:54,544][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 55.39% examples, 2874283 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:55,528][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5073452 effective words) took 2.0s, 2556656 effective words/s
[2023-02-07 19:43:56,537][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 53.42% examples, 2760795 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:57,357][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5073452 effective words) took 1.8s, 2779194 effective words/s
[2023-02-07 19:43:58,360][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 55.39% examples, 2873107 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:43:59,133][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5073452 effective words) took 1.8s, 2859152 effective words/s
[2023-02-07 19:44:00,140][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 41.08% examples, 2129072 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:44:01,140][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 84.46% examples, 2157692 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:44:01,482][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5073452 effective words) took 2.3s, 2162168 effective words/s
[2023-02-07 19:44:02,490][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 41.45% examples, 2154101 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:44:03,492][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 83.67% examples, 2138143 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:44:03,854][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5073452 effective words) took 2.4s, 2141554 effective words/s
[2023-02-07 19:44:04,857][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 51.89% examples, 2692369 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:44:05,724][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5073452 effective words) took 1.9s, 2715860 effective words/s
[2023-02-07 19:44:05,726][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (76101780 effective words) took 31.9s, 2382125 effective words/s', 'datetime': '2023-02-07T19:44:05.725974', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:44:05.726 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:44:08,356][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194308-72fxz6y0/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:44:08.356589', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:44:08,357][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:44:08,444][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194308-72fxz6y0/files/../tmp/embedding_model.pt
2023-02-07 19:44:08.444 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:44:09.782 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:44:10.230 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:44:14.209 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.304267581180697, 'test_mae': 1.1165421540160418, 'test_r2': -1.7135893846485355}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.69
wandb: percentage 0.28551
wandb:   test_mae 1.11654
wandb:   test_mse 2.30427
wandb:    test_r2 -1.71359
wandb: 
wandb: üöÄ View run ancient-sweep-58 at: https://wandb.ai/xiaoqiz/mof2vec/runs/72fxz6y0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_194308-72fxz6y0/logs
wandb: Agent Starting Run: 5f1hy1lm with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 942
wandb: 	model.gensim.alpha: 0.050338630522506055
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.8910731201300817
wandb: 	model.gensim.vector_size: 295
wandb: 	model.gensim.window: 10
wandb: 	model.sklearn.learning_rate: 0.046942574158555773
wandb: 	model.sklearn.max_depth: 30
wandb: 	model.sklearn.min_child_weight: 0.09362370957389245
wandb: 	model.sklearn.n_estimators: 2034
wandb: 	model.sklearn.num_leaves: 468
wandb: 	model.sklearn.reg_alpha: 0.2299098201823609
wandb: 	model.sklearn.reg_lambda: 0.8209420964140617
wandb: 	model.sklearn.subsample: 0.6714295822328891
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194428-5f1hy1lm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-59
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/5f1hy1lm
2023-02-07 19:44:36.577 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 19:44:36.578 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 942 for sweep.
2023-02-07 19:44:36.578 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.050338630522506055 for sweep.
2023-02-07 19:44:36.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 19:44:36.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 19:44:36.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.8910731201300817 for sweep.
2023-02-07 19:44:36.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 295 for sweep.
2023-02-07 19:44:36.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 10 for sweep.
2023-02-07 19:44:36.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.046942574158555773 for sweep.
2023-02-07 19:44:36.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 30 for sweep.
2023-02-07 19:44:36.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.09362370957389245 for sweep.
2023-02-07 19:44:36.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2034 for sweep.
2023-02-07 19:44:36.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 468 for sweep.
2023-02-07 19:44:36.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.2299098201823609 for sweep.
2023-02-07 19:44:36.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.8209420964140617 for sweep.
2023-02-07 19:44:36.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6714295822328891 for sweep.
2023-02-07 19:44:36.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:44:36.595 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194428-5f1hy1lm/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 942, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 295, 'window': 10, 'min_count': 2, 'dm': 1, 'sample': 0.8910731201300817, 'workers': 4, 'alpha': 0.050338630522506055, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2034, 'max_depth': 30, 'num_leaves': 468, 'reg_alpha': 0.2299098201823609, 'reg_lambda': 0.8209420964140617, 'subsample': 0.6714295822328891, 'min_child_weight': 0.09362370957389245, 'n_jobs': 4, 'learning_rate': 0.046942574158555773}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 16/3257 [00:00<00:20, 157.13it/s]  1%|          | 34/3257 [00:00<00:20, 158.15it/s]  2%|‚ñè         | 52/3257 [00:00<00:19, 165.80it/s]  2%|‚ñè         | 69/3257 [00:00<00:19, 162.07it/s]  3%|‚ñé         | 90/3257 [00:00<00:18, 172.77it/s]  3%|‚ñé         | 108/3257 [00:00<00:19, 162.51it/s]  4%|‚ñç         | 125/3257 [00:00<00:19, 163.00it/s]  4%|‚ñç         | 145/3257 [00:00<00:17, 173.86it/s]  5%|‚ñå         | 163/3257 [00:00<00:18, 165.25it/s]  6%|‚ñå         | 180/3257 [00:01<00:18, 166.27it/s]  6%|‚ñå         | 199/3257 [00:01<00:17, 171.49it/s]  7%|‚ñã         | 218/3257 [00:01<00:17, 175.32it/s]  7%|‚ñã         | 237/3257 [00:01<00:16, 179.03it/s]  8%|‚ñä         | 255/3257 [00:01<00:16, 178.95it/s]  8%|‚ñä         | 273/3257 [00:01<00:16, 177.96it/s]  9%|‚ñâ         | 296/3257 [00:01<00:15, 190.21it/s] 10%|‚ñâ         | 316/3257 [00:01<00:16, 180.59it/s] 10%|‚ñà         | 336/3257 [00:01<00:16, 182.06it/s] 11%|‚ñà         | 355/3257 [00:02<00:15, 183.00it/s] 11%|‚ñà‚ñè        | 374/3257 [00:02<00:15, 183.73it/s] 12%|‚ñà‚ñè        | 393/3257 [00:02<00:16, 170.40it/s] 13%|‚ñà‚ñé        | 415/3257 [00:02<00:15, 183.68it/s] 13%|‚ñà‚ñé        | 434/3257 [00:02<00:17, 161.52it/s] 14%|‚ñà‚ñç        | 453/3257 [00:02<00:16, 168.74it/s] 15%|‚ñà‚ñç        | 474/3257 [00:02<00:15, 178.80it/s] 15%|‚ñà‚ñå        | 493/3257 [00:02<00:15, 179.03it/s] 16%|‚ñà‚ñå        | 516/3257 [00:02<00:14, 192.72it/s] 16%|‚ñà‚ñã        | 536/3257 [00:03<00:14, 188.06it/s] 17%|‚ñà‚ñã        | 557/3257 [00:03<00:13, 193.98it/s] 18%|‚ñà‚ñä        | 577/3257 [00:03<00:15, 172.35it/s] 18%|‚ñà‚ñä        | 600/3257 [00:03<00:14, 185.81it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:20, 126.49it/s] 20%|‚ñà‚ñâ        | 641/3257 [00:03<00:18, 141.46it/s] 20%|‚ñà‚ñà        | 658/3257 [00:03<00:18, 142.19it/s] 21%|‚ñà‚ñà        | 679/3257 [00:04<00:16, 157.90it/s] 21%|‚ñà‚ñà‚ñè       | 697/3257 [00:04<00:16, 158.33it/s] 22%|‚ñà‚ñà‚ñè       | 718/3257 [00:04<00:14, 171.06it/s] 23%|‚ñà‚ñà‚ñé       | 737/3257 [00:04<00:15, 163.04it/s] 23%|‚ñà‚ñà‚ñé       | 756/3257 [00:04<00:14, 169.13it/s] 24%|‚ñà‚ñà‚ñç       | 775/3257 [00:04<00:14, 172.27it/s] 24%|‚ñà‚ñà‚ñç       | 793/3257 [00:04<00:14, 173.99it/s] 25%|‚ñà‚ñà‚ñç       | 812/3257 [00:04<00:13, 177.91it/s] 26%|‚ñà‚ñà‚ñå       | 831/3257 [00:04<00:13, 173.62it/s] 26%|‚ñà‚ñà‚ñå       | 849/3257 [00:04<00:14, 165.74it/s] 27%|‚ñà‚ñà‚ñã       | 868/3257 [00:05<00:13, 172.32it/s] 27%|‚ñà‚ñà‚ñã       | 886/3257 [00:05<00:13, 171.81it/s] 28%|‚ñà‚ñà‚ñä       | 907/3257 [00:05<00:12, 181.25it/s] 28%|‚ñà‚ñà‚ñä       | 928/3257 [00:05<00:12, 188.19it/s] 29%|‚ñà‚ñà‚ñâ       | 947/3257 [00:05<00:12, 187.50it/s] 30%|‚ñà‚ñà‚ñâ       | 967/3257 [00:05<00:12, 189.91it/s] 30%|‚ñà‚ñà‚ñà       | 987/3257 [00:05<00:12, 183.09it/s] 31%|‚ñà‚ñà‚ñà       | 1006/3257 [00:05<00:12, 184.56it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1025/3257 [00:05<00:12, 181.77it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1044/3257 [00:06<00:13, 168.35it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1064/3257 [00:06<00:12, 175.05it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1082/3257 [00:06<00:12, 173.66it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1100/3257 [00:06<00:12, 174.05it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1119/3257 [00:06<00:12, 173.27it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1139/3257 [00:06<00:12, 174.96it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1158/3257 [00:06<00:11, 178.62it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1177/3257 [00:06<00:11, 179.04it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1195/3257 [00:06<00:12, 165.83it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1212/3257 [00:07<00:12, 162.05it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1234/3257 [00:07<00:11, 178.01it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1253/3257 [00:07<00:11, 175.96it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1272/3257 [00:07<00:11, 178.26it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1290/3257 [00:07<00:11, 167.50it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1309/3257 [00:07<00:11, 171.09it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1329/3257 [00:07<00:10, 178.23it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:07<00:10, 178.79it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1368/3257 [00:07<00:10, 174.24it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1386/3257 [00:08<00:10, 171.29it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1409/3257 [00:08<00:09, 185.69it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:08<00:09, 190.93it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1455/3257 [00:08<00:08, 201.34it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1476/3257 [00:08<00:08, 203.54it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1498/3257 [00:08<00:08, 206.47it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:08<00:08, 193.20it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1539/3257 [00:08<00:09, 181.46it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1558/3257 [00:08<00:09, 178.63it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1577/3257 [00:09<00:09, 180.49it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1599/3257 [00:09<00:08, 189.25it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:09<00:08, 191.91it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1639/3257 [00:09<00:08, 184.93it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1658/3257 [00:09<00:09, 174.76it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1676/3257 [00:09<00:09, 170.95it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1695/3257 [00:09<00:08, 175.24it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:09<00:08, 178.24it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1733/3257 [00:09<00:09, 167.75it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1753/3257 [00:10<00:08, 174.09it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1774/3257 [00:10<00:08, 181.50it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1795/3257 [00:10<00:07, 188.05it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1814/3257 [00:10<00:07, 180.50it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1833/3257 [00:10<00:07, 179.55it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1855/3257 [00:10<00:07, 188.09it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1877/3257 [00:10<00:07, 196.23it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1897/3257 [00:10<00:07, 190.75it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1917/3257 [00:11<00:10, 128.55it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1938/3257 [00:11<00:09, 145.74it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1964/3257 [00:11<00:07, 171.93it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1984/3257 [00:11<00:07, 169.60it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:11<00:07, 175.15it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2024/3257 [00:11<00:06, 181.28it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2044/3257 [00:11<00:06, 181.39it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2063/3257 [00:11<00:07, 169.18it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2084/3257 [00:11<00:06, 178.29it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2103/3257 [00:12<00:06, 176.48it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2122/3257 [00:12<00:06, 171.44it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2142/3257 [00:12<00:06, 178.93it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2161/3257 [00:12<00:06, 179.15it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2182/3257 [00:12<00:05, 182.83it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2205/3257 [00:12<00:05, 195.16it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2225/3257 [00:12<00:05, 187.12it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2244/3257 [00:12<00:05, 184.10it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2264/3257 [00:12<00:05, 187.82it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2283/3257 [00:13<00:05, 183.04it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2303/3257 [00:13<00:05, 186.78it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2330/3257 [00:13<00:04, 208.79it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2359/3257 [00:13<00:03, 229.45it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2383/3257 [00:13<00:03, 219.83it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2406/3257 [00:13<00:04, 211.60it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2428/3257 [00:13<00:03, 209.91it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2450/3257 [00:13<00:04, 197.02it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2476/3257 [00:13<00:03, 211.34it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2499/3257 [00:13<00:03, 216.43it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2523/3257 [00:14<00:03, 220.83it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2546/3257 [00:14<00:03, 222.10it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2569/3257 [00:14<00:03, 198.94it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2590/3257 [00:14<00:03, 193.31it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2619/3257 [00:14<00:02, 217.79it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2642/3257 [00:14<00:02, 217.53it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2665/3257 [00:14<00:02, 207.60it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2688/3257 [00:14<00:02, 212.34it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2710/3257 [00:15<00:02, 187.07it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:15<00:02, 194.99it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2757/3257 [00:15<00:02, 207.38it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2779/3257 [00:15<00:02, 201.85it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:15<00:02, 215.51it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2828/3257 [00:15<00:02, 202.79it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2850/3257 [00:15<00:01, 205.27it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2879/3257 [00:15<00:01, 226.68it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2902/3257 [00:15<00:01, 204.75it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:16<00:01, 210.12it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:16<00:01, 197.94it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2972/3257 [00:16<00:01, 203.07it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2993/3257 [00:16<00:01, 193.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3016/3257 [00:16<00:01, 202.66it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3040/3257 [00:16<00:01, 212.47it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3066/3257 [00:16<00:00, 225.22it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3089/3257 [00:16<00:00, 221.56it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3114/3257 [00:16<00:00, 228.56it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3138/3257 [00:17<00:00, 221.13it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3161/3257 [00:17<00:00, 209.56it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3183/3257 [00:17<00:00, 195.35it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3206/3257 [00:17<00:00, 204.07it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3227/3257 [00:17<00:00, 195.41it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:17<00:00, 199.39it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:17<00:00, 184.42it/s]
2023-02-07 19:44:55.018 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:44:55,019][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d295,n5,w10,mc2,s0.891073,t4>', 'datetime': '2023-02-07T19:44:55.019889', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:44:55,020][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:44:55,020][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:44:55,541][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 19:44:55,542][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:44:55,632][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 36619 unique words (85.76% of original 42701, drops 6082)', 'datetime': '2023-02-07T19:44:55.632315', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:44:55,632][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 5816910 word corpus (99.90% of original 5822992, drops 6082)', 'datetime': '2023-02-07T19:44:55.632725', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:44:55,757][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 19:44:55,758][gensim.models.word2vec][INFO] - sample=0.891073 downsamples 0 most-common words
[2023-02-07 19:44:55,759][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5816910 word corpus (100.0%% of prior 5816910)', 'datetime': '2023-02-07T19:44:55.759192', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:44:55,963][gensim.models.word2vec][INFO] - estimated required memory for 36619 words and 295 dimensions: 109225000 bytes
[2023-02-07 19:44:55,963][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:44:56,002][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 36619 vocabulary and 295 features, using sg=0 hs=0 sample=0.8910731201300817 negative=5 window=10 shrink_windows=True', 'datetime': '2023-02-07T19:44:56.002853', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:44:57,008][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 20.17% examples, 1143350 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:44:58,012][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 39.61% examples, 1175940 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:44:59,015][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 61.13% examples, 1195724 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:00,018][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 82.81% examples, 1205451 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:00,768][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5790320 effective words) took 4.8s, 1215755 effective words/s
[2023-02-07 19:45:01,770][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 21.40% examples, 1222555 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:02,772][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 41.97% examples, 1244097 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:03,776][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 64.32% examples, 1259284 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:04,778][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 85.94% examples, 1251384 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:05,339][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5790320 effective words) took 4.6s, 1266921 effective words/s
[2023-02-07 19:45:06,341][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 23.79% examples, 1366700 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:07,348][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 45.90% examples, 1347100 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:45:08,360][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 67.82% examples, 1328501 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:09,371][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 90.64% examples, 1311650 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:09,748][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5790320 effective words) took 4.4s, 1313913 effective words/s
[2023-02-07 19:45:10,768][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 22.38% examples, 1269443 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:45:11,769][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 44.03% examples, 1294815 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:12,769][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 66.84% examples, 1306808 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:13,774][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 90.51% examples, 1311659 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:14,144][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5790320 effective words) took 4.4s, 1317894 effective words/s
[2023-02-07 19:45:15,149][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 23.70% examples, 1357211 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:16,150][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 45.69% examples, 1345723 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:17,152][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 68.47% examples, 1346921 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:45:18,153][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 92.94% examples, 1352215 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:45:18,424][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5790320 effective words) took 4.3s, 1353889 effective words/s
[2023-02-07 19:45:19,426][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 23.15% examples, 1331027 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:20,426][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 45.38% examples, 1339324 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:21,440][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 67.82% examples, 1330536 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:22,455][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 92.42% examples, 1335421 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:45:22,758][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5790320 effective words) took 4.3s, 1336233 effective words/s
[2023-02-07 19:45:23,764][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 23.70% examples, 1352375 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:24,768][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 46.36% examples, 1356546 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:25,768][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 68.84% examples, 1353582 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:26,772][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 97.21% examples, 1404830 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:26,856][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5790320 effective words) took 4.1s, 1413356 effective words/s
[2023-02-07 19:45:27,872][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 23.92% examples, 1349365 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:28,876][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 46.64% examples, 1362111 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:29,877][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 69.48% examples, 1361968 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:30,889][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 94.96% examples, 1367575 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:45:31,072][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5790320 effective words) took 4.2s, 1373908 effective words/s
[2023-02-07 19:45:32,075][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 26.01% examples, 1510303 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:33,088][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 50.14% examples, 1466245 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:45:34,101][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 74.33% examples, 1441691 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:35,102][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 99.51% examples, 1430149 words/s, in_qsize 3, out_qsize 1
[2023-02-07 19:45:35,112][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5790320 effective words) took 4.0s, 1433809 effective words/s
[2023-02-07 19:45:36,129][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 23.92% examples, 1356045 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:37,135][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 47.13% examples, 1374533 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:38,142][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 70.80% examples, 1384025 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:39,148][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 96.38% examples, 1383942 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:39,287][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5790320 effective words) took 4.2s, 1387254 effective words/s
[2023-02-07 19:45:40,294][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 23.92% examples, 1371096 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:41,299][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 46.76% examples, 1371110 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:42,308][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 70.80% examples, 1388343 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:43,314][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 96.75% examples, 1393451 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:43,428][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5790320 effective words) took 4.1s, 1398973 effective words/s
[2023-02-07 19:45:44,430][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 24.04% examples, 1385959 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:45,432][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 46.48% examples, 1368945 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:45:46,442][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 69.14% examples, 1357591 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:47,444][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 97.67% examples, 1412311 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:47,516][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5790320 effective words) took 4.1s, 1417049 effective words/s
[2023-02-07 19:45:48,521][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 23.40% examples, 1336773 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:49,521][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 45.90% examples, 1349696 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:50,524][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 69.14% examples, 1359925 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:51,536][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 95.18% examples, 1374291 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:45:51,707][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5790320 effective words) took 4.2s, 1381851 effective words/s
[2023-02-07 19:45:52,718][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 27.30% examples, 1574187 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:53,728][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 54.77% examples, 1606504 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:54,728][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 82.84% examples, 1606429 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:45:55,303][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5790320 effective words) took 3.6s, 1611621 effective words/s
[2023-02-07 19:45:56,309][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 24.87% examples, 1420269 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:57,315][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 47.96% examples, 1409393 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:45:58,319][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 71.26% examples, 1396781 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:59,330][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 96.28% examples, 1386362 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:45:59,485][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5790320 effective words) took 4.2s, 1385038 effective words/s
[2023-02-07 19:45:59,487][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86854800 effective words) took 63.5s, 1368131 effective words/s', 'datetime': '2023-02-07T19:45:59.487550', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:45:59.488 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:46:03,698][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194428-5f1hy1lm/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:46:03.698707', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:46:03,699][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194428-5f1hy1lm/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 19:46:03,746][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194428-5f1hy1lm/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 19:46:03,791][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:46:03,835][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194428-5f1hy1lm/files/../tmp/embedding_model.pt
2023-02-07 19:46:03.836 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:46:05.635 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:46:06.242 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:46:08.623 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.438879124807809, 'test_mae': 1.1557497057815203, 'test_r2': -2.9291860714437727}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.82
wandb: percentage 0.14243
wandb:   test_mae 1.15575
wandb:   test_mse 2.43888
wandb:    test_r2 -2.92919
wandb: 
wandb: üöÄ View run jumping-sweep-59 at: https://wandb.ai/xiaoqiz/mof2vec/runs/5f1hy1lm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_194428-5f1hy1lm/logs
wandb: Agent Starting Run: bno00d5i with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 849
wandb: 	model.gensim.alpha: 0.0027956092048453405
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.8427695571689817
wandb: 	model.gensim.vector_size: 202
wandb: 	model.gensim.window: 4
wandb: 	model.sklearn.learning_rate: 0.6991645170235115
wandb: 	model.sklearn.max_depth: 22
wandb: 	model.sklearn.min_child_weight: 0.07510632364021447
wandb: 	model.sklearn.n_estimators: 1170
wandb: 	model.sklearn.num_leaves: 434
wandb: 	model.sklearn.reg_alpha: 0.17674026695051778
wandb: 	model.sklearn.reg_lambda: 0.07275926882273161
wandb: 	model.sklearn.subsample: 0.8894541647021275
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194618-bno00d5i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-60
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/bno00d5i
2023-02-07 19:46:26.750 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 19:46:26.750 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 849 for sweep.
2023-02-07 19:46:26.751 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0027956092048453405 for sweep.
2023-02-07 19:46:26.751 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:46:26.751 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 19:46:26.751 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.8427695571689817 for sweep.
2023-02-07 19:46:26.752 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 202 for sweep.
2023-02-07 19:46:26.752 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 4 for sweep.
2023-02-07 19:46:26.752 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.6991645170235115 for sweep.
2023-02-07 19:46:26.752 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 22 for sweep.
2023-02-07 19:46:26.752 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.07510632364021447 for sweep.
2023-02-07 19:46:26.753 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1170 for sweep.
2023-02-07 19:46:26.753 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 434 for sweep.
2023-02-07 19:46:26.753 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.17674026695051778 for sweep.
2023-02-07 19:46:26.753 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.07275926882273161 for sweep.
2023-02-07 19:46:26.753 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8894541647021275 for sweep.
2023-02-07 19:46:26.754 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:46:26.760 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194618-bno00d5i/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 849, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 202, 'window': 4, 'min_count': 4, 'dm': 0, 'sample': 0.8427695571689817, 'workers': 4, 'alpha': 0.0027956092048453405, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1170, 'max_depth': 22, 'num_leaves': 434, 'reg_alpha': 0.17674026695051778, 'reg_lambda': 0.07275926882273161, 'subsample': 0.8894541647021275, 'min_child_weight': 0.07510632364021447, 'n_jobs': 4, 'learning_rate': 0.6991645170235115}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 16/3257 [00:00<00:20, 155.93it/s]  1%|          | 34/3257 [00:00<00:20, 157.32it/s]  2%|‚ñè         | 53/3257 [00:00<00:19, 162.53it/s]  2%|‚ñè         | 70/3257 [00:00<00:28, 110.91it/s]  3%|‚ñé         | 90/3257 [00:00<00:24, 128.56it/s]  3%|‚ñé         | 105/3257 [00:00<00:23, 133.87it/s]  4%|‚ñé         | 121/3257 [00:00<00:22, 139.59it/s]  4%|‚ñç         | 141/3257 [00:00<00:20, 154.54it/s]  5%|‚ñç         | 159/3257 [00:01<00:19, 160.13it/s]  5%|‚ñå         | 176/3257 [00:01<00:19, 158.82it/s]  6%|‚ñå         | 196/3257 [00:01<00:18, 169.87it/s]  7%|‚ñã         | 215/3257 [00:01<00:17, 175.09it/s]  7%|‚ñã         | 236/3257 [00:01<00:16, 185.20it/s]  8%|‚ñä         | 255/3257 [00:01<00:16, 185.08it/s]  8%|‚ñä         | 274/3257 [00:01<00:16, 183.90it/s]  9%|‚ñâ         | 296/3257 [00:01<00:15, 194.38it/s] 10%|‚ñâ         | 316/3257 [00:01<00:16, 182.79it/s] 10%|‚ñà         | 336/3257 [00:02<00:15, 185.55it/s] 11%|‚ñà         | 356/3257 [00:02<00:15, 188.54it/s] 12%|‚ñà‚ñè        | 375/3257 [00:02<00:16, 177.51it/s] 12%|‚ñà‚ñè        | 393/3257 [00:02<00:16, 168.72it/s] 13%|‚ñà‚ñé        | 414/3257 [00:02<00:16, 177.61it/s] 13%|‚ñà‚ñé        | 432/3257 [00:02<00:18, 154.59it/s] 14%|‚ñà‚ñç        | 449/3257 [00:02<00:17, 156.47it/s] 14%|‚ñà‚ñç        | 470/3257 [00:02<00:16, 168.60it/s] 15%|‚ñà‚ñç        | 488/3257 [00:02<00:16, 168.59it/s] 16%|‚ñà‚ñå        | 510/3257 [00:03<00:15, 181.31it/s] 16%|‚ñà‚ñå        | 529/3257 [00:03<00:16, 167.28it/s] 17%|‚ñà‚ñã        | 547/3257 [00:03<00:16, 165.96it/s] 17%|‚ñà‚ñã        | 564/3257 [00:03<00:17, 157.63it/s] 18%|‚ñà‚ñä        | 581/3257 [00:03<00:17, 157.38it/s] 18%|‚ñà‚ñä        | 602/3257 [00:03<00:15, 169.63it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:15, 167.76it/s] 20%|‚ñà‚ñâ        | 640/3257 [00:03<00:15, 173.36it/s] 20%|‚ñà‚ñà        | 658/3257 [00:03<00:16, 158.55it/s] 21%|‚ñà‚ñà        | 678/3257 [00:04<00:15, 167.51it/s] 21%|‚ñà‚ñà‚ñè       | 696/3257 [00:04<00:15, 160.12it/s] 22%|‚ñà‚ñà‚ñè       | 717/3257 [00:04<00:14, 173.13it/s] 23%|‚ñà‚ñà‚ñé       | 735/3257 [00:04<00:15, 165.32it/s] 23%|‚ñà‚ñà‚ñé       | 752/3257 [00:04<00:16, 155.97it/s] 24%|‚ñà‚ñà‚ñé       | 772/3257 [00:04<00:15, 164.81it/s] 24%|‚ñà‚ñà‚ñç       | 789/3257 [00:04<00:15, 158.43it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:04<00:15, 160.91it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:05<00:15, 161.79it/s] 26%|‚ñà‚ñà‚ñå       | 840/3257 [00:05<00:14, 162.05it/s] 26%|‚ñà‚ñà‚ñã       | 857/3257 [00:05<00:14, 161.05it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:05<00:14, 162.22it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:05<00:14, 158.80it/s] 28%|‚ñà‚ñà‚ñä       | 911/3257 [00:05<00:13, 169.41it/s] 29%|‚ñà‚ñà‚ñä       | 930/3257 [00:05<00:13, 173.52it/s] 29%|‚ñà‚ñà‚ñâ       | 948/3257 [00:05<00:13, 171.10it/s] 30%|‚ñà‚ñà‚ñâ       | 967/3257 [00:05<00:12, 176.41it/s] 30%|‚ñà‚ñà‚ñà       | 985/3257 [00:05<00:13, 173.80it/s] 31%|‚ñà‚ñà‚ñà       | 1003/3257 [00:06<00:13, 169.28it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1021/3257 [00:06<00:13, 170.34it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1039/3257 [00:06<00:14, 156.15it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1056/3257 [00:06<00:13, 157.63it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1077/3257 [00:06<00:12, 171.28it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1095/3257 [00:06<00:13, 162.19it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1115/3257 [00:06<00:12, 169.43it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1133/3257 [00:06<00:13, 161.11it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1150/3257 [00:06<00:13, 158.01it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:07<00:12, 169.59it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1189/3257 [00:07<00:13, 150.82it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1205/3257 [00:07<00:13, 152.99it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1222/3257 [00:07<00:12, 157.19it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1243/3257 [00:07<00:11, 168.32it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1261/3257 [00:07<00:11, 167.93it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1278/3257 [00:07<00:12, 158.97it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1295/3257 [00:07<00:12, 156.49it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1314/3257 [00:07<00:11, 164.49it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1331/3257 [00:08<00:17, 108.18it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:08<00:15, 120.08it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1368/3257 [00:08<00:14, 132.87it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:08<00:13, 136.65it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1403/3257 [00:08<00:12, 149.49it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1425/3257 [00:08<00:10, 166.96it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1445/3257 [00:08<00:10, 175.11it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1467/3257 [00:09<00:09, 186.82it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1487/3257 [00:09<00:09, 185.31it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1510/3257 [00:09<00:08, 197.32it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1531/3257 [00:09<00:09, 176.30it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1550/3257 [00:09<00:09, 172.20it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1568/3257 [00:09<00:09, 172.48it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1586/3257 [00:09<00:09, 170.55it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1605/3257 [00:09<00:09, 173.11it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1623/3257 [00:09<00:09, 174.45it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1641/3257 [00:10<00:09, 170.53it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:10<00:09, 166.40it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1676/3257 [00:10<00:09, 158.70it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1694/3257 [00:10<00:09, 162.07it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:10<00:09, 170.84it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1733/3257 [00:10<00:09, 160.03it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1751/3257 [00:10<00:09, 164.46it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1771/3257 [00:10<00:08, 170.36it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1792/3257 [00:10<00:08, 179.08it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1811/3257 [00:11<00:08, 169.54it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1829/3257 [00:11<00:08, 164.50it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1847/3257 [00:11<00:08, 168.52it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:11<00:07, 174.20it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1888/3257 [00:11<00:07, 177.07it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1908/3257 [00:11<00:07, 182.20it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1927/3257 [00:11<00:07, 171.84it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1951/3257 [00:11<00:06, 188.48it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1973/3257 [00:11<00:06, 195.00it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1993/3257 [00:12<00:06, 186.16it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2012/3257 [00:12<00:06, 182.55it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2033/3257 [00:12<00:06, 186.39it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2052/3257 [00:12<00:07, 171.79it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2070/3257 [00:12<00:07, 167.94it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2089/3257 [00:12<00:06, 172.55it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2107/3257 [00:12<00:06, 169.08it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2125/3257 [00:12<00:07, 156.75it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2142/3257 [00:12<00:06, 159.52it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2160/3257 [00:13<00:06, 163.83it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2180/3257 [00:13<00:06, 171.88it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2199/3257 [00:13<00:06, 175.62it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2217/3257 [00:13<00:06, 168.96it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:13<00:05, 172.95it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2254/3257 [00:13<00:05, 170.96it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2272/3257 [00:13<00:06, 159.51it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2294/3257 [00:13<00:05, 174.80it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2312/3257 [00:13<00:05, 174.06it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2337/3257 [00:14<00:04, 192.57it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:14<00:04, 203.18it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2381/3257 [00:14<00:04, 205.00it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2403/3257 [00:14<00:04, 209.05it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2425/3257 [00:14<00:04, 192.59it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2445/3257 [00:14<00:04, 177.25it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2465/3257 [00:14<00:04, 181.62it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2484/3257 [00:14<00:04, 183.30it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2508/3257 [00:14<00:03, 197.70it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2529/3257 [00:15<00:03, 197.92it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2549/3257 [00:15<00:03, 194.77it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2569/3257 [00:15<00:03, 174.15it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2587/3257 [00:15<00:03, 173.21it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2609/3257 [00:15<00:03, 185.80it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2633/3257 [00:15<00:03, 199.33it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2654/3257 [00:15<00:03, 189.68it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2674/3257 [00:15<00:03, 185.50it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2694/3257 [00:15<00:03, 183.84it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2713/3257 [00:16<00:05, 96.21it/s]  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:16<00:04, 111.83it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2753/3257 [00:16<00:03, 130.12it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2771/3257 [00:16<00:03, 135.45it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2793/3257 [00:16<00:02, 154.79it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2812/3257 [00:16<00:02, 163.33it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2831/3257 [00:17<00:02, 158.04it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2852/3257 [00:17<00:02, 169.33it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2880/3257 [00:17<00:01, 196.38it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2901/3257 [00:17<00:02, 174.61it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2924/3257 [00:17<00:01, 187.53it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2944/3257 [00:17<00:01, 172.03it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2965/3257 [00:17<00:01, 180.22it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2984/3257 [00:17<00:01, 169.45it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3008/3257 [00:17<00:01, 185.22it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3028/3257 [00:18<00:01, 179.60it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:18<00:01, 187.67it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3075/3257 [00:18<00:00, 203.38it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3096/3257 [00:18<00:00, 196.20it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3121/3257 [00:18<00:00, 209.98it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3143/3257 [00:18<00:00, 193.08it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3163/3257 [00:18<00:00, 191.67it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3183/3257 [00:18<00:00, 180.43it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3205/3257 [00:18<00:00, 190.69it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3225/3257 [00:19<00:00, 177.26it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:19<00:00, 184.70it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 168.91it/s]
2023-02-07 19:46:46.828 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:46:46,829][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d202,n5,mc4,s0.84277,t4>', 'datetime': '2023-02-07T19:46:46.829524', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:46:46,830][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:46:46,830][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:46:47,390][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 19:46:47,390][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:46:47,481][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 36921 unique words (68.30% of original 54054, drops 17133)', 'datetime': '2023-02-07T19:46:47.481734', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:46:47,482][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 6522479 word corpus (99.57% of original 6550866, drops 28387)', 'datetime': '2023-02-07T19:46:47.482117', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:46:47,600][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 19:46:47,601][gensim.models.word2vec][INFO] - sample=0.84277 downsamples 0 most-common words
[2023-02-07 19:46:47,601][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6522479 word corpus (100.0%% of prior 6522479)', 'datetime': '2023-02-07T19:46:47.601740', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:46:47,801][gensim.models.word2vec][INFO] - estimated required memory for 36921 words and 202 dimensions: 81407892 bytes
[2023-02-07 19:46:47,801][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:46:47,837][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 36921 vocabulary and 202 features, using sg=1 hs=0 sample=0.8427695571689817 negative=5 window=4 shrink_windows=True', 'datetime': '2023-02-07T19:46:47.837401', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:46:48,841][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 28.89% examples, 1873045 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:46:49,843][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 68.19% examples, 2245915 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:46:50,590][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6467889 effective words) took 2.8s, 2351848 effective words/s
[2023-02-07 19:46:51,605][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 41.30% examples, 2726124 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:46:52,610][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 84.00% examples, 2732054 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:46:52,968][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6467889 effective words) took 2.4s, 2732633 effective words/s
[2023-02-07 19:46:53,975][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 32.27% examples, 2096227 words/s, in_qsize 8, out_qsize 2
[2023-02-07 19:46:55,005][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 64.51% examples, 2086969 words/s, in_qsize 4, out_qsize 7
[2023-02-07 19:46:56,021][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 98.83% examples, 2100030 words/s, in_qsize 3, out_qsize 6
[2023-02-07 19:46:56,028][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6467889 effective words) took 3.1s, 2115003 effective words/s
[2023-02-07 19:46:57,031][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 30.52% examples, 1980615 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:46:58,032][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 60.09% examples, 1972331 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:46:59,032][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 91.34% examples, 1980692 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:46:59,287][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6467889 effective words) took 3.3s, 1985950 effective words/s
[2023-02-07 19:47:00,303][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 30.89% examples, 1998977 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:01,306][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 60.73% examples, 1990792 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:02,307][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 91.07% examples, 1971864 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:02,568][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6467889 effective words) took 3.3s, 1979242 effective words/s
[2023-02-07 19:47:03,573][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 30.30% examples, 1962915 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:04,575][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 60.73% examples, 1992694 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:05,539][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6467889 effective words) took 3.0s, 2178538 effective words/s
[2023-02-07 19:47:06,549][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 29.23% examples, 1878919 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:07,552][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 57.94% examples, 1906354 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:08,552][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 88.61% examples, 1912211 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:08,916][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6467889 effective words) took 3.4s, 1916637 effective words/s
[2023-02-07 19:47:09,921][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 29.87% examples, 1934297 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:10,926][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 59.38% examples, 1952107 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:11,935][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 90.33% examples, 1950013 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:12,230][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6467889 effective words) took 3.3s, 1954398 effective words/s
[2023-02-07 19:47:13,233][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 38.38% examples, 2534714 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:14,235][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 78.02% examples, 2544260 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:14,747][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6467889 effective words) took 2.5s, 2571048 effective words/s
[2023-02-07 19:47:15,753][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 32.88% examples, 2137512 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:47:16,755][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 64.42% examples, 2111721 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:17,642][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6467889 effective words) took 2.9s, 2235499 effective words/s
[2023-02-07 19:47:18,649][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 30.15% examples, 1947103 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:47:19,651][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 62.27% examples, 2031790 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:20,661][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 94.66% examples, 2033582 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:20,816][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6467889 effective words) took 3.2s, 2038494 effective words/s
[2023-02-07 19:47:21,820][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 30.92% examples, 2014200 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:22,822][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 61.59% examples, 2019920 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:23,828][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 94.14% examples, 2031100 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:23,994][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6467889 effective words) took 3.2s, 2037829 effective words/s
[2023-02-07 19:47:24,996][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 31.69% examples, 2064553 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:25,997][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 64.17% examples, 2109998 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:27,002][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 96.99% examples, 2093736 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:27,090][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6467889 effective words) took 3.1s, 2090351 effective words/s
[2023-02-07 19:47:28,092][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 31.65% examples, 2065168 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:29,096][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 61.93% examples, 2026171 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:30,103][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 92.78% examples, 2006216 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:30,312][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6467889 effective words) took 3.2s, 2007891 effective words/s
[2023-02-07 19:47:31,317][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 30.06% examples, 1949172 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:32,326][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 59.26% examples, 1947714 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:33,327][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 89.90% examples, 1943516 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:47:33,639][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6467889 effective words) took 3.3s, 1946423 effective words/s
[2023-02-07 19:47:33,640][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (97018335 effective words) took 45.8s, 2118191 effective words/s', 'datetime': '2023-02-07T19:47:33.640867', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:47:33.641 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:47:37,054][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194618-bno00d5i/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:47:37.053858', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:47:37,055][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:47:37,201][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194618-bno00d5i/files/../tmp/embedding_model.pt
2023-02-07 19:47:37.202 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:47:38.821 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:47:39.365 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:47:48.718 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.011794772835546, 'test_mae': 1.0553229402913569, 'test_r2': -1.918398014168862}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.58
wandb: percentage 0.31696
wandb:   test_mae 1.05532
wandb:   test_mse 2.01179
wandb:    test_r2 -1.9184
wandb: 
wandb: üöÄ View run wandering-sweep-60 at: https://wandb.ai/xiaoqiz/mof2vec/runs/bno00d5i
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_194618-bno00d5i/logs
wandb: Agent Starting Run: x8sg3xjw with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 532
wandb: 	model.gensim.alpha: 0.0028089913173300705
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.5603940938226961
wandb: 	model.gensim.vector_size: 478
wandb: 	model.gensim.window: 9
wandb: 	model.sklearn.learning_rate: 0.3864118310232233
wandb: 	model.sklearn.max_depth: 66
wandb: 	model.sklearn.min_child_weight: 0.06316141710925649
wandb: 	model.sklearn.n_estimators: 2055
wandb: 	model.sklearn.num_leaves: 161
wandb: 	model.sklearn.reg_alpha: 0.07150946620251582
wandb: 	model.sklearn.reg_lambda: 0.13831565168112622
wandb: 	model.sklearn.subsample: 0.46133605221326984
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194758-x8sg3xjw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-61
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/x8sg3xjw
2023-02-07 19:48:06.570 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 19:48:06.571 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 532 for sweep.
2023-02-07 19:48:06.571 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0028089913173300705 for sweep.
2023-02-07 19:48:06.571 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:48:06.571 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 19:48:06.572 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5603940938226961 for sweep.
2023-02-07 19:48:06.572 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 478 for sweep.
2023-02-07 19:48:06.572 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 9 for sweep.
2023-02-07 19:48:06.572 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.3864118310232233 for sweep.
2023-02-07 19:48:06.573 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 66 for sweep.
2023-02-07 19:48:06.573 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.06316141710925649 for sweep.
2023-02-07 19:48:06.573 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2055 for sweep.
2023-02-07 19:48:06.573 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 161 for sweep.
2023-02-07 19:48:06.574 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.07150946620251582 for sweep.
2023-02-07 19:48:06.574 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.13831565168112622 for sweep.
2023-02-07 19:48:06.574 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.46133605221326984 for sweep.
2023-02-07 19:48:06.575 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:48:06.581 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194758-x8sg3xjw/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 532, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 478, 'window': 9, 'min_count': 3, 'dm': 0, 'sample': 0.5603940938226961, 'workers': 4, 'alpha': 0.0028089913173300705, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2055, 'max_depth': 66, 'num_leaves': 161, 'reg_alpha': 0.07150946620251582, 'reg_lambda': 0.13831565168112622, 'subsample': 0.46133605221326984, 'min_child_weight': 0.06316141710925649, 'n_jobs': 4, 'learning_rate': 0.3864118310232233}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:20, 161.26it/s]  1%|          | 34/3257 [00:00<00:19, 164.25it/s]  2%|‚ñè         | 52/3257 [00:00<00:19, 168.54it/s]  2%|‚ñè         | 69/3257 [00:00<00:18, 168.61it/s]  3%|‚ñé         | 90/3257 [00:00<00:17, 182.57it/s]  3%|‚ñé         | 109/3257 [00:00<00:17, 175.47it/s]  4%|‚ñç         | 130/3257 [00:00<00:16, 185.15it/s]  5%|‚ñç         | 151/3257 [00:00<00:16, 189.86it/s]  5%|‚ñå         | 171/3257 [00:00<00:16, 185.66it/s]  6%|‚ñå         | 192/3257 [00:01<00:16, 190.76it/s]  7%|‚ñã         | 212/3257 [00:01<00:15, 192.14it/s]  7%|‚ñã         | 235/3257 [00:01<00:15, 201.35it/s]  8%|‚ñä         | 256/3257 [00:01<00:15, 195.36it/s]  8%|‚ñä         | 276/3257 [00:01<00:15, 196.51it/s]  9%|‚ñâ         | 298/3257 [00:01<00:14, 200.53it/s] 10%|‚ñâ         | 319/3257 [00:01<00:14, 200.71it/s] 10%|‚ñà         | 340/3257 [00:01<00:14, 195.60it/s] 11%|‚ñà         | 362/3257 [00:01<00:14, 199.93it/s] 12%|‚ñà‚ñè        | 383/3257 [00:02<00:15, 187.45it/s] 12%|‚ñà‚ñè        | 403/3257 [00:02<00:15, 189.58it/s] 13%|‚ñà‚ñé        | 423/3257 [00:02<00:15, 188.16it/s] 14%|‚ñà‚ñé        | 442/3257 [00:02<00:16, 170.99it/s] 14%|‚ñà‚ñç        | 463/3257 [00:02<00:15, 180.55it/s] 15%|‚ñà‚ñç        | 482/3257 [00:02<00:15, 175.31it/s] 15%|‚ñà‚ñå        | 504/3257 [00:02<00:14, 183.63it/s] 16%|‚ñà‚ñå        | 524/3257 [00:02<00:14, 185.96it/s] 17%|‚ñà‚ñã        | 545/3257 [00:02<00:14, 191.44it/s] 17%|‚ñà‚ñã        | 565/3257 [00:03<00:15, 177.00it/s] 18%|‚ñà‚ñä        | 583/3257 [00:03<00:15, 174.42it/s] 19%|‚ñà‚ñä        | 603/3257 [00:03<00:14, 180.96it/s] 19%|‚ñà‚ñâ        | 622/3257 [00:03<00:14, 176.02it/s] 20%|‚ñà‚ñâ        | 644/3257 [00:03<00:14, 186.32it/s] 20%|‚ñà‚ñà        | 663/3257 [00:03<00:15, 172.14it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:14, 175.47it/s] 22%|‚ñà‚ñà‚ñè       | 703/3257 [00:03<00:14, 179.84it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:03<00:14, 175.68it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:04<00:21, 115.22it/s] 23%|‚ñà‚ñà‚ñé       | 762/3257 [00:04<00:18, 136.52it/s] 24%|‚ñà‚ñà‚ñç       | 779/3257 [00:04<00:17, 142.95it/s] 25%|‚ñà‚ñà‚ñç       | 798/3257 [00:04<00:15, 153.97it/s] 25%|‚ñà‚ñà‚ñå       | 817/3257 [00:04<00:15, 161.89it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:04<00:15, 160.41it/s] 26%|‚ñà‚ñà‚ñå       | 852/3257 [00:04<00:15, 156.70it/s] 27%|‚ñà‚ñà‚ñã       | 871/3257 [00:04<00:14, 163.87it/s] 27%|‚ñà‚ñà‚ñã       | 890/3257 [00:05<00:13, 170.05it/s] 28%|‚ñà‚ñà‚ñä       | 912/3257 [00:05<00:12, 183.40it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:05<00:13, 178.03it/s] 29%|‚ñà‚ñà‚ñâ       | 950/3257 [00:05<00:13, 176.04it/s] 30%|‚ñà‚ñà‚ñâ       | 969/3257 [00:05<00:12, 178.94it/s] 30%|‚ñà‚ñà‚ñà       | 988/3257 [00:05<00:12, 174.78it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:05<00:13, 172.43it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:05<00:12, 172.22it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1045/3257 [00:05<00:12, 171.37it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1064/3257 [00:06<00:12, 175.58it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1082/3257 [00:06<00:12, 173.74it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1100/3257 [00:06<00:12, 174.74it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1118/3257 [00:06<00:12, 175.28it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1136/3257 [00:06<00:12, 173.32it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1154/3257 [00:06<00:12, 171.39it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:06<00:11, 176.57it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:06<00:13, 158.59it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1208/3257 [00:06<00:12, 157.75it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1231/3257 [00:07<00:11, 176.72it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:07<00:11, 170.17it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:07<00:11, 177.16it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1289/3257 [00:07<00:12, 161.38it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1307/3257 [00:07<00:11, 166.07it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1327/3257 [00:07<00:11, 175.13it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1347/3257 [00:07<00:10, 179.89it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1366/3257 [00:07<00:10, 174.92it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:07<00:10, 173.35it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1405/3257 [00:08<00:10, 182.57it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1428/3257 [00:08<00:09, 195.72it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1448/3257 [00:08<00:09, 192.98it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1471/3257 [00:08<00:08, 202.86it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1492/3257 [00:08<00:08, 201.43it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:08<00:08, 199.89it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1534/3257 [00:08<00:09, 181.84it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1553/3257 [00:08<00:09, 177.37it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1574/3257 [00:08<00:09, 181.15it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1593/3257 [00:09<00:09, 182.91it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:09<00:08, 193.00it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1635/3257 [00:09<00:08, 183.20it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1654/3257 [00:09<00:08, 180.40it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1673/3257 [00:09<00:09, 172.32it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1691/3257 [00:09<00:09, 166.87it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1712/3257 [00:09<00:08, 178.48it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:09<00:09, 164.86it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1749/3257 [00:09<00:08, 168.86it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1769/3257 [00:10<00:08, 176.77it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1791/3257 [00:10<00:07, 186.58it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1810/3257 [00:10<00:08, 172.82it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1828/3257 [00:10<00:08, 170.47it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1849/3257 [00:10<00:07, 179.64it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1870/3257 [00:10<00:07, 187.89it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1889/3257 [00:10<00:07, 188.26it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1910/3257 [00:10<00:07, 191.49it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1930/3257 [00:10<00:07, 185.74it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1957/3257 [00:11<00:10, 120.04it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:11<00:09, 133.83it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1998/3257 [00:11<00:08, 149.74it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2018/3257 [00:11<00:07, 160.24it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2038/3257 [00:11<00:07, 169.35it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2057/3257 [00:11<00:07, 159.92it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2077/3257 [00:11<00:06, 168.86it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2096/3257 [00:12<00:06, 169.10it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2117/3257 [00:12<00:06, 178.48it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2136/3257 [00:12<00:06, 171.77it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2154/3257 [00:12<00:06, 172.67it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2177/3257 [00:12<00:05, 187.27it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2197/3257 [00:12<00:05, 187.04it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2216/3257 [00:12<00:05, 182.06it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:12<00:05, 185.23it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2255/3257 [00:12<00:05, 178.99it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2274/3257 [00:13<00:05, 170.33it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2297/3257 [00:13<00:05, 186.02it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2316/3257 [00:13<00:05, 184.60it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2341/3257 [00:13<00:04, 201.67it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2365/3257 [00:13<00:04, 212.50it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2388/3257 [00:13<00:04, 216.99it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2410/3257 [00:13<00:04, 205.70it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2431/3257 [00:13<00:04, 197.57it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2451/3257 [00:13<00:04, 193.31it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2475/3257 [00:13<00:03, 204.88it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2497/3257 [00:14<00:03, 208.12it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2519/3257 [00:14<00:03, 209.36it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2541/3257 [00:14<00:03, 203.55it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2562/3257 [00:14<00:03, 196.56it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2582/3257 [00:14<00:03, 190.17it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2602/3257 [00:14<00:03, 190.42it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2627/3257 [00:14<00:03, 205.29it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2648/3257 [00:14<00:03, 194.28it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2668/3257 [00:14<00:03, 193.63it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2690/3257 [00:15<00:02, 199.68it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2711/3257 [00:15<00:03, 178.25it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2733/3257 [00:15<00:02, 189.11it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:15<00:02, 195.03it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2776/3257 [00:15<00:02, 189.32it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2802/3257 [00:15<00:02, 208.33it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2824/3257 [00:15<00:02, 193.78it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:15<00:02, 195.37it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2873/3257 [00:15<00:01, 220.90it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2896/3257 [00:16<00:01, 205.93it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2919/3257 [00:16<00:01, 211.88it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2941/3257 [00:16<00:01, 211.51it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2963/3257 [00:16<00:01, 201.61it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2984/3257 [00:16<00:01, 187.36it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3008/3257 [00:16<00:01, 201.10it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3029/3257 [00:16<00:01, 198.16it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3053/3257 [00:16<00:00, 209.07it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3077/3257 [00:16<00:00, 217.80it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3100/3257 [00:17<00:00, 205.77it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3124/3257 [00:17<00:00, 215.02it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3146/3257 [00:17<00:00, 198.01it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3167/3257 [00:17<00:00, 194.29it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3187/3257 [00:17<00:00, 186.18it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:17<00:00, 189.41it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3227/3257 [00:17<00:00, 188.97it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3251/3257 [00:17<00:00, 202.82it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:17<00:00, 181.95it/s]
2023-02-07 19:48:25.271 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:48:25,273][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d478,n5,mc3,s0.560394,t4>', 'datetime': '2023-02-07T19:48:25.273198', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:48:25,274][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:48:25,274][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:48:25,790][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 19:48:25,790][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:48:25,867][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 30516 unique words (71.46% of original 42701, drops 12185)', 'datetime': '2023-02-07T19:48:25.867346', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:48:25,867][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 5804704 word corpus (99.69% of original 5822992, drops 18288)', 'datetime': '2023-02-07T19:48:25.867737', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:48:25,968][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 19:48:25,969][gensim.models.word2vec][INFO] - sample=0.560394 downsamples 0 most-common words
[2023-02-07 19:48:25,969][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5804704 word corpus (100.0%% of prior 5804704)', 'datetime': '2023-02-07T19:48:25.969707', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:48:26,134][gensim.models.word2vec][INFO] - estimated required memory for 30516 words and 478 dimensions: 138829968 bytes
[2023-02-07 19:48:26,134][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:48:26,193][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 30516 vocabulary and 478 features, using sg=1 hs=0 sample=0.5603940938226961 negative=5 window=9 shrink_windows=True', 'datetime': '2023-02-07T19:48:26.193637', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:48:27,203][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 22.11% examples, 1252705 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:48:28,208][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 44.86% examples, 1315610 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:48:29,210][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 68.22% examples, 1335208 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:30,217][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 93.12% examples, 1346281 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:30,475][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5778362 effective words) took 4.3s, 1350593 effective words/s
[2023-02-07 19:48:31,482][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 19.01% examples, 1061946 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:32,482][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 36.48% examples, 1072335 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:33,489][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 55.20% examples, 1082733 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:48:34,491][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 74.58% examples, 1089052 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:35,492][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 96.90% examples, 1117922 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:48:35,634][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5778362 effective words) took 5.2s, 1120530 effective words/s
[2023-02-07 19:48:36,638][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 27.11% examples, 1569346 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:37,641][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 52.93% examples, 1558426 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:38,647][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 75.99% examples, 1478142 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:48:39,673][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 97.67% examples, 1401127 words/s, in_qsize 8, out_qsize 3
[2023-02-07 19:48:39,740][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5778362 effective words) took 4.1s, 1407606 effective words/s
[2023-02-07 19:48:40,747][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 21.31% examples, 1206818 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:41,748][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 41.14% examples, 1214177 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:42,754][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 62.45% examples, 1212522 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:43,755][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 83.57% examples, 1217171 words/s, in_qsize 5, out_qsize 0
[2023-02-07 19:48:44,475][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5778362 effective words) took 4.7s, 1220894 effective words/s
[2023-02-07 19:48:45,485][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 22.01% examples, 1241724 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:48:46,487][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 42.03% examples, 1242884 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:47,488][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 63.22% examples, 1235489 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:48,507][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 85.20% examples, 1233224 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:49,148][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5778362 effective words) took 4.7s, 1237614 effective words/s
[2023-02-07 19:48:50,150][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 21.58% examples, 1230044 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:48:51,153][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 42.25% examples, 1253373 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:52,160][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 63.99% examples, 1249941 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:53,163][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 86.15% examples, 1249960 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:53,755][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5778362 effective words) took 4.6s, 1254746 effective words/s
[2023-02-07 19:48:54,761][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 19.22% examples, 1072079 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:48:55,768][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 39.94% examples, 1180203 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:56,769][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 61.90% examples, 1205316 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:57,783][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 83.27% examples, 1209276 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:48:58,493][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5778362 effective words) took 4.7s, 1220064 effective words/s
[2023-02-07 19:48:59,501][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 21.58% examples, 1229187 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:00,501][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 41.97% examples, 1240789 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:01,502][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 63.22% examples, 1237528 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:49:02,508][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 85.20% examples, 1238529 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:03,145][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5778362 effective words) took 4.6s, 1243205 effective words/s
[2023-02-07 19:49:04,153][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 21.83% examples, 1238238 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:05,157][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 42.37% examples, 1252499 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:49:06,161][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 63.99% examples, 1248027 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:07,163][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 86.46% examples, 1255001 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:07,723][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5778362 effective words) took 4.6s, 1262925 effective words/s
[2023-02-07 19:49:08,727][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 22.35% examples, 1278308 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:09,728][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 42.98% examples, 1276319 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:49:10,745][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 65.37% examples, 1275272 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:11,752][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 88.42% examples, 1277314 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:12,239][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5778362 effective words) took 4.5s, 1280720 effective words/s
[2023-02-07 19:49:13,249][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 22.57% examples, 1284094 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:14,250][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 44.03% examples, 1297965 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:15,255][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 65.83% examples, 1289444 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:49:16,270][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 89.01% examples, 1284674 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:49:16,738][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5778362 effective words) took 4.5s, 1284830 effective words/s
[2023-02-07 19:49:17,749][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 22.01% examples, 1241046 words/s, in_qsize 7, out_qsize 1
[2023-02-07 19:49:18,756][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 42.52% examples, 1255294 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:19,768][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 65.21% examples, 1267449 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:20,769][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 87.93% examples, 1271153 words/s, in_qsize 7, out_qsize 1
[2023-02-07 19:49:21,265][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5778362 effective words) took 4.5s, 1277279 effective words/s
[2023-02-07 19:49:22,270][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 21.92% examples, 1247367 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:23,271][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 42.98% examples, 1274138 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:24,281][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 65.31% examples, 1273550 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:49:25,291][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 87.69% examples, 1270142 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:49:25,819][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5778362 effective words) took 4.6s, 1269399 effective words/s
[2023-02-07 19:49:26,827][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 21.58% examples, 1224311 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:49:27,835][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 42.03% examples, 1238694 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:49:28,838][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 63.77% examples, 1241952 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:49:29,840][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 85.60% examples, 1241115 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:49:30,473][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5778362 effective words) took 4.7s, 1242290 effective words/s
[2023-02-07 19:49:31,489][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 22.11% examples, 1242214 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:32,492][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 42.65% examples, 1256458 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:33,496][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 64.38% examples, 1254477 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:49:34,502][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 86.58% examples, 1252448 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:49:35,076][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5778362 effective words) took 4.6s, 1255970 effective words/s
[2023-02-07 19:49:35,076][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86675430 effective words) took 68.9s, 1258331 effective words/s', 'datetime': '2023-02-07T19:49:35.076936', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:49:35.077 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:49:40,130][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194758-x8sg3xjw/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:49:40.130692', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:49:40,131][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194758-x8sg3xjw/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 19:49:40,191][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194758-x8sg3xjw/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 19:49:40,249][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:49:40,286][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194758-x8sg3xjw/files/../tmp/embedding_model.pt
2023-02-07 19:49:40.286 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:49:42.888 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:49:43.663 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:49:46.782 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2155824169471487, 'test_mae': 1.1122752563310845, 'test_r2': -1.4801566572648324}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.035 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.035 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.58
wandb: percentage 0.28536
wandb:   test_mae 1.11228
wandb:   test_mse 2.21558
wandb:    test_r2 -1.48016
wandb: 
wandb: üöÄ View run jolly-sweep-61 at: https://wandb.ai/xiaoqiz/mof2vec/runs/x8sg3xjw
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_194758-x8sg3xjw/logs
wandb: Agent Starting Run: c0p3q4bs with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 763
wandb: 	model.gensim.alpha: 0.007167308610772795
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.6178467458021984
wandb: 	model.gensim.vector_size: 280
wandb: 	model.gensim.window: 18
wandb: 	model.sklearn.learning_rate: 0.03407569130907915
wandb: 	model.sklearn.max_depth: 42
wandb: 	model.sklearn.min_child_weight: 0.05193405896774928
wandb: 	model.sklearn.n_estimators: 1037
wandb: 	model.sklearn.num_leaves: 318
wandb: 	model.sklearn.reg_alpha: 0.1895877738827081
wandb: 	model.sklearn.reg_lambda: 0.026297190788353804
wandb: 	model.sklearn.subsample: 0.43379172104709574
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194955-c0p3q4bs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-sweep-62
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/c0p3q4bs
2023-02-07 19:50:04.258 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 19:50:04.259 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 763 for sweep.
2023-02-07 19:50:04.259 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.007167308610772795 for sweep.
2023-02-07 19:50:04.260 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:50:04.260 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 19:50:04.260 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6178467458021984 for sweep.
2023-02-07 19:50:04.260 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 280 for sweep.
2023-02-07 19:50:04.261 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 18 for sweep.
2023-02-07 19:50:04.261 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.03407569130907915 for sweep.
2023-02-07 19:50:04.261 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 42 for sweep.
2023-02-07 19:50:04.261 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05193405896774928 for sweep.
2023-02-07 19:50:04.261 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1037 for sweep.
2023-02-07 19:50:04.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 318 for sweep.
2023-02-07 19:50:04.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.1895877738827081 for sweep.
2023-02-07 19:50:04.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.026297190788353804 for sweep.
2023-02-07 19:50:04.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.43379172104709574 for sweep.
2023-02-07 19:50:04.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:50:04.268 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194955-c0p3q4bs/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 763, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 280, 'window': 18, 'min_count': 4, 'dm': 0, 'sample': 0.6178467458021984, 'workers': 4, 'alpha': 0.007167308610772795, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1037, 'max_depth': 42, 'num_leaves': 318, 'reg_alpha': 0.1895877738827081, 'reg_lambda': 0.026297190788353804, 'subsample': 0.43379172104709574, 'min_child_weight': 0.05193405896774928, 'n_jobs': 4, 'learning_rate': 0.03407569130907915}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:19, 163.24it/s]  1%|          | 35/3257 [00:00<00:18, 170.50it/s]  2%|‚ñè         | 53/3257 [00:00<00:19, 168.23it/s]  2%|‚ñè         | 71/3257 [00:00<00:18, 172.29it/s]  3%|‚ñé         | 91/3257 [00:00<00:17, 180.16it/s]  3%|‚ñé         | 110/3257 [00:00<00:18, 168.92it/s]  4%|‚ñç         | 129/3257 [00:00<00:17, 174.56it/s]  5%|‚ñç         | 149/3257 [00:00<00:17, 178.48it/s]  5%|‚ñå         | 167/3257 [00:00<00:17, 171.95it/s]  6%|‚ñå         | 185/3257 [00:01<00:17, 173.92it/s]  6%|‚ñå         | 203/3257 [00:01<00:17, 173.92it/s]  7%|‚ñã         | 229/3257 [00:01<00:15, 198.62it/s]  8%|‚ñä         | 249/3257 [00:01<00:15, 192.91it/s]  8%|‚ñä         | 269/3257 [00:01<00:16, 184.16it/s]  9%|‚ñâ         | 293/3257 [00:01<00:14, 199.80it/s] 10%|‚ñâ         | 314/3257 [00:01<00:15, 185.05it/s] 10%|‚ñà         | 335/3257 [00:01<00:15, 189.54it/s] 11%|‚ñà         | 355/3257 [00:01<00:15, 190.85it/s] 12%|‚ñà‚ñè        | 375/3257 [00:02<00:15, 183.50it/s] 12%|‚ñà‚ñè        | 394/3257 [00:02<00:16, 174.51it/s] 13%|‚ñà‚ñé        | 415/3257 [00:02<00:15, 183.36it/s] 13%|‚ñà‚ñé        | 434/3257 [00:02<00:18, 155.39it/s] 14%|‚ñà‚ñç        | 453/3257 [00:02<00:17, 161.81it/s] 15%|‚ñà‚ñç        | 473/3257 [00:02<00:16, 170.08it/s] 15%|‚ñà‚ñå        | 491/3257 [00:02<00:16, 169.09it/s] 16%|‚ñà‚ñå        | 512/3257 [00:02<00:15, 179.38it/s] 16%|‚ñà‚ñã        | 531/3257 [00:02<00:15, 177.37it/s] 17%|‚ñà‚ñã        | 550/3257 [00:03<00:15, 179.28it/s] 17%|‚ñà‚ñã        | 569/3257 [00:03<00:15, 173.66it/s] 18%|‚ñà‚ñä        | 587/3257 [00:03<00:15, 169.11it/s] 19%|‚ñà‚ñä        | 607/3257 [00:03<00:15, 176.24it/s] 19%|‚ñà‚ñâ        | 625/3257 [00:03<00:15, 174.20it/s] 20%|‚ñà‚ñâ        | 644/3257 [00:03<00:14, 177.84it/s] 20%|‚ñà‚ñà        | 662/3257 [00:03<00:15, 165.72it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:15, 170.96it/s] 22%|‚ñà‚ñà‚ñè       | 702/3257 [00:03<00:14, 174.81it/s] 22%|‚ñà‚ñà‚ñè       | 721/3257 [00:04<00:14, 177.29it/s] 23%|‚ñà‚ñà‚ñé       | 739/3257 [00:04<00:15, 166.75it/s] 23%|‚ñà‚ñà‚ñé       | 762/3257 [00:04<00:13, 182.21it/s] 24%|‚ñà‚ñà‚ñç       | 781/3257 [00:04<00:14, 175.78it/s] 25%|‚ñà‚ñà‚ñç       | 802/3257 [00:04<00:13, 184.76it/s] 25%|‚ñà‚ñà‚ñå       | 821/3257 [00:04<00:13, 176.45it/s] 26%|‚ñà‚ñà‚ñå       | 839/3257 [00:04<00:14, 172.58it/s] 26%|‚ñà‚ñà‚ñã       | 857/3257 [00:04<00:14, 167.08it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:04<00:14, 166.81it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:05<00:14, 163.42it/s] 28%|‚ñà‚ñà‚ñä       | 911/3257 [00:05<00:13, 172.30it/s] 29%|‚ñà‚ñà‚ñä       | 930/3257 [00:05<00:13, 174.73it/s] 29%|‚ñà‚ñà‚ñâ       | 948/3257 [00:05<00:13, 170.73it/s] 30%|‚ñà‚ñà‚ñâ       | 967/3257 [00:05<00:13, 175.14it/s] 30%|‚ñà‚ñà‚ñà       | 985/3257 [00:05<00:13, 172.99it/s] 31%|‚ñà‚ñà‚ñà       | 1003/3257 [00:05<00:13, 168.37it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1022/3257 [00:05<00:12, 172.54it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1040/3257 [00:05<00:13, 163.49it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1057/3257 [00:06<00:13, 162.08it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1079/3257 [00:06<00:12, 177.46it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1097/3257 [00:06<00:13, 162.67it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:06<00:12, 169.44it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1135/3257 [00:06<00:12, 169.03it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1153/3257 [00:06<00:12, 165.98it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:06<00:12, 173.52it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:06<00:13, 155.91it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1207/3257 [00:06<00:13, 154.42it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1228/3257 [00:07<00:12, 168.49it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1246/3257 [00:07<00:11, 171.16it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1265/3257 [00:07<00:11, 176.49it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1283/3257 [00:07<00:12, 160.69it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1300/3257 [00:07<00:19, 98.94it/s]  41%|‚ñà‚ñà‚ñà‚ñà      | 1320/3257 [00:07<00:16, 118.14it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:07<00:14, 132.77it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1356/3257 [00:08<00:13, 138.93it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1373/3257 [00:08<00:12, 145.25it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1390/3257 [00:08<00:12, 151.39it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1414/3257 [00:08<00:10, 174.23it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1433/3257 [00:08<00:10, 177.55it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1453/3257 [00:08<00:09, 183.72it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1474/3257 [00:08<00:09, 191.17it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1494/3257 [00:08<00:09, 190.41it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1515/3257 [00:08<00:08, 194.35it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1535/3257 [00:09<00:10, 170.75it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1553/3257 [00:09<00:10, 168.89it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1574/3257 [00:09<00:09, 176.70it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1593/3257 [00:09<00:09, 175.97it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:09<00:08, 186.24it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:09<00:09, 176.36it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1652/3257 [00:09<00:09, 174.02it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1670/3257 [00:09<00:09, 173.96it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1688/3257 [00:09<00:09, 174.00it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1707/3257 [00:10<00:08, 175.12it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:10<00:09, 169.65it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1746/3257 [00:10<00:09, 166.69it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1765/3257 [00:10<00:08, 172.78it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1786/3257 [00:10<00:08, 181.53it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:10<00:08, 174.98it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1823/3257 [00:10<00:08, 173.37it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1841/3257 [00:10<00:08, 170.71it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1859/3257 [00:10<00:08, 170.57it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1879/3257 [00:10<00:07, 177.74it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1897/3257 [00:11<00:07, 175.55it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:11<00:07, 177.24it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1937/3257 [00:11<00:07, 186.06it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1963/3257 [00:11<00:06, 206.65it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1984/3257 [00:11<00:06, 188.84it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2005/3257 [00:11<00:06, 193.08it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2025/3257 [00:11<00:06, 194.17it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2045/3257 [00:11<00:06, 189.62it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2065/3257 [00:12<00:06, 173.04it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2085/3257 [00:12<00:06, 175.76it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2103/3257 [00:12<00:06, 175.02it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2121/3257 [00:12<00:06, 166.97it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2140/3257 [00:12<00:06, 172.64it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2158/3257 [00:12<00:06, 173.67it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:12<00:06, 177.22it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2199/3257 [00:12<00:05, 184.06it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2218/3257 [00:12<00:05, 179.62it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2237/3257 [00:12<00:05, 180.54it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2256/3257 [00:13<00:05, 179.77it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2275/3257 [00:13<00:05, 169.21it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:13<00:05, 182.63it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2321/3257 [00:13<00:04, 192.62it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2346/3257 [00:13<00:04, 208.46it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2368/3257 [00:13<00:04, 209.29it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2392/3257 [00:13<00:03, 217.51it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2414/3257 [00:13<00:04, 201.64it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2435/3257 [00:13<00:04, 188.82it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2455/3257 [00:14<00:04, 183.89it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2479/3257 [00:14<00:03, 197.07it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2503/3257 [00:14<00:03, 208.82it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2525/3257 [00:14<00:03, 207.30it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2546/3257 [00:14<00:03, 205.03it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2567/3257 [00:14<00:03, 188.23it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2587/3257 [00:14<00:03, 179.31it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2610/3257 [00:14<00:03, 192.18it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2634/3257 [00:14<00:03, 203.99it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2655/3257 [00:15<00:05, 107.73it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2673/3257 [00:15<00:04, 120.12it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2693/3257 [00:15<00:04, 135.28it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2711/3257 [00:15<00:04, 132.28it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:15<00:03, 146.92it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2754/3257 [00:15<00:03, 162.67it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2773/3257 [00:16<00:03, 159.16it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2796/3257 [00:16<00:02, 175.17it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2815/3257 [00:16<00:02, 173.23it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2834/3257 [00:16<00:02, 167.43it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2854/3257 [00:16<00:02, 175.70it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:16<00:01, 196.61it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2902/3257 [00:16<00:01, 180.11it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2924/3257 [00:16<00:01, 189.09it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2944/3257 [00:17<00:01, 172.56it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2964/3257 [00:17<00:01, 179.52it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2983/3257 [00:17<00:01, 170.91it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3005/3257 [00:17<00:01, 183.89it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3024/3257 [00:17<00:01, 179.89it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3048/3257 [00:17<00:01, 192.45it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3071/3257 [00:17<00:00, 200.40it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3092/3257 [00:17<00:00, 193.27it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3116/3257 [00:17<00:00, 206.12it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3137/3257 [00:18<00:00, 194.94it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3157/3257 [00:18<00:00, 185.68it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3176/3257 [00:18<00:00, 181.36it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3196/3257 [00:18<00:00, 185.06it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3215/3257 [00:18<00:00, 171.70it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3238/3257 [00:18<00:00, 186.50it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 187.01it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 174.40it/s]
2023-02-07 19:50:23.790 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:50:23,792][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d280,n5,mc4,s0.617847,t4>', 'datetime': '2023-02-07T19:50:23.792195', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:50:23,792][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:50:23,792][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:50:24,375][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 19:50:24,375][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:50:24,473][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 36921 unique words (68.30% of original 54054, drops 17133)', 'datetime': '2023-02-07T19:50:24.473710', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:50:24,474][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 6522479 word corpus (99.57% of original 6550866, drops 28387)', 'datetime': '2023-02-07T19:50:24.474121', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:50:24,597][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 19:50:24,598][gensim.models.word2vec][INFO] - sample=0.617847 downsamples 0 most-common words
[2023-02-07 19:50:24,598][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6522479 word corpus (100.0%% of prior 6522479)', 'datetime': '2023-02-07T19:50:24.598732', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:50:24,799][gensim.models.word2vec][INFO] - estimated required memory for 36921 words and 280 dimensions: 105462780 bytes
[2023-02-07 19:50:24,800][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:50:24,837][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 36921 vocabulary and 280 features, using sg=1 hs=0 sample=0.6178467458021984 negative=5 window=18 shrink_windows=True', 'datetime': '2023-02-07T19:50:24.837798', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:50:25,843][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 28.03% examples, 1815147 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:26,847][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 59.53% examples, 1957542 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:27,852][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 93.46% examples, 2017949 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:28,027][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6467889 effective words) took 3.2s, 2030690 effective words/s
[2023-02-07 19:50:29,029][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 35.89% examples, 2345058 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:30,035][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 70.86% examples, 2335973 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:30,775][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6467889 effective words) took 2.7s, 2355051 effective words/s
[2023-02-07 19:50:31,778][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 28.06% examples, 1824144 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:32,779][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 56.03% examples, 1852536 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:33,783][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 85.20% examples, 1853043 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:34,267][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6467889 effective words) took 3.5s, 1853747 effective words/s
[2023-02-07 19:50:35,271][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 28.28% examples, 1838391 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:50:36,277][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 55.97% examples, 1846349 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:37,280][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 85.20% examples, 1850732 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:37,772][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6467889 effective words) took 3.5s, 1846931 effective words/s
[2023-02-07 19:50:38,774][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 30.73% examples, 1995682 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:39,779][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 65.80% examples, 2168901 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:40,678][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6467889 effective words) took 2.9s, 2227236 effective words/s
[2023-02-07 19:50:41,682][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 32.55% examples, 2124946 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:42,685][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 64.32% examples, 2110066 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:43,685][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 94.72% examples, 2044217 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:43,847][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6467889 effective words) took 3.2s, 2041753 effective words/s
[2023-02-07 19:50:44,852][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 29.38% examples, 1896627 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:45,863][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 57.35% examples, 1880370 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:50:46,866][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 87.04% examples, 1884649 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:50:47,269][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6467889 effective words) took 3.4s, 1891274 effective words/s
[2023-02-07 19:50:48,278][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 29.66% examples, 1908907 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:49,287][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 57.72% examples, 1894890 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:50,285][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 93.71% examples, 2018772 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:50,440][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6467889 effective words) took 3.2s, 2042183 effective words/s
[2023-02-07 19:50:51,443][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 35.00% examples, 2292438 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:50:52,448][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 69.85% examples, 2306851 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:50:53,214][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6467889 effective words) took 2.8s, 2333412 effective words/s
[2023-02-07 19:50:54,220][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 29.75% examples, 1919242 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:55,228][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 58.46% examples, 1917783 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:56,231][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 89.41% examples, 1930891 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:56,514][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6467889 effective words) took 3.3s, 1961824 effective words/s
[2023-02-07 19:50:57,522][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 30.12% examples, 1948724 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:58,522][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 66.44% examples, 2186584 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:50:59,384][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6467889 effective words) took 2.9s, 2255498 effective words/s
[2023-02-07 19:51:00,389][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 30.03% examples, 1941309 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:51:01,392][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 58.27% examples, 1921109 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:51:02,395][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 88.73% examples, 1918075 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:51:02,759][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6467889 effective words) took 3.4s, 1918843 effective words/s
[2023-02-07 19:51:03,763][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 29.90% examples, 1930595 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:51:04,769][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 58.80% examples, 1933836 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:51:05,773][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 89.01% examples, 1927013 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:51:06,125][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6467889 effective words) took 3.4s, 1923630 effective words/s
[2023-02-07 19:51:07,134][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 29.23% examples, 1884055 words/s, in_qsize 5, out_qsize 2
[2023-02-07 19:51:08,136][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 58.21% examples, 1916326 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:51:09,145][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 88.67% examples, 1912202 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:51:09,509][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6467889 effective words) took 3.4s, 1913178 effective words/s
[2023-02-07 19:51:10,517][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 29.87% examples, 1927222 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:51:11,522][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 58.27% examples, 1915424 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:51:12,522][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 88.61% examples, 1913000 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:51:12,894][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6467889 effective words) took 3.4s, 1912836 effective words/s
[2023-02-07 19:51:12,895][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (97018335 effective words) took 48.1s, 2018824 effective words/s', 'datetime': '2023-02-07T19:51:12.895132', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:51:12.895 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:51:16,977][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194955-c0p3q4bs/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:51:16.977872', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:51:16,978][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:51:17,171][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_194955-c0p3q4bs/files/../tmp/embedding_model.pt
2023-02-07 19:51:17.171 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:51:18.955 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:51:19.552 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:51:22.184 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.285716044223276, 'test_mae': 1.1330149796251436, 'test_r2': -1.9694308157942015}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.83
wandb: percentage 0.31696
wandb:   test_mae 1.13301
wandb:   test_mse 2.28572
wandb:    test_r2 -1.96943
wandb: 
wandb: üöÄ View run dashing-sweep-62 at: https://wandb.ai/xiaoqiz/mof2vec/runs/c0p3q4bs
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_194955-c0p3q4bs/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: xb6i8lgi with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 751
wandb: 	model.gensim.alpha: 0.02731706637928713
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.5793546191065002
wandb: 	model.gensim.vector_size: 178
wandb: 	model.gensim.window: 19
wandb: 	model.sklearn.learning_rate: 0.02487275827140468
wandb: 	model.sklearn.max_depth: 44
wandb: 	model.sklearn.min_child_weight: 0.05044163446218793
wandb: 	model.sklearn.n_estimators: 915
wandb: 	model.sklearn.num_leaves: 433
wandb: 	model.sklearn.reg_alpha: 0.02571669816516703
wandb: 	model.sklearn.reg_lambda: 0.2469917206273639
wandb: 	model.sklearn.subsample: 0.8846414314860289
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195141-xb6i8lgi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-63
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/xb6i8lgi
2023-02-07 19:51:49.292 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 19:51:49.293 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 751 for sweep.
2023-02-07 19:51:49.293 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.02731706637928713 for sweep.
2023-02-07 19:51:49.294 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:51:49.294 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 19:51:49.294 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5793546191065002 for sweep.
2023-02-07 19:51:49.294 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 178 for sweep.
2023-02-07 19:51:49.294 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 19 for sweep.
2023-02-07 19:51:49.295 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.02487275827140468 for sweep.
2023-02-07 19:51:49.295 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 44 for sweep.
2023-02-07 19:51:49.295 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05044163446218793 for sweep.
2023-02-07 19:51:49.295 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 915 for sweep.
2023-02-07 19:51:49.295 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 433 for sweep.
2023-02-07 19:51:49.296 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.02571669816516703 for sweep.
2023-02-07 19:51:49.296 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.2469917206273639 for sweep.
2023-02-07 19:51:49.296 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8846414314860289 for sweep.
2023-02-07 19:51:49.296 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:51:49.314 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195141-xb6i8lgi/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 751, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 178, 'window': 19, 'min_count': 2, 'dm': 0, 'sample': 0.5793546191065002, 'workers': 4, 'alpha': 0.02731706637928713, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 915, 'max_depth': 44, 'num_leaves': 433, 'reg_alpha': 0.02571669816516703, 'reg_lambda': 0.2469917206273639, 'subsample': 0.8846414314860289, 'min_child_weight': 0.05044163446218793, 'n_jobs': 4, 'learning_rate': 0.02487275827140468}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:18, 173.16it/s]  1%|          | 40/3257 [00:00<00:16, 199.79it/s]  2%|‚ñè         | 61/3257 [00:00<00:15, 201.37it/s]  3%|‚ñé         | 86/3257 [00:00<00:14, 220.22it/s]  3%|‚ñé         | 109/3257 [00:00<00:15, 203.36it/s]  4%|‚ñç         | 134/3257 [00:00<00:14, 214.25it/s]  5%|‚ñç         | 158/3257 [00:00<00:14, 221.29it/s]  6%|‚ñå         | 181/3257 [00:00<00:14, 215.68it/s]  6%|‚ñã         | 204/3257 [00:00<00:13, 219.08it/s]  7%|‚ñã         | 234/3257 [00:01<00:12, 241.34it/s]  8%|‚ñä         | 259/3257 [00:01<00:12, 233.40it/s]  9%|‚ñâ         | 286/3257 [00:01<00:12, 242.74it/s] 10%|‚ñâ         | 311/3257 [00:01<00:12, 239.18it/s] 10%|‚ñà         | 336/3257 [00:01<00:12, 234.40it/s] 11%|‚ñà         | 360/3257 [00:01<00:12, 233.29it/s] 12%|‚ñà‚ñè        | 384/3257 [00:01<00:13, 220.08it/s] 12%|‚ñà‚ñè        | 407/3257 [00:01<00:12, 220.19it/s] 13%|‚ñà‚ñé        | 430/3257 [00:01<00:14, 198.48it/s] 14%|‚ñà‚ñç        | 452/3257 [00:02<00:13, 203.44it/s] 15%|‚ñà‚ñç        | 475/3257 [00:02<00:13, 209.81it/s] 15%|‚ñà‚ñå        | 498/3257 [00:02<00:12, 214.10it/s] 16%|‚ñà‚ñå        | 522/3257 [00:02<00:12, 218.60it/s] 17%|‚ñà‚ñã        | 546/3257 [00:02<00:12, 216.44it/s] 17%|‚ñà‚ñã        | 568/3257 [00:02<00:12, 210.28it/s] 18%|‚ñà‚ñä        | 590/3257 [00:02<00:12, 205.63it/s] 19%|‚ñà‚ñâ        | 611/3257 [00:02<00:18, 144.37it/s] 19%|‚ñà‚ñâ        | 633/3257 [00:03<00:16, 160.69it/s] 20%|‚ñà‚ñà        | 652/3257 [00:03<00:15, 166.92it/s] 21%|‚ñà‚ñà        | 674/3257 [00:03<00:14, 178.70it/s] 21%|‚ñà‚ñà‚ñè       | 694/3257 [00:03<00:13, 184.25it/s] 22%|‚ñà‚ñà‚ñè       | 717/3257 [00:03<00:12, 196.11it/s] 23%|‚ñà‚ñà‚ñé       | 738/3257 [00:03<00:13, 187.35it/s] 23%|‚ñà‚ñà‚ñé       | 763/3257 [00:03<00:12, 203.72it/s] 24%|‚ñà‚ñà‚ñç       | 784/3257 [00:03<00:12, 196.27it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:03<00:12, 199.78it/s] 25%|‚ñà‚ñà‚ñå       | 828/3257 [00:04<00:11, 203.47it/s] 26%|‚ñà‚ñà‚ñå       | 849/3257 [00:04<00:12, 194.87it/s] 27%|‚ñà‚ñà‚ñã       | 871/3257 [00:04<00:11, 201.24it/s] 27%|‚ñà‚ñà‚ñã       | 892/3257 [00:04<00:11, 201.75it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:04<00:11, 205.00it/s] 29%|‚ñà‚ñà‚ñä       | 935/3257 [00:04<00:11, 204.69it/s] 29%|‚ñà‚ñà‚ñâ       | 959/3257 [00:04<00:10, 211.32it/s] 30%|‚ñà‚ñà‚ñà       | 981/3257 [00:04<00:10, 210.61it/s] 31%|‚ñà‚ñà‚ñà       | 1003/3257 [00:04<00:10, 209.36it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1025/3257 [00:04<00:10, 207.18it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1046/3257 [00:05<00:11, 194.27it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1068/3257 [00:05<00:10, 200.22it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1090/3257 [00:05<00:10, 202.90it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1113/3257 [00:05<00:10, 208.49it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1134/3257 [00:05<00:10, 201.99it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1155/3257 [00:05<00:10, 200.99it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1176/3257 [00:05<00:10, 198.84it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1196/3257 [00:05<00:11, 183.93it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1215/3257 [00:05<00:11, 182.44it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1241/3257 [00:06<00:10, 199.61it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1262/3257 [00:06<00:10, 198.88it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1282/3257 [00:06<00:10, 189.59it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1302/3257 [00:06<00:10, 191.93it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1327/3257 [00:06<00:09, 207.97it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1349/3257 [00:06<00:09, 208.49it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:06<00:09, 202.12it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1391/3257 [00:06<00:09, 200.89it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1420/3257 [00:06<00:08, 225.33it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1443/3257 [00:07<00:08, 219.94it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1470/3257 [00:07<00:07, 233.65it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1494/3257 [00:07<00:07, 235.15it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:07<00:07, 229.72it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1543/3257 [00:07<00:07, 219.76it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1566/3257 [00:07<00:07, 211.80it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1588/3257 [00:07<00:07, 212.47it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1614/3257 [00:07<00:07, 224.19it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1637/3257 [00:07<00:07, 212.35it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:08<00:07, 206.00it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1680/3257 [00:08<00:08, 195.35it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1701/3257 [00:08<00:07, 197.40it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1721/3257 [00:08<00:07, 195.53it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1741/3257 [00:08<00:08, 176.11it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1764/3257 [00:08<00:07, 188.70it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1787/3257 [00:08<00:07, 198.92it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1808/3257 [00:08<00:07, 188.43it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1828/3257 [00:08<00:07, 191.19it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1850/3257 [00:09<00:07, 195.11it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1874/3257 [00:09<00:06, 206.43it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1895/3257 [00:09<00:06, 203.28it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:09<00:06, 202.03it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1941/3257 [00:09<00:06, 213.76it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1963/3257 [00:09<00:09, 134.20it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1982/3257 [00:09<00:08, 144.78it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:09<00:07, 160.61it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2029/3257 [00:10<00:06, 180.47it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2050/3257 [00:10<00:06, 173.80it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2069/3257 [00:10<00:06, 174.15it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2091/3257 [00:10<00:06, 184.31it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2111/3257 [00:10<00:06, 186.32it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2131/3257 [00:10<00:06, 177.96it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2150/3257 [00:10<00:06, 179.10it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2174/3257 [00:10<00:05, 195.73it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2194/3257 [00:10<00:05, 195.54it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2214/3257 [00:11<00:05, 188.93it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:11<00:05, 195.21it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2256/3257 [00:11<00:05, 193.47it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2276/3257 [00:11<00:05, 183.24it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2300/3257 [00:11<00:04, 198.84it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2329/3257 [00:11<00:04, 222.86it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2357/3257 [00:11<00:03, 237.78it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2382/3257 [00:11<00:03, 230.66it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2406/3257 [00:11<00:03, 219.67it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2429/3257 [00:12<00:03, 219.45it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2452/3257 [00:12<00:03, 204.94it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2478/3257 [00:12<00:03, 218.36it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2505/3257 [00:12<00:03, 231.16it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2529/3257 [00:12<00:03, 233.53it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2553/3257 [00:12<00:03, 229.17it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2577/3257 [00:12<00:03, 213.60it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2599/3257 [00:12<00:03, 209.78it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:12<00:02, 230.69it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2652/3257 [00:13<00:02, 215.81it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2675/3257 [00:13<00:02, 219.37it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2698/3257 [00:13<00:02, 205.65it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2719/3257 [00:13<00:02, 195.91it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2744/3257 [00:13<00:02, 209.43it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2766/3257 [00:13<00:02, 209.29it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2788/3257 [00:13<00:02, 211.95it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2810/3257 [00:13<00:02, 213.02it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2832/3257 [00:13<00:02, 206.24it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2856/3257 [00:14<00:01, 213.70it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:14<00:01, 232.63it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2910/3257 [00:14<00:01, 220.79it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2933/3257 [00:14<00:01, 223.19it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2956/3257 [00:14<00:01, 204.78it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2978/3257 [00:14<00:01, 205.91it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3001/3257 [00:14<00:01, 211.72it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3023/3257 [00:14<00:01, 201.03it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:14<00:00, 212.41it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3078/3257 [00:15<00:00, 231.89it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3102/3257 [00:15<00:00, 229.03it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3127/3257 [00:15<00:00, 231.98it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3151/3257 [00:15<00:00, 217.31it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3174/3257 [00:15<00:00, 220.13it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3197/3257 [00:15<00:00, 213.03it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3219/3257 [00:15<00:00, 205.43it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3245/3257 [00:15<00:00, 220.25it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:15<00:00, 204.70it/s]
2023-02-07 19:52:05.907 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:52:05,908][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d178,n5,mc2,s0.579355,t4>', 'datetime': '2023-02-07T19:52:05.908258', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:52:05,908][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:52:05,908][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:52:06,342][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 19:52:06,343][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:52:06,410][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 27186 unique words (85.48% of original 31803, drops 4617)', 'datetime': '2023-02-07T19:52:06.410026', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:52:06,410][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 5090501 word corpus (99.91% of original 5095118, drops 4617)', 'datetime': '2023-02-07T19:52:06.410404', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:52:06,503][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 19:52:06,506][gensim.models.word2vec][INFO] - sample=0.579355 downsamples 0 most-common words
[2023-02-07 19:52:06,506][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5090501 word corpus (100.0%% of prior 5090501)', 'datetime': '2023-02-07T19:52:06.506739', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:52:06,653][gensim.models.word2vec][INFO] - estimated required memory for 27186 words and 178 dimensions: 55276248 bytes
[2023-02-07 19:52:06,654][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:52:06,675][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 27186 vocabulary and 178 features, using sg=1 hs=0 sample=0.5793546191065002 negative=5 window=19 shrink_windows=True', 'datetime': '2023-02-07T19:52:06.675093', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:52:07,683][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 41.63% examples, 2163351 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:08,688][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 86.92% examples, 2217883 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:52:08,952][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5082292 effective words) took 2.3s, 2234373 effective words/s
[2023-02-07 19:52:09,956][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 47.44% examples, 2459843 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:10,959][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 95.43% examples, 2421079 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:11,047][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5082292 effective words) took 2.1s, 2427429 effective words/s
[2023-02-07 19:52:12,048][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 50.97% examples, 2643372 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:12,825][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5082292 effective words) took 1.8s, 2860041 effective words/s
[2023-02-07 19:52:13,831][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 61.81% examples, 3179209 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:14,456][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5082292 effective words) took 1.6s, 3122056 effective words/s
[2023-02-07 19:52:15,460][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 50.63% examples, 2619610 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:16,411][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5082292 effective words) took 2.0s, 2600578 effective words/s
[2023-02-07 19:52:17,419][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 61.01% examples, 3136969 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:18,128][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5082292 effective words) took 1.7s, 2964721 effective words/s
[2023-02-07 19:52:19,132][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 49.46% examples, 2556144 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:20,116][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5082292 effective words) took 2.0s, 2559048 effective words/s
[2023-02-07 19:52:21,118][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 49.40% examples, 2561032 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:52:22,119][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 97.36% examples, 2474106 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:22,167][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5082292 effective words) took 2.0s, 2479311 effective words/s
[2023-02-07 19:52:23,173][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 47.62% examples, 2459608 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:24,185][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 96.59% examples, 2441216 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:24,249][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5082292 effective words) took 2.1s, 2442476 effective words/s
[2023-02-07 19:52:25,252][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 46.76% examples, 2413075 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:26,256][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 95.15% examples, 2417275 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:26,348][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5082292 effective words) took 2.1s, 2422723 effective words/s
[2023-02-07 19:52:27,357][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 47.90% examples, 2487477 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:28,354][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 98.13% examples, 2493298 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:28,390][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5082292 effective words) took 2.0s, 2493824 effective words/s
[2023-02-07 19:52:29,399][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 55.66% examples, 2885366 words/s, in_qsize 5, out_qsize 2
[2023-02-07 19:52:30,330][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5082292 effective words) took 1.9s, 2622563 effective words/s
[2023-02-07 19:52:31,337][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 48.57% examples, 2515811 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:32,288][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5082292 effective words) took 2.0s, 2600442 effective words/s
[2023-02-07 19:52:33,301][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 50.94% examples, 2639367 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:34,221][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5082292 effective words) took 1.9s, 2645270 effective words/s
[2023-02-07 19:52:35,225][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 60.85% examples, 3142918 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:52:35,964][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5082292 effective words) took 1.7s, 2920635 effective words/s
[2023-02-07 19:52:35,965][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (76234380 effective words) took 29.3s, 2602755 effective words/s', 'datetime': '2023-02-07T19:52:35.965387', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:52:35.965 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:52:39,241][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195141-xb6i8lgi/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:52:39.241171', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:52:39,243][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:52:39,325][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195141-xb6i8lgi/files/../tmp/embedding_model.pt
2023-02-07 19:52:39.326 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:52:40.653 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:52:41.133 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:53:11.656 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2400907296160044, 'test_mae': 1.1095972776575802, 'test_r2': -2.4995090430658427}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.98
wandb: percentage 0.14517
wandb:   test_mae 1.1096
wandb:   test_mse 2.24009
wandb:    test_r2 -2.49951
wandb: 
wandb: üöÄ View run cerulean-sweep-63 at: https://wandb.ai/xiaoqiz/mof2vec/runs/xb6i8lgi
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_195141-xb6i8lgi/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ttdl7e6p with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 857
wandb: 	model.gensim.alpha: 0.003751334013300339
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.7076408583708917
wandb: 	model.gensim.vector_size: 340
wandb: 	model.gensim.window: 15
wandb: 	model.sklearn.learning_rate: 0.18888916558910387
wandb: 	model.sklearn.max_depth: 17
wandb: 	model.sklearn.min_child_weight: 0.06790436454738671
wandb: 	model.sklearn.n_estimators: 2220
wandb: 	model.sklearn.num_leaves: 352
wandb: 	model.sklearn.reg_alpha: 0.1609737918892687
wandb: 	model.sklearn.reg_lambda: 0.09882736141859436
wandb: 	model.sklearn.subsample: 0.8156358866597633
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195331-ttdl7e6p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-sweep-64
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/ttdl7e6p
2023-02-07 19:53:39.240 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 19:53:39.241 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 857 for sweep.
2023-02-07 19:53:39.241 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.003751334013300339 for sweep.
2023-02-07 19:53:39.242 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:53:39.242 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 19:53:39.242 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7076408583708917 for sweep.
2023-02-07 19:53:39.243 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 340 for sweep.
2023-02-07 19:53:39.243 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 15 for sweep.
2023-02-07 19:53:39.243 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.18888916558910387 for sweep.
2023-02-07 19:53:39.243 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 17 for sweep.
2023-02-07 19:53:39.244 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.06790436454738671 for sweep.
2023-02-07 19:53:39.244 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2220 for sweep.
2023-02-07 19:53:39.244 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 352 for sweep.
2023-02-07 19:53:39.244 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.1609737918892687 for sweep.
2023-02-07 19:53:39.244 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.09882736141859436 for sweep.
2023-02-07 19:53:39.245 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8156358866597633 for sweep.
2023-02-07 19:53:39.245 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:53:39.252 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195331-ttdl7e6p/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 857, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 340, 'window': 15, 'min_count': 6, 'dm': 0, 'sample': 0.7076408583708917, 'workers': 4, 'alpha': 0.003751334013300339, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2220, 'max_depth': 17, 'num_leaves': 352, 'reg_alpha': 0.1609737918892687, 'reg_lambda': 0.09882736141859436, 'subsample': 0.8156358866597633, 'min_child_weight': 0.06790436454738671, 'n_jobs': 4, 'learning_rate': 0.18888916558910387}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 20/3257 [00:00<00:16, 193.37it/s]  1%|‚ñè         | 41/3257 [00:00<00:15, 201.00it/s]  2%|‚ñè         | 62/3257 [00:00<00:15, 204.49it/s]  3%|‚ñé         | 84/3257 [00:00<00:15, 208.20it/s]  3%|‚ñé         | 105/3257 [00:00<00:15, 202.82it/s]  4%|‚ñç         | 126/3257 [00:00<00:22, 137.29it/s]  5%|‚ñç         | 151/3257 [00:00<00:19, 162.10it/s]  5%|‚ñå         | 171/3257 [00:00<00:18, 171.08it/s]  6%|‚ñå         | 193/3257 [00:01<00:16, 182.25it/s]  7%|‚ñã         | 216/3257 [00:01<00:15, 193.25it/s]  7%|‚ñã         | 241/3257 [00:01<00:14, 208.19it/s]  8%|‚ñä         | 263/3257 [00:01<00:14, 204.92it/s]  9%|‚ñâ         | 290/3257 [00:01<00:13, 219.21it/s] 10%|‚ñâ         | 313/3257 [00:01<00:13, 210.65it/s] 10%|‚ñà         | 337/3257 [00:01<00:13, 217.98it/s] 11%|‚ñà         | 360/3257 [00:01<00:13, 217.24it/s] 12%|‚ñà‚ñè        | 382/3257 [00:01<00:13, 206.99it/s] 12%|‚ñà‚ñè        | 404/3257 [00:02<00:13, 210.10it/s] 13%|‚ñà‚ñé        | 426/3257 [00:02<00:14, 192.98it/s] 14%|‚ñà‚ñé        | 447/3257 [00:02<00:14, 194.55it/s] 14%|‚ñà‚ñç        | 472/3257 [00:02<00:13, 208.03it/s] 15%|‚ñà‚ñå        | 494/3257 [00:02<00:13, 206.93it/s] 16%|‚ñà‚ñå        | 518/3257 [00:02<00:12, 215.24it/s] 17%|‚ñà‚ñã        | 541/3257 [00:02<00:12, 218.14it/s] 17%|‚ñà‚ñã        | 563/3257 [00:02<00:13, 195.52it/s] 18%|‚ñà‚ñä        | 584/3257 [00:02<00:13, 191.54it/s] 19%|‚ñà‚ñä        | 607/3257 [00:03<00:13, 201.89it/s] 19%|‚ñà‚ñâ        | 628/3257 [00:03<00:13, 201.78it/s] 20%|‚ñà‚ñâ        | 649/3257 [00:03<00:13, 198.63it/s] 21%|‚ñà‚ñà        | 670/3257 [00:03<00:13, 193.62it/s] 21%|‚ñà‚ñà        | 690/3257 [00:03<00:13, 191.20it/s] 22%|‚ñà‚ñà‚ñè       | 713/3257 [00:03<00:12, 200.65it/s] 23%|‚ñà‚ñà‚ñé       | 734/3257 [00:03<00:13, 192.54it/s] 23%|‚ñà‚ñà‚ñé       | 754/3257 [00:03<00:13, 188.48it/s] 24%|‚ñà‚ñà‚ñç       | 775/3257 [00:03<00:12, 192.65it/s] 24%|‚ñà‚ñà‚ñç       | 795/3257 [00:04<00:12, 191.66it/s] 25%|‚ñà‚ñà‚ñå       | 815/3257 [00:04<00:12, 191.10it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:04<00:12, 187.52it/s] 26%|‚ñà‚ñà‚ñå       | 854/3257 [00:04<00:13, 176.21it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:04<00:13, 181.92it/s] 27%|‚ñà‚ñà‚ñã       | 893/3257 [00:04<00:13, 179.27it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:04<00:13, 179.24it/s] 29%|‚ñà‚ñà‚ñä       | 934/3257 [00:04<00:12, 183.87it/s] 29%|‚ñà‚ñà‚ñâ       | 956/3257 [00:04<00:12, 191.44it/s] 30%|‚ñà‚ñà‚ñâ       | 976/3257 [00:05<00:11, 193.60it/s] 31%|‚ñà‚ñà‚ñà       | 996/3257 [00:05<00:12, 183.24it/s] 31%|‚ñà‚ñà‚ñà       | 1015/3257 [00:05<00:12, 184.86it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1034/3257 [00:05<00:12, 180.91it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1053/3257 [00:05<00:12, 180.23it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1077/3257 [00:05<00:11, 195.17it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1097/3257 [00:05<00:11, 186.42it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1118/3257 [00:05<00:11, 192.30it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1138/3257 [00:05<00:11, 186.93it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1157/3257 [00:06<00:11, 185.69it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1176/3257 [00:06<00:11, 184.22it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1195/3257 [00:06<00:11, 172.43it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1213/3257 [00:06<00:12, 168.89it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1236/3257 [00:06<00:10, 185.61it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1255/3257 [00:06<00:10, 183.75it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:06<00:10, 183.33it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1294/3257 [00:06<00:11, 176.54it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1313/3257 [00:06<00:10, 180.14it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1335/3257 [00:06<00:10, 190.57it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1355/3257 [00:07<00:10, 185.78it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1374/3257 [00:07<00:10, 184.28it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1393/3257 [00:07<00:10, 182.67it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1420/3257 [00:07<00:08, 206.54it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1441/3257 [00:07<00:08, 204.03it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1468/3257 [00:07<00:08, 221.93it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1492/3257 [00:07<00:07, 225.46it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1515/3257 [00:08<00:11, 147.49it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1534/3257 [00:08<00:11, 152.08it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1554/3257 [00:08<00:10, 161.81it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1575/3257 [00:08<00:09, 169.89it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1599/3257 [00:08<00:08, 186.63it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1621/3257 [00:08<00:08, 193.20it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1642/3257 [00:08<00:08, 192.26it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1662/3257 [00:08<00:08, 187.61it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1682/3257 [00:08<00:08, 185.17it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1703/3257 [00:08<00:08, 190.05it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1726/3257 [00:09<00:07, 201.19it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1747/3257 [00:09<00:08, 185.89it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1770/3257 [00:09<00:07, 195.98it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1793/3257 [00:09<00:07, 203.54it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1814/3257 [00:09<00:07, 195.54it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1834/3257 [00:09<00:07, 196.42it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1856/3257 [00:09<00:06, 202.51it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1880/3257 [00:09<00:06, 211.06it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1902/3257 [00:09<00:06, 208.82it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1923/3257 [00:10<00:06, 205.52it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1952/3257 [00:10<00:05, 227.79it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1976/3257 [00:10<00:05, 230.35it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2000/3257 [00:10<00:05, 221.54it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2023/3257 [00:10<00:05, 220.47it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2046/3257 [00:10<00:05, 215.71it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2068/3257 [00:10<00:05, 198.80it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2091/3257 [00:10<00:05, 205.11it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2112/3257 [00:10<00:05, 200.37it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2133/3257 [00:11<00:05, 191.48it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2153/3257 [00:11<00:05, 191.94it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2177/3257 [00:11<00:05, 201.85it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2198/3257 [00:11<00:05, 201.18it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2219/3257 [00:11<00:05, 195.20it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2239/3257 [00:11<00:05, 190.34it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2261/3257 [00:11<00:05, 197.86it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2281/3257 [00:11<00:04, 195.44it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2304/3257 [00:11<00:04, 203.29it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2332/3257 [00:12<00:04, 225.13it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:12<00:03, 230.61it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2384/3257 [00:12<00:03, 230.51it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2408/3257 [00:12<00:03, 218.19it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2430/3257 [00:12<00:03, 212.02it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2452/3257 [00:12<00:04, 198.79it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2478/3257 [00:12<00:03, 212.04it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2504/3257 [00:12<00:03, 222.58it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2527/3257 [00:12<00:03, 220.92it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:13<00:03, 222.52it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2574/3257 [00:13<00:03, 204.72it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2595/3257 [00:13<00:03, 201.47it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2623/3257 [00:13<00:02, 222.39it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2646/3257 [00:13<00:02, 213.40it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2668/3257 [00:13<00:02, 206.43it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2689/3257 [00:13<00:02, 206.91it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2710/3257 [00:13<00:02, 184.49it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2733/3257 [00:13<00:02, 195.82it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:14<00:02, 203.81it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2777/3257 [00:14<00:02, 193.95it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2800/3257 [00:14<00:02, 201.99it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2821/3257 [00:14<00:02, 197.19it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2841/3257 [00:14<00:02, 188.95it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2870/3257 [00:14<00:01, 215.04it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2892/3257 [00:14<00:01, 215.01it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2914/3257 [00:15<00:02, 137.27it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2934/3257 [00:15<00:02, 149.34it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2953/3257 [00:15<00:01, 155.14it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:15<00:01, 168.06it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2996/3257 [00:15<00:01, 175.80it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3018/3257 [00:15<00:01, 185.46it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3044/3257 [00:15<00:01, 204.37it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3070/3257 [00:15<00:00, 217.65it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3093/3257 [00:15<00:00, 216.28it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3120/3257 [00:15<00:00, 230.90it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3144/3257 [00:16<00:00, 217.70it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3167/3257 [00:16<00:00, 214.84it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3189/3257 [00:16<00:00, 214.09it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3211/3257 [00:16<00:00, 213.55it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3235/3257 [00:16<00:00, 220.13it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 196.25it/s]
2023-02-07 19:53:56.474 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:53:56,476][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d340,n5,mc6,s0.707641,t4>', 'datetime': '2023-02-07T19:53:56.476131', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:53:56,476][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:53:56,476][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:53:56,904][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 19:53:56,905][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:53:56,948][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 15588 unique words (49.01% of original 31803, drops 16215)', 'datetime': '2023-02-07T19:53:56.948616', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:53:56,949][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 5053508 word corpus (99.18% of original 5095118, drops 41610)', 'datetime': '2023-02-07T19:53:56.948998', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:53:56,998][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 19:53:56,999][gensim.models.word2vec][INFO] - sample=0.707641 downsamples 0 most-common words
[2023-02-07 19:53:56,999][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5053508 word corpus (100.0%% of prior 5053508)', 'datetime': '2023-02-07T19:53:56.999834', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:53:57,081][gensim.models.word2vec][INFO] - estimated required memory for 15588 words and 340 dimensions: 55274280 bytes
[2023-02-07 19:53:57,082][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:53:57,108][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 15588 vocabulary and 340 features, using sg=1 hs=0 sample=0.7076408583708917 negative=5 window=15 shrink_windows=True', 'datetime': '2023-02-07T19:53:57.108703', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:53:58,113][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 31.65% examples, 1616217 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:53:59,116][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 62.91% examples, 1611658 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:00,080][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5045550 effective words) took 3.0s, 1699483 effective words/s
[2023-02-07 19:54:01,086][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 33.47% examples, 1703125 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:02,095][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 67.64% examples, 1729298 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:02,979][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5045550 effective words) took 2.9s, 1741295 effective words/s
[2023-02-07 19:54:03,983][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 44.89% examples, 2310018 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:04,988][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 92.14% examples, 2330721 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:05,137][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5045550 effective words) took 2.2s, 2340683 effective words/s
[2023-02-07 19:54:06,150][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 35.65% examples, 1804523 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:07,156][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 71.11% examples, 1816622 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:07,918][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5045550 effective words) took 2.8s, 1814670 effective words/s
[2023-02-07 19:54:08,922][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 45.04% examples, 2320847 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:54:09,925][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 92.14% examples, 2334731 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:10,079][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5045550 effective words) took 2.2s, 2337616 effective words/s
[2023-02-07 19:54:11,082][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 44.43% examples, 2293682 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:54:12,085][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 90.33% examples, 2286574 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:12,345][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5045550 effective words) took 2.3s, 2228020 effective words/s
[2023-02-07 19:54:13,348][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 33.83% examples, 1728919 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:14,352][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 68.50% examples, 1761206 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:15,214][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5045550 effective words) took 2.9s, 1760440 effective words/s
[2023-02-07 19:54:16,224][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 34.48% examples, 1752287 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:17,227][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 68.13% examples, 1746940 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:18,103][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5045550 effective words) took 2.9s, 1748459 effective words/s
[2023-02-07 19:54:19,110][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 33.71% examples, 1712417 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:20,116][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 68.13% examples, 1745514 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:54:20,966][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5045550 effective words) took 2.9s, 1763996 effective words/s
[2023-02-07 19:54:21,973][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 36.35% examples, 1857939 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:22,978][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 72.21% examples, 1844853 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:23,722][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5045550 effective words) took 2.8s, 1832598 effective words/s
[2023-02-07 19:54:24,732][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 33.71% examples, 1710024 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:54:25,734][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 68.28% examples, 1753253 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:26,594][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5045550 effective words) took 2.9s, 1759280 effective words/s
[2023-02-07 19:54:27,596][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 42.55% examples, 2214662 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:54:28,598][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 87.29% examples, 2220557 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:28,857][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5045550 effective words) took 2.3s, 2231178 effective words/s
[2023-02-07 19:54:29,866][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 35.40% examples, 1809017 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:30,874][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 69.88% examples, 1797288 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:31,665][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5045550 effective words) took 2.8s, 1799115 effective words/s
[2023-02-07 19:54:32,677][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 34.94% examples, 1770356 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:33,682][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 70.40% examples, 1801265 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:34,430][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5045550 effective words) took 2.8s, 1826397 effective words/s
[2023-02-07 19:54:35,433][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 45.04% examples, 2318268 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:54:36,435][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 89.93% examples, 2278757 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:54:36,707][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5045550 effective words) took 2.3s, 2217287 effective words/s
[2023-02-07 19:54:36,708][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (75683250 effective words) took 39.6s, 1911308 effective words/s', 'datetime': '2023-02-07T19:54:36.707977', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:54:36.708 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:54:39,738][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195331-ttdl7e6p/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:54:39.738659', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:54:39,739][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:54:39,816][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195331-ttdl7e6p/files/../tmp/embedding_model.pt
2023-02-07 19:54:39.817 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:54:41.768 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:54:42.420 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:54:44.609 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.147621460163272, 'test_mae': 1.0690110139848745, 'test_r2': -1.707415338292242}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.79
wandb: percentage 0.50986
wandb:   test_mae 1.06901
wandb:   test_mse 2.14762
wandb:    test_r2 -1.70742
wandb: 
wandb: üöÄ View run still-sweep-64 at: https://wandb.ai/xiaoqiz/mof2vec/runs/ttdl7e6p
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_195331-ttdl7e6p/logs
wandb: Agent Starting Run: dsv0bbq3 with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 504
wandb: 	model.gensim.alpha: 0.050313083447448725
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.6809325050618278
wandb: 	model.gensim.vector_size: 503
wandb: 	model.gensim.window: 15
wandb: 	model.sklearn.learning_rate: 0.053771820155473195
wandb: 	model.sklearn.max_depth: 79
wandb: 	model.sklearn.min_child_weight: 0.08933709770748398
wandb: 	model.sklearn.n_estimators: 1651
wandb: 	model.sklearn.num_leaves: 362
wandb: 	model.sklearn.reg_alpha: 0.5182196363533957
wandb: 	model.sklearn.reg_lambda: 0.05638034243753526
wandb: 	model.sklearn.subsample: 0.7061116029747103
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195455-dsv0bbq3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-65
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/dsv0bbq3
2023-02-07 19:55:04.534 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 19:55:04.535 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 504 for sweep.
2023-02-07 19:55:04.535 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.050313083447448725 for sweep.
2023-02-07 19:55:04.536 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:55:04.536 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 19:55:04.536 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6809325050618278 for sweep.
2023-02-07 19:55:04.536 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 503 for sweep.
2023-02-07 19:55:04.536 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 15 for sweep.
2023-02-07 19:55:04.537 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.053771820155473195 for sweep.
2023-02-07 19:55:04.537 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 79 for sweep.
2023-02-07 19:55:04.537 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.08933709770748398 for sweep.
2023-02-07 19:55:04.538 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1651 for sweep.
2023-02-07 19:55:04.538 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 362 for sweep.
2023-02-07 19:55:04.538 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.5182196363533957 for sweep.
2023-02-07 19:55:04.538 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.05638034243753526 for sweep.
2023-02-07 19:55:04.539 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.7061116029747103 for sweep.
2023-02-07 19:55:04.539 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:55:04.545 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195455-dsv0bbq3/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 504, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 503, 'window': 15, 'min_count': 2, 'dm': 0, 'sample': 0.6809325050618278, 'workers': 4, 'alpha': 0.050313083447448725, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1651, 'max_depth': 79, 'num_leaves': 362, 'reg_alpha': 0.5182196363533957, 'reg_lambda': 0.05638034243753526, 'subsample': 0.7061116029747103, 'min_child_weight': 0.08933709770748398, 'n_jobs': 4, 'learning_rate': 0.053771820155473195}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 15/3257 [00:00<00:21, 148.47it/s]  1%|          | 31/3257 [00:00<00:21, 152.89it/s]  1%|‚ñè         | 47/3257 [00:00<00:20, 155.30it/s]  2%|‚ñè         | 64/3257 [00:00<00:19, 160.11it/s]  2%|‚ñè         | 81/3257 [00:00<00:19, 163.63it/s]  3%|‚ñé         | 98/3257 [00:00<00:19, 162.04it/s]  4%|‚ñé         | 115/3257 [00:00<00:19, 158.85it/s]  4%|‚ñç         | 132/3257 [00:00<00:19, 160.88it/s]  5%|‚ñç         | 151/3257 [00:00<00:18, 168.87it/s]  5%|‚ñå         | 168/3257 [00:01<00:18, 164.64it/s]  6%|‚ñå         | 185/3257 [00:01<00:18, 165.30it/s]  6%|‚ñå         | 202/3257 [00:01<00:18, 166.00it/s]  7%|‚ñã         | 228/3257 [00:01<00:15, 192.10it/s]  8%|‚ñä         | 248/3257 [00:01<00:16, 186.74it/s]  8%|‚ñä         | 267/3257 [00:01<00:16, 176.37it/s]  9%|‚ñâ         | 291/3257 [00:01<00:15, 191.17it/s] 10%|‚ñâ         | 311/3257 [00:01<00:15, 186.43it/s] 10%|‚ñà         | 331/3257 [00:01<00:15, 186.80it/s] 11%|‚ñà         | 350/3257 [00:02<00:16, 180.69it/s] 11%|‚ñà‚ñè        | 369/3257 [00:02<00:15, 181.33it/s] 12%|‚ñà‚ñè        | 388/3257 [00:02<00:17, 167.22it/s] 13%|‚ñà‚ñé        | 408/3257 [00:02<00:16, 175.79it/s] 13%|‚ñà‚ñé        | 426/3257 [00:02<00:18, 157.01it/s] 14%|‚ñà‚ñé        | 443/3257 [00:02<00:17, 158.06it/s] 14%|‚ñà‚ñç        | 462/3257 [00:02<00:16, 166.23it/s] 15%|‚ñà‚ñç        | 479/3257 [00:02<00:16, 163.71it/s] 15%|‚ñà‚ñå        | 499/3257 [00:02<00:15, 173.64it/s] 16%|‚ñà‚ñå        | 518/3257 [00:03<00:15, 177.52it/s] 16%|‚ñà‚ñã        | 536/3257 [00:03<00:15, 172.50it/s] 17%|‚ñà‚ñã        | 554/3257 [00:03<00:15, 173.95it/s] 18%|‚ñà‚ñä        | 572/3257 [00:03<00:17, 151.05it/s] 18%|‚ñà‚ñä        | 593/3257 [00:03<00:16, 165.52it/s] 19%|‚ñà‚ñâ        | 613/3257 [00:03<00:15, 173.56it/s] 19%|‚ñà‚ñâ        | 631/3257 [00:03<00:15, 171.25it/s] 20%|‚ñà‚ñâ        | 649/3257 [00:03<00:15, 166.47it/s] 20%|‚ñà‚ñà        | 666/3257 [00:03<00:16, 158.05it/s] 21%|‚ñà‚ñà        | 683/3257 [00:04<00:16, 158.40it/s] 21%|‚ñà‚ñà‚ñè       | 699/3257 [00:04<00:16, 157.59it/s] 22%|‚ñà‚ñà‚ñè       | 719/3257 [00:04<00:14, 169.33it/s] 23%|‚ñà‚ñà‚ñé       | 737/3257 [00:04<00:15, 158.11it/s] 23%|‚ñà‚ñà‚ñé       | 754/3257 [00:04<00:15, 159.89it/s] 24%|‚ñà‚ñà‚ñé       | 773/3257 [00:04<00:15, 165.33it/s] 24%|‚ñà‚ñà‚ñç       | 790/3257 [00:04<00:14, 165.67it/s] 25%|‚ñà‚ñà‚ñç       | 807/3257 [00:04<00:14, 165.41it/s] 25%|‚ñà‚ñà‚ñå       | 824/3257 [00:04<00:14, 164.46it/s] 26%|‚ñà‚ñà‚ñå       | 841/3257 [00:05<00:15, 153.76it/s] 26%|‚ñà‚ñà‚ñã       | 859/3257 [00:05<00:14, 160.52it/s] 27%|‚ñà‚ñà‚ñã       | 876/3257 [00:05<00:14, 159.43it/s] 27%|‚ñà‚ñà‚ñã       | 894/3257 [00:05<00:14, 162.78it/s] 28%|‚ñà‚ñà‚ñä       | 911/3257 [00:05<00:21, 108.65it/s] 29%|‚ñà‚ñà‚ñä       | 930/3257 [00:05<00:18, 124.43it/s] 29%|‚ñà‚ñà‚ñâ       | 948/3257 [00:05<00:17, 135.14it/s] 30%|‚ñà‚ñà‚ñâ       | 968/3257 [00:05<00:15, 150.43it/s] 30%|‚ñà‚ñà‚ñà       | 985/3257 [00:06<00:14, 155.05it/s] 31%|‚ñà‚ñà‚ñà       | 1002/3257 [00:06<00:14, 154.83it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1020/3257 [00:06<00:13, 161.13it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1037/3257 [00:06<00:14, 155.95it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1054/3257 [00:06<00:14, 157.35it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1074/3257 [00:06<00:12, 168.88it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1092/3257 [00:06<00:13, 160.65it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1112/3257 [00:06<00:12, 171.30it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1130/3257 [00:06<00:13, 162.35it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:07<00:13, 158.90it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1169/3257 [00:07<00:11, 174.92it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1187/3257 [00:07<00:13, 158.15it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1204/3257 [00:07<00:13, 148.42it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1221/3257 [00:07<00:13, 153.42it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:07<00:12, 167.71it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1260/3257 [00:07<00:12, 165.15it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1277/3257 [00:07<00:12, 156.82it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1293/3257 [00:07<00:12, 152.47it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1309/3257 [00:08<00:12, 153.05it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:08<00:12, 159.49it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1349/3257 [00:08<00:11, 172.21it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1367/3257 [00:08<00:11, 164.73it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:08<00:11, 161.54it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1403/3257 [00:08<00:11, 168.06it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:08<00:10, 174.78it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1441/3257 [00:08<00:10, 171.59it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1465/3257 [00:08<00:09, 189.67it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1485/3257 [00:09<00:09, 186.77it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1509/3257 [00:09<00:08, 199.29it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1530/3257 [00:09<00:09, 177.85it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1549/3257 [00:09<00:09, 171.79it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1568/3257 [00:09<00:09, 174.79it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1586/3257 [00:09<00:09, 174.39it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1605/3257 [00:09<00:09, 177.49it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1623/3257 [00:09<00:09, 176.63it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1641/3257 [00:09<00:09, 172.68it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:10<00:09, 164.24it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1676/3257 [00:10<00:09, 158.32it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1694/3257 [00:10<00:09, 161.11it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1714/3257 [00:10<00:09, 169.85it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1732/3257 [00:10<00:09, 153.36it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1749/3257 [00:10<00:09, 156.25it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1767/3257 [00:10<00:09, 161.71it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1787/3257 [00:10<00:08, 169.76it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:10<00:09, 161.12it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1823/3257 [00:11<00:08, 165.02it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1840/3257 [00:11<00:08, 159.19it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1857/3257 [00:11<00:08, 159.88it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1877/3257 [00:11<00:08, 170.40it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1895/3257 [00:11<00:08, 163.42it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:11<00:08, 167.36it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1936/3257 [00:11<00:07, 175.44it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1960/3257 [00:11<00:06, 193.11it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1980/3257 [00:11<00:07, 178.07it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1999/3257 [00:12<00:07, 179.05it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2018/3257 [00:12<00:06, 180.40it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2037/3257 [00:12<00:06, 181.80it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2056/3257 [00:12<00:11, 103.42it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2072/3257 [00:12<00:10, 113.55it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2088/3257 [00:12<00:09, 122.78it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2104/3257 [00:12<00:08, 129.64it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:13<00:08, 134.15it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2136/3257 [00:13<00:07, 140.73it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2152/3257 [00:13<00:07, 144.76it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2172/3257 [00:13<00:06, 158.09it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2189/3257 [00:13<00:06, 155.86it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:13<00:06, 159.53it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2224/3257 [00:13<00:06, 160.12it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2241/3257 [00:13<00:06, 157.47it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2259/3257 [00:13<00:06, 162.59it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2276/3257 [00:14<00:06, 150.43it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2298/3257 [00:14<00:05, 168.43it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2316/3257 [00:14<00:05, 166.68it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2338/3257 [00:14<00:05, 181.49it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:14<00:04, 188.55it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2380/3257 [00:14<00:04, 189.83it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2400/3257 [00:14<00:04, 190.40it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2420/3257 [00:14<00:04, 177.82it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2439/3257 [00:14<00:04, 163.61it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2456/3257 [00:15<00:04, 160.65it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2476/3257 [00:15<00:04, 171.11it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2497/3257 [00:15<00:04, 180.98it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2519/3257 [00:15<00:03, 186.44it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2540/3257 [00:15<00:03, 186.21it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2559/3257 [00:15<00:03, 177.26it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2577/3257 [00:15<00:04, 168.00it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2594/3257 [00:15<00:04, 163.42it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2619/3257 [00:15<00:03, 184.22it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:16<00:03, 187.40it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2659/3257 [00:16<00:03, 171.88it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2678/3257 [00:16<00:03, 175.60it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2696/3257 [00:16<00:03, 170.89it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2714/3257 [00:16<00:03, 161.97it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:16<00:03, 166.52it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2753/3257 [00:16<00:02, 178.24it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2772/3257 [00:16<00:02, 169.45it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2795/3257 [00:16<00:02, 184.61it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2814/3257 [00:17<00:02, 179.10it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2833/3257 [00:17<00:02, 169.24it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2852/3257 [00:17<00:02, 173.85it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2878/3257 [00:17<00:01, 197.63it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2899/3257 [00:17<00:02, 177.49it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2918/3257 [00:17<00:01, 175.29it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2936/3257 [00:17<00:01, 174.87it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2954/3257 [00:17<00:01, 166.79it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2973/3257 [00:17<00:01, 171.92it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2991/3257 [00:18<00:01, 160.31it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:18<00:01, 171.94it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3030/3257 [00:18<00:01, 172.58it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3052/3257 [00:18<00:01, 185.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3075/3257 [00:18<00:00, 197.27it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3095/3257 [00:18<00:00, 192.33it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3118/3257 [00:18<00:00, 199.83it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3139/3257 [00:18<00:00, 191.26it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3159/3257 [00:18<00:00, 182.96it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3178/3257 [00:19<00:00, 169.19it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3200/3257 [00:19<00:00, 182.12it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3219/3257 [00:19<00:00, 172.48it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3243/3257 [00:19<00:00, 190.08it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 166.73it/s]
2023-02-07 19:55:24.888 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:55:24,889][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d503,n5,mc2,s0.680933,t4>', 'datetime': '2023-02-07T19:55:24.889714', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:55:24,891][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:55:24,891][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:55:25,460][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 19:55:25,460][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:55:25,581][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 46469 unique words (85.97% of original 54054, drops 7585)', 'datetime': '2023-02-07T19:55:25.581144', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:55:25,581][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 6543281 word corpus (99.88% of original 6550866, drops 7585)', 'datetime': '2023-02-07T19:55:25.581552', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:55:25,731][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 19:55:25,733][gensim.models.word2vec][INFO] - sample=0.680933 downsamples 0 most-common words
[2023-02-07 19:55:25,733][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6543281 word corpus (100.0%% of prior 6543281)', 'datetime': '2023-02-07T19:55:25.733252', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:55:25,982][gensim.models.word2vec][INFO] - estimated required memory for 46469 words and 503 dimensions: 217430240 bytes
[2023-02-07 19:55:25,982][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:55:26,073][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 46469 vocabulary and 503 features, using sg=1 hs=0 sample=0.6809325050618278 negative=5 window=15 shrink_windows=True', 'datetime': '2023-02-07T19:55:26.073134', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:55:27,078][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 21.37% examples, 1367278 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:55:28,085][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 44.89% examples, 1478285 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:55:29,089][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 65.00% examples, 1426436 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:55:30,092][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 85.08% examples, 1387988 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:55:30,848][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6488146 effective words) took 4.8s, 1359282 effective words/s
[2023-02-07 19:55:31,863][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 22.29% examples, 1421823 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:55:32,863][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 42.09% examples, 1395699 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:55:33,864][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 62.97% examples, 1378348 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:55:34,868][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 84.16% examples, 1372965 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:55:35,567][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6488146 effective words) took 4.7s, 1376211 effective words/s
[2023-02-07 19:55:36,586][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 22.72% examples, 1439928 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:55:37,592][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 43.11% examples, 1420635 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:55:38,597][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 64.66% examples, 1408838 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:55:39,601][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 86.46% examples, 1402272 words/s, in_qsize 8, out_qsize 1
[2023-02-07 19:55:40,188][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6488146 effective words) took 4.6s, 1404424 effective words/s
[2023-02-07 19:55:41,194][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 22.81% examples, 1470689 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:55:42,197][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 44.43% examples, 1470408 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:55:43,204][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 66.96% examples, 1473854 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:55:44,209][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 89.62% examples, 1456863 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:55:44,652][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6488146 effective words) took 4.5s, 1455362 effective words/s
[2023-02-07 19:55:45,669][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 22.69% examples, 1440465 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:55:46,685][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 46.24% examples, 1501819 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:55:47,684][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 72.89% examples, 1585459 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:55:48,662][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6488146 effective words) took 4.0s, 1618785 effective words/s
[2023-02-07 19:55:49,664][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 22.60% examples, 1453153 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:55:50,668][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 43.32% examples, 1439073 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:55:51,674][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 65.37% examples, 1435918 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:55:52,678][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 89.16% examples, 1451769 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:55:53,063][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6488146 effective words) took 4.4s, 1474986 effective words/s
[2023-02-07 19:55:54,067][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 22.87% examples, 1471032 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:55:55,073][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 45.04% examples, 1485094 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:55:56,088][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 67.55% examples, 1479016 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:55:57,097][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 91.19% examples, 1479467 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:55:57,450][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6488146 effective words) took 4.4s, 1480122 effective words/s
[2023-02-07 19:55:58,454][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 26.50% examples, 1716288 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:55:59,457][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 51.37% examples, 1698042 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:56:00,459][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 79.15% examples, 1727018 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:01,159][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6488146 effective words) took 3.7s, 1750244 effective words/s
[2023-02-07 19:56:02,172][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 23.30% examples, 1482618 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:56:03,181][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 49.06% examples, 1608006 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:56:04,186][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 78.23% examples, 1693183 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:04,926][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6488146 effective words) took 3.8s, 1723406 effective words/s
[2023-02-07 19:56:05,927][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 23.86% examples, 1535950 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:06,937][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 46.36% examples, 1517800 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:07,941][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 68.99% examples, 1517223 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:08,944][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 92.75% examples, 1507979 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:09,216][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6488146 effective words) took 4.3s, 1512877 effective words/s
[2023-02-07 19:56:10,220][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 28.40% examples, 1842021 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:11,223][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 55.66% examples, 1844128 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:12,228][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 83.97% examples, 1828613 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:12,752][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6488146 effective words) took 3.5s, 1836201 effective words/s
[2023-02-07 19:56:13,759][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 26.34% examples, 1703127 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:56:14,760][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 52.69% examples, 1738713 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:15,766][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 75.35% examples, 1649574 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:56:16,772][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 98.68% examples, 1597927 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:16,818][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6488146 effective words) took 4.1s, 1596979 effective words/s
[2023-02-07 19:56:17,826][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 28.55% examples, 1840643 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:18,831][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 55.70% examples, 1839316 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:19,832][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 84.43% examples, 1834632 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:20,361][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6488146 effective words) took 3.5s, 1831680 effective words/s
[2023-02-07 19:56:21,371][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 28.55% examples, 1838629 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:22,378][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 56.09% examples, 1849106 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:23,386][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 84.74% examples, 1834315 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:23,905][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6488146 effective words) took 3.5s, 1831662 effective words/s
[2023-02-07 19:56:24,916][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 23.12% examples, 1469770 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:25,918][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 44.58% examples, 1470397 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:26,920][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 66.84% examples, 1466973 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:56:27,921][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 89.90% examples, 1462587 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:56:28,336][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6488146 effective words) took 4.4s, 1464961 effective words/s
[2023-02-07 19:56:28,337][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (97322190 effective words) took 62.3s, 1563064 effective words/s', 'datetime': '2023-02-07T19:56:28.337290', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:56:28.337 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:56:33,582][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195455-dsv0bbq3/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:56:33.582790', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:56:33,584][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195455-dsv0bbq3/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 19:56:33,670][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195455-dsv0bbq3/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 19:56:33,754][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:56:33,800][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195455-dsv0bbq3/files/../tmp/embedding_model.pt
2023-02-07 19:56:33.800 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:56:36.200 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:56:37.018 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:56:40.167 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.232919937963808, 'test_mae': 1.0892567355942804, 'test_r2': -2.57683782460898}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.96
wandb: percentage 0.14032
wandb:   test_mae 1.08926
wandb:   test_mse 2.23292
wandb:    test_r2 -2.57684
wandb: 
wandb: üöÄ View run polar-sweep-65 at: https://wandb.ai/xiaoqiz/mof2vec/runs/dsv0bbq3
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_195455-dsv0bbq3/logs
wandb: Agent Starting Run: gjkvys06 with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 717
wandb: 	model.gensim.alpha: 0.000430251721698311
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 7
wandb: 	model.gensim.sample: 0.8975283244504111
wandb: 	model.gensim.vector_size: 285
wandb: 	model.gensim.window: 11
wandb: 	model.sklearn.learning_rate: 0.05633275017099118
wandb: 	model.sklearn.max_depth: 22
wandb: 	model.sklearn.min_child_weight: 0.09084341934498236
wandb: 	model.sklearn.n_estimators: 154
wandb: 	model.sklearn.num_leaves: 389
wandb: 	model.sklearn.reg_alpha: 0.02605868157361371
wandb: 	model.sklearn.reg_lambda: 0.10156902460996003
wandb: 	model.sklearn.subsample: 0.6640000701887592
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195650-gjkvys06
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-66
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/gjkvys06
2023-02-07 19:56:58.504 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 19:56:58.504 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 717 for sweep.
2023-02-07 19:56:58.505 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.000430251721698311 for sweep.
2023-02-07 19:56:58.505 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:56:58.506 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 7 for sweep.
2023-02-07 19:56:58.506 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.8975283244504111 for sweep.
2023-02-07 19:56:58.506 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 285 for sweep.
2023-02-07 19:56:58.506 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 11 for sweep.
2023-02-07 19:56:58.507 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.05633275017099118 for sweep.
2023-02-07 19:56:58.507 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 22 for sweep.
2023-02-07 19:56:58.507 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.09084341934498236 for sweep.
2023-02-07 19:56:58.507 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 154 for sweep.
2023-02-07 19:56:58.507 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 389 for sweep.
2023-02-07 19:56:58.508 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.02605868157361371 for sweep.
2023-02-07 19:56:58.508 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.10156902460996003 for sweep.
2023-02-07 19:56:58.508 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6640000701887592 for sweep.
2023-02-07 19:56:58.508 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:56:58.515 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195650-gjkvys06/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 717, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 285, 'window': 11, 'min_count': 7, 'dm': 0, 'sample': 0.8975283244504111, 'workers': 4, 'alpha': 0.000430251721698311, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 154, 'max_depth': 22, 'num_leaves': 389, 'reg_alpha': 0.02605868157361371, 'reg_lambda': 0.10156902460996003, 'subsample': 0.6640000701887592, 'min_child_weight': 0.09084341934498236, 'n_jobs': 4, 'learning_rate': 0.05633275017099118}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 12/3257 [00:00<00:28, 114.88it/s]  1%|          | 29/3257 [00:00<00:22, 144.38it/s]  1%|‚ñè         | 44/3257 [00:00<00:22, 144.34it/s]  2%|‚ñè         | 59/3257 [00:00<00:22, 140.80it/s]  2%|‚ñè         | 75/3257 [00:00<00:21, 146.24it/s]  3%|‚ñé         | 90/3257 [00:00<00:22, 140.73it/s]  3%|‚ñé         | 105/3257 [00:00<00:22, 139.78it/s]  4%|‚ñé         | 120/3257 [00:00<00:22, 141.40it/s]  4%|‚ñç         | 135/3257 [00:01<00:31, 99.22it/s]   5%|‚ñç         | 155/3257 [00:01<00:25, 121.85it/s]  5%|‚ñå         | 172/3257 [00:01<00:23, 132.40it/s]  6%|‚ñå         | 192/3257 [00:01<00:20, 149.05it/s]  6%|‚ñã         | 211/3257 [00:01<00:19, 159.65it/s]  7%|‚ñã         | 236/3257 [00:01<00:16, 181.51it/s]  8%|‚ñä         | 256/3257 [00:01<00:16, 184.89it/s]  9%|‚ñä         | 278/3257 [00:01<00:15, 192.93it/s]  9%|‚ñâ         | 298/3257 [00:01<00:15, 194.17it/s] 10%|‚ñâ         | 318/3257 [00:02<00:15, 192.66it/s] 10%|‚ñà         | 339/3257 [00:02<00:15, 193.82it/s] 11%|‚ñà         | 360/3257 [00:02<00:14, 196.93it/s] 12%|‚ñà‚ñè        | 380/3257 [00:02<00:15, 184.47it/s] 12%|‚ñà‚ñè        | 399/3257 [00:02<00:15, 182.59it/s] 13%|‚ñà‚ñé        | 420/3257 [00:02<00:15, 185.32it/s] 13%|‚ñà‚ñé        | 439/3257 [00:02<00:17, 162.55it/s] 14%|‚ñà‚ñç        | 459/3257 [00:02<00:16, 170.55it/s] 15%|‚ñà‚ñç        | 477/3257 [00:02<00:16, 171.36it/s] 15%|‚ñà‚ñå        | 497/3257 [00:03<00:15, 179.18it/s] 16%|‚ñà‚ñå        | 517/3257 [00:03<00:15, 182.06it/s] 16%|‚ñà‚ñã        | 536/3257 [00:03<00:15, 178.87it/s] 17%|‚ñà‚ñã        | 555/3257 [00:03<00:15, 179.63it/s] 18%|‚ñà‚ñä        | 574/3257 [00:03<00:17, 152.97it/s] 18%|‚ñà‚ñä        | 596/3257 [00:03<00:15, 168.59it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:03<00:15, 173.07it/s] 19%|‚ñà‚ñâ        | 634/3257 [00:03<00:14, 176.27it/s] 20%|‚ñà‚ñà        | 653/3257 [00:03<00:15, 167.48it/s] 21%|‚ñà‚ñà        | 671/3257 [00:04<00:15, 163.64it/s] 21%|‚ñà‚ñà        | 688/3257 [00:04<00:16, 156.23it/s] 22%|‚ñà‚ñà‚ñè       | 706/3257 [00:04<00:15, 161.96it/s] 22%|‚ñà‚ñà‚ñè       | 723/3257 [00:04<00:15, 160.61it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:04<00:15, 159.64it/s] 23%|‚ñà‚ñà‚ñé       | 761/3257 [00:04<00:14, 172.01it/s] 24%|‚ñà‚ñà‚ñç       | 779/3257 [00:04<00:14, 169.05it/s] 25%|‚ñà‚ñà‚ñç       | 798/3257 [00:04<00:14, 174.60it/s] 25%|‚ñà‚ñà‚ñå       | 816/3257 [00:04<00:14, 173.21it/s] 26%|‚ñà‚ñà‚ñå       | 834/3257 [00:05<00:14, 167.21it/s] 26%|‚ñà‚ñà‚ñå       | 851/3257 [00:05<00:15, 157.86it/s] 27%|‚ñà‚ñà‚ñã       | 868/3257 [00:05<00:14, 160.41it/s] 27%|‚ñà‚ñà‚ñã       | 885/3257 [00:05<00:14, 158.94it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:05<00:13, 172.40it/s] 28%|‚ñà‚ñà‚ñä       | 926/3257 [00:05<00:13, 179.03it/s] 29%|‚ñà‚ñà‚ñâ       | 945/3257 [00:05<00:13, 173.55it/s] 30%|‚ñà‚ñà‚ñâ       | 964/3257 [00:05<00:12, 177.47it/s] 30%|‚ñà‚ñà‚ñà       | 982/3257 [00:05<00:13, 171.99it/s] 31%|‚ñà‚ñà‚ñà       | 1000/3257 [00:06<00:13, 171.73it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1018/3257 [00:06<00:12, 173.39it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1036/3257 [00:06<00:13, 164.59it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1053/3257 [00:06<00:13, 161.68it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1073/3257 [00:06<00:12, 171.71it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:06<00:12, 166.89it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1112/3257 [00:06<00:12, 175.08it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1130/3257 [00:06<00:12, 168.51it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:06<00:12, 167.04it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:07<00:11, 179.85it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1189/3257 [00:07<00:13, 158.34it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1206/3257 [00:07<00:12, 160.46it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1225/3257 [00:07<00:12, 168.19it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1244/3257 [00:07<00:11, 171.59it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1262/3257 [00:07<00:11, 172.80it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1280/3257 [00:07<00:12, 164.66it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1297/3257 [00:07<00:12, 162.31it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1318/3257 [00:07<00:11, 173.42it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:08<00:10, 179.15it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1358/3257 [00:08<00:11, 170.89it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1376/3257 [00:08<00:11, 164.43it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1393/3257 [00:08<00:16, 110.59it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1417/3257 [00:08<00:13, 137.00it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1435/3257 [00:08<00:12, 146.23it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1457/3257 [00:08<00:10, 164.09it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1477/3257 [00:08<00:10, 170.02it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1499/3257 [00:09<00:09, 182.44it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:09<00:09, 175.76it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1538/3257 [00:09<00:10, 169.78it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1556/3257 [00:09<00:10, 166.36it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1575/3257 [00:09<00:10, 165.50it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1595/3257 [00:09<00:09, 174.74it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:09<00:09, 181.10it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:09<00:09, 170.22it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1652/3257 [00:09<00:09, 165.85it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1669/3257 [00:10<00:09, 164.58it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1686/3257 [00:10<00:09, 164.17it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1706/3257 [00:10<00:09, 172.14it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1724/3257 [00:10<00:08, 174.19it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1742/3257 [00:10<00:09, 154.68it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:10<00:08, 169.30it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1784/3257 [00:10<00:08, 179.05it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1803/3257 [00:10<00:08, 177.97it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1822/3257 [00:10<00:08, 179.14it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1841/3257 [00:11<00:08, 174.98it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1859/3257 [00:11<00:08, 172.44it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1880/3257 [00:11<00:07, 180.32it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1899/3257 [00:11<00:07, 176.99it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1918/3257 [00:11<00:07, 175.02it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1941/3257 [00:11<00:06, 189.48it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:11<00:06, 197.26it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1985/3257 [00:11<00:06, 183.76it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:11<00:06, 184.57it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2024/3257 [00:12<00:06, 186.25it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2043/3257 [00:12<00:06, 177.46it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2061/3257 [00:12<00:07, 162.82it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2082/3257 [00:12<00:06, 175.19it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2100/3257 [00:12<00:06, 166.12it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:12<00:06, 169.95it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2138/3257 [00:12<00:06, 169.13it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2156/3257 [00:12<00:06, 168.85it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:12<00:06, 176.76it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2198/3257 [00:13<00:05, 179.67it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2217/3257 [00:13<00:06, 171.67it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:13<00:05, 176.25it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2254/3257 [00:13<00:05, 174.57it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2272/3257 [00:13<00:05, 164.59it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2294/3257 [00:13<00:05, 178.59it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2315/3257 [00:13<00:05, 182.64it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2343/3257 [00:13<00:04, 208.49it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2367/3257 [00:13<00:04, 214.53it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2393/3257 [00:14<00:03, 223.14it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2416/3257 [00:14<00:04, 209.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2438/3257 [00:14<00:04, 200.44it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2460/3257 [00:14<00:03, 203.28it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2483/3257 [00:14<00:03, 208.06it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2509/3257 [00:14<00:03, 220.23it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2533/3257 [00:14<00:03, 224.95it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2556/3257 [00:14<00:03, 210.55it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2578/3257 [00:14<00:03, 200.14it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2599/3257 [00:15<00:03, 196.79it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:15<00:02, 218.81it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2651/3257 [00:15<00:04, 138.39it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2672/3257 [00:15<00:03, 152.15it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2693/3257 [00:15<00:03, 164.25it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2713/3257 [00:15<00:03, 154.25it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2734/3257 [00:15<00:03, 166.81it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:16<00:02, 178.82it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2776/3257 [00:16<00:02, 176.19it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2801/3257 [00:16<00:02, 195.34it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2822/3257 [00:16<00:02, 189.46it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2842/3257 [00:16<00:02, 180.07it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2870/3257 [00:16<00:01, 206.52it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2892/3257 [00:16<00:01, 203.61it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2913/3257 [00:16<00:01, 194.70it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2933/3257 [00:16<00:01, 193.07it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2953/3257 [00:17<00:01, 184.46it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2975/3257 [00:17<00:01, 192.82it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2995/3257 [00:17<00:01, 182.09it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3016/3257 [00:17<00:01, 187.85it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3037/3257 [00:17<00:01, 191.12it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3059/3257 [00:17<00:00, 198.27it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3080/3257 [00:17<00:00, 201.16it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3101/3257 [00:17<00:00, 202.61it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3125/3257 [00:17<00:00, 213.38it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3147/3257 [00:18<00:00, 195.21it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3167/3257 [00:18<00:00, 185.83it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3186/3257 [00:18<00:00, 180.67it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:18<00:00, 186.10it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3226/3257 [00:18<00:00, 184.94it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:18<00:00, 196.42it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 174.58it/s]
2023-02-07 19:57:18.041 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:57:18,042][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d285,n5,mc7,s0.897528,t4>', 'datetime': '2023-02-07T19:57:18.042714', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:57:18,043][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:57:18,043][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:57:18,568][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 19:57:18,569][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:57:18,620][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 retains 18716 unique words (43.83% of original 42701, drops 23985)', 'datetime': '2023-02-07T19:57:18.620187', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:57:18,621][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 leaves 5754306 word corpus (98.82% of original 5822992, drops 68686)', 'datetime': '2023-02-07T19:57:18.621744', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:57:18,685][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 19:57:18,686][gensim.models.word2vec][INFO] - sample=0.897528 downsamples 0 most-common words
[2023-02-07 19:57:18,686][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5754306 word corpus (100.0%% of prior 5754306)', 'datetime': '2023-02-07T19:57:18.686929', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:57:18,789][gensim.models.word2vec][INFO] - estimated required memory for 18716 words and 285 dimensions: 56394860 bytes
[2023-02-07 19:57:18,790][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:57:18,813][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 18716 vocabulary and 285 features, using sg=1 hs=0 sample=0.8975283244504111 negative=5 window=11 shrink_windows=True', 'datetime': '2023-02-07T19:57:18.813074', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:57:19,816][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 26.77% examples, 1540066 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:20,817][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 60.58% examples, 1758781 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:21,820][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 97.67% examples, 1866249 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:21,873][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5728381 effective words) took 3.1s, 1872930 effective words/s
[2023-02-07 19:57:22,882][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 26.99% examples, 1541791 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:23,886][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 53.33% examples, 1558699 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:24,886][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 82.16% examples, 1575970 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:25,510][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5728381 effective words) took 3.6s, 1575885 effective words/s
[2023-02-07 19:57:26,525][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 26.44% examples, 1503673 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:27,528][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 58.83% examples, 1709140 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:28,532][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 96.07% examples, 1823592 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:28,636][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5728381 effective words) took 3.1s, 1833470 effective words/s
[2023-02-07 19:57:29,638][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 26.59% examples, 1530984 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:30,643][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 52.59% examples, 1535150 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:31,644][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 79.71% examples, 1537029 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:57:32,367][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5728381 effective words) took 3.7s, 1535895 effective words/s
[2023-02-07 19:57:33,370][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 34.33% examples, 1988885 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:34,372][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 68.65% examples, 2004366 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:35,206][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5728381 effective words) took 2.8s, 2019214 effective words/s
[2023-02-07 19:57:36,216][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 34.85% examples, 2011521 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:57:37,217][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 69.14% examples, 2015772 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:57:38,054][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5728381 effective words) took 2.8s, 2013927 effective words/s
[2023-02-07 19:57:39,057][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 27.11% examples, 1560443 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:40,058][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 53.21% examples, 1561227 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:41,065][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 80.81% examples, 1552706 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:41,741][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5728381 effective words) took 3.7s, 1554308 effective words/s
[2023-02-07 19:57:42,746][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 26.99% examples, 1555937 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:43,748][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 53.09% examples, 1551627 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:57:44,761][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 80.81% examples, 1547032 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:45,444][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5728381 effective words) took 3.7s, 1548488 effective words/s
[2023-02-07 19:57:46,448][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 34.45% examples, 1997666 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:57:47,451][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 68.84% examples, 2013719 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:48,280][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5728381 effective words) took 2.8s, 2022578 effective words/s
[2023-02-07 19:57:49,284][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 26.87% examples, 1547576 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:50,284][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 52.29% examples, 1529847 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:57:51,291][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 79.28% examples, 1527363 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:52,013][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5728381 effective words) took 3.7s, 1535893 effective words/s
[2023-02-07 19:57:53,017][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 25.73% examples, 1475142 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:54,023][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 51.21% examples, 1492739 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:57:55,025][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 81.82% examples, 1570629 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:55,539][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5728381 effective words) took 3.5s, 1625251 effective words/s
[2023-02-07 19:57:56,542][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 25.79% examples, 1478794 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:57,542][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 51.18% examples, 1497905 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:57:58,547][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 78.69% examples, 1515243 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:57:59,305][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5728381 effective words) took 3.8s, 1522080 effective words/s
[2023-02-07 19:58:00,310][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 26.34% examples, 1511207 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:58:01,315][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 53.02% examples, 1545692 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:58:02,317][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 81.42% examples, 1561391 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:58:02,965][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5728381 effective words) took 3.7s, 1566750 effective words/s
[2023-02-07 19:58:03,976][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 27.82% examples, 1584650 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:58:04,988][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 62.85% examples, 1811028 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:58:05,994][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 97.21% examples, 1842418 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:58:06,084][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5728381 effective words) took 3.1s, 1837729 effective words/s
[2023-02-07 19:58:07,088][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 26.68% examples, 1529003 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:58:08,092][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 53.02% examples, 1546912 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:58:09,101][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 80.81% examples, 1547898 words/s, in_qsize 6, out_qsize 0
[2023-02-07 19:58:09,770][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5728381 effective words) took 3.7s, 1555280 effective words/s
[2023-02-07 19:58:09,771][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (85925715 effective words) took 51.0s, 1686222 effective words/s', 'datetime': '2023-02-07T19:58:09.771023', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 19:58:09.771 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 19:58:13,174][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195650-gjkvys06/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T19:58:13.174220', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 19:58:13,175][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 19:58:13,239][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195650-gjkvys06/files/../tmp/embedding_model.pt
2023-02-07 19:58:13.239 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 19:58:14.891 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 19:58:15.460 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 19:58:21.533 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.573745889869871, 'test_mae': 1.1780791114630074, 'test_r2': -2.0849972034824455}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.17
wandb: percentage 0.5617
wandb:   test_mae 1.17808
wandb:   test_mse 2.57375
wandb:    test_r2 -2.085
wandb: 
wandb: üöÄ View run cool-sweep-66 at: https://wandb.ai/xiaoqiz/mof2vec/runs/gjkvys06
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_195650-gjkvys06/logs
wandb: Agent Starting Run: g4lwh93k with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 722
wandb: 	model.gensim.alpha: 0.002149765230932854
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.8615868229567296
wandb: 	model.gensim.vector_size: 351
wandb: 	model.gensim.window: 16
wandb: 	model.sklearn.learning_rate: 0.4923205686739597
wandb: 	model.sklearn.max_depth: 21
wandb: 	model.sklearn.min_child_weight: 0.02464094363512701
wandb: 	model.sklearn.n_estimators: 1888
wandb: 	model.sklearn.num_leaves: 303
wandb: 	model.sklearn.reg_alpha: 0.14656193260904968
wandb: 	model.sklearn.reg_lambda: 0.028574574226138526
wandb: 	model.sklearn.subsample: 0.7654885981235102
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195835-g4lwh93k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-67
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/g4lwh93k
2023-02-07 19:58:43.368 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 19:58:43.369 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 722 for sweep.
2023-02-07 19:58:43.369 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.002149765230932854 for sweep.
2023-02-07 19:58:43.369 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 19:58:43.370 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 19:58:43.370 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.8615868229567296 for sweep.
2023-02-07 19:58:43.370 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 351 for sweep.
2023-02-07 19:58:43.370 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 16 for sweep.
2023-02-07 19:58:43.370 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.4923205686739597 for sweep.
2023-02-07 19:58:43.371 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 21 for sweep.
2023-02-07 19:58:43.371 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.02464094363512701 for sweep.
2023-02-07 19:58:43.371 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1888 for sweep.
2023-02-07 19:58:43.371 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 303 for sweep.
2023-02-07 19:58:43.372 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.14656193260904968 for sweep.
2023-02-07 19:58:43.372 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.028574574226138526 for sweep.
2023-02-07 19:58:43.372 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.7654885981235102 for sweep.
2023-02-07 19:58:43.372 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 19:58:43.386 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195835-g4lwh93k/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 722, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 351, 'window': 16, 'min_count': 2, 'dm': 0, 'sample': 0.8615868229567296, 'workers': 4, 'alpha': 0.002149765230932854, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1888, 'max_depth': 21, 'num_leaves': 303, 'reg_alpha': 0.14656193260904968, 'reg_lambda': 0.028574574226138526, 'subsample': 0.7654885981235102, 'min_child_weight': 0.02464094363512701, 'n_jobs': 4, 'learning_rate': 0.4923205686739597}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:18, 174.59it/s]  1%|          | 39/3257 [00:00<00:16, 193.86it/s]  2%|‚ñè         | 59/3257 [00:00<00:17, 183.19it/s]  3%|‚ñé         | 82/3257 [00:00<00:16, 197.69it/s]  3%|‚ñé         | 102/3257 [00:00<00:16, 196.38it/s]  4%|‚ñé         | 122/3257 [00:00<00:17, 181.03it/s]  4%|‚ñç         | 145/3257 [00:00<00:16, 192.70it/s]  5%|‚ñå         | 165/3257 [00:00<00:17, 180.68it/s]  6%|‚ñå         | 185/3257 [00:00<00:16, 185.89it/s]  6%|‚ñã         | 205/3257 [00:01<00:16, 189.65it/s]  7%|‚ñã         | 230/3257 [00:01<00:14, 207.15it/s]  8%|‚ñä         | 251/3257 [00:01<00:14, 204.40it/s]  8%|‚ñä         | 272/3257 [00:01<00:14, 203.12it/s]  9%|‚ñâ         | 297/3257 [00:01<00:13, 216.48it/s] 10%|‚ñâ         | 319/3257 [00:01<00:13, 212.14it/s] 10%|‚ñà         | 341/3257 [00:01<00:14, 206.30it/s] 11%|‚ñà         | 363/3257 [00:01<00:13, 210.15it/s] 12%|‚ñà‚ñè        | 385/3257 [00:01<00:14, 200.14it/s] 12%|‚ñà‚ñè        | 406/3257 [00:02<00:14, 199.22it/s] 13%|‚ñà‚ñé        | 427/3257 [00:02<00:15, 180.29it/s] 14%|‚ñà‚ñé        | 447/3257 [00:02<00:15, 181.07it/s] 14%|‚ñà‚ñç        | 471/3257 [00:02<00:14, 194.79it/s] 15%|‚ñà‚ñå        | 491/3257 [00:02<00:14, 192.47it/s] 16%|‚ñà‚ñå        | 513/3257 [00:02<00:13, 198.45it/s] 16%|‚ñà‚ñã        | 534/3257 [00:02<00:14, 193.69it/s] 17%|‚ñà‚ñã        | 555/3257 [00:02<00:13, 195.91it/s] 18%|‚ñà‚ñä        | 575/3257 [00:02<00:15, 168.58it/s] 18%|‚ñà‚ñä        | 597/3257 [00:03<00:14, 180.82it/s] 19%|‚ñà‚ñâ        | 619/3257 [00:03<00:13, 189.29it/s] 20%|‚ñà‚ñâ        | 639/3257 [00:03<00:14, 180.83it/s] 20%|‚ñà‚ñà        | 658/3257 [00:03<00:15, 166.42it/s] 21%|‚ñà‚ñà        | 679/3257 [00:03<00:14, 177.25it/s] 21%|‚ñà‚ñà‚ñè       | 698/3257 [00:03<00:15, 169.75it/s] 22%|‚ñà‚ñà‚ñè       | 719/3257 [00:03<00:14, 179.99it/s] 23%|‚ñà‚ñà‚ñé       | 738/3257 [00:03<00:15, 164.74it/s] 23%|‚ñà‚ñà‚ñé       | 758/3257 [00:04<00:14, 173.43it/s] 24%|‚ñà‚ñà‚ñç       | 776/3257 [00:04<00:14, 170.67it/s] 24%|‚ñà‚ñà‚ñç       | 795/3257 [00:04<00:14, 174.93it/s] 25%|‚ñà‚ñà‚ñç       | 813/3257 [00:04<00:20, 119.44it/s] 25%|‚ñà‚ñà‚ñå       | 828/3257 [00:04<00:19, 123.95it/s] 26%|‚ñà‚ñà‚ñå       | 843/3257 [00:04<00:19, 126.72it/s] 26%|‚ñà‚ñà‚ñã       | 861/3257 [00:04<00:17, 138.85it/s] 27%|‚ñà‚ñà‚ñã       | 877/3257 [00:04<00:16, 141.40it/s] 28%|‚ñà‚ñà‚ñä       | 896/3257 [00:05<00:15, 154.19it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:05<00:14, 159.87it/s] 29%|‚ñà‚ñà‚ñä       | 933/3257 [00:05<00:13, 167.55it/s] 29%|‚ñà‚ñà‚ñâ       | 955/3257 [00:05<00:12, 182.21it/s] 30%|‚ñà‚ñà‚ñâ       | 974/3257 [00:05<00:12, 181.82it/s] 30%|‚ñà‚ñà‚ñà       | 993/3257 [00:05<00:12, 174.73it/s] 31%|‚ñà‚ñà‚ñà       | 1011/3257 [00:05<00:13, 169.20it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1029/3257 [00:05<00:13, 170.47it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:05<00:13, 168.09it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:06<00:12, 174.70it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1086/3257 [00:06<00:12, 176.14it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1104/3257 [00:06<00:12, 175.39it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1122/3257 [00:06<00:12, 172.87it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1140/3257 [00:06<00:12, 165.71it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1159/3257 [00:06<00:12, 171.70it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1177/3257 [00:06<00:12, 167.55it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1194/3257 [00:06<00:13, 153.56it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1210/3257 [00:06<00:13, 151.44it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1230/3257 [00:07<00:12, 163.95it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1247/3257 [00:07<00:12, 161.52it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1265/3257 [00:07<00:12, 165.54it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1282/3257 [00:07<00:12, 153.66it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1299/3257 [00:07<00:12, 157.33it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1318/3257 [00:07<00:11, 165.33it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1338/3257 [00:07<00:10, 174.84it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1356/3257 [00:07<00:11, 163.98it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1373/3257 [00:07<00:11, 160.61it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1390/3257 [00:08<00:11, 155.85it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1412/3257 [00:08<00:10, 171.15it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:08<00:10, 177.41it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1452/3257 [00:08<00:09, 181.55it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1473/3257 [00:08<00:09, 187.88it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1493/3257 [00:08<00:09, 189.31it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:08<00:09, 188.62it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1532/3257 [00:08<00:09, 177.29it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1550/3257 [00:08<00:10, 167.26it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1567/3257 [00:08<00:10, 165.12it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1585/3257 [00:09<00:10, 166.01it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1605/3257 [00:09<00:09, 173.84it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1624/3257 [00:09<00:09, 177.79it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1642/3257 [00:09<00:09, 171.95it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1660/3257 [00:09<00:09, 172.29it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1678/3257 [00:09<00:09, 163.19it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1696/3257 [00:09<00:09, 167.83it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:09<00:08, 172.08it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1733/3257 [00:09<00:09, 158.25it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1752/3257 [00:10<00:09, 164.30it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1773/3257 [00:10<00:08, 176.13it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1792/3257 [00:10<00:08, 179.97it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1811/3257 [00:10<00:08, 169.97it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1829/3257 [00:10<00:08, 168.18it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1849/3257 [00:10<00:08, 175.48it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1870/3257 [00:10<00:07, 183.88it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1889/3257 [00:10<00:07, 183.21it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1909/3257 [00:10<00:07, 186.62it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1928/3257 [00:11<00:07, 177.48it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1954/3257 [00:11<00:06, 198.52it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1975/3257 [00:11<00:06, 199.48it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1996/3257 [00:11<00:06, 195.77it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2016/3257 [00:11<00:06, 188.92it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2036/3257 [00:11<00:06, 191.19it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2056/3257 [00:11<00:06, 173.85it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2075/3257 [00:11<00:06, 176.26it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2093/3257 [00:11<00:06, 175.33it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2111/3257 [00:12<00:06, 171.58it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2129/3257 [00:12<00:07, 160.15it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2146/3257 [00:12<00:06, 161.33it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2166/3257 [00:12<00:06, 171.43it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2184/3257 [00:12<00:09, 112.80it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2205/3257 [00:12<00:07, 132.54it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2224/3257 [00:12<00:07, 142.27it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2241/3257 [00:12<00:06, 147.64it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2260/3257 [00:13<00:06, 157.73it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2278/3257 [00:13<00:06, 161.73it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2300/3257 [00:13<00:05, 175.71it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2326/3257 [00:13<00:04, 198.10it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2352/3257 [00:13<00:04, 214.69it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2375/3257 [00:13<00:04, 209.65it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2399/3257 [00:13<00:03, 217.71it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2422/3257 [00:13<00:04, 208.08it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2444/3257 [00:13<00:04, 196.37it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2465/3257 [00:14<00:03, 198.83it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2486/3257 [00:14<00:03, 201.54it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2511/3257 [00:14<00:03, 214.76it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2535/3257 [00:14<00:03, 220.26it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2558/3257 [00:14<00:03, 205.67it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2579/3257 [00:14<00:03, 192.72it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2600/3257 [00:14<00:03, 197.31it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:14<00:02, 218.38it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2651/3257 [00:14<00:02, 206.84it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2673/3257 [00:15<00:02, 206.90it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2694/3257 [00:15<00:02, 198.78it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2715/3257 [00:15<00:03, 176.22it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2735/3257 [00:15<00:02, 181.86it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2757/3257 [00:15<00:02, 190.27it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2777/3257 [00:15<00:02, 185.71it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2803/3257 [00:15<00:02, 203.12it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2824/3257 [00:15<00:02, 185.44it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2843/3257 [00:16<00:02, 184.90it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2872/3257 [00:16<00:01, 208.04it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2894/3257 [00:16<00:01, 198.13it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2916/3257 [00:16<00:01, 201.12it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2937/3257 [00:16<00:01, 201.69it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2958/3257 [00:16<00:01, 187.93it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2978/3257 [00:16<00:01, 189.52it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3002/3257 [00:16<00:01, 200.92it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3023/3257 [00:16<00:01, 196.33it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3048/3257 [00:16<00:01, 208.55it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3074/3257 [00:17<00:00, 222.84it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3097/3257 [00:17<00:00, 212.31it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3121/3257 [00:17<00:00, 219.33it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3144/3257 [00:17<00:00, 202.56it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3165/3257 [00:17<00:00, 199.63it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3186/3257 [00:17<00:00, 197.94it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3208/3257 [00:17<00:00, 201.78it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3229/3257 [00:17<00:00, 198.60it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:17<00:00, 204.44it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 180.81it/s]
2023-02-07 19:59:02.102 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 19:59:02,103][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d351,n5,mc2,s0.861587,t4>', 'datetime': '2023-02-07T19:59:02.103094', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 19:59:02,103][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 19:59:02,103][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 19:59:02,617][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 19:59:02,617][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 19:59:02,710][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 36619 unique words (85.76% of original 42701, drops 6082)', 'datetime': '2023-02-07T19:59:02.710196', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:59:02,710][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 5816910 word corpus (99.90% of original 5822992, drops 6082)', 'datetime': '2023-02-07T19:59:02.710594', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:59:02,829][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 19:59:02,830][gensim.models.word2vec][INFO] - sample=0.861587 downsamples 0 most-common words
[2023-02-07 19:59:02,830][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5816910 word corpus (100.0%% of prior 5816910)', 'datetime': '2023-02-07T19:59:02.830873', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 19:59:03,033][gensim.models.word2vec][INFO] - estimated required memory for 36619 words and 351 dimensions: 126359880 bytes
[2023-02-07 19:59:03,034][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 19:59:03,081][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 36619 vocabulary and 351 features, using sg=1 hs=0 sample=0.8615868229567296 negative=5 window=16 shrink_windows=True', 'datetime': '2023-02-07T19:59:03.081711', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 19:59:04,088][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 29.57% examples, 1706810 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:05,096][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 54.53% examples, 1605989 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:06,106][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 78.17% examples, 1513663 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:06,960][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5790320 effective words) took 3.9s, 1493708 effective words/s
[2023-02-07 19:59:07,969][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 25.24% examples, 1446873 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:08,969][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 49.68% examples, 1462258 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:09,978][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 72.77% examples, 1419514 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:10,990][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 97.67% examples, 1408226 words/s, in_qsize 4, out_qsize 0
[2023-02-07 19:59:11,062][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5790320 effective words) took 4.1s, 1413126 effective words/s
[2023-02-07 19:59:12,067][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 25.15% examples, 1443097 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:13,068][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 49.37% examples, 1454240 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:14,073][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 74.03% examples, 1445727 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:15,073][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 98.62% examples, 1426793 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:15,121][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5790320 effective words) took 4.1s, 1427637 effective words/s
[2023-02-07 19:59:16,128][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 24.50% examples, 1404449 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:17,134][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 48.33% examples, 1421245 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:18,137][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 74.18% examples, 1445275 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:19,111][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5790320 effective words) took 4.0s, 1452056 effective words/s
[2023-02-07 19:59:20,116][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 24.13% examples, 1379941 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:21,117][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 47.71% examples, 1409029 words/s, in_qsize 6, out_qsize 0
[2023-02-07 19:59:22,124][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 72.43% examples, 1413056 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:23,125][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 97.54% examples, 1410850 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:23,220][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5790320 effective words) took 4.1s, 1409600 effective words/s
[2023-02-07 19:59:24,232][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 24.26% examples, 1380874 words/s, in_qsize 8, out_qsize 2
[2023-02-07 19:59:25,233][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 47.37% examples, 1391306 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:59:26,243][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 71.26% examples, 1395344 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:27,254][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 97.67% examples, 1405970 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:27,325][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5790320 effective words) took 4.1s, 1411258 effective words/s
[2023-02-07 19:59:28,333][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 26.01% examples, 1502594 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:29,335][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 49.09% examples, 1445465 words/s, in_qsize 7, out_qsize 1
[2023-02-07 19:59:30,339][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 73.84% examples, 1440430 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:31,339][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 99.11% examples, 1432252 words/s, in_qsize 6, out_qsize 0
[2023-02-07 19:59:31,361][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5790320 effective words) took 4.0s, 1435247 effective words/s
[2023-02-07 19:59:32,373][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 30.92% examples, 1790157 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:33,382][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 61.71% examples, 1798331 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:34,389][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 93.74% examples, 1802588 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:34,566][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5790320 effective words) took 3.2s, 1808462 effective words/s
[2023-02-07 19:59:35,568][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 32.36% examples, 1896574 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:36,572][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 58.03% examples, 1714347 words/s, in_qsize 8, out_qsize 2
[2023-02-07 19:59:37,581][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 84.03% examples, 1630501 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:59:38,166][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5790320 effective words) took 3.6s, 1609259 effective words/s
[2023-02-07 19:59:39,170][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 24.53% examples, 1415149 words/s, in_qsize 6, out_qsize 1
[2023-02-07 19:59:40,220][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 47.93% examples, 1382931 words/s, in_qsize 7, out_qsize 11
[2023-02-07 19:59:41,223][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 74.64% examples, 1436863 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:59:42,198][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5790320 effective words) took 4.0s, 1436828 effective words/s
[2023-02-07 19:59:43,212][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 25.58% examples, 1462482 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:59:44,213][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 51.06% examples, 1500048 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:45,214][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 76.70% examples, 1491395 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:46,110][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5790320 effective words) took 3.9s, 1480690 effective words/s
[2023-02-07 19:59:47,123][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 25.30% examples, 1448157 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:59:48,125][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 49.77% examples, 1459448 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:49,130][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 74.64% examples, 1455827 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:50,089][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5790320 effective words) took 4.0s, 1456804 effective words/s
[2023-02-07 19:59:51,093][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 25.15% examples, 1444085 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:52,100][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 49.55% examples, 1455144 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:53,104][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 75.01% examples, 1467273 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:59:54,038][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5790320 effective words) took 3.9s, 1467315 effective words/s
[2023-02-07 19:59:55,043][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 30.55% examples, 1771551 words/s, in_qsize 0, out_qsize 0
[2023-02-07 19:59:56,053][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 53.33% examples, 1572943 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:57,055][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 79.28% examples, 1539738 words/s, in_qsize 7, out_qsize 0
[2023-02-07 19:59:57,848][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5790320 effective words) took 3.8s, 1520512 effective words/s
[2023-02-07 19:59:58,859][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 24.81% examples, 1420808 words/s, in_qsize 8, out_qsize 0
[2023-02-07 19:59:59,869][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 48.97% examples, 1436095 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:00:00,871][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 73.84% examples, 1437949 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:00:01,859][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5790320 effective words) took 4.0s, 1445625 effective words/s
[2023-02-07 20:00:01,860][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86854800 effective words) took 58.8s, 1477668 effective words/s', 'datetime': '2023-02-07T20:00:01.860507', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:00:01.860 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:00:06,460][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195835-g4lwh93k/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:00:06.460484', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:00:06,461][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195835-g4lwh93k/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 20:00:06,528][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195835-g4lwh93k/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 20:00:06,582][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:00:06,622][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_195835-g4lwh93k/files/../tmp/embedding_model.pt
2023-02-07 20:00:06.623 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:00:08.592 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:00:09.257 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:00:11.488 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.3986442327238633, 'test_mae': 1.1453311075327883, 'test_r2': -1.6137779667891468}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.49
wandb: percentage 0.14243
wandb:   test_mae 1.14533
wandb:   test_mse 2.39864
wandb:    test_r2 -1.61378
wandb: 
wandb: üöÄ View run glowing-sweep-67 at: https://wandb.ai/xiaoqiz/mof2vec/runs/g4lwh93k
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_195835-g4lwh93k/logs
wandb: Agent Starting Run: 579n3t9q with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 630
wandb: 	model.gensim.alpha: 0.01246447868558996
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.26171173234504
wandb: 	model.gensim.vector_size: 171
wandb: 	model.gensim.window: 11
wandb: 	model.sklearn.learning_rate: 0.052499441016260966
wandb: 	model.sklearn.max_depth: 40
wandb: 	model.sklearn.min_child_weight: 0.04001820262545791
wandb: 	model.sklearn.n_estimators: 2274
wandb: 	model.sklearn.num_leaves: 312
wandb: 	model.sklearn.reg_alpha: 0.0042040243952715176
wandb: 	model.sklearn.reg_lambda: 0.020431118330971124
wandb: 	model.sklearn.subsample: 0.480500965710002
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200023-579n3t9q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sweep-68
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/579n3t9q
2023-02-07 20:00:31.678 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 20:00:31.678 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 630 for sweep.
2023-02-07 20:00:31.678 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.01246447868558996 for sweep.
2023-02-07 20:00:31.679 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:00:31.679 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 20:00:31.679 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.26171173234504 for sweep.
2023-02-07 20:00:31.679 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 171 for sweep.
2023-02-07 20:00:31.680 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 11 for sweep.
2023-02-07 20:00:31.680 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.052499441016260966 for sweep.
2023-02-07 20:00:31.680 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 40 for sweep.
2023-02-07 20:00:31.680 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.04001820262545791 for sweep.
2023-02-07 20:00:31.680 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2274 for sweep.
2023-02-07 20:00:31.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 312 for sweep.
2023-02-07 20:00:31.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.0042040243952715176 for sweep.
2023-02-07 20:00:31.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.020431118330971124 for sweep.
2023-02-07 20:00:31.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.480500965710002 for sweep.
2023-02-07 20:00:31.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:00:31.687 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200023-579n3t9q/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 630, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 171, 'window': 11, 'min_count': 1, 'dm': 0, 'sample': 0.26171173234504, 'workers': 4, 'alpha': 0.01246447868558996, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2274, 'max_depth': 40, 'num_leaves': 312, 'reg_alpha': 0.0042040243952715176, 'reg_lambda': 0.020431118330971124, 'subsample': 0.480500965710002, 'min_child_weight': 0.04001820262545791, 'n_jobs': 4, 'learning_rate': 0.052499441016260966}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 16/3257 [00:00<00:21, 152.76it/s]  1%|          | 34/3257 [00:00<00:19, 164.45it/s]  2%|‚ñè         | 53/3257 [00:00<00:18, 169.98it/s]  2%|‚ñè         | 73/3257 [00:00<00:17, 179.54it/s]  3%|‚ñé         | 92/3257 [00:00<00:17, 183.13it/s]  3%|‚ñé         | 111/3257 [00:00<00:18, 172.91it/s]  4%|‚ñç         | 131/3257 [00:00<00:17, 178.54it/s]  5%|‚ñç         | 151/3257 [00:00<00:16, 184.51it/s]  5%|‚ñå         | 170/3257 [00:00<00:17, 180.79it/s]  6%|‚ñå         | 189/3257 [00:01<00:17, 180.08it/s]  6%|‚ñã         | 208/3257 [00:01<00:16, 181.09it/s]  7%|‚ñã         | 232/3257 [00:01<00:15, 195.92it/s]  8%|‚ñä         | 252/3257 [00:01<00:15, 194.29it/s]  8%|‚ñä         | 272/3257 [00:01<00:22, 131.96it/s]  9%|‚ñâ         | 296/3257 [00:01<00:19, 154.17it/s] 10%|‚ñâ         | 315/3257 [00:01<00:18, 157.46it/s] 10%|‚ñà         | 336/3257 [00:01<00:17, 169.03it/s] 11%|‚ñà         | 358/3257 [00:02<00:15, 181.28it/s] 12%|‚ñà‚ñè        | 378/3257 [00:02<00:16, 170.58it/s] 12%|‚ñà‚ñè        | 397/3257 [00:02<00:16, 171.96it/s] 13%|‚ñà‚ñé        | 418/3257 [00:02<00:15, 181.73it/s] 13%|‚ñà‚ñé        | 437/3257 [00:02<00:17, 157.05it/s] 14%|‚ñà‚ñç        | 457/3257 [00:02<00:16, 167.80it/s] 15%|‚ñà‚ñç        | 476/3257 [00:02<00:16, 172.90it/s] 15%|‚ñà‚ñå        | 498/3257 [00:02<00:15, 182.67it/s] 16%|‚ñà‚ñå        | 517/3257 [00:02<00:14, 184.61it/s] 16%|‚ñà‚ñã        | 536/3257 [00:03<00:14, 184.62it/s] 17%|‚ñà‚ñã        | 556/3257 [00:03<00:14, 187.11it/s] 18%|‚ñà‚ñä        | 575/3257 [00:03<00:16, 161.87it/s] 18%|‚ñà‚ñä        | 595/3257 [00:03<00:15, 171.83it/s] 19%|‚ñà‚ñâ        | 616/3257 [00:03<00:14, 181.20it/s] 19%|‚ñà‚ñâ        | 635/3257 [00:03<00:14, 183.36it/s] 20%|‚ñà‚ñà        | 654/3257 [00:03<00:15, 172.81it/s] 21%|‚ñà‚ñà        | 672/3257 [00:03<00:15, 170.00it/s] 21%|‚ñà‚ñà        | 690/3257 [00:03<00:15, 166.12it/s] 22%|‚ñà‚ñà‚ñè       | 711/3257 [00:04<00:14, 175.28it/s] 22%|‚ñà‚ñà‚ñè       | 729/3257 [00:04<00:15, 168.41it/s] 23%|‚ñà‚ñà‚ñé       | 746/3257 [00:04<00:15, 167.33it/s] 24%|‚ñà‚ñà‚ñé       | 768/3257 [00:04<00:13, 181.55it/s] 24%|‚ñà‚ñà‚ñç       | 787/3257 [00:04<00:14, 171.05it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:04<00:14, 171.90it/s] 25%|‚ñà‚ñà‚ñå       | 824/3257 [00:04<00:14, 173.02it/s] 26%|‚ñà‚ñà‚ñå       | 842/3257 [00:04<00:14, 166.32it/s] 26%|‚ñà‚ñà‚ñã       | 862/3257 [00:04<00:13, 174.58it/s] 27%|‚ñà‚ñà‚ñã       | 880/3257 [00:05<00:14, 169.74it/s] 28%|‚ñà‚ñà‚ñä       | 901/3257 [00:05<00:13, 180.20it/s] 28%|‚ñà‚ñà‚ñä       | 920/3257 [00:05<00:12, 181.09it/s] 29%|‚ñà‚ñà‚ñâ       | 939/3257 [00:05<00:12, 181.75it/s] 29%|‚ñà‚ñà‚ñâ       | 959/3257 [00:05<00:12, 184.46it/s] 30%|‚ñà‚ñà‚ñà       | 978/3257 [00:05<00:12, 183.99it/s] 31%|‚ñà‚ñà‚ñà       | 997/3257 [00:05<00:12, 174.23it/s] 31%|‚ñà‚ñà‚ñà       | 1015/3257 [00:05<00:13, 172.37it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1033/3257 [00:05<00:13, 163.65it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1050/3257 [00:06<00:13, 162.58it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1070/3257 [00:06<00:12, 171.91it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1088/3257 [00:06<00:12, 168.82it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1105/3257 [00:06<00:12, 169.00it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1122/3257 [00:06<00:12, 168.69it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1139/3257 [00:06<00:12, 165.52it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1158/3257 [00:06<00:12, 169.48it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1176/3257 [00:06<00:12, 170.31it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1194/3257 [00:06<00:13, 157.86it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1210/3257 [00:07<00:13, 154.19it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1232/3257 [00:07<00:11, 170.35it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1250/3257 [00:07<00:12, 164.70it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:07<00:11, 172.36it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1289/3257 [00:07<00:12, 157.84it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1306/3257 [00:07<00:12, 159.51it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1324/3257 [00:07<00:11, 163.85it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1343/3257 [00:07<00:11, 169.64it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1361/3257 [00:07<00:11, 167.05it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1378/3257 [00:08<00:11, 160.97it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1395/3257 [00:08<00:11, 163.27it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1417/3257 [00:08<00:10, 179.37it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1436/3257 [00:08<00:16, 109.09it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1459/3257 [00:08<00:13, 132.78it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1478/3257 [00:08<00:12, 144.61it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1500/3257 [00:08<00:10, 160.77it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:09<00:10, 160.00it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1537/3257 [00:09<00:11, 156.33it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1554/3257 [00:09<00:10, 156.60it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1573/3257 [00:09<00:10, 164.27it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1591/3257 [00:09<00:10, 162.27it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1610/3257 [00:09<00:09, 168.45it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1628/3257 [00:09<00:09, 170.90it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1646/3257 [00:09<00:09, 164.31it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1663/3257 [00:09<00:10, 157.35it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1679/3257 [00:10<00:09, 157.81it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1695/3257 [00:10<00:09, 156.65it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1714/3257 [00:10<00:09, 164.77it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:10<00:09, 155.52it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1748/3257 [00:10<00:09, 159.50it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1765/3257 [00:10<00:09, 160.88it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1783/3257 [00:10<00:08, 165.04it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1800/3257 [00:10<00:09, 160.34it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1817/3257 [00:10<00:08, 160.45it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1834/3257 [00:10<00:08, 159.14it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1853/3257 [00:11<00:08, 167.69it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1874/3257 [00:11<00:07, 179.25it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1893/3257 [00:11<00:08, 169.09it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:11<00:07, 175.38it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1935/3257 [00:11<00:07, 176.97it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1958/3257 [00:11<00:06, 189.22it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1978/3257 [00:11<00:06, 183.33it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1999/3257 [00:11<00:06, 186.04it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2018/3257 [00:11<00:06, 186.29it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2038/3257 [00:12<00:06, 186.38it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2057/3257 [00:12<00:07, 167.80it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2075/3257 [00:12<00:07, 168.04it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2093/3257 [00:12<00:06, 169.19it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2111/3257 [00:12<00:06, 168.30it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2128/3257 [00:12<00:07, 158.18it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2144/3257 [00:12<00:07, 154.03it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2165/3257 [00:12<00:06, 168.73it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2183/3257 [00:12<00:06, 163.11it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2204/3257 [00:13<00:06, 172.02it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2222/3257 [00:13<00:06, 167.13it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2239/3257 [00:13<00:06, 160.58it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2259/3257 [00:13<00:05, 168.41it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2276/3257 [00:13<00:06, 156.17it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2298/3257 [00:13<00:05, 171.55it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2316/3257 [00:13<00:05, 171.01it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2340/3257 [00:13<00:04, 188.94it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2362/3257 [00:13<00:04, 196.61it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2382/3257 [00:14<00:04, 195.71it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2404/3257 [00:14<00:04, 200.83it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2425/3257 [00:14<00:04, 187.20it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2444/3257 [00:14<00:04, 171.53it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2464/3257 [00:14<00:04, 178.59it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2484/3257 [00:14<00:04, 181.24it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2507/3257 [00:14<00:03, 192.80it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2527/3257 [00:14<00:03, 190.64it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2548/3257 [00:14<00:03, 194.81it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2568/3257 [00:15<00:03, 173.95it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2586/3257 [00:15<00:03, 171.36it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2605/3257 [00:15<00:03, 176.30it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2629/3257 [00:15<00:03, 193.57it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2649/3257 [00:15<00:03, 185.30it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2668/3257 [00:15<00:03, 182.54it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2688/3257 [00:15<00:03, 185.70it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2707/3257 [00:15<00:03, 165.55it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2725/3257 [00:16<00:03, 166.42it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2747/3257 [00:16<00:02, 179.36it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2766/3257 [00:16<00:02, 180.70it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2785/3257 [00:16<00:02, 181.87it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2807/3257 [00:16<00:02, 187.48it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2826/3257 [00:16<00:04, 101.51it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2845/3257 [00:16<00:03, 117.06it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2872/3257 [00:17<00:02, 147.75it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2893/3257 [00:17<00:02, 155.66it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2914/3257 [00:17<00:02, 167.89it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2934/3257 [00:17<00:01, 172.95it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2954/3257 [00:17<00:01, 172.16it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:17<00:01, 177.69it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2995/3257 [00:17<00:01, 177.48it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3017/3257 [00:17<00:01, 187.04it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3040/3257 [00:17<00:01, 197.81it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3065/3257 [00:18<00:00, 210.65it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3087/3257 [00:18<00:00, 206.68it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3111/3257 [00:18<00:00, 215.14it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3133/3257 [00:18<00:00, 213.29it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3155/3257 [00:18<00:00, 204.59it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3176/3257 [00:18<00:00, 196.07it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3197/3257 [00:18<00:00, 192.61it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3217/3257 [00:18<00:00, 183.88it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3240/3257 [00:18<00:00, 194.41it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 171.55it/s]
2023-02-07 20:00:51.453 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:00:51,454][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d171,n5,s0.261712,t4>', 'datetime': '2023-02-07T20:00:51.454855', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:00:51,455][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:00:51,455][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:00:51,978][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 20:00:51,978][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:00:52,087][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 42701 unique words (100.00% of original 42701, drops 0)', 'datetime': '2023-02-07T20:00:52.087608', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:00:52,088][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 5822992 word corpus (100.00% of original 5822992, drops 0)', 'datetime': '2023-02-07T20:00:52.087999', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:00:52,227][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 20:00:52,228][gensim.models.word2vec][INFO] - sample=0.261712 downsamples 0 most-common words
[2023-02-07 20:00:52,228][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5822992 word corpus (100.0%% of prior 5822992)', 'datetime': '2023-02-07T20:00:52.228403', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:00:52,458][gensim.models.word2vec][INFO] - estimated required memory for 42701 words and 171 dimensions: 82644656 bytes
[2023-02-07 20:00:52,458][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:00:52,492][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 42701 vocabulary and 171 features, using sg=1 hs=0 sample=0.26171173234504 negative=5 window=11 shrink_windows=True', 'datetime': '2023-02-07T20:00:52.492822', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:00:53,496][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 35.22% examples, 2072582 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:00:54,497][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 72.43% examples, 2127436 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:00:55,186][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5795785 effective words) took 2.7s, 2153839 effective words/s
[2023-02-07 20:00:56,192][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 53.05% examples, 3134972 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:00:57,033][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5795785 effective words) took 1.8s, 3141571 effective words/s
[2023-02-07 20:00:58,036][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 41.97% examples, 2500085 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:00:59,036][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 81.82% examples, 2389246 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:00:59,457][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5795785 effective words) took 2.4s, 2392351 effective words/s
[2023-02-07 20:01:00,461][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 49.89% examples, 2939441 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:01,383][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5795785 effective words) took 1.9s, 3011592 effective words/s
[2023-02-07 20:01:02,391][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 31.53% examples, 1835569 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:03,391][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 72.64% examples, 2130370 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:04,014][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5795785 effective words) took 2.6s, 2204621 effective words/s
[2023-02-07 20:01:05,017][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 53.12% examples, 3148792 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:05,871][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5795785 effective words) took 1.9s, 3123073 effective words/s
[2023-02-07 20:01:06,876][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 46.12% examples, 2708190 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:07,878][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 88.64% examples, 2576506 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:08,146][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5795785 effective words) took 2.3s, 2549728 effective words/s
[2023-02-07 20:01:09,150][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 51.92% examples, 3072763 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:09,985][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5795785 effective words) took 1.8s, 3155989 effective words/s
[2023-02-07 20:01:10,991][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 42.62% examples, 2534719 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:11,993][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 86.58% examples, 2522794 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:12,231][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5795785 effective words) took 2.2s, 2583144 effective words/s
[2023-02-07 20:01:13,235][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 41.66% examples, 2476976 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:14,241][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 94.53% examples, 2735270 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:14,333][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5795785 effective words) took 2.1s, 2759181 effective words/s
[2023-02-07 20:01:15,336][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 41.26% examples, 2451017 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:16,340][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 84.03% examples, 2453877 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:16,666][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5795785 effective words) took 2.3s, 2486044 effective words/s
[2023-02-07 20:01:17,669][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 44.27% examples, 2624236 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:18,673][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 89.90% examples, 2615343 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:18,887][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5795785 effective words) took 2.2s, 2612286 effective words/s
[2023-02-07 20:01:19,889][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 42.22% examples, 2512946 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:20,892][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 86.74% examples, 2534288 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:21,175][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5795785 effective words) took 2.3s, 2534567 effective words/s
[2023-02-07 20:01:22,180][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 42.09% examples, 2506070 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:23,188][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 85.20% examples, 2480056 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:23,504][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5795785 effective words) took 2.3s, 2492160 effective words/s
[2023-02-07 20:01:24,509][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 52.29% examples, 3092623 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:01:25,348][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5795785 effective words) took 1.8s, 3147793 effective words/s
[2023-02-07 20:01:25,349][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86936775 effective words) took 32.9s, 2645983 effective words/s', 'datetime': '2023-02-07T20:01:25.349376', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:01:25.350 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:01:28,098][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200023-579n3t9q/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:01:28.098061', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:01:28,099][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:01:28,214][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200023-579n3t9q/files/../tmp/embedding_model.pt
2023-02-07 20:01:28.214 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:01:29.623 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:01:30.093 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:01:31.603 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.18692907192995, 'test_mae': 1.094863189300065, 'test_r2': -2.065877790812152}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.85
wandb: percentage 0.0
wandb:   test_mae 1.09486
wandb:   test_mse 2.18693
wandb:    test_r2 -2.06588
wandb: 
wandb: üöÄ View run apricot-sweep-68 at: https://wandb.ai/xiaoqiz/mof2vec/runs/579n3t9q
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_200023-579n3t9q/logs
wandb: Agent Starting Run: yk6fv8qg with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 817
wandb: 	model.gensim.alpha: 0.006910952727310831
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.2676342324431398
wandb: 	model.gensim.vector_size: 276
wandb: 	model.gensim.window: 17
wandb: 	model.sklearn.learning_rate: 0.1202674484532635
wandb: 	model.sklearn.max_depth: 50
wandb: 	model.sklearn.min_child_weight: 0.07080772864274397
wandb: 	model.sklearn.n_estimators: 1001
wandb: 	model.sklearn.num_leaves: 255
wandb: 	model.sklearn.reg_alpha: 0.04377562156704118
wandb: 	model.sklearn.reg_lambda: 0.4796403457295339
wandb: 	model.sklearn.subsample: 0.6370744333334968
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200143-yk6fv8qg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-69
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/yk6fv8qg
2023-02-07 20:01:51.231 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 20:01:51.232 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 817 for sweep.
2023-02-07 20:01:51.232 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.006910952727310831 for sweep.
2023-02-07 20:01:51.232 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:01:51.233 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 20:01:51.233 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2676342324431398 for sweep.
2023-02-07 20:01:51.233 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 276 for sweep.
2023-02-07 20:01:51.233 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 17 for sweep.
2023-02-07 20:01:51.234 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.1202674484532635 for sweep.
2023-02-07 20:01:51.234 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 50 for sweep.
2023-02-07 20:01:51.234 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.07080772864274397 for sweep.
2023-02-07 20:01:51.234 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1001 for sweep.
2023-02-07 20:01:51.234 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 255 for sweep.
2023-02-07 20:01:51.235 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.04377562156704118 for sweep.
2023-02-07 20:01:51.235 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.4796403457295339 for sweep.
2023-02-07 20:01:51.235 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6370744333334968 for sweep.
2023-02-07 20:01:51.235 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:01:51.243 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200143-yk6fv8qg/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 817, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 276, 'window': 17, 'min_count': 5, 'dm': 0, 'sample': 0.2676342324431398, 'workers': 4, 'alpha': 0.006910952727310831, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1001, 'max_depth': 50, 'num_leaves': 255, 'reg_alpha': 0.04377562156704118, 'reg_lambda': 0.4796403457295339, 'subsample': 0.6370744333334968, 'min_child_weight': 0.07080772864274397, 'n_jobs': 4, 'learning_rate': 0.1202674484532635}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 16/3257 [00:00<00:20, 158.89it/s]  1%|          | 34/3257 [00:00<00:19, 164.58it/s]  2%|‚ñè         | 53/3257 [00:00<00:19, 168.04it/s]  2%|‚ñè         | 71/3257 [00:00<00:18, 170.96it/s]  3%|‚ñé         | 90/3257 [00:00<00:18, 173.22it/s]  3%|‚ñé         | 108/3257 [00:00<00:18, 165.76it/s]  4%|‚ñç         | 125/3257 [00:00<00:18, 166.76it/s]  4%|‚ñç         | 146/3257 [00:00<00:17, 179.88it/s]  5%|‚ñå         | 165/3257 [00:00<00:18, 164.69it/s]  6%|‚ñå         | 184/3257 [00:01<00:18, 169.86it/s]  6%|‚ñã         | 204/3257 [00:01<00:17, 177.61it/s]  7%|‚ñã         | 230/3257 [00:01<00:15, 194.80it/s]  8%|‚ñä         | 250/3257 [00:01<00:15, 190.72it/s]  8%|‚ñä         | 270/3257 [00:01<00:15, 187.43it/s]  9%|‚ñâ         | 294/3257 [00:01<00:14, 201.61it/s] 10%|‚ñâ         | 315/3257 [00:01<00:15, 188.37it/s] 10%|‚ñà         | 336/3257 [00:01<00:15, 190.46it/s] 11%|‚ñà         | 356/3257 [00:01<00:15, 192.73it/s] 12%|‚ñà‚ñè        | 376/3257 [00:02<00:15, 185.31it/s] 12%|‚ñà‚ñè        | 395/3257 [00:02<00:15, 182.79it/s] 13%|‚ñà‚ñé        | 415/3257 [00:02<00:15, 185.89it/s] 13%|‚ñà‚ñé        | 434/3257 [00:02<00:17, 157.01it/s] 14%|‚ñà‚ñç        | 455/3257 [00:02<00:16, 169.75it/s] 15%|‚ñà‚ñç        | 475/3257 [00:02<00:15, 177.29it/s] 15%|‚ñà‚ñå        | 494/3257 [00:02<00:15, 178.28it/s] 16%|‚ñà‚ñå        | 515/3257 [00:02<00:14, 186.55it/s] 16%|‚ñà‚ñã        | 535/3257 [00:02<00:15, 178.79it/s] 17%|‚ñà‚ñã        | 554/3257 [00:03<00:15, 179.98it/s] 18%|‚ñà‚ñä        | 573/3257 [00:03<00:16, 158.14it/s] 18%|‚ñà‚ñä        | 595/3257 [00:03<00:15, 172.40it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:03<00:14, 179.47it/s] 19%|‚ñà‚ñâ        | 634/3257 [00:03<00:14, 180.82it/s] 20%|‚ñà‚ñà        | 653/3257 [00:03<00:15, 173.59it/s] 21%|‚ñà‚ñà        | 671/3257 [00:03<00:14, 172.67it/s] 21%|‚ñà‚ñà        | 689/3257 [00:03<00:15, 165.44it/s] 22%|‚ñà‚ñà‚ñè       | 707/3257 [00:04<00:15, 169.39it/s] 22%|‚ñà‚ñà‚ñè       | 725/3257 [00:04<00:15, 165.72it/s] 23%|‚ñà‚ñà‚ñé       | 742/3257 [00:04<00:15, 161.75it/s] 23%|‚ñà‚ñà‚ñé       | 763/3257 [00:04<00:14, 175.05it/s] 24%|‚ñà‚ñà‚ñç       | 781/3257 [00:04<00:14, 171.07it/s] 25%|‚ñà‚ñà‚ñç       | 803/3257 [00:04<00:13, 183.38it/s] 25%|‚ñà‚ñà‚ñå       | 822/3257 [00:04<00:13, 174.23it/s] 26%|‚ñà‚ñà‚ñå       | 840/3257 [00:04<00:14, 164.31it/s] 26%|‚ñà‚ñà‚ñã       | 857/3257 [00:05<00:22, 107.62it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:05<00:19, 120.07it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:05<00:18, 130.27it/s] 28%|‚ñà‚ñà‚ñä       | 912/3257 [00:05<00:15, 148.79it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:05<00:14, 157.15it/s] 29%|‚ñà‚ñà‚ñâ       | 951/3257 [00:05<00:13, 167.75it/s] 30%|‚ñà‚ñà‚ñâ       | 970/3257 [00:05<00:13, 171.46it/s] 30%|‚ñà‚ñà‚ñà       | 988/3257 [00:05<00:13, 169.18it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:05<00:13, 168.28it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:06<00:13, 170.08it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1045/3257 [00:06<00:13, 166.50it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1065/3257 [00:06<00:12, 174.10it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1083/3257 [00:06<00:12, 171.01it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1101/3257 [00:06<00:12, 168.05it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1118/3257 [00:06<00:13, 161.00it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1135/3257 [00:06<00:13, 158.22it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1151/3257 [00:06<00:13, 158.37it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:06<00:12, 169.50it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1189/3257 [00:07<00:13, 152.51it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1205/3257 [00:07<00:13, 154.44it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1224/3257 [00:07<00:12, 163.58it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1245/3257 [00:07<00:11, 171.97it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1264/3257 [00:07<00:11, 176.08it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1282/3257 [00:07<00:12, 161.80it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1299/3257 [00:07<00:11, 163.80it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1319/3257 [00:07<00:11, 173.60it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:07<00:10, 178.35it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1358/3257 [00:08<00:11, 172.38it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1376/3257 [00:08<00:11, 169.71it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1394/3257 [00:08<00:11, 168.15it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1416/3257 [00:08<00:10, 181.57it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1435/3257 [00:08<00:10, 175.79it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1455/3257 [00:08<00:09, 182.18it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1475/3257 [00:08<00:09, 185.75it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1495/3257 [00:08<00:09, 189.81it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1515/3257 [00:08<00:09, 191.96it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1535/3257 [00:09<00:10, 168.23it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1553/3257 [00:09<00:10, 164.69it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1572/3257 [00:09<00:09, 169.26it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1590/3257 [00:09<00:09, 167.86it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1610/3257 [00:09<00:09, 175.37it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1629/3257 [00:09<00:09, 178.67it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1648/3257 [00:09<00:10, 159.82it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1665/3257 [00:09<00:09, 160.69it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1682/3257 [00:09<00:09, 160.01it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1699/3257 [00:10<00:09, 162.21it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1716/3257 [00:10<00:09, 161.45it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1733/3257 [00:10<00:10, 151.55it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1751/3257 [00:10<00:09, 156.70it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1770/3257 [00:10<00:09, 164.30it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1789/3257 [00:10<00:08, 170.85it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1807/3257 [00:10<00:09, 158.89it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1826/3257 [00:10<00:08, 166.68it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1845/3257 [00:10<00:08, 170.87it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1864/3257 [00:11<00:07, 176.22it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1883/3257 [00:11<00:07, 176.39it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1901/3257 [00:11<00:07, 175.92it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1919/3257 [00:11<00:07, 171.71it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1943/3257 [00:11<00:06, 190.43it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:11<00:06, 197.16it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1985/3257 [00:11<00:06, 185.35it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:11<00:06, 184.76it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2024/3257 [00:11<00:06, 187.66it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2043/3257 [00:11<00:06, 177.86it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2061/3257 [00:12<00:07, 160.44it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2081/3257 [00:12<00:06, 170.25it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2099/3257 [00:12<00:07, 161.49it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:12<00:06, 164.13it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2137/3257 [00:12<00:06, 164.99it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2154/3257 [00:12<00:06, 163.11it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2174/3257 [00:12<00:06, 172.96it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2192/3257 [00:12<00:06, 168.40it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2209/3257 [00:13<00:06, 162.62it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2227/3257 [00:13<00:06, 167.16it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2244/3257 [00:13<00:10, 97.57it/s]  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2262/3257 [00:13<00:08, 112.65it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2277/3257 [00:13<00:08, 119.93it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:13<00:06, 139.73it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2320/3257 [00:13<00:05, 156.33it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2343/3257 [00:13<00:05, 173.86it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2364/3257 [00:14<00:04, 183.27it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2384/3257 [00:14<00:04, 185.11it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:14<00:04, 184.38it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2425/3257 [00:14<00:04, 186.55it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2445/3257 [00:14<00:04, 173.95it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2465/3257 [00:14<00:04, 180.66it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2484/3257 [00:14<00:04, 180.66it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2508/3257 [00:14<00:03, 194.95it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2528/3257 [00:14<00:03, 193.01it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2548/3257 [00:15<00:03, 191.44it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2568/3257 [00:15<00:03, 175.03it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2586/3257 [00:15<00:03, 175.28it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2605/3257 [00:15<00:03, 178.05it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:15<00:03, 191.66it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2648/3257 [00:15<00:03, 182.44it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2667/3257 [00:15<00:03, 175.81it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2687/3257 [00:15<00:03, 181.08it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2706/3257 [00:15<00:03, 168.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2724/3257 [00:16<00:03, 165.79it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2747/3257 [00:16<00:02, 182.73it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2766/3257 [00:16<00:02, 182.21it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2785/3257 [00:16<00:02, 180.76it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:16<00:02, 185.00it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2825/3257 [00:16<00:02, 174.79it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:16<00:02, 175.87it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2871/3257 [00:16<00:01, 201.83it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2892/3257 [00:16<00:01, 196.07it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2912/3257 [00:17<00:01, 186.33it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2931/3257 [00:17<00:01, 182.39it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:17<00:01, 174.86it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2972/3257 [00:17<00:01, 184.25it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2991/3257 [00:17<00:01, 176.05it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3014/3257 [00:17<00:01, 188.92it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3035/3257 [00:17<00:01, 193.77it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3056/3257 [00:17<00:01, 198.32it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:17<00:00, 203.25it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3100/3257 [00:18<00:00, 200.71it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3124/3257 [00:18<00:00, 208.91it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3145/3257 [00:18<00:00, 190.55it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3165/3257 [00:18<00:00, 185.80it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3184/3257 [00:18<00:00, 179.66it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:18<00:00, 188.22it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3226/3257 [00:18<00:00, 184.31it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:18<00:00, 196.15it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 172.42it/s]
2023-02-07 20:02:10.889 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:02:10,890][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d276,n5,mc5,s0.267634,t4>', 'datetime': '2023-02-07T20:02:10.890223', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:02:10,890][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:02:10,890][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:02:11,395][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 20:02:11,396][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:02:11,455][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 21312 unique words (49.91% of original 42701, drops 21389)', 'datetime': '2023-02-07T20:02:11.455393', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:02:11,455][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5769229 word corpus (99.08% of original 5822992, drops 53763)', 'datetime': '2023-02-07T20:02:11.455737', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:02:11,526][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 20:02:11,527][gensim.models.word2vec][INFO] - sample=0.267634 downsamples 0 most-common words
[2023-02-07 20:02:11,527][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5769229 word corpus (100.0%% of prior 5769229)', 'datetime': '2023-02-07T20:02:11.527852', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:02:11,642][gensim.models.word2vec][INFO] - estimated required memory for 21312 words and 276 dimensions: 61960024 bytes
[2023-02-07 20:02:11,642][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:02:11,666][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 21312 vocabulary and 276 features, using sg=1 hs=0 sample=0.2676342324431398 negative=5 window=17 shrink_windows=True', 'datetime': '2023-02-07T20:02:11.665993', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:02:12,669][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 36.48% examples, 2140145 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:13,671][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 78.39% examples, 2269202 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:14,294][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5743174 effective words) took 2.6s, 2187386 effective words/s
[2023-02-07 20:02:15,296][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 36.05% examples, 2097744 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:16,297][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 70.62% examples, 2071734 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:17,077][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5743174 effective words) took 2.8s, 2064888 effective words/s
[2023-02-07 20:02:18,085][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 35.31% examples, 2052532 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:19,086][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 70.62% examples, 2069469 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:02:19,837][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5743174 effective words) took 2.8s, 2084140 effective words/s
[2023-02-07 20:02:20,844][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 36.05% examples, 2090612 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:21,847][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 72.64% examples, 2108298 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:22,577][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5743174 effective words) took 2.7s, 2096986 effective words/s
[2023-02-07 20:02:23,583][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 35.71% examples, 2073476 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:24,585][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 71.66% examples, 2092752 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:02:25,329][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5743174 effective words) took 2.7s, 2090097 effective words/s
[2023-02-07 20:02:26,341][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 36.35% examples, 2113274 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:27,342][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 71.51% examples, 2080210 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:02:28,117][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5743174 effective words) took 2.8s, 2061022 effective words/s
[2023-02-07 20:02:29,129][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 36.14% examples, 2089216 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:30,129][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 71.26% examples, 2077769 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:02:30,873][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5743174 effective words) took 2.8s, 2085915 effective words/s
[2023-02-07 20:02:31,879][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 35.92% examples, 2092390 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:32,879][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 71.69% examples, 2095709 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:33,633][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5743174 effective words) took 2.8s, 2085107 effective words/s
[2023-02-07 20:02:34,641][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 36.17% examples, 2091166 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:35,645][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 73.32% examples, 2123935 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:36,296][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5743174 effective words) took 2.7s, 2158044 effective words/s
[2023-02-07 20:02:37,299][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 37.40% examples, 2203870 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:38,306][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 74.82% examples, 2174538 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:38,937][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5743174 effective words) took 2.6s, 2177350 effective words/s
[2023-02-07 20:02:39,941][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 36.20% examples, 2116380 words/s, in_qsize 6, out_qsize 1
[2023-02-07 20:02:40,947][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 72.77% examples, 2113786 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:41,646][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5743174 effective words) took 2.7s, 2122202 effective words/s
[2023-02-07 20:02:42,650][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 36.20% examples, 2115172 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:43,656][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 72.92% examples, 2118737 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:44,355][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5743174 effective words) took 2.7s, 2121976 effective words/s
[2023-02-07 20:02:45,360][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 36.84% examples, 2158540 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:46,364][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 74.64% examples, 2169575 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:47,003][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5743174 effective words) took 2.6s, 2170416 effective words/s
[2023-02-07 20:02:48,007][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 36.81% examples, 2161475 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:49,010][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 74.18% examples, 2155644 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:02:49,668][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5743174 effective words) took 2.7s, 2157751 effective words/s
[2023-02-07 20:02:50,671][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 36.35% examples, 2132625 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:02:51,675][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 74.03% examples, 2152239 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:02:52,345][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5743174 effective words) took 2.7s, 2147424 effective words/s
[2023-02-07 20:02:52,346][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86147610 effective words) took 40.7s, 2117688 effective words/s', 'datetime': '2023-02-07T20:02:52.346444', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:02:52.346 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:02:55,459][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200143-yk6fv8qg/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:02:55.459417', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:02:55,460][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:02:55,555][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200143-yk6fv8qg/files/../tmp/embedding_model.pt
2023-02-07 20:02:55.555 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:02:57.300 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:02:57.884 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:02:59.869 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1726777654680656, 'test_mae': 1.0891114689046917, 'test_r2': -2.049074378549519}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.86
wandb: percentage 0.5009
wandb:   test_mae 1.08911
wandb:   test_mse 2.17268
wandb:    test_r2 -2.04907
wandb: 
wandb: üöÄ View run different-sweep-69 at: https://wandb.ai/xiaoqiz/mof2vec/runs/yk6fv8qg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_200143-yk6fv8qg/logs
wandb: Agent Starting Run: u2qfddgq with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 1014
wandb: 	model.gensim.alpha: 0.007382122282062164
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.3768857289405251
wandb: 	model.gensim.vector_size: 102
wandb: 	model.gensim.window: 15
wandb: 	model.sklearn.learning_rate: 0.30651408484950865
wandb: 	model.sklearn.max_depth: 24
wandb: 	model.sklearn.min_child_weight: 0.08055333996404447
wandb: 	model.sklearn.n_estimators: 2129
wandb: 	model.sklearn.num_leaves: 473
wandb: 	model.sklearn.reg_alpha: 0.11309461384126004
wandb: 	model.sklearn.reg_lambda: 0.16281947760488108
wandb: 	model.sklearn.subsample: 0.8090934209905145
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200313-u2qfddgq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-sweep-70
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/u2qfddgq
2023-02-07 20:03:21.417 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 20:03:21.417 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 1014 for sweep.
2023-02-07 20:03:21.418 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.007382122282062164 for sweep.
2023-02-07 20:03:21.418 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:03:21.418 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 20:03:21.418 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3768857289405251 for sweep.
2023-02-07 20:03:21.419 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 102 for sweep.
2023-02-07 20:03:21.419 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 15 for sweep.
2023-02-07 20:03:21.419 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.30651408484950865 for sweep.
2023-02-07 20:03:21.419 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 24 for sweep.
2023-02-07 20:03:21.419 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.08055333996404447 for sweep.
2023-02-07 20:03:21.420 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2129 for sweep.
2023-02-07 20:03:21.420 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 473 for sweep.
2023-02-07 20:03:21.420 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.11309461384126004 for sweep.
2023-02-07 20:03:21.420 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.16281947760488108 for sweep.
2023-02-07 20:03:21.420 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8090934209905145 for sweep.
2023-02-07 20:03:21.421 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:03:21.427 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200313-u2qfddgq/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 1014, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 102, 'window': 15, 'min_count': 2, 'dm': 0, 'sample': 0.3768857289405251, 'workers': 4, 'alpha': 0.007382122282062164, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2129, 'max_depth': 24, 'num_leaves': 473, 'reg_alpha': 0.11309461384126004, 'reg_lambda': 0.16281947760488108, 'subsample': 0.8090934209905145, 'min_child_weight': 0.08055333996404447, 'n_jobs': 4, 'learning_rate': 0.30651408484950865}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 23/3257 [00:00<00:14, 226.09it/s]  1%|‚ñè         | 47/3257 [00:00<00:13, 230.70it/s]  2%|‚ñè         | 71/3257 [00:00<00:14, 226.48it/s]  3%|‚ñé         | 96/3257 [00:00<00:13, 230.98it/s]  4%|‚ñé         | 120/3257 [00:00<00:13, 231.65it/s]  4%|‚ñç         | 145/3257 [00:00<00:13, 235.97it/s]  5%|‚ñå         | 169/3257 [00:00<00:13, 228.32it/s]  6%|‚ñå         | 193/3257 [00:00<00:13, 230.05it/s]  7%|‚ñã         | 217/3257 [00:00<00:13, 227.24it/s]  7%|‚ñã         | 243/3257 [00:01<00:12, 234.89it/s]  8%|‚ñä         | 267/3257 [00:01<00:13, 222.79it/s]  9%|‚ñâ         | 290/3257 [00:01<00:17, 165.66it/s] 10%|‚ñâ         | 312/3257 [00:01<00:16, 176.99it/s] 10%|‚ñà         | 338/3257 [00:01<00:14, 196.33it/s] 11%|‚ñà         | 363/3257 [00:01<00:13, 210.07it/s] 12%|‚ñà‚ñè        | 386/3257 [00:01<00:13, 207.76it/s] 13%|‚ñà‚ñé        | 413/3257 [00:01<00:12, 224.31it/s] 13%|‚ñà‚ñé        | 437/3257 [00:02<00:13, 202.87it/s] 14%|‚ñà‚ñç        | 464/3257 [00:02<00:12, 218.34it/s] 15%|‚ñà‚ñç        | 487/3257 [00:02<00:12, 219.18it/s] 16%|‚ñà‚ñå        | 513/3257 [00:02<00:12, 228.07it/s] 16%|‚ñà‚ñã        | 537/3257 [00:02<00:11, 229.32it/s] 17%|‚ñà‚ñã        | 561/3257 [00:02<00:12, 220.78it/s] 18%|‚ñà‚ñä        | 584/3257 [00:02<00:12, 221.66it/s] 19%|‚ñà‚ñâ        | 611/3257 [00:02<00:11, 234.71it/s] 20%|‚ñà‚ñâ        | 636/3257 [00:02<00:10, 238.53it/s] 20%|‚ñà‚ñà        | 661/3257 [00:03<00:11, 221.82it/s] 21%|‚ñà‚ñà        | 687/3257 [00:03<00:11, 231.08it/s] 22%|‚ñà‚ñà‚ñè       | 715/3257 [00:03<00:10, 244.64it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:03<00:11, 226.87it/s] 24%|‚ñà‚ñà‚ñé       | 768/3257 [00:03<00:10, 239.21it/s] 24%|‚ñà‚ñà‚ñç       | 793/3257 [00:03<00:10, 230.36it/s] 25%|‚ñà‚ñà‚ñå       | 819/3257 [00:03<00:10, 236.59it/s] 26%|‚ñà‚ñà‚ñå       | 843/3257 [00:03<00:10, 222.99it/s] 27%|‚ñà‚ñà‚ñã       | 866/3257 [00:03<00:10, 224.58it/s] 27%|‚ñà‚ñà‚ñã       | 890/3257 [00:04<00:10, 227.47it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:04<00:10, 229.49it/s] 29%|‚ñà‚ñà‚ñâ       | 941/3257 [00:04<00:09, 238.81it/s] 30%|‚ñà‚ñà‚ñâ       | 968/3257 [00:04<00:09, 247.39it/s] 30%|‚ñà‚ñà‚ñà       | 993/3257 [00:04<00:09, 241.52it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1019/3257 [00:04<00:09, 243.83it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1044/3257 [00:04<00:09, 227.38it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1071/3257 [00:04<00:09, 238.12it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1096/3257 [00:04<00:09, 236.46it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1120/3257 [00:04<00:09, 235.19it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1145/3257 [00:05<00:08, 234.72it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:05<00:08, 241.81it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1196/3257 [00:05<00:08, 229.01it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1220/3257 [00:05<00:09, 225.68it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1247/3257 [00:05<00:08, 236.54it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:05<00:08, 237.39it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1295/3257 [00:05<00:08, 226.61it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1322/3257 [00:05<00:08, 237.58it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:05<00:07, 241.74it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1375/3257 [00:06<00:11, 162.79it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1402/3257 [00:06<00:10, 183.71it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:06<00:08, 210.24it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1462/3257 [00:06<00:07, 232.02it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1488/3257 [00:06<00:07, 239.15it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1517/3257 [00:06<00:06, 251.03it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1544/3257 [00:06<00:07, 236.51it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1571/3257 [00:06<00:06, 244.60it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1598/3257 [00:07<00:06, 248.90it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1624/3257 [00:07<00:06, 252.00it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1650/3257 [00:07<00:06, 242.34it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1675/3257 [00:07<00:06, 239.32it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1701/3257 [00:07<00:06, 243.35it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:07<00:06, 239.28it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1753/3257 [00:07<00:06, 240.32it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1783/3257 [00:07<00:05, 255.68it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1809/3257 [00:07<00:05, 247.79it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1836/3257 [00:08<00:05, 253.81it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1863/3257 [00:08<00:05, 258.05it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1889/3257 [00:08<00:05, 257.84it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:08<00:05, 251.95it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1947/3257 [00:08<00:04, 267.22it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:08<00:04, 270.20it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2005/3257 [00:08<00:04, 256.62it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2034/3257 [00:08<00:04, 262.58it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2061/3257 [00:08<00:04, 240.66it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2086/3257 [00:09<00:04, 240.73it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2112/3257 [00:09<00:04, 242.24it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2137/3257 [00:09<00:04, 237.00it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2161/3257 [00:09<00:04, 237.20it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2187/3257 [00:09<00:04, 242.81it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2212/3257 [00:09<00:04, 241.46it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2238/3257 [00:09<00:04, 244.34it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2265/3257 [00:09<00:03, 249.36it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2290/3257 [00:09<00:03, 249.00it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2315/3257 [00:09<00:03, 247.45it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2347/3257 [00:10<00:03, 268.37it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2374/3257 [00:10<00:03, 267.74it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2404/3257 [00:10<00:03, 274.96it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2432/3257 [00:10<00:03, 252.88it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2461/3257 [00:10<00:03, 261.66it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2490/3257 [00:10<00:02, 267.63it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2522/3257 [00:10<00:02, 281.80it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:10<00:02, 278.43it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2580/3257 [00:10<00:02, 258.29it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2610/3257 [00:11<00:02, 268.26it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:11<00:02, 277.05it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2669/3257 [00:11<00:02, 267.11it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2696/3257 [00:11<00:02, 259.58it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2723/3257 [00:11<00:02, 247.74it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2753/3257 [00:11<00:01, 261.05it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2780/3257 [00:11<00:03, 158.98it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2809/3257 [00:12<00:02, 183.98it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2833/3257 [00:12<00:02, 193.83it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2864/3257 [00:12<00:01, 219.93it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2891/3257 [00:12<00:01, 231.95it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2917/3257 [00:12<00:01, 234.94it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2943/3257 [00:12<00:01, 233.08it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2969/3257 [00:12<00:01, 240.19it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2994/3257 [00:12<00:01, 241.14it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3022/3257 [00:12<00:00, 251.39it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3054/3257 [00:12<00:00, 269.93it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3085/3257 [00:13<00:00, 279.73it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3116/3257 [00:13<00:00, 287.53it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3145/3257 [00:13<00:00, 270.93it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3173/3257 [00:13<00:00, 272.46it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3201/3257 [00:13<00:00, 269.41it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3229/3257 [00:13<00:00, 262.08it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 237.59it/s]
2023-02-07 20:03:35.601 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:03:35,602][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d102,n5,mc2,s0.376886,t4>', 'datetime': '2023-02-07T20:03:35.602073', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:03:35,602][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:03:35,602][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:03:35,929][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 20:03:35,930][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:03:35,958][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 11155 unique words (85.41% of original 13061, drops 1906)', 'datetime': '2023-02-07T20:03:35.958086', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:03:35,958][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 3637464 word corpus (99.95% of original 3639370, drops 1906)', 'datetime': '2023-02-07T20:03:35.958457', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:03:35,993][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 20:03:35,994][gensim.models.word2vec][INFO] - sample=0.376886 downsamples 0 most-common words
[2023-02-07 20:03:35,995][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3637464 word corpus (100.0%% of prior 3637464)', 'datetime': '2023-02-07T20:03:35.995941', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:03:36,056][gensim.models.word2vec][INFO] - estimated required memory for 11155 words and 102 dimensions: 16660236 bytes
[2023-02-07 20:03:36,056][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:03:36,062][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 11155 vocabulary and 102 features, using sg=1 hs=0 sample=0.3768857289405251 negative=5 window=15 shrink_windows=True', 'datetime': '2023-02-07T20:03:36.062525', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:03:37,066][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 71.05% examples, 2641422 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:03:37,421][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3640721 effective words) took 1.4s, 2682620 effective words/s
[2023-02-07 20:03:38,374][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3640721 effective words) took 1.0s, 3827916 effective words/s
[2023-02-07 20:03:39,385][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 81.92% examples, 2981842 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:03:39,594][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3640721 effective words) took 1.2s, 2988389 effective words/s
[2023-02-07 20:03:40,542][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3640721 effective words) took 0.9s, 3848651 effective words/s
[2023-02-07 20:03:41,545][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 80.69% examples, 2958659 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:03:41,778][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3640721 effective words) took 1.2s, 2950524 effective words/s
[2023-02-07 20:03:42,717][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3640721 effective words) took 0.9s, 3880067 effective words/s
[2023-02-07 20:03:43,722][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 81.64% examples, 2988298 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:03:43,931][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3640721 effective words) took 1.2s, 3001803 effective words/s
[2023-02-07 20:03:44,933][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 80.38% examples, 2950705 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:03:45,167][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3640721 effective words) took 1.2s, 2947964 effective words/s
[2023-02-07 20:03:46,174][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 79.37% examples, 2914669 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:03:46,413][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3640721 effective words) took 1.2s, 2929356 effective words/s
[2023-02-07 20:03:47,420][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 81.64% examples, 2982585 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:03:47,629][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3640721 effective words) took 1.2s, 2996832 effective words/s
[2023-02-07 20:03:48,652][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 86.52% examples, 3166636 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:03:48,797][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3640721 effective words) took 1.1s, 3169070 effective words/s
[2023-02-07 20:03:49,719][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3640721 effective words) took 0.9s, 3956921 effective words/s
[2023-02-07 20:03:50,724][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 86.98% examples, 3188508 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:03:50,866][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3640721 effective words) took 1.1s, 3182293 effective words/s
[2023-02-07 20:03:51,873][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 78.54% examples, 2872696 words/s, in_qsize 5, out_qsize 2
[2023-02-07 20:03:52,108][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3640721 effective words) took 1.2s, 2935381 effective words/s
[2023-02-07 20:03:53,113][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 82.99% examples, 3036435 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:03:53,306][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3640721 effective words) took 1.2s, 3040512 effective words/s
[2023-02-07 20:03:53,307][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54610815 effective words) took 17.2s, 3166817 effective words/s', 'datetime': '2023-02-07T20:03:53.307621', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:03:53.307 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:03:54,534][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200313-u2qfddgq/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:03:54.534412', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:03:54,535][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:03:54,563][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200313-u2qfddgq/files/../tmp/embedding_model.pt
2023-02-07 20:03:54.564 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:03:55.689 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:03:56.108 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:04:23.402 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.17559841171029, 'test_mae': 1.0733834305975267, 'test_r2': -1.7556602201442915}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.76
wandb: percentage 0.14593
wandb:   test_mae 1.07338
wandb:   test_mse 2.1756
wandb:    test_r2 -1.75566
wandb: 
wandb: üöÄ View run woven-sweep-70 at: https://wandb.ai/xiaoqiz/mof2vec/runs/u2qfddgq
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_200313-u2qfddgq/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: vhlgahvv with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 764
wandb: 	model.gensim.alpha: 0.008910720909907077
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.3357584919143528
wandb: 	model.gensim.vector_size: 169
wandb: 	model.gensim.window: 9
wandb: 	model.sklearn.learning_rate: 0.001172209167267394
wandb: 	model.sklearn.max_depth: 48
wandb: 	model.sklearn.min_child_weight: 0.05927862177924851
wandb: 	model.sklearn.n_estimators: 2513
wandb: 	model.sklearn.num_leaves: 302
wandb: 	model.sklearn.reg_alpha: 0.016608607723684877
wandb: 	model.sklearn.reg_lambda: 0.012998004849039449
wandb: 	model.sklearn.subsample: 0.6537512903165248
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200442-vhlgahvv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-71
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/vhlgahvv
2023-02-07 20:04:50.519 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 20:04:50.520 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 764 for sweep.
2023-02-07 20:04:50.520 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.008910720909907077 for sweep.
2023-02-07 20:04:50.520 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:04:50.520 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 20:04:50.521 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3357584919143528 for sweep.
2023-02-07 20:04:50.521 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 169 for sweep.
2023-02-07 20:04:50.521 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 9 for sweep.
2023-02-07 20:04:50.521 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.001172209167267394 for sweep.
2023-02-07 20:04:50.521 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 48 for sweep.
2023-02-07 20:04:50.522 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05927862177924851 for sweep.
2023-02-07 20:04:50.522 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2513 for sweep.
2023-02-07 20:04:50.522 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 302 for sweep.
2023-02-07 20:04:50.522 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.016608607723684877 for sweep.
2023-02-07 20:04:50.523 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.012998004849039449 for sweep.
2023-02-07 20:04:50.523 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6537512903165248 for sweep.
2023-02-07 20:04:50.523 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:04:50.530 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200442-vhlgahvv/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 764, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 169, 'window': 9, 'min_count': 2, 'dm': 0, 'sample': 0.3357584919143528, 'workers': 4, 'alpha': 0.008910720909907077, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2513, 'max_depth': 48, 'num_leaves': 302, 'reg_alpha': 0.016608607723684877, 'reg_lambda': 0.012998004849039449, 'subsample': 0.6537512903165248, 'min_child_weight': 0.05927862177924851, 'n_jobs': 4, 'learning_rate': 0.001172209167267394}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:19, 165.50it/s]  1%|          | 34/3257 [00:00<00:19, 168.05it/s]  2%|‚ñè         | 53/3257 [00:00<00:18, 169.90it/s]  2%|‚ñè         | 70/3257 [00:00<00:18, 169.58it/s]  3%|‚ñé         | 89/3257 [00:00<00:18, 174.30it/s]  3%|‚ñé         | 107/3257 [00:00<00:19, 161.93it/s]  4%|‚ñç         | 126/3257 [00:00<00:18, 168.53it/s]  4%|‚ñç         | 146/3257 [00:00<00:17, 177.40it/s]  5%|‚ñå         | 164/3257 [00:00<00:18, 165.35it/s]  6%|‚ñå         | 181/3257 [00:01<00:18, 166.56it/s]  6%|‚ñå         | 201/3257 [00:01<00:17, 175.05it/s]  7%|‚ñã         | 228/3257 [00:01<00:15, 201.30it/s]  8%|‚ñä         | 249/3257 [00:01<00:15, 189.99it/s]  8%|‚ñä         | 269/3257 [00:01<00:16, 179.93it/s]  9%|‚ñâ         | 294/3257 [00:01<00:14, 198.10it/s] 10%|‚ñâ         | 315/3257 [00:01<00:15, 188.46it/s] 10%|‚ñà         | 336/3257 [00:01<00:15, 189.65it/s] 11%|‚ñà         | 357/3257 [00:01<00:14, 195.08it/s] 12%|‚ñà‚ñè        | 377/3257 [00:02<00:16, 178.55it/s] 12%|‚ñà‚ñè        | 396/3257 [00:02<00:15, 180.93it/s] 13%|‚ñà‚ñé        | 416/3257 [00:02<00:15, 185.87it/s] 13%|‚ñà‚ñé        | 435/3257 [00:02<00:17, 159.72it/s] 14%|‚ñà‚ñç        | 458/3257 [00:02<00:15, 175.76it/s] 15%|‚ñà‚ñç        | 479/3257 [00:02<00:15, 179.65it/s] 15%|‚ñà‚ñå        | 502/3257 [00:02<00:14, 193.13it/s] 16%|‚ñà‚ñå        | 523/3257 [00:02<00:13, 195.45it/s] 17%|‚ñà‚ñã        | 543/3257 [00:02<00:13, 196.37it/s] 17%|‚ñà‚ñã        | 563/3257 [00:03<00:15, 179.08it/s] 18%|‚ñà‚ñä        | 582/3257 [00:03<00:15, 177.78it/s] 19%|‚ñà‚ñä        | 604/3257 [00:03<00:14, 188.00it/s] 19%|‚ñà‚ñâ        | 624/3257 [00:03<00:13, 191.07it/s] 20%|‚ñà‚ñâ        | 645/3257 [00:03<00:13, 191.97it/s] 20%|‚ñà‚ñà        | 665/3257 [00:03<00:14, 184.32it/s] 21%|‚ñà‚ñà        | 685/3257 [00:03<00:13, 185.24it/s] 22%|‚ñà‚ñà‚ñè       | 705/3257 [00:03<00:13, 189.18it/s] 22%|‚ñà‚ñà‚ñè       | 725/3257 [00:03<00:13, 186.75it/s] 23%|‚ñà‚ñà‚ñé       | 744/3257 [00:04<00:13, 182.80it/s] 24%|‚ñà‚ñà‚ñé       | 766/3257 [00:04<00:13, 190.72it/s] 24%|‚ñà‚ñà‚ñç       | 786/3257 [00:04<00:13, 180.23it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:04<00:13, 180.92it/s] 25%|‚ñà‚ñà‚ñå       | 825/3257 [00:04<00:13, 179.94it/s] 26%|‚ñà‚ñà‚ñå       | 844/3257 [00:04<00:13, 176.64it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:04<00:13, 176.06it/s] 27%|‚ñà‚ñà‚ñã       | 882/3257 [00:04<00:13, 174.59it/s] 28%|‚ñà‚ñà‚ñä       | 903/3257 [00:04<00:12, 183.93it/s] 28%|‚ñà‚ñà‚ñä       | 922/3257 [00:05<00:12, 185.20it/s] 29%|‚ñà‚ñà‚ñâ       | 941/3257 [00:05<00:12, 180.46it/s] 29%|‚ñà‚ñà‚ñâ       | 960/3257 [00:05<00:12, 182.53it/s] 30%|‚ñà‚ñà‚ñà       | 979/3257 [00:05<00:13, 174.32it/s] 31%|‚ñà‚ñà‚ñà       | 997/3257 [00:05<00:19, 115.01it/s] 31%|‚ñà‚ñà‚ñà       | 1012/3257 [00:05<00:18, 121.40it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1028/3257 [00:05<00:17, 129.07it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1045/3257 [00:05<00:15, 138.51it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1065/3257 [00:06<00:14, 153.36it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1083/3257 [00:06<00:13, 159.87it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1102/3257 [00:06<00:12, 167.92it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1120/3257 [00:06<00:12, 169.28it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1138/3257 [00:06<00:12, 171.88it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1156/3257 [00:06<00:12, 171.32it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1175/3257 [00:06<00:12, 172.68it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1193/3257 [00:06<00:13, 158.26it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1210/3257 [00:06<00:12, 158.83it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1234/3257 [00:07<00:11, 179.81it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1253/3257 [00:07<00:11, 180.23it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1272/3257 [00:07<00:11, 179.75it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1291/3257 [00:07<00:11, 164.88it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1309/3257 [00:07<00:11, 165.91it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:07<00:11, 166.84it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1348/3257 [00:07<00:10, 175.88it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1366/3257 [00:07<00:11, 168.17it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:07<00:11, 167.71it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1404/3257 [00:08<00:10, 175.54it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1426/3257 [00:08<00:09, 187.41it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1445/3257 [00:08<00:09, 186.31it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1468/3257 [00:08<00:09, 197.24it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1488/3257 [00:08<00:09, 191.15it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:08<00:08, 199.47it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1532/3257 [00:08<00:09, 183.94it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1551/3257 [00:08<00:09, 177.79it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1570/3257 [00:08<00:09, 179.88it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1589/3257 [00:09<00:09, 181.93it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1612/3257 [00:09<00:08, 193.86it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:09<00:08, 187.46it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1653/3257 [00:09<00:08, 184.28it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1672/3257 [00:09<00:08, 176.17it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1691/3257 [00:09<00:08, 177.26it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1713/3257 [00:09<00:08, 188.14it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1732/3257 [00:09<00:08, 169.45it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1750/3257 [00:09<00:08, 170.31it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1768/3257 [00:10<00:08, 172.00it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1788/3257 [00:10<00:08, 178.20it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1806/3257 [00:10<00:08, 174.99it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1825/3257 [00:10<00:08, 175.81it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1843/3257 [00:10<00:08, 173.77it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1862/3257 [00:10<00:07, 177.66it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1881/3257 [00:10<00:07, 181.01it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1900/3257 [00:10<00:07, 181.59it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1919/3257 [00:10<00:07, 178.51it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1944/3257 [00:10<00:06, 198.33it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1967/3257 [00:11<00:06, 206.60it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1988/3257 [00:11<00:06, 194.72it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2008/3257 [00:11<00:06, 189.56it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2030/3257 [00:11<00:06, 197.78it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2050/3257 [00:11<00:06, 180.65it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2069/3257 [00:11<00:06, 176.20it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2087/3257 [00:11<00:06, 176.74it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2105/3257 [00:11<00:06, 177.28it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2123/3257 [00:12<00:06, 167.29it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2142/3257 [00:12<00:06, 172.92it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2160/3257 [00:12<00:06, 172.46it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:12<00:06, 174.59it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2198/3257 [00:12<00:05, 178.84it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2216/3257 [00:12<00:05, 173.85it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:12<00:05, 176.56it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2254/3257 [00:12<00:05, 172.76it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2272/3257 [00:12<00:06, 159.60it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2295/3257 [00:12<00:05, 177.73it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2314/3257 [00:13<00:05, 180.07it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2337/3257 [00:13<00:04, 194.02it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2362/3257 [00:13<00:04, 207.51it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2383/3257 [00:13<00:04, 206.04it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2404/3257 [00:13<00:06, 133.36it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2421/3257 [00:13<00:05, 140.64it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2438/3257 [00:13<00:05, 144.43it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2457/3257 [00:13<00:05, 155.07it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2480/3257 [00:14<00:04, 173.95it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2505/3257 [00:14<00:03, 194.20it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2526/3257 [00:14<00:03, 197.08it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2549/3257 [00:14<00:03, 205.13it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2571/3257 [00:14<00:03, 191.93it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2591/3257 [00:14<00:03, 188.87it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2620/3257 [00:14<00:02, 215.33it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2643/3257 [00:14<00:02, 214.36it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2665/3257 [00:14<00:02, 208.23it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2688/3257 [00:15<00:02, 212.54it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2710/3257 [00:15<00:02, 184.63it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:15<00:02, 191.16it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:15<00:02, 200.56it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2777/3257 [00:15<00:02, 193.79it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2802/3257 [00:15<00:02, 207.81it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2824/3257 [00:15<00:02, 189.14it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:15<00:02, 189.03it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2872/3257 [00:16<00:01, 211.96it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2894/3257 [00:16<00:01, 200.76it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2915/3257 [00:16<00:01, 203.02it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2936/3257 [00:16<00:01, 199.10it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2957/3257 [00:16<00:01, 183.96it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2978/3257 [00:16<00:01, 189.88it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3001/3257 [00:16<00:01, 200.29it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3022/3257 [00:16<00:01, 193.98it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3047/3257 [00:16<00:01, 208.31it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3071/3257 [00:17<00:00, 215.32it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3093/3257 [00:17<00:00, 212.81it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3120/3257 [00:17<00:00, 227.16it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3143/3257 [00:17<00:00, 211.09it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3165/3257 [00:17<00:00, 207.75it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3186/3257 [00:17<00:00, 203.44it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3208/3257 [00:17<00:00, 205.45it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3229/3257 [00:17<00:00, 203.58it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:17<00:00, 209.09it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:17<00:00, 181.94it/s]
2023-02-07 20:05:09.201 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:05:09,203][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d169,n5,mc2,s0.335758,t4>', 'datetime': '2023-02-07T20:05:09.202997', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:05:09,204][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:05:09,204][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:05:09,712][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 20:05:09,713][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:05:09,807][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 36619 unique words (85.76% of original 42701, drops 6082)', 'datetime': '2023-02-07T20:05:09.807284', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:05:09,808][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 5816910 word corpus (99.90% of original 5822992, drops 6082)', 'datetime': '2023-02-07T20:05:09.808422', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:05:09,927][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 20:05:09,928][gensim.models.word2vec][INFO] - sample=0.335758 downsamples 0 most-common words
[2023-02-07 20:05:09,929][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5816910 word corpus (100.0%% of prior 5816910)', 'datetime': '2023-02-07T20:05:09.929136', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:05:10,123][gensim.models.word2vec][INFO] - estimated required memory for 36619 words and 169 dimensions: 70671520 bytes
[2023-02-07 20:05:10,123][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:05:10,149][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 36619 vocabulary and 169 features, using sg=1 hs=0 sample=0.3357584919143528 negative=5 window=9 shrink_windows=True', 'datetime': '2023-02-07T20:05:10.149314', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:05:11,152][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 35.12% examples, 2060643 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:12,153][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 84.37% examples, 2463424 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:12,442][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5790320 effective words) took 2.3s, 2527371 effective words/s
[2023-02-07 20:05:13,452][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 41.36% examples, 2437666 words/s, in_qsize 5, out_qsize 3
[2023-02-07 20:05:14,453][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 96.90% examples, 2795293 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:14,504][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5790320 effective words) took 2.1s, 2810154 effective words/s
[2023-02-07 20:05:15,505][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 53.70% examples, 3183009 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:16,310][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5790320 effective words) took 1.8s, 3206634 effective words/s
[2023-02-07 20:05:17,313][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 54.34% examples, 3215348 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:18,092][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5790320 effective words) took 1.8s, 3251640 effective words/s
[2023-02-07 20:05:19,100][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 42.43% examples, 2515063 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:20,104][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 85.32% examples, 2483247 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:20,425][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5790320 effective words) took 2.3s, 2483883 effective words/s
[2023-02-07 20:05:21,431][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 41.42% examples, 2449413 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:22,432][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 86.95% examples, 2534118 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:22,651][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5790320 effective words) took 2.2s, 2603209 effective words/s
[2023-02-07 20:05:23,654][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 43.14% examples, 2564217 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:24,654][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 86.95% examples, 2538911 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:24,941][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5790320 effective words) took 2.3s, 2529697 effective words/s
[2023-02-07 20:05:25,948][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 43.35% examples, 2568131 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:26,950][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 88.76% examples, 2579185 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:27,200][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5790320 effective words) took 2.3s, 2567622 effective words/s
[2023-02-07 20:05:28,202][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 42.77% examples, 2550484 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:29,205][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 87.81% examples, 2555690 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:29,470][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5790320 effective words) took 2.3s, 2553159 effective words/s
[2023-02-07 20:05:30,474][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 42.52% examples, 2525205 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:31,478][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 87.07% examples, 2537437 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:31,734][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5790320 effective words) took 2.3s, 2558618 effective words/s
[2023-02-07 20:05:32,737][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 54.53% examples, 3230620 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:33,517][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5790320 effective words) took 1.8s, 3251161 effective words/s
[2023-02-07 20:05:34,519][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 43.69% examples, 2593632 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:35,523][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 89.47% examples, 2604939 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:35,743][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5790320 effective words) took 2.2s, 2602570 effective words/s
[2023-02-07 20:05:36,745][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 43.48% examples, 2584621 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:37,748][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 89.50% examples, 2603464 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:37,961][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5790320 effective words) took 2.2s, 2613082 effective words/s
[2023-02-07 20:05:38,965][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 44.03% examples, 2607627 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:39,964][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 88.36% examples, 2568450 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:40,217][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5790320 effective words) took 2.3s, 2567897 effective words/s
[2023-02-07 20:05:41,222][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 42.98% examples, 2558220 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:42,223][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 87.26% examples, 2549762 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:05:42,501][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5790320 effective words) took 2.3s, 2540348 effective words/s
[2023-02-07 20:05:42,502][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86854800 effective words) took 32.4s, 2684624 effective words/s', 'datetime': '2023-02-07T20:05:42.502467', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:05:42.503 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:05:45,229][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200442-vhlgahvv/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:05:45.228815', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:05:45,231][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:05:45,332][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200442-vhlgahvv/files/../tmp/embedding_model.pt
2023-02-07 20:05:45.333 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:05:46.707 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:05:47.182 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:05:48.566 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.262141827716428, 'test_mae': 1.1115487607244539, 'test_r2': -1.5564026118730867}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.76
wandb: percentage 0.14243
wandb:   test_mae 1.11155
wandb:   test_mse 2.26214
wandb:    test_r2 -1.5564
wandb: 
wandb: üöÄ View run whole-sweep-71 at: https://wandb.ai/xiaoqiz/mof2vec/runs/vhlgahvv
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_200442-vhlgahvv/logs
wandb: Agent Starting Run: 32y95ews with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 582
wandb: 	model.gensim.alpha: 0.01262909468123046
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.5461122589943851
wandb: 	model.gensim.vector_size: 512
wandb: 	model.gensim.window: 11
wandb: 	model.sklearn.learning_rate: 0.5108912355765425
wandb: 	model.sklearn.max_depth: 62
wandb: 	model.sklearn.min_child_weight: 0.0444290793874021
wandb: 	model.sklearn.n_estimators: 887
wandb: 	model.sklearn.num_leaves: 347
wandb: 	model.sklearn.reg_alpha: 0.004369993042027277
wandb: 	model.sklearn.reg_lambda: 0.08967190501054734
wandb: 	model.sklearn.subsample: 0.4267303407334908
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200602-32y95ews
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-72
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/32y95ews
2023-02-07 20:06:10.766 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 20:06:10.767 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 582 for sweep.
2023-02-07 20:06:10.767 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.01262909468123046 for sweep.
2023-02-07 20:06:10.767 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:06:10.768 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 20:06:10.768 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5461122589943851 for sweep.
2023-02-07 20:06:10.768 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 512 for sweep.
2023-02-07 20:06:10.768 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 11 for sweep.
2023-02-07 20:06:10.768 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.5108912355765425 for sweep.
2023-02-07 20:06:10.769 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 62 for sweep.
2023-02-07 20:06:10.769 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0444290793874021 for sweep.
2023-02-07 20:06:10.769 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 887 for sweep.
2023-02-07 20:06:10.769 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 347 for sweep.
2023-02-07 20:06:10.770 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.004369993042027277 for sweep.
2023-02-07 20:06:10.770 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.08967190501054734 for sweep.
2023-02-07 20:06:10.770 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.4267303407334908 for sweep.
2023-02-07 20:06:10.770 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:06:10.780 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200602-32y95ews/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 582, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 512, 'window': 11, 'min_count': 1, 'dm': 0, 'sample': 0.5461122589943851, 'workers': 4, 'alpha': 0.01262909468123046, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 887, 'max_depth': 62, 'num_leaves': 347, 'reg_alpha': 0.004369993042027277, 'reg_lambda': 0.08967190501054734, 'subsample': 0.4267303407334908, 'min_child_weight': 0.0444290793874021, 'n_jobs': 4, 'learning_rate': 0.5108912355765425}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 15/3257 [00:00<00:21, 149.77it/s]  1%|          | 33/3257 [00:00<00:19, 167.47it/s]  2%|‚ñè         | 50/3257 [00:00<00:19, 161.20it/s]  2%|‚ñè         | 67/3257 [00:00<00:20, 158.27it/s]  3%|‚ñé         | 89/3257 [00:00<00:18, 175.87it/s]  3%|‚ñé         | 107/3257 [00:00<00:20, 155.91it/s]  4%|‚ñç         | 124/3257 [00:00<00:19, 158.52it/s]  4%|‚ñç         | 145/3257 [00:00<00:18, 171.43it/s]  5%|‚ñå         | 163/3257 [00:01<00:19, 158.86it/s]  6%|‚ñå         | 180/3257 [00:01<00:19, 159.82it/s]  6%|‚ñå         | 199/3257 [00:01<00:18, 167.30it/s]  7%|‚ñã         | 218/3257 [00:01<00:17, 171.66it/s]  7%|‚ñã         | 237/3257 [00:01<00:17, 175.78it/s]  8%|‚ñä         | 256/3257 [00:01<00:16, 179.72it/s]  8%|‚ñä         | 275/3257 [00:01<00:16, 178.15it/s]  9%|‚ñâ         | 297/3257 [00:01<00:15, 186.88it/s] 10%|‚ñâ         | 316/3257 [00:01<00:16, 176.66it/s] 10%|‚ñà         | 336/3257 [00:01<00:16, 178.93it/s] 11%|‚ñà         | 355/3257 [00:02<00:16, 180.71it/s] 11%|‚ñà‚ñè        | 374/3257 [00:02<00:16, 174.73it/s] 12%|‚ñà‚ñè        | 392/3257 [00:02<00:18, 157.02it/s] 13%|‚ñà‚ñé        | 412/3257 [00:02<00:17, 166.84it/s] 13%|‚ñà‚ñé        | 430/3257 [00:02<00:19, 148.54it/s] 14%|‚ñà‚ñé        | 447/3257 [00:02<00:18, 150.12it/s] 14%|‚ñà‚ñç        | 467/3257 [00:02<00:17, 159.75it/s] 15%|‚ñà‚ñç        | 484/3257 [00:02<00:17, 158.18it/s] 15%|‚ñà‚ñå        | 501/3257 [00:03<00:24, 113.86it/s] 16%|‚ñà‚ñå        | 519/3257 [00:03<00:21, 127.98it/s] 16%|‚ñà‚ñã        | 536/3257 [00:03<00:19, 136.22it/s] 17%|‚ñà‚ñã        | 554/3257 [00:03<00:18, 146.78it/s] 18%|‚ñà‚ñä        | 570/3257 [00:03<00:18, 145.36it/s] 18%|‚ñà‚ñä        | 586/3257 [00:03<00:18, 147.59it/s] 19%|‚ñà‚ñä        | 605/3257 [00:03<00:16, 157.91it/s] 19%|‚ñà‚ñâ        | 622/3257 [00:03<00:16, 155.89it/s] 20%|‚ñà‚ñâ        | 643/3257 [00:04<00:15, 169.06it/s] 20%|‚ñà‚ñà        | 661/3257 [00:04<00:17, 150.40it/s] 21%|‚ñà‚ñà        | 681/3257 [00:04<00:15, 162.22it/s] 21%|‚ñà‚ñà‚ñè       | 698/3257 [00:04<00:16, 153.01it/s] 22%|‚ñà‚ñà‚ñè       | 718/3257 [00:04<00:15, 162.96it/s] 23%|‚ñà‚ñà‚ñé       | 735/3257 [00:04<00:15, 158.20it/s] 23%|‚ñà‚ñà‚ñé       | 752/3257 [00:04<00:16, 153.77it/s] 24%|‚ñà‚ñà‚ñé       | 772/3257 [00:04<00:15, 162.96it/s] 24%|‚ñà‚ñà‚ñç       | 789/3257 [00:04<00:15, 159.87it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:05<00:15, 162.23it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:05<00:15, 160.76it/s] 26%|‚ñà‚ñà‚ñå       | 840/3257 [00:05<00:15, 159.58it/s] 26%|‚ñà‚ñà‚ñã       | 857/3257 [00:05<00:15, 156.84it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:05<00:14, 160.20it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:05<00:14, 159.30it/s] 28%|‚ñà‚ñà‚ñä       | 912/3257 [00:05<00:13, 172.20it/s] 29%|‚ñà‚ñà‚ñä       | 930/3257 [00:05<00:13, 174.27it/s] 29%|‚ñà‚ñà‚ñâ       | 948/3257 [00:05<00:13, 170.44it/s] 30%|‚ñà‚ñà‚ñâ       | 967/3257 [00:06<00:13, 174.50it/s] 30%|‚ñà‚ñà‚ñà       | 985/3257 [00:06<00:13, 170.76it/s] 31%|‚ñà‚ñà‚ñà       | 1003/3257 [00:06<00:13, 167.02it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1020/3257 [00:06<00:13, 165.65it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1037/3257 [00:06<00:14, 156.83it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1053/3257 [00:06<00:14, 154.10it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1072/3257 [00:06<00:13, 162.84it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1089/3257 [00:06<00:13, 161.97it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1106/3257 [00:06<00:13, 162.77it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1123/3257 [00:06<00:13, 160.96it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1140/3257 [00:07<00:13, 157.79it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1159/3257 [00:07<00:12, 165.59it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1176/3257 [00:07<00:12, 162.73it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1193/3257 [00:07<00:13, 147.70it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1209/3257 [00:07<00:14, 143.20it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1230/3257 [00:07<00:12, 160.30it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1247/3257 [00:07<00:12, 157.68it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1265/3257 [00:07<00:12, 163.75it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1282/3257 [00:08<00:13, 149.07it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1298/3257 [00:08<00:13, 148.07it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1315/3257 [00:08<00:12, 153.26it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1332/3257 [00:08<00:12, 156.21it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:08<00:12, 154.98it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1367/3257 [00:08<00:11, 158.07it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1383/3257 [00:08<00:12, 154.21it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1402/3257 [00:08<00:11, 160.04it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1424/3257 [00:08<00:10, 173.99it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1442/3257 [00:08<00:10, 175.30it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1463/3257 [00:09<00:09, 184.23it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1483/3257 [00:09<00:09, 187.44it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1503/3257 [00:09<00:09, 190.64it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1523/3257 [00:09<00:09, 174.63it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1541/3257 [00:09<00:10, 165.96it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1558/3257 [00:09<00:10, 160.53it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1575/3257 [00:09<00:10, 160.22it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1592/3257 [00:10<00:16, 101.37it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1612/3257 [00:10<00:13, 120.18it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1631/3257 [00:10<00:12, 135.35it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1648/3257 [00:10<00:11, 134.42it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1664/3257 [00:10<00:11, 139.56it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1681/3257 [00:10<00:10, 145.82it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1699/3257 [00:10<00:10, 154.44it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1716/3257 [00:10<00:09, 154.43it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1733/3257 [00:10<00:10, 147.39it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1750/3257 [00:11<00:09, 152.21it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1768/3257 [00:11<00:09, 158.37it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1788/3257 [00:11<00:08, 168.39it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1806/3257 [00:11<00:08, 164.97it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1825/3257 [00:11<00:08, 168.11it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1843/3257 [00:11<00:08, 168.77it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1860/3257 [00:11<00:08, 169.10it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1880/3257 [00:11<00:07, 177.44it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1898/3257 [00:11<00:07, 175.09it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1917/3257 [00:12<00:07, 176.79it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1937/3257 [00:12<00:07, 183.41it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1961/3257 [00:12<00:06, 199.32it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1981/3257 [00:12<00:06, 188.16it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2001/3257 [00:12<00:06, 189.45it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2021/3257 [00:12<00:06, 189.79it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2041/3257 [00:12<00:06, 174.64it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2059/3257 [00:12<00:07, 163.08it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2078/3257 [00:12<00:07, 166.60it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2096/3257 [00:13<00:07, 162.46it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2116/3257 [00:13<00:06, 172.23it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2134/3257 [00:13<00:07, 158.67it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2151/3257 [00:13<00:07, 157.34it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:13<00:06, 163.12it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2186/3257 [00:13<00:06, 162.92it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:13<00:06, 167.97it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2224/3257 [00:13<00:06, 167.13it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2241/3257 [00:13<00:06, 161.25it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2260/3257 [00:14<00:06, 165.89it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2277/3257 [00:14<00:06, 160.45it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:14<00:05, 173.59it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2320/3257 [00:14<00:05, 183.67it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2340/3257 [00:14<00:04, 187.53it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:14<00:04, 187.97it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2379/3257 [00:14<00:04, 185.00it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2398/3257 [00:14<00:04, 186.43it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2417/3257 [00:14<00:04, 173.80it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2435/3257 [00:15<00:04, 165.00it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2452/3257 [00:15<00:05, 160.22it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2475/3257 [00:15<00:04, 175.59it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2494/3257 [00:15<00:04, 178.72it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2514/3257 [00:15<00:04, 182.70it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2533/3257 [00:15<00:04, 179.09it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:15<00:04, 169.99it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2569/3257 [00:15<00:04, 152.30it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2585/3257 [00:15<00:04, 152.69it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2605/3257 [00:16<00:04, 162.50it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:16<00:03, 178.96it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2647/3257 [00:16<00:03, 165.68it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2664/3257 [00:16<00:03, 161.35it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2681/3257 [00:16<00:03, 163.67it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2698/3257 [00:16<00:03, 149.76it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2714/3257 [00:16<00:03, 149.41it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:16<00:03, 154.81it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2752/3257 [00:16<00:03, 166.55it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2769/3257 [00:17<00:03, 156.22it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2790/3257 [00:17<00:02, 170.05it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2808/3257 [00:17<00:02, 166.47it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2825/3257 [00:17<00:02, 157.49it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2841/3257 [00:17<00:02, 157.62it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2864/3257 [00:17<00:02, 177.20it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:17<00:02, 181.15it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2905/3257 [00:18<00:03, 91.74it/s]  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2926/3257 [00:18<00:02, 111.21it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2943/3257 [00:18<00:02, 114.84it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2959/3257 [00:18<00:02, 122.54it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:18<00:02, 132.31it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2992/3257 [00:18<00:01, 136.10it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:18<00:01, 151.87it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3029/3257 [00:18<00:01, 155.01it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:19<00:01, 165.54it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3072/3257 [00:19<00:01, 181.78it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3091/3257 [00:19<00:00, 173.97it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3113/3257 [00:19<00:00, 184.19it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3132/3257 [00:19<00:00, 177.01it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3151/3257 [00:19<00:00, 169.97it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3169/3257 [00:19<00:00, 171.73it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3187/3257 [00:19<00:00, 165.55it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:19<00:00, 168.13it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3224/3257 [00:20<00:00, 168.18it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:20<00:00, 180.36it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:20<00:00, 160.98it/s]
2023-02-07 20:06:31.927 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:06:31,928][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d512,n5,s0.546112,t4>', 'datetime': '2023-02-07T20:06:31.928298', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:06:31,928][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:06:31,928][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:06:32,498][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 20:06:32,498][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:06:32,637][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 54054 unique words (100.00% of original 54054, drops 0)', 'datetime': '2023-02-07T20:06:32.637107', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:06:32,637][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 6550866 word corpus (100.00% of original 6550866, drops 0)', 'datetime': '2023-02-07T20:06:32.637502', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:06:32,813][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 20:06:32,814][gensim.models.word2vec][INFO] - sample=0.546112 downsamples 0 most-common words
[2023-02-07 20:06:32,815][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6550866 word corpus (100.0%% of prior 6550866)', 'datetime': '2023-02-07T20:06:32.815037', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:06:33,112][gensim.models.word2vec][INFO] - estimated required memory for 54054 words and 512 dimensions: 255753920 bytes
[2023-02-07 20:06:33,113][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:06:33,220][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 54054 vocabulary and 512 features, using sg=1 hs=0 sample=0.5461122589943851 negative=5 window=11 shrink_windows=True', 'datetime': '2023-02-07T20:06:33.220616', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:06:34,226][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 22.04% examples, 1401808 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:35,231][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 44.89% examples, 1480802 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:36,233][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 68.99% examples, 1520852 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:37,234][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 96.53% examples, 1565413 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:37,362][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6494971 effective words) took 4.1s, 1569445 effective words/s
[2023-02-07 20:06:38,376][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 22.81% examples, 1454631 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:06:39,384][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 43.91% examples, 1443152 words/s, in_qsize 6, out_qsize 1
[2023-02-07 20:06:40,386][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 65.77% examples, 1441088 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:41,392][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 88.67% examples, 1437654 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:41,891][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6494971 effective words) took 4.5s, 1434371 effective words/s
[2023-02-07 20:06:42,894][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 28.34% examples, 1839301 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:43,907][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 55.48% examples, 1832344 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:44,910][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 78.42% examples, 1705496 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:45,864][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6494971 effective words) took 4.0s, 1635608 effective words/s
[2023-02-07 20:06:46,881][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 22.29% examples, 1416081 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:06:47,886][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 43.20% examples, 1428394 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:48,887][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 65.37% examples, 1432050 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:49,889][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 87.17% examples, 1421486 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:06:50,431][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6494971 effective words) took 4.6s, 1422611 effective words/s
[2023-02-07 20:06:51,434][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 23.00% examples, 1481143 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:52,440][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 44.43% examples, 1470917 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:53,440][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 67.45% examples, 1484599 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:54,441][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 91.16% examples, 1487114 words/s, in_qsize 6, out_qsize 0
[2023-02-07 20:06:54,804][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6494971 effective words) took 4.4s, 1485887 effective words/s
[2023-02-07 20:06:55,813][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 28.55% examples, 1842225 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:56,816][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 55.20% examples, 1821921 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:57,819][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 83.33% examples, 1815658 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:06:58,381][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6494971 effective words) took 3.6s, 1816674 effective words/s
[2023-02-07 20:06:59,398][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 23.30% examples, 1481897 words/s, in_qsize 6, out_qsize 0
[2023-02-07 20:07:00,407][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 44.21% examples, 1455391 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:01,409][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 67.27% examples, 1473583 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:02,410][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 95.61% examples, 1544170 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:07:02,582][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6494971 effective words) took 4.2s, 1547411 effective words/s
[2023-02-07 20:07:03,593][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 23.21% examples, 1480302 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:04,594][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 47.37% examples, 1561464 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:05,600][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 78.23% examples, 1701021 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:06,277][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6494971 effective words) took 3.7s, 1759625 effective words/s
[2023-02-07 20:07:07,279][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 30.06% examples, 1957513 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:08,279][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 59.26% examples, 1964178 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:09,289][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 90.97% examples, 1973472 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:07:09,564][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6494971 effective words) took 3.3s, 1976960 effective words/s
[2023-02-07 20:07:10,568][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 30.92% examples, 2015808 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:11,577][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 54.65% examples, 1804634 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:12,580][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 78.94% examples, 1718246 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:13,417][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6494971 effective words) took 3.9s, 1685864 effective words/s
[2023-02-07 20:07:14,426][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 28.46% examples, 1845470 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:15,429][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 56.74% examples, 1877042 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:16,431][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 83.91% examples, 1830207 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:17,049][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6494971 effective words) took 3.6s, 1790322 effective words/s
[2023-02-07 20:07:18,064][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 24.29% examples, 1546841 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:19,072][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 48.05% examples, 1579650 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:20,078][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 77.86% examples, 1689169 words/s, in_qsize 6, out_qsize 0
[2023-02-07 20:07:20,793][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6494971 effective words) took 3.7s, 1736692 effective words/s
[2023-02-07 20:07:21,810][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 24.19% examples, 1542928 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:22,822][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 47.44% examples, 1553907 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:07:23,825][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 70.77% examples, 1551504 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:24,839][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 95.70% examples, 1539980 words/s, in_qsize 6, out_qsize 1
[2023-02-07 20:07:25,005][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6494971 effective words) took 4.2s, 1543652 effective words/s
[2023-02-07 20:07:26,007][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 24.78% examples, 1597249 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:07:27,017][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 47.28% examples, 1556863 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:07:28,016][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 69.85% examples, 1543824 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:07:29,019][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 94.60% examples, 1535570 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:29,245][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6494971 effective words) took 4.2s, 1532495 effective words/s
[2023-02-07 20:07:30,251][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 23.00% examples, 1479345 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:31,254][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 44.67% examples, 1478486 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:32,260][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 67.27% examples, 1479614 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:07:33,267][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 92.14% examples, 1498330 words/s, in_qsize 6, out_qsize 0
[2023-02-07 20:07:33,579][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6494971 effective words) took 4.3s, 1499713 effective words/s
[2023-02-07 20:07:33,580][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (97424565 effective words) took 60.4s, 1614077 effective words/s', 'datetime': '2023-02-07T20:07:33.580397', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:07:33.580 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:07:38,899][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200602-32y95ews/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:07:38.899655', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:07:38,901][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200602-32y95ews/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 20:07:39,021][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200602-32y95ews/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 20:07:39,121][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:07:39,193][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200602-32y95ews/files/../tmp/embedding_model.pt
2023-02-07 20:07:39.194 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:07:41.883 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:07:42.760 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:07:46.349 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.23101288684143, 'test_mae': 1.1139002404378533, 'test_r2': -1.961044443588957}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: / 0.008 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.8
wandb: percentage 0.0
wandb:   test_mae 1.1139
wandb:   test_mse 2.23101
wandb:    test_r2 -1.96104
wandb: 
wandb: üöÄ View run light-sweep-72 at: https://wandb.ai/xiaoqiz/mof2vec/runs/32y95ews
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_200602-32y95ews/logs
wandb: Agent Starting Run: rjorlhs6 with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 860
wandb: 	model.gensim.alpha: 0.0004536263436573713
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.7771617792003425
wandb: 	model.gensim.vector_size: 378
wandb: 	model.gensim.window: 14
wandb: 	model.sklearn.learning_rate: 0.0543861114800062
wandb: 	model.sklearn.max_depth: 81
wandb: 	model.sklearn.min_child_weight: 0.07999933624004071
wandb: 	model.sklearn.n_estimators: 2277
wandb: 	model.sklearn.num_leaves: 460
wandb: 	model.sklearn.reg_alpha: 0.27823909352109044
wandb: 	model.sklearn.reg_lambda: 0.6642693873855724
wandb: 	model.sklearn.subsample: 0.7959824986059127
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200759-rjorlhs6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-73
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/rjorlhs6
2023-02-07 20:08:06.680 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 20:08:06.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 860 for sweep.
2023-02-07 20:08:06.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0004536263436573713 for sweep.
2023-02-07 20:08:06.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:08:06.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 20:08:06.682 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7771617792003425 for sweep.
2023-02-07 20:08:06.682 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 378 for sweep.
2023-02-07 20:08:06.682 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 14 for sweep.
2023-02-07 20:08:06.682 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0543861114800062 for sweep.
2023-02-07 20:08:06.683 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 81 for sweep.
2023-02-07 20:08:06.683 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.07999933624004071 for sweep.
2023-02-07 20:08:06.683 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2277 for sweep.
2023-02-07 20:08:06.683 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 460 for sweep.
2023-02-07 20:08:06.684 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.27823909352109044 for sweep.
2023-02-07 20:08:06.684 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.6642693873855724 for sweep.
2023-02-07 20:08:06.684 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.7959824986059127 for sweep.
2023-02-07 20:08:06.684 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:08:06.691 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200759-rjorlhs6/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 860, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 378, 'window': 14, 'min_count': 3, 'dm': 0, 'sample': 0.7771617792003425, 'workers': 4, 'alpha': 0.0004536263436573713, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2277, 'max_depth': 81, 'num_leaves': 460, 'reg_alpha': 0.27823909352109044, 'reg_lambda': 0.6642693873855724, 'subsample': 0.7959824986059127, 'min_child_weight': 0.07999933624004071, 'n_jobs': 4, 'learning_rate': 0.0543861114800062}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:18, 171.04it/s]  1%|          | 38/3257 [00:00<00:17, 186.17it/s]  2%|‚ñè         | 57/3257 [00:00<00:18, 176.46it/s]  2%|‚ñè         | 78/3257 [00:00<00:17, 185.65it/s]  3%|‚ñé         | 97/3257 [00:00<00:17, 181.39it/s]  4%|‚ñé         | 116/3257 [00:00<00:17, 178.46it/s]  4%|‚ñç         | 137/3257 [00:00<00:16, 187.41it/s]  5%|‚ñç         | 158/3257 [00:00<00:16, 189.78it/s]  5%|‚ñå         | 178/3257 [00:00<00:16, 183.65it/s]  6%|‚ñå         | 199/3257 [00:01<00:16, 190.98it/s]  7%|‚ñã         | 220/3257 [00:01<00:15, 196.04it/s]  7%|‚ñã         | 242/3257 [00:01<00:14, 202.18it/s]  8%|‚ñä         | 263/3257 [00:01<00:15, 191.99it/s]  9%|‚ñâ         | 286/3257 [00:01<00:14, 202.58it/s]  9%|‚ñâ         | 307/3257 [00:01<00:15, 193.31it/s] 10%|‚ñà         | 328/3257 [00:01<00:14, 196.24it/s] 11%|‚ñà         | 348/3257 [00:01<00:15, 191.16it/s] 11%|‚ñà‚ñè        | 370/3257 [00:01<00:14, 198.72it/s] 12%|‚ñà‚ñè        | 390/3257 [00:02<00:15, 181.47it/s] 13%|‚ñà‚ñé        | 411/3257 [00:02<00:15, 188.15it/s] 13%|‚ñà‚ñé        | 431/3257 [00:02<00:16, 166.46it/s] 14%|‚ñà‚ñç        | 449/3257 [00:02<00:16, 168.56it/s] 14%|‚ñà‚ñç        | 469/3257 [00:02<00:15, 176.07it/s] 15%|‚ñà‚ñç        | 488/3257 [00:02<00:15, 175.31it/s] 16%|‚ñà‚ñå        | 511/3257 [00:02<00:14, 189.60it/s] 16%|‚ñà‚ñã        | 531/3257 [00:02<00:14, 184.01it/s] 17%|‚ñà‚ñã        | 550/3257 [00:02<00:14, 185.24it/s] 17%|‚ñà‚ñã        | 569/3257 [00:03<00:15, 179.16it/s] 18%|‚ñà‚ñä        | 588/3257 [00:03<00:15, 174.14it/s] 19%|‚ñà‚ñä        | 610/3257 [00:03<00:14, 183.86it/s] 19%|‚ñà‚ñâ        | 629/3257 [00:03<00:14, 184.02it/s] 20%|‚ñà‚ñâ        | 648/3257 [00:03<00:14, 181.36it/s] 20%|‚ñà‚ñà        | 667/3257 [00:03<00:14, 174.95it/s] 21%|‚ñà‚ñà        | 685/3257 [00:03<00:14, 173.22it/s] 22%|‚ñà‚ñà‚ñè       | 704/3257 [00:03<00:14, 176.55it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:03<00:14, 177.02it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:04<00:14, 169.88it/s] 23%|‚ñà‚ñà‚ñé       | 762/3257 [00:04<00:13, 182.77it/s] 24%|‚ñà‚ñà‚ñç       | 781/3257 [00:04<00:14, 173.13it/s] 25%|‚ñà‚ñà‚ñç       | 799/3257 [00:04<00:14, 173.80it/s] 25%|‚ñà‚ñà‚ñå       | 817/3257 [00:04<00:13, 175.47it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:04<00:22, 109.09it/s] 26%|‚ñà‚ñà‚ñå       | 850/3257 [00:04<00:20, 116.10it/s] 27%|‚ñà‚ñà‚ñã       | 869/3257 [00:05<00:18, 131.76it/s] 27%|‚ñà‚ñà‚ñã       | 886/3257 [00:05<00:16, 140.31it/s] 28%|‚ñà‚ñà‚ñä       | 908/3257 [00:05<00:14, 158.36it/s] 29%|‚ñà‚ñà‚ñä       | 929/3257 [00:05<00:13, 171.78it/s] 29%|‚ñà‚ñà‚ñâ       | 948/3257 [00:05<00:13, 170.62it/s] 30%|‚ñà‚ñà‚ñâ       | 968/3257 [00:05<00:12, 178.20it/s] 30%|‚ñà‚ñà‚ñà       | 987/3257 [00:05<00:12, 176.07it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:05<00:12, 173.89it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:05<00:12, 175.97it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1045/3257 [00:05<00:12, 172.99it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1064/3257 [00:06<00:12, 177.40it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1082/3257 [00:06<00:12, 174.44it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1100/3257 [00:06<00:12, 172.46it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1118/3257 [00:06<00:12, 170.50it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1136/3257 [00:06<00:12, 167.37it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1153/3257 [00:06<00:12, 162.61it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1172/3257 [00:06<00:12, 170.17it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1190/3257 [00:06<00:13, 150.32it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1206/3257 [00:06<00:13, 150.78it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1223/3257 [00:07<00:13, 155.18it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1243/3257 [00:07<00:12, 166.57it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1260/3257 [00:07<00:12, 166.27it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1277/3257 [00:07<00:12, 159.22it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1294/3257 [00:07<00:12, 156.55it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1313/3257 [00:07<00:11, 162.93it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1331/3257 [00:07<00:11, 167.43it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:07<00:11, 166.85it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1367/3257 [00:07<00:11, 163.14it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:08<00:11, 157.69it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1403/3257 [00:08<00:11, 165.85it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1424/3257 [00:08<00:10, 176.75it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1443/3257 [00:08<00:10, 179.40it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1465/3257 [00:08<00:09, 190.33it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1485/3257 [00:08<00:09, 184.24it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1508/3257 [00:08<00:08, 195.74it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1528/3257 [00:08<00:09, 173.22it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1546/3257 [00:08<00:10, 167.82it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1565/3257 [00:09<00:09, 172.35it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1583/3257 [00:09<00:09, 172.86it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1604/3257 [00:09<00:09, 180.72it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1623/3257 [00:09<00:09, 178.61it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1641/3257 [00:09<00:09, 174.29it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:09<00:09, 166.83it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1676/3257 [00:09<00:09, 160.46it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1693/3257 [00:09<00:09, 161.51it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1713/3257 [00:09<00:09, 170.64it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:10<00:09, 159.50it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1748/3257 [00:10<00:09, 161.42it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1766/3257 [00:10<00:08, 165.87it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1786/3257 [00:10<00:08, 175.10it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1804/3257 [00:10<00:08, 175.41it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1822/3257 [00:10<00:08, 172.24it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1840/3257 [00:10<00:08, 164.96it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1858/3257 [00:10<00:08, 164.25it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1879/3257 [00:10<00:07, 176.69it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1897/3257 [00:11<00:07, 174.04it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1917/3257 [00:11<00:07, 180.41it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1938/3257 [00:11<00:06, 188.89it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1963/3257 [00:11<00:06, 205.51it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1984/3257 [00:11<00:06, 188.88it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:11<00:06, 190.77it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2024/3257 [00:11<00:06, 190.75it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2044/3257 [00:11<00:06, 182.84it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2063/3257 [00:11<00:07, 167.75it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2083/3257 [00:12<00:06, 171.52it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2101/3257 [00:12<00:06, 166.59it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:12<00:06, 168.10it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2137/3257 [00:12<00:06, 166.44it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2154/3257 [00:12<00:06, 164.60it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2171/3257 [00:12<00:11, 92.05it/s]  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2187/3257 [00:12<00:10, 103.46it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:13<00:08, 119.82it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2223/3257 [00:13<00:07, 130.82it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2239/3257 [00:13<00:07, 134.35it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2259/3257 [00:13<00:06, 150.36it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2276/3257 [00:13<00:06, 145.43it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:13<00:05, 161.58it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2322/3257 [00:13<00:05, 179.29it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2346/3257 [00:13<00:04, 195.07it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2367/3257 [00:13<00:04, 196.97it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2390/3257 [00:14<00:04, 202.95it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2411/3257 [00:14<00:04, 189.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2431/3257 [00:14<00:04, 180.31it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2450/3257 [00:14<00:04, 169.35it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2475/3257 [00:14<00:04, 187.94it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2496/3257 [00:14<00:03, 191.19it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2519/3257 [00:14<00:03, 198.52it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2540/3257 [00:14<00:03, 197.80it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2560/3257 [00:14<00:03, 191.31it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2580/3257 [00:15<00:03, 177.71it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2600/3257 [00:15<00:03, 182.42it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2626/3257 [00:15<00:03, 202.01it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2647/3257 [00:15<00:03, 192.58it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2667/3257 [00:15<00:03, 193.75it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2690/3257 [00:15<00:02, 201.86it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2711/3257 [00:15<00:03, 174.68it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:15<00:02, 182.69it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:15<00:02, 193.23it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2776/3257 [00:16<00:02, 188.29it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2802/3257 [00:16<00:02, 206.38it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2824/3257 [00:16<00:02, 187.55it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:16<00:02, 187.21it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2872/3257 [00:16<00:01, 209.97it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2894/3257 [00:16<00:01, 196.44it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2916/3257 [00:16<00:01, 199.84it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2937/3257 [00:16<00:01, 199.16it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2958/3257 [00:17<00:01, 183.98it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2978/3257 [00:17<00:01, 185.21it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3001/3257 [00:17<00:01, 196.46it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3021/3257 [00:17<00:01, 192.51it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3047/3257 [00:17<00:00, 210.01it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3073/3257 [00:17<00:00, 222.61it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3096/3257 [00:17<00:00, 214.74it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3123/3257 [00:17<00:00, 228.01it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3146/3257 [00:17<00:00, 213.31it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3168/3257 [00:18<00:00, 211.38it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3190/3257 [00:18<00:00, 208.26it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3211/3257 [00:18<00:00, 206.61it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3234/3257 [00:18<00:00, 210.75it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3256/3257 [00:18<00:00, 211.52it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 176.74it/s]
2023-02-07 20:08:25.981 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:08:25,982][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d378,n5,mc3,s0.777162,t4>', 'datetime': '2023-02-07T20:08:25.982627', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:08:25,983][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:08:25,983][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:08:26,502][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 20:08:26,503][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:08:26,586][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 30516 unique words (71.46% of original 42701, drops 12185)', 'datetime': '2023-02-07T20:08:26.586626', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:08:26,587][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 5804704 word corpus (99.69% of original 5822992, drops 18288)', 'datetime': '2023-02-07T20:08:26.587881', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:08:26,690][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 20:08:26,691][gensim.models.word2vec][INFO] - sample=0.777162 downsamples 0 most-common words
[2023-02-07 20:08:26,691][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5804704 word corpus (100.0%% of prior 5804704)', 'datetime': '2023-02-07T20:08:26.691585', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:08:26,868][gensim.models.word2vec][INFO] - estimated required memory for 30516 words and 378 dimensions: 113114368 bytes
[2023-02-07 20:08:26,868][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:08:26,915][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 30516 vocabulary and 378 features, using sg=1 hs=0 sample=0.7771617792003425 negative=5 window=14 shrink_windows=True', 'datetime': '2023-02-07T20:08:26.915328', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:08:27,919][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 22.35% examples, 1276851 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:28,921][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 43.94% examples, 1297041 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:08:29,922][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 66.41% examples, 1304736 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:30,928][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 90.14% examples, 1306862 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:31,336][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5778362 effective words) took 4.4s, 1307931 effective words/s
[2023-02-07 20:08:32,350][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 22.60% examples, 1284637 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:33,355][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 44.43% examples, 1303683 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:34,355][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 66.99% examples, 1311795 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:35,360][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 90.73% examples, 1313908 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:35,728][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5778362 effective words) took 4.4s, 1316990 effective words/s
[2023-02-07 20:08:36,738][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 25.30% examples, 1446040 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:08:37,747][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 54.77% examples, 1603058 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:38,751][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 77.10% examples, 1490571 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:39,759][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 99.51% examples, 1426771 words/s, in_qsize 3, out_qsize 1
[2023-02-07 20:08:39,768][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5778362 effective words) took 4.0s, 1430635 effective words/s
[2023-02-07 20:08:40,782][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 21.92% examples, 1238008 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:41,790][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 42.22% examples, 1245146 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:42,796][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 63.89% examples, 1241240 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:08:43,804][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 91.34% examples, 1317543 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:08:44,081][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5778362 effective words) took 4.3s, 1341004 effective words/s
[2023-02-07 20:08:45,105][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 22.17% examples, 1231451 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:08:46,108][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 43.35% examples, 1270237 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:47,123][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 65.83% examples, 1278390 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:48,128][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 89.71% examples, 1288176 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:48,551][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5778362 effective words) took 4.5s, 1293274 effective words/s
[2023-02-07 20:08:49,553][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 28.71% examples, 1664983 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:50,556][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 52.16% examples, 1537552 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:51,559][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 75.84% examples, 1477719 words/s, in_qsize 6, out_qsize 2
[2023-02-07 20:08:52,518][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5778362 effective words) took 4.0s, 1456924 effective words/s
[2023-02-07 20:08:53,526][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 23.83% examples, 1360160 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:54,527][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 46.91% examples, 1376500 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:55,532][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 69.76% examples, 1370703 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:56,535][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 94.96% examples, 1370455 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:56,732][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5778362 effective words) took 4.2s, 1372031 effective words/s
[2023-02-07 20:08:57,743][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 24.13% examples, 1372699 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:58,749][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 46.98% examples, 1373917 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:08:59,749][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 70.80% examples, 1387245 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:00,751][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 96.28% examples, 1386693 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:09:00,895][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5778362 effective words) took 4.2s, 1388926 effective words/s
[2023-02-07 20:09:01,904][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 24.13% examples, 1375686 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:02,915][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 46.76% examples, 1360767 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:09:03,917][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 69.33% examples, 1354893 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:04,924][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 94.38% examples, 1357975 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:05,142][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5778362 effective words) took 4.2s, 1361411 effective words/s
[2023-02-07 20:09:06,145][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 23.92% examples, 1372344 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:07,146][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 46.91% examples, 1378422 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:08,150][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 70.22% examples, 1379793 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:09,151][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 95.21% examples, 1375537 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:09,335][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5778362 effective words) took 4.2s, 1378927 effective words/s
[2023-02-07 20:09:10,338][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 23.27% examples, 1327471 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:11,341][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 46.33% examples, 1355271 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:12,345][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 69.05% examples, 1354247 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:13,355][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 94.78% examples, 1367090 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:09:13,549][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5778362 effective words) took 4.2s, 1371471 effective words/s
[2023-02-07 20:09:14,558][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 24.65% examples, 1401961 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:15,562][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 47.19% examples, 1385085 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:16,563][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 70.43% examples, 1382914 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:17,564][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 95.79% examples, 1380380 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:17,733][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5778362 effective words) took 4.2s, 1382255 effective words/s
[2023-02-07 20:09:18,743][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 29.23% examples, 1677956 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:09:19,752][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 52.41% examples, 1534375 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:20,763][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 76.94% examples, 1483905 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:21,679][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5778362 effective words) took 3.9s, 1465078 effective words/s
[2023-02-07 20:09:22,682][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 24.13% examples, 1380783 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:09:23,682][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 47.01% examples, 1379138 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:24,688][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 70.43% examples, 1384623 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:25,689][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 96.47% examples, 1391879 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:25,829][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5778362 effective words) took 4.1s, 1392825 effective words/s
[2023-02-07 20:09:26,835][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 29.23% examples, 1686387 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:27,837][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 58.15% examples, 1712931 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:28,839][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 88.73% examples, 1712844 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:09:29,201][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5778362 effective words) took 3.4s, 1714987 effective words/s
[2023-02-07 20:09:29,201][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86675430 effective words) took 62.3s, 1391574 effective words/s', 'datetime': '2023-02-07T20:09:29.201635', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:09:29.202 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:09:33,410][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200759-rjorlhs6/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:09:33.410480', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:09:33,411][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200759-rjorlhs6/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 20:09:33,469][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200759-rjorlhs6/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 20:09:33,518][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:09:33,563][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200759-rjorlhs6/files/../tmp/embedding_model.pt
2023-02-07 20:09:33.563 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:09:35.716 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:09:36.395 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:09:39.978 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.5585570411750553, 'test_mae': 1.1936339827131572, 'test_r2': -2.4103929816149052}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.15
wandb: percentage 0.28536
wandb:   test_mae 1.19363
wandb:   test_mse 2.55856
wandb:    test_r2 -2.41039
wandb: 
wandb: üöÄ View run twilight-sweep-73 at: https://wandb.ai/xiaoqiz/mof2vec/runs/rjorlhs6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_200759-rjorlhs6/logs
wandb: Agent Starting Run: ptw1ga94 with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 933
wandb: 	model.gensim.alpha: 0.015485153922563572
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.4839119497924183
wandb: 	model.gensim.vector_size: 467
wandb: 	model.gensim.window: 6
wandb: 	model.sklearn.learning_rate: 0.8461323723843461
wandb: 	model.sklearn.max_depth: 8
wandb: 	model.sklearn.min_child_weight: 0.05379744979531957
wandb: 	model.sklearn.n_estimators: 911
wandb: 	model.sklearn.num_leaves: 493
wandb: 	model.sklearn.reg_alpha: 0.13186246334107993
wandb: 	model.sklearn.reg_lambda: 0.26357128993169227
wandb: 	model.sklearn.subsample: 0.3529429487185405
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200949-ptw1ga94
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-74
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/ptw1ga94
2023-02-07 20:09:57.623 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 20:09:57.624 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 933 for sweep.
2023-02-07 20:09:57.624 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.015485153922563572 for sweep.
2023-02-07 20:09:57.624 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:09:57.625 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 20:09:57.625 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4839119497924183 for sweep.
2023-02-07 20:09:57.625 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 467 for sweep.
2023-02-07 20:09:57.625 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 6 for sweep.
2023-02-07 20:09:57.626 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.8461323723843461 for sweep.
2023-02-07 20:09:57.626 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 8 for sweep.
2023-02-07 20:09:57.626 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05379744979531957 for sweep.
2023-02-07 20:09:57.626 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 911 for sweep.
2023-02-07 20:09:57.626 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 493 for sweep.
2023-02-07 20:09:57.627 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.13186246334107993 for sweep.
2023-02-07 20:09:57.627 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.26357128993169227 for sweep.
2023-02-07 20:09:57.628 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3529429487185405 for sweep.
2023-02-07 20:09:57.628 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:09:57.638 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200949-ptw1ga94/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 933, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 467, 'window': 6, 'min_count': 3, 'dm': 0, 'sample': 0.4839119497924183, 'workers': 4, 'alpha': 0.015485153922563572, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 911, 'max_depth': 8, 'num_leaves': 493, 'reg_alpha': 0.13186246334107993, 'reg_lambda': 0.26357128993169227, 'subsample': 0.3529429487185405, 'min_child_weight': 0.05379744979531957, 'n_jobs': 4, 'learning_rate': 0.8461323723843461}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:20, 159.44it/s]  1%|          | 36/3257 [00:00<00:18, 172.57it/s]  2%|‚ñè         | 54/3257 [00:00<00:18, 172.59it/s]  2%|‚ñè         | 73/3257 [00:00<00:17, 178.76it/s]  3%|‚ñé         | 92/3257 [00:00<00:17, 180.97it/s]  3%|‚ñé         | 111/3257 [00:00<00:17, 175.02it/s]  4%|‚ñç         | 131/3257 [00:00<00:17, 182.30it/s]  5%|‚ñç         | 152/3257 [00:00<00:16, 189.42it/s]  5%|‚ñå         | 171/3257 [00:00<00:16, 185.15it/s]  6%|‚ñå         | 190/3257 [00:01<00:16, 180.97it/s]  6%|‚ñã         | 209/3257 [00:01<00:17, 170.44it/s]  7%|‚ñã         | 231/3257 [00:01<00:16, 182.80it/s]  8%|‚ñä         | 252/3257 [00:01<00:15, 189.18it/s]  8%|‚ñä         | 272/3257 [00:01<00:15, 187.61it/s]  9%|‚ñâ         | 296/3257 [00:01<00:14, 202.33it/s] 10%|‚ñâ         | 317/3257 [00:01<00:15, 195.84it/s] 10%|‚ñà         | 337/3257 [00:01<00:21, 138.00it/s] 11%|‚ñà         | 359/3257 [00:02<00:18, 153.94it/s] 12%|‚ñà‚ñè        | 377/3257 [00:02<00:18, 153.60it/s] 12%|‚ñà‚ñè        | 397/3257 [00:02<00:17, 163.54it/s] 13%|‚ñà‚ñé        | 418/3257 [00:02<00:16, 175.13it/s] 13%|‚ñà‚ñé        | 437/3257 [00:02<00:17, 158.04it/s] 14%|‚ñà‚ñç        | 459/3257 [00:02<00:16, 172.68it/s] 15%|‚ñà‚ñç        | 479/3257 [00:02<00:16, 172.51it/s] 15%|‚ñà‚ñå        | 498/3257 [00:02<00:15, 176.63it/s] 16%|‚ñà‚ñå        | 518/3257 [00:02<00:14, 182.84it/s] 16%|‚ñà‚ñã        | 537/3257 [00:03<00:14, 184.00it/s] 17%|‚ñà‚ñã        | 557/3257 [00:03<00:14, 186.93it/s] 18%|‚ñà‚ñä        | 576/3257 [00:03<00:16, 165.68it/s] 18%|‚ñà‚ñä        | 599/3257 [00:03<00:14, 181.16it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:14, 179.59it/s] 20%|‚ñà‚ñâ        | 641/3257 [00:03<00:13, 187.06it/s] 20%|‚ñà‚ñà        | 661/3257 [00:03<00:14, 174.50it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:14, 177.41it/s] 22%|‚ñà‚ñà‚ñè       | 703/3257 [00:03<00:14, 181.99it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:04<00:13, 183.20it/s] 23%|‚ñà‚ñà‚ñé       | 741/3257 [00:04<00:14, 174.60it/s] 23%|‚ñà‚ñà‚ñé       | 763/3257 [00:04<00:13, 185.20it/s] 24%|‚ñà‚ñà‚ñç       | 782/3257 [00:04<00:13, 180.89it/s] 25%|‚ñà‚ñà‚ñç       | 801/3257 [00:04<00:13, 179.93it/s] 25%|‚ñà‚ñà‚ñå       | 820/3257 [00:04<00:14, 173.27it/s] 26%|‚ñà‚ñà‚ñå       | 838/3257 [00:04<00:14, 167.66it/s] 26%|‚ñà‚ñà‚ñã       | 855/3257 [00:04<00:14, 164.56it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:04<00:13, 171.17it/s] 27%|‚ñà‚ñà‚ñã       | 892/3257 [00:05<00:13, 169.86it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:05<00:13, 175.79it/s] 29%|‚ñà‚ñà‚ñä       | 933/3257 [00:05<00:13, 177.87it/s] 29%|‚ñà‚ñà‚ñâ       | 953/3257 [00:05<00:12, 182.82it/s] 30%|‚ñà‚ñà‚ñâ       | 972/3257 [00:05<00:12, 179.99it/s] 30%|‚ñà‚ñà‚ñà       | 991/3257 [00:05<00:12, 177.45it/s] 31%|‚ñà‚ñà‚ñà       | 1009/3257 [00:05<00:12, 174.19it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:05<00:12, 175.36it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1045/3257 [00:05<00:12, 172.28it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1065/3257 [00:06<00:12, 177.62it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1083/3257 [00:06<00:12, 176.37it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1103/3257 [00:06<00:11, 180.82it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1122/3257 [00:06<00:11, 182.33it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1141/3257 [00:06<00:12, 175.51it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1160/3257 [00:06<00:11, 179.26it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1178/3257 [00:06<00:12, 170.66it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1196/3257 [00:06<00:12, 166.40it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1213/3257 [00:06<00:12, 160.47it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1237/3257 [00:07<00:11, 181.14it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1256/3257 [00:07<00:11, 176.91it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1274/3257 [00:07<00:11, 176.63it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1292/3257 [00:07<00:11, 165.44it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1312/3257 [00:07<00:11, 174.21it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1332/3257 [00:07<00:10, 180.09it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1351/3257 [00:07<00:10, 179.57it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:07<00:10, 178.35it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1388/3257 [00:07<00:10, 178.15it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1412/3257 [00:08<00:09, 193.73it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:08<00:09, 194.70it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1454/3257 [00:08<00:08, 201.50it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1475/3257 [00:08<00:08, 199.27it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1499/3257 [00:08<00:08, 209.97it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1521/3257 [00:08<00:08, 196.50it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1541/3257 [00:08<00:09, 188.00it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1560/3257 [00:08<00:09, 183.00it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1579/3257 [00:08<00:09, 181.70it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1600/3257 [00:08<00:08, 189.15it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1621/3257 [00:09<00:08, 191.28it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1641/3257 [00:09<00:13, 122.82it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:09<00:11, 133.93it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1676/3257 [00:09<00:11, 141.48it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1696/3257 [00:09<00:10, 155.24it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1716/3257 [00:09<00:09, 164.36it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1734/3257 [00:09<00:09, 154.17it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1755/3257 [00:10<00:08, 168.18it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1776/3257 [00:10<00:08, 178.99it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1797/3257 [00:10<00:07, 185.07it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1817/3257 [00:10<00:07, 183.02it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1836/3257 [00:10<00:07, 183.07it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1855/3257 [00:10<00:07, 183.35it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1876/3257 [00:10<00:07, 187.32it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1895/3257 [00:10<00:07, 181.38it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:10<00:07, 189.34it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1940/3257 [00:10<00:06, 201.83it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:11<00:06, 209.22it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1986/3257 [00:11<00:06, 200.02it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2007/3257 [00:11<00:06, 201.69it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2032/3257 [00:11<00:05, 213.00it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2054/3257 [00:11<00:06, 192.67it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2074/3257 [00:11<00:06, 190.98it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2094/3257 [00:11<00:06, 190.83it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2114/3257 [00:11<00:05, 192.21it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2134/3257 [00:12<00:06, 179.45it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2153/3257 [00:12<00:06, 177.82it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2176/3257 [00:12<00:05, 190.79it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2196/3257 [00:12<00:05, 185.98it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2215/3257 [00:12<00:05, 183.19it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:12<00:05, 189.45it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2256/3257 [00:12<00:05, 185.91it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2275/3257 [00:12<00:05, 175.36it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:12<00:05, 189.02it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2323/3257 [00:12<00:04, 202.24it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2349/3257 [00:13<00:04, 218.16it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2372/3257 [00:13<00:04, 216.43it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2398/3257 [00:13<00:03, 226.17it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2421/3257 [00:13<00:03, 213.90it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2443/3257 [00:13<00:04, 190.44it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2464/3257 [00:13<00:04, 194.69it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2485/3257 [00:13<00:03, 197.25it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2512/3257 [00:13<00:03, 214.62it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2536/3257 [00:13<00:03, 219.31it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2559/3257 [00:14<00:03, 206.04it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2580/3257 [00:14<00:03, 194.84it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2602/3257 [00:14<00:03, 200.55it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:14<00:02, 220.72it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2654/3257 [00:14<00:02, 213.14it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2676/3257 [00:14<00:02, 209.62it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2698/3257 [00:14<00:02, 192.93it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2718/3257 [00:14<00:02, 188.16it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2744/3257 [00:15<00:02, 207.12it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2766/3257 [00:15<00:02, 204.81it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2787/3257 [00:15<00:02, 205.11it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2808/3257 [00:15<00:02, 205.27it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2829/3257 [00:15<00:02, 191.40it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2852/3257 [00:15<00:02, 200.16it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:15<00:01, 221.50it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2904/3257 [00:15<00:01, 202.29it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:15<00:01, 206.81it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:16<00:01, 194.89it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2972/3257 [00:16<00:01, 199.02it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2993/3257 [00:16<00:01, 192.72it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3016/3257 [00:16<00:01, 202.60it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3040/3257 [00:16<00:01, 211.10it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3062/3257 [00:16<00:01, 137.44it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3083/3257 [00:16<00:01, 151.63it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3106/3257 [00:16<00:00, 168.66it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3128/3257 [00:17<00:00, 180.45it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3149/3257 [00:17<00:00, 180.03it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3169/3257 [00:17<00:00, 183.59it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3189/3257 [00:17<00:00, 185.70it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3209/3257 [00:17<00:00, 189.56it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3229/3257 [00:17<00:00, 192.37it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:17<00:00, 197.87it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:17<00:00, 183.69it/s]
2023-02-07 20:10:16.067 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:10:16,068][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d467,n5,mc3,s0.483912,t4>', 'datetime': '2023-02-07T20:10:16.068150', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:10:16,068][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:10:16,068][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:10:16,557][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 20:10:16,557][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:10:16,638][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 30516 unique words (71.46% of original 42701, drops 12185)', 'datetime': '2023-02-07T20:10:16.638070', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:10:16,638][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 5804704 word corpus (99.69% of original 5822992, drops 18288)', 'datetime': '2023-02-07T20:10:16.638433', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:10:16,739][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 20:10:16,740][gensim.models.word2vec][INFO] - sample=0.483912 downsamples 0 most-common words
[2023-02-07 20:10:16,740][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5804704 word corpus (100.0%% of prior 5804704)', 'datetime': '2023-02-07T20:10:16.740848', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:10:16,908][gensim.models.word2vec][INFO] - estimated required memory for 30516 words and 467 dimensions: 136001252 bytes
[2023-02-07 20:10:16,908][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:10:16,970][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 30516 vocabulary and 467 features, using sg=1 hs=0 sample=0.4839119497924183 negative=5 window=6 shrink_windows=True', 'datetime': '2023-02-07T20:10:16.970744', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:10:17,975][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 28.52% examples, 1649420 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:18,976][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 57.88% examples, 1709029 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:19,976][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 89.99% examples, 1743015 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:20,281][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5778362 effective words) took 3.3s, 1747804 effective words/s
[2023-02-07 20:10:21,290][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 26.50% examples, 1521945 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:22,291][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 50.97% examples, 1494528 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:23,294][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 74.64% examples, 1454282 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:10:24,295][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 99.51% examples, 1432864 words/s, in_qsize 3, out_qsize 1
[2023-02-07 20:10:24,299][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5778362 effective words) took 4.0s, 1438394 effective words/s
[2023-02-07 20:10:25,305][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 24.50% examples, 1400539 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:26,306][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 50.17% examples, 1473776 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:27,307][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 74.21% examples, 1446139 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:28,308][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 98.25% examples, 1418012 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:10:28,371][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5778362 effective words) took 4.1s, 1419374 effective words/s
[2023-02-07 20:10:29,376][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 24.72% examples, 1412833 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:30,377][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 48.45% examples, 1428326 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:31,378][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 73.63% examples, 1434337 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:32,380][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 99.97% examples, 1439875 words/s, in_qsize 1, out_qsize 1
[2023-02-07 20:10:32,382][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5778362 effective words) took 4.0s, 1441426 effective words/s
[2023-02-07 20:10:33,389][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 27.11% examples, 1565696 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:34,391][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 54.53% examples, 1606519 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:35,394][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 83.57% examples, 1622928 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:35,954][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5778362 effective words) took 3.6s, 1618431 effective words/s
[2023-02-07 20:10:36,960][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 28.34% examples, 1637648 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:37,964][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 55.27% examples, 1630513 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:38,965][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 84.43% examples, 1636671 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:39,503][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5778362 effective words) took 3.5s, 1629904 effective words/s
[2023-02-07 20:10:40,512][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 34.33% examples, 1997662 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:41,518][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 67.79% examples, 1988117 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:42,522][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 97.21% examples, 1866137 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:42,620][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5778362 effective words) took 3.1s, 1856420 effective words/s
[2023-02-07 20:10:43,629][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 29.26% examples, 1685141 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:44,636][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 57.88% examples, 1701058 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:45,635][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 86.95% examples, 1684585 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:46,064][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5778362 effective words) took 3.4s, 1679921 effective words/s
[2023-02-07 20:10:47,072][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 28.40% examples, 1636287 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:48,077][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 55.57% examples, 1634729 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:49,082][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 84.03% examples, 1626691 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:49,600][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5778362 effective words) took 3.5s, 1634654 effective words/s
[2023-02-07 20:10:50,604][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 27.11% examples, 1573481 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:51,613][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 54.04% examples, 1590386 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:52,613][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 86.61% examples, 1678975 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:10:52,977][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5778362 effective words) took 3.4s, 1712891 effective words/s
[2023-02-07 20:10:53,989][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 27.57% examples, 1587139 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:10:54,993][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 55.79% examples, 1637556 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:55,997][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 84.83% examples, 1639726 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:10:56,500][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5778362 effective words) took 3.5s, 1641181 effective words/s
[2023-02-07 20:10:57,502][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 33.47% examples, 1953699 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:58,505][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 67.67% examples, 1990428 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:10:59,399][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5778362 effective words) took 2.9s, 1994416 effective words/s
[2023-02-07 20:11:00,402][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 30.40% examples, 1763376 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:11:01,402][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 56.74% examples, 1677021 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:11:02,403][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 84.83% examples, 1648480 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:11:02,919][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5778362 effective words) took 3.5s, 1642533 effective words/s
[2023-02-07 20:11:03,927][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 28.06% examples, 1625689 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:11:04,930][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 55.27% examples, 1625671 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:11:05,932][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 83.27% examples, 1617880 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:11:06,503][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5778362 effective words) took 3.6s, 1614307 effective words/s
[2023-02-07 20:11:07,508][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 30.92% examples, 1796383 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:11:08,508][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 57.23% examples, 1686360 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:11:09,523][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 84.03% examples, 1623730 words/s, in_qsize 3, out_qsize 1
[2023-02-07 20:11:10,062][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5778362 effective words) took 3.6s, 1624348 effective words/s
[2023-02-07 20:11:10,063][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86675430 effective words) took 53.1s, 1632575 effective words/s', 'datetime': '2023-02-07T20:11:10.063259', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:11:10.063 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:11:15,199][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200949-ptw1ga94/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:11:15.199337', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:11:15,200][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200949-ptw1ga94/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 20:11:15,256][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200949-ptw1ga94/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 20:11:15,311][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:11:15,348][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_200949-ptw1ga94/files/../tmp/embedding_model.pt
2023-02-07 20:11:15.348 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:11:17.654 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:11:18.425 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:11:21.778 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.049718317643826, 'test_mae': 1.0559832570710836, 'test_r2': -1.6773138616651195}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.8
wandb: percentage 0.28536
wandb:   test_mae 1.05598
wandb:   test_mse 2.04972
wandb:    test_r2 -1.67731
wandb: 
wandb: üöÄ View run earthy-sweep-74 at: https://wandb.ai/xiaoqiz/mof2vec/runs/ptw1ga94
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_200949-ptw1ga94/logs
wandb: Agent Starting Run: tlee1eox with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 669
wandb: 	model.gensim.alpha: 0.05704180885232819
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.4985720369540982
wandb: 	model.gensim.vector_size: 397
wandb: 	model.gensim.window: 7
wandb: 	model.sklearn.learning_rate: 0.2662162878202157
wandb: 	model.sklearn.max_depth: 46
wandb: 	model.sklearn.min_child_weight: 0.03211808505822338
wandb: 	model.sklearn.n_estimators: 320
wandb: 	model.sklearn.num_leaves: 379
wandb: 	model.sklearn.reg_alpha: 0.037095386333457624
wandb: 	model.sklearn.reg_lambda: 0.14158063264514997
wandb: 	model.sklearn.subsample: 0.32622675053847694
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201135-tlee1eox
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-75
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/tlee1eox
2023-02-07 20:11:43.049 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 20:11:43.050 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 669 for sweep.
2023-02-07 20:11:43.050 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.05704180885232819 for sweep.
2023-02-07 20:11:43.050 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:11:43.050 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 20:11:43.051 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4985720369540982 for sweep.
2023-02-07 20:11:43.051 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 397 for sweep.
2023-02-07 20:11:43.051 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 7 for sweep.
2023-02-07 20:11:43.051 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.2662162878202157 for sweep.
2023-02-07 20:11:43.052 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 46 for sweep.
2023-02-07 20:11:43.052 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03211808505822338 for sweep.
2023-02-07 20:11:43.052 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 320 for sweep.
2023-02-07 20:11:43.052 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 379 for sweep.
2023-02-07 20:11:43.053 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.037095386333457624 for sweep.
2023-02-07 20:11:43.053 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.14158063264514997 for sweep.
2023-02-07 20:11:43.053 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.32622675053847694 for sweep.
2023-02-07 20:11:43.053 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:11:43.061 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201135-tlee1eox/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 669, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 397, 'window': 7, 'min_count': 2, 'dm': 0, 'sample': 0.4985720369540982, 'workers': 4, 'alpha': 0.05704180885232819, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 320, 'max_depth': 46, 'num_leaves': 379, 'reg_alpha': 0.037095386333457624, 'reg_lambda': 0.14158063264514997, 'subsample': 0.32622675053847694, 'min_child_weight': 0.03211808505822338, 'n_jobs': 4, 'learning_rate': 0.2662162878202157}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 15/3257 [00:00<00:21, 147.53it/s]  1%|          | 34/3257 [00:00<00:19, 169.29it/s]  2%|‚ñè         | 53/3257 [00:00<00:18, 170.25it/s]  2%|‚ñè         | 73/3257 [00:00<00:17, 181.22it/s]  3%|‚ñé         | 93/3257 [00:00<00:17, 185.83it/s]  3%|‚ñé         | 112/3257 [00:00<00:18, 172.82it/s]  4%|‚ñç         | 131/3257 [00:00<00:17, 176.81it/s]  5%|‚ñç         | 153/3257 [00:00<00:16, 188.85it/s]  5%|‚ñå         | 173/3257 [00:00<00:17, 177.93it/s]  6%|‚ñå         | 194/3257 [00:01<00:16, 184.88it/s]  7%|‚ñã         | 215/3257 [00:01<00:15, 190.28it/s]  7%|‚ñã         | 237/3257 [00:01<00:15, 191.48it/s]  8%|‚ñä         | 257/3257 [00:01<00:15, 192.13it/s]  9%|‚ñä         | 279/3257 [00:01<00:14, 198.98it/s]  9%|‚ñâ         | 300/3257 [00:01<00:15, 195.93it/s] 10%|‚ñâ         | 321/3257 [00:01<00:14, 198.56it/s] 10%|‚ñà         | 341/3257 [00:01<00:15, 191.08it/s] 11%|‚ñà         | 361/3257 [00:01<00:15, 191.79it/s] 12%|‚ñà‚ñè        | 381/3257 [00:02<00:16, 179.68it/s] 12%|‚ñà‚ñè        | 400/3257 [00:02<00:15, 179.33it/s] 13%|‚ñà‚ñé        | 420/3257 [00:02<00:15, 181.12it/s] 13%|‚ñà‚ñé        | 439/3257 [00:02<00:17, 160.01it/s] 14%|‚ñà‚ñç        | 459/3257 [00:02<00:16, 168.96it/s] 15%|‚ñà‚ñç        | 478/3257 [00:02<00:15, 174.20it/s] 15%|‚ñà‚ñå        | 498/3257 [00:02<00:15, 179.19it/s] 16%|‚ñà‚ñå        | 519/3257 [00:02<00:14, 187.63it/s] 17%|‚ñà‚ñã        | 539/3257 [00:02<00:14, 183.40it/s] 17%|‚ñà‚ñã        | 558/3257 [00:03<00:15, 173.34it/s] 18%|‚ñà‚ñä        | 576/3257 [00:03<00:16, 164.02it/s] 18%|‚ñà‚ñä        | 598/3257 [00:03<00:14, 178.33it/s] 19%|‚ñà‚ñâ        | 618/3257 [00:03<00:14, 183.84it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:03<00:14, 177.40it/s] 20%|‚ñà‚ñà        | 655/3257 [00:03<00:15, 165.47it/s] 21%|‚ñà‚ñà        | 674/3257 [00:03<00:15, 170.34it/s] 21%|‚ñà‚ñà        | 692/3257 [00:03<00:15, 168.04it/s] 22%|‚ñà‚ñà‚ñè       | 712/3257 [00:03<00:14, 176.56it/s] 22%|‚ñà‚ñà‚ñè       | 730/3257 [00:04<00:15, 164.68it/s] 23%|‚ñà‚ñà‚ñé       | 747/3257 [00:04<00:15, 165.25it/s] 24%|‚ñà‚ñà‚ñé       | 769/3257 [00:04<00:13, 178.39it/s] 24%|‚ñà‚ñà‚ñç       | 788/3257 [00:04<00:14, 167.38it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:04<00:14, 169.70it/s] 25%|‚ñà‚ñà‚ñå       | 824/3257 [00:04<00:14, 170.32it/s] 26%|‚ñà‚ñà‚ñå       | 842/3257 [00:04<00:14, 164.48it/s] 26%|‚ñà‚ñà‚ñã       | 860/3257 [00:04<00:14, 167.56it/s] 27%|‚ñà‚ñà‚ñã       | 877/3257 [00:04<00:14, 159.80it/s] 28%|‚ñà‚ñà‚ñä       | 896/3257 [00:05<00:14, 168.00it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:05<00:14, 167.06it/s] 29%|‚ñà‚ñà‚ñä       | 933/3257 [00:05<00:13, 172.02it/s] 29%|‚ñà‚ñà‚ñâ       | 953/3257 [00:05<00:12, 177.58it/s] 30%|‚ñà‚ñà‚ñâ       | 971/3257 [00:05<00:12, 176.66it/s] 30%|‚ñà‚ñà‚ñà       | 989/3257 [00:05<00:13, 171.28it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:05<00:13, 163.08it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1026/3257 [00:05<00:13, 170.03it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1044/3257 [00:05<00:14, 157.81it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1062/3257 [00:06<00:13, 162.96it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:06<00:13, 165.43it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1097/3257 [00:06<00:20, 103.19it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1116/3257 [00:06<00:17, 120.25it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1131/3257 [00:06<00:17, 123.93it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:06<00:16, 131.74it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1168/3257 [00:06<00:13, 150.27it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1185/3257 [00:07<00:14, 143.26it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1201/3257 [00:07<00:15, 137.01it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1217/3257 [00:07<00:14, 142.31it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1238/3257 [00:07<00:12, 159.27it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1255/3257 [00:07<00:12, 157.86it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1272/3257 [00:07<00:12, 158.66it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1289/3257 [00:07<00:13, 147.88it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1307/3257 [00:07<00:12, 156.05it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1325/3257 [00:07<00:12, 160.79it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1342/3257 [00:08<00:11, 161.57it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1359/3257 [00:08<00:11, 160.10it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1376/3257 [00:08<00:11, 158.40it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1392/3257 [00:08<00:11, 157.52it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1414/3257 [00:08<00:10, 174.85it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1433/3257 [00:08<00:10, 178.10it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1452/3257 [00:08<00:09, 180.68it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1473/3257 [00:08<00:09, 187.43it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1493/3257 [00:08<00:09, 189.70it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1514/3257 [00:08<00:09, 193.12it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1534/3257 [00:09<00:09, 178.49it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1553/3257 [00:09<00:09, 175.81it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1574/3257 [00:09<00:09, 181.64it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1593/3257 [00:09<00:09, 183.94it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:09<00:08, 192.05it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1635/3257 [00:09<00:08, 180.69it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1654/3257 [00:09<00:09, 177.92it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1672/3257 [00:09<00:09, 169.30it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1690/3257 [00:09<00:09, 169.88it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1709/3257 [00:10<00:08, 175.03it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:10<00:08, 170.31it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1746/3257 [00:10<00:09, 166.84it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1766/3257 [00:10<00:08, 173.86it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1787/3257 [00:10<00:08, 182.93it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1806/3257 [00:10<00:08, 175.36it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1826/3257 [00:10<00:08, 177.58it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1846/3257 [00:10<00:07, 179.24it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:10<00:07, 186.87it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1888/3257 [00:11<00:07, 186.36it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1910/3257 [00:11<00:06, 193.80it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1930/3257 [00:11<00:07, 186.90it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1958/3257 [00:11<00:06, 212.81it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1980/3257 [00:11<00:06, 203.12it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2003/3257 [00:11<00:05, 209.95it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2025/3257 [00:11<00:05, 207.82it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2046/3257 [00:11<00:05, 202.49it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2067/3257 [00:11<00:06, 184.50it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2089/3257 [00:12<00:06, 193.14it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2109/3257 [00:12<00:05, 192.06it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2129/3257 [00:12<00:06, 177.45it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2148/3257 [00:12<00:06, 175.36it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:12<00:05, 183.07it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2188/3257 [00:12<00:05, 183.23it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2208/3257 [00:12<00:05, 186.05it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2227/3257 [00:12<00:05, 185.08it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2246/3257 [00:12<00:05, 181.25it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2266/3257 [00:13<00:05, 183.93it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2285/3257 [00:13<00:09, 106.18it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2302/3257 [00:13<00:08, 118.13it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2327/3257 [00:13<00:06, 146.15it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2353/3257 [00:13<00:05, 172.49it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2374/3257 [00:13<00:04, 178.41it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2399/3257 [00:13<00:04, 194.84it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2421/3257 [00:14<00:04, 192.35it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2442/3257 [00:14<00:04, 181.72it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2464/3257 [00:14<00:04, 190.79it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2484/3257 [00:14<00:04, 192.42it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2509/3257 [00:14<00:03, 208.06it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2532/3257 [00:14<00:03, 214.03it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2554/3257 [00:14<00:03, 201.41it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2575/3257 [00:14<00:03, 189.90it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2595/3257 [00:14<00:03, 184.49it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2622/3257 [00:15<00:03, 206.01it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2643/3257 [00:15<00:03, 204.48it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2664/3257 [00:15<00:02, 198.19it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2685/3257 [00:15<00:02, 200.88it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2706/3257 [00:15<00:03, 180.55it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2725/3257 [00:15<00:02, 180.14it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2749/3257 [00:15<00:02, 194.75it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2769/3257 [00:15<00:02, 186.44it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2793/3257 [00:15<00:02, 199.82it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2814/3257 [00:16<00:02, 199.06it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2835/3257 [00:16<00:02, 188.51it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2858/3257 [00:16<00:01, 199.76it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2885/3257 [00:16<00:01, 218.58it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2908/3257 [00:16<00:01, 195.02it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2929/3257 [00:16<00:01, 195.94it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:16<00:01, 188.36it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2972/3257 [00:16<00:01, 194.41it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2992/3257 [00:16<00:01, 186.44it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3016/3257 [00:17<00:01, 195.78it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3039/3257 [00:17<00:01, 202.34it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3064/3257 [00:17<00:00, 215.19it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3086/3257 [00:17<00:00, 210.13it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3109/3257 [00:17<00:00, 212.60it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3131/3257 [00:17<00:00, 210.51it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3153/3257 [00:17<00:00, 200.83it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3175/3257 [00:17<00:00, 204.08it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3196/3257 [00:17<00:00, 202.72it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3217/3257 [00:18<00:00, 189.12it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:18<00:00, 204.81it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 178.35it/s]
2023-02-07 20:12:02.102 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:12:02,104][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d397,n5,mc2,s0.498572,t4>', 'datetime': '2023-02-07T20:12:02.103998', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:12:02,104][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:12:02,105][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:12:02,608][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 20:12:02,609][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:12:02,700][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 36619 unique words (85.76% of original 42701, drops 6082)', 'datetime': '2023-02-07T20:12:02.700465', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:12:02,700][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 5816910 word corpus (99.90% of original 5822992, drops 6082)', 'datetime': '2023-02-07T20:12:02.700868', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:12:02,821][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 20:12:02,822][gensim.models.word2vec][INFO] - sample=0.498572 downsamples 0 most-common words
[2023-02-07 20:12:02,822][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5816910 word corpus (100.0%% of prior 5816910)', 'datetime': '2023-02-07T20:12:02.822938', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:12:03,028][gensim.models.word2vec][INFO] - estimated required memory for 36619 words and 397 dimensions: 140434960 bytes
[2023-02-07 20:12:03,029][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:12:03,087][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 36619 vocabulary and 397 features, using sg=1 hs=0 sample=0.4985720369540982 negative=5 window=7 shrink_windows=True', 'datetime': '2023-02-07T20:12:03.087795', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:12:04,093][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 25.30% examples, 1458335 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:05,103][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 50.60% examples, 1485463 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:06,108][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 76.70% examples, 1489432 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:06,970][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5790320 effective words) took 3.9s, 1492215 effective words/s
[2023-02-07 20:12:07,975][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 36.44% examples, 2142152 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:08,978][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 65.09% examples, 1915029 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:12:09,983][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 95.43% examples, 1837299 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:10,131][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5790320 effective words) took 3.2s, 1832446 effective words/s
[2023-02-07 20:12:11,144][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 30.03% examples, 1721985 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:12,146][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 58.61% examples, 1725148 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:13,149][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 88.79% examples, 1715205 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:13,500][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5790320 effective words) took 3.4s, 1719831 effective words/s
[2023-02-07 20:12:14,504][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 38.44% examples, 2276189 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:15,507][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 77.86% examples, 2270536 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:12:16,076][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5790320 effective words) took 2.6s, 2250996 effective words/s
[2023-02-07 20:12:17,081][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 36.17% examples, 2115336 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:18,084][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 73.81% examples, 2157978 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:18,775][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5790320 effective words) took 2.7s, 2146956 effective words/s
[2023-02-07 20:12:19,790][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 28.89% examples, 1656542 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:12:20,792][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 56.46% examples, 1661304 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:21,792][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 87.07% examples, 1689248 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:22,153][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5790320 effective words) took 3.4s, 1715215 effective words/s
[2023-02-07 20:12:23,159][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 28.89% examples, 1675080 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:12:24,164][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 56.89% examples, 1678575 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:12:25,165][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 86.58% examples, 1680008 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:25,610][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5790320 effective words) took 3.5s, 1676884 effective words/s
[2023-02-07 20:12:26,615][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 30.55% examples, 1774723 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:12:27,618][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 62.54% examples, 1831903 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:28,624][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 93.74% examples, 1811618 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:28,804][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5790320 effective words) took 3.2s, 1815588 effective words/s
[2023-02-07 20:12:29,818][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 30.03% examples, 1719125 words/s, in_qsize 5, out_qsize 2
[2023-02-07 20:12:30,821][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 60.58% examples, 1767404 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:31,821][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 91.62% examples, 1773058 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:32,069][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5790320 effective words) took 3.3s, 1774401 effective words/s
[2023-02-07 20:12:33,076][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 30.73% examples, 1787035 words/s, in_qsize 6, out_qsize 0
[2023-02-07 20:12:34,078][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 62.36% examples, 1820993 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:35,085][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 93.77% examples, 1808307 words/s, in_qsize 6, out_qsize 2
[2023-02-07 20:12:35,258][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5790320 effective words) took 3.2s, 1816502 effective words/s
[2023-02-07 20:12:36,263][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 30.92% examples, 1799972 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:37,266][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 62.24% examples, 1823105 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:38,273][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 94.53% examples, 1821476 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:38,431][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5790320 effective words) took 3.2s, 1826182 effective words/s
[2023-02-07 20:12:39,439][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 36.48% examples, 2141400 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:40,443][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 67.55% examples, 1986207 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:41,427][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5790320 effective words) took 3.0s, 1934005 effective words/s
[2023-02-07 20:12:42,429][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 31.65% examples, 1840246 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:43,434][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 63.09% examples, 1853141 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:44,438][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 94.96% examples, 1832454 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:44,594][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5790320 effective words) took 3.2s, 1829715 effective words/s
[2023-02-07 20:12:45,596][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 30.33% examples, 1763062 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:46,598][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 59.69% examples, 1757984 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:47,602][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 90.76% examples, 1761442 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:12:47,874][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5790320 effective words) took 3.3s, 1765793 effective words/s
[2023-02-07 20:12:48,877][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 30.55% examples, 1772660 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:49,882][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 61.13% examples, 1794665 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:50,885][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 92.20% examples, 1786073 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:12:51,122][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5790320 effective words) took 3.2s, 1783783 effective words/s
[2023-02-07 20:12:51,123][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86854800 effective words) took 48.0s, 1808153 effective words/s', 'datetime': '2023-02-07T20:12:51.123316', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:12:51.123 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:12:54,773][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201135-tlee1eox/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:12:54.773451', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:12:54,776][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201135-tlee1eox/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 20:12:54,881][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201135-tlee1eox/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 20:12:54,935][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:12:54,976][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201135-tlee1eox/files/../tmp/embedding_model.pt
2023-02-07 20:12:54.977 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:12:56.978 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:12:57.671 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:13:16.366 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2418418864604197, 'test_mae': 1.0868962133043314, 'test_r2': -2.4039710718266103}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.95
wandb: percentage 0.14243
wandb:   test_mae 1.0869
wandb:   test_mse 2.24184
wandb:    test_r2 -2.40397
wandb: 
wandb: üöÄ View run zesty-sweep-75 at: https://wandb.ai/xiaoqiz/mof2vec/runs/tlee1eox
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_201135-tlee1eox/logs
wandb: Agent Starting Run: f9gidpyb with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 865
wandb: 	model.gensim.alpha: 0.04548899853727606
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.5800048670498017
wandb: 	model.gensim.vector_size: 342
wandb: 	model.gensim.window: 12
wandb: 	model.sklearn.learning_rate: 0.0012629246717692547
wandb: 	model.sklearn.max_depth: 17
wandb: 	model.sklearn.min_child_weight: 0.0837342304677558
wandb: 	model.sklearn.n_estimators: 2668
wandb: 	model.sklearn.num_leaves: 354
wandb: 	model.sklearn.reg_alpha: 0.7408005685992803
wandb: 	model.sklearn.reg_lambda: 0.1938628786691731
wandb: 	model.sklearn.subsample: 0.5172687334816457
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201326-f9gidpyb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-sweep-76
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/f9gidpyb
2023-02-07 20:13:34.717 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 20:13:34.718 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 865 for sweep.
2023-02-07 20:13:34.718 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.04548899853727606 for sweep.
2023-02-07 20:13:34.718 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:13:34.718 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 20:13:34.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5800048670498017 for sweep.
2023-02-07 20:13:34.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 342 for sweep.
2023-02-07 20:13:34.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 12 for sweep.
2023-02-07 20:13:34.720 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0012629246717692547 for sweep.
2023-02-07 20:13:34.720 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 17 for sweep.
2023-02-07 20:13:34.720 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0837342304677558 for sweep.
2023-02-07 20:13:34.720 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2668 for sweep.
2023-02-07 20:13:34.721 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 354 for sweep.
2023-02-07 20:13:34.721 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.7408005685992803 for sweep.
2023-02-07 20:13:34.721 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.1938628786691731 for sweep.
2023-02-07 20:13:34.721 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5172687334816457 for sweep.
2023-02-07 20:13:34.721 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:13:34.729 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201326-f9gidpyb/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 865, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 342, 'window': 12, 'min_count': 3, 'dm': 0, 'sample': 0.5800048670498017, 'workers': 4, 'alpha': 0.04548899853727606, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2668, 'max_depth': 17, 'num_leaves': 354, 'reg_alpha': 0.7408005685992803, 'reg_lambda': 0.1938628786691731, 'subsample': 0.5172687334816457, 'min_child_weight': 0.0837342304677558, 'n_jobs': 4, 'learning_rate': 0.0012629246717692547}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 19/3257 [00:00<00:17, 188.33it/s]  1%|‚ñè         | 41/3257 [00:00<00:16, 194.78it/s]  2%|‚ñè         | 62/3257 [00:00<00:15, 200.14it/s]  3%|‚ñé         | 86/3257 [00:00<00:14, 215.00it/s]  3%|‚ñé         | 108/3257 [00:00<00:15, 201.20it/s]  4%|‚ñç         | 129/3257 [00:00<00:15, 202.43it/s]  5%|‚ñç         | 152/3257 [00:00<00:14, 209.28it/s]  5%|‚ñå         | 174/3257 [00:00<00:15, 201.28it/s]  6%|‚ñå         | 197/3257 [00:00<00:14, 206.81it/s]  7%|‚ñã         | 220/3257 [00:01<00:14, 212.62it/s]  7%|‚ñã         | 244/3257 [00:01<00:13, 218.71it/s]  8%|‚ñä         | 266/3257 [00:01<00:14, 212.32it/s]  9%|‚ñâ         | 292/3257 [00:01<00:13, 225.74it/s] 10%|‚ñâ         | 315/3257 [00:01<00:13, 218.07it/s] 10%|‚ñà         | 337/3257 [00:01<00:13, 217.24it/s] 11%|‚ñà         | 360/3257 [00:01<00:13, 220.73it/s] 12%|‚ñà‚ñè        | 383/3257 [00:01<00:13, 208.84it/s] 12%|‚ñà‚ñè        | 405/3257 [00:01<00:13, 205.50it/s] 13%|‚ñà‚ñé        | 426/3257 [00:02<00:21, 131.03it/s] 14%|‚ñà‚ñé        | 446/3257 [00:02<00:19, 144.78it/s] 14%|‚ñà‚ñç        | 469/3257 [00:02<00:17, 163.06it/s] 15%|‚ñà‚ñå        | 489/3257 [00:02<00:16, 170.13it/s] 16%|‚ñà‚ñå        | 512/3257 [00:02<00:14, 183.89it/s] 16%|‚ñà‚ñã        | 533/3257 [00:02<00:14, 183.34it/s] 17%|‚ñà‚ñã        | 556/3257 [00:02<00:13, 193.39it/s] 18%|‚ñà‚ñä        | 577/3257 [00:03<00:15, 174.05it/s] 18%|‚ñà‚ñä        | 600/3257 [00:03<00:14, 186.61it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:14, 187.69it/s] 20%|‚ñà‚ñâ        | 643/3257 [00:03<00:13, 198.12it/s] 20%|‚ñà‚ñà        | 664/3257 [00:03<00:13, 187.26it/s] 21%|‚ñà‚ñà        | 684/3257 [00:03<00:13, 189.84it/s] 22%|‚ñà‚ñà‚ñè       | 704/3257 [00:03<00:13, 189.24it/s] 22%|‚ñà‚ñà‚ñè       | 724/3257 [00:03<00:13, 184.22it/s] 23%|‚ñà‚ñà‚ñé       | 743/3257 [00:03<00:13, 183.39it/s] 23%|‚ñà‚ñà‚ñé       | 765/3257 [00:03<00:12, 192.11it/s] 24%|‚ñà‚ñà‚ñç       | 785/3257 [00:04<00:13, 181.58it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:04<00:13, 185.88it/s] 25%|‚ñà‚ñà‚ñå       | 825/3257 [00:04<00:13, 181.85it/s] 26%|‚ñà‚ñà‚ñå       | 844/3257 [00:04<00:13, 175.34it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:04<00:13, 177.38it/s] 27%|‚ñà‚ñà‚ñã       | 883/3257 [00:04<00:13, 180.66it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:04<00:12, 190.57it/s] 28%|‚ñà‚ñà‚ñä       | 927/3257 [00:04<00:11, 198.82it/s] 29%|‚ñà‚ñà‚ñâ       | 947/3257 [00:04<00:11, 197.83it/s] 30%|‚ñà‚ñà‚ñâ       | 969/3257 [00:05<00:11, 203.42it/s] 30%|‚ñà‚ñà‚ñà       | 990/3257 [00:05<00:11, 195.43it/s] 31%|‚ñà‚ñà‚ñà       | 1010/3257 [00:05<00:12, 186.83it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1030/3257 [00:05<00:11, 187.89it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1049/3257 [00:05<00:11, 185.98it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1071/3257 [00:05<00:11, 195.50it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:05<00:11, 189.84it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1115/3257 [00:05<00:10, 200.15it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1136/3257 [00:05<00:10, 195.47it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1156/3257 [00:06<00:11, 190.38it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1176/3257 [00:06<00:10, 190.81it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1196/3257 [00:06<00:11, 174.83it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1214/3257 [00:06<00:11, 170.65it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1238/3257 [00:06<00:10, 187.03it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1257/3257 [00:06<00:10, 182.49it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1276/3257 [00:06<00:10, 181.77it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1295/3257 [00:06<00:11, 177.16it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1317/3257 [00:06<00:10, 188.26it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:07<00:09, 192.76it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1359/3257 [00:07<00:10, 188.61it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1378/3257 [00:07<00:10, 185.67it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1398/3257 [00:07<00:09, 187.54it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:07<00:09, 198.03it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1443/3257 [00:07<00:09, 196.37it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1467/3257 [00:07<00:08, 207.39it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1490/3257 [00:07<00:08, 213.57it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:07<00:08, 217.95it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1535/3257 [00:08<00:08, 198.52it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1556/3257 [00:08<00:13, 130.79it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1576/3257 [00:08<00:11, 143.85it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1599/3257 [00:08<00:10, 161.91it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1620/3257 [00:08<00:09, 172.75it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1640/3257 [00:08<00:09, 177.86it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1660/3257 [00:08<00:08, 177.51it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1679/3257 [00:08<00:08, 175.87it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1701/3257 [00:09<00:08, 185.74it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1723/3257 [00:09<00:07, 191.88it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1743/3257 [00:09<00:08, 177.37it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1765/3257 [00:09<00:07, 188.24it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1788/3257 [00:09<00:07, 196.42it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1808/3257 [00:09<00:07, 185.02it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1828/3257 [00:09<00:07, 187.43it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1850/3257 [00:09<00:07, 193.95it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1875/3257 [00:09<00:06, 208.18it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1897/3257 [00:10<00:06, 202.11it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1918/3257 [00:10<00:06, 198.10it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1943/3257 [00:10<00:06, 212.04it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1966/3257 [00:10<00:05, 215.26it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1988/3257 [00:10<00:06, 206.59it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2009/3257 [00:10<00:06, 207.01it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2032/3257 [00:10<00:05, 211.58it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2054/3257 [00:10<00:06, 190.12it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2074/3257 [00:10<00:06, 190.02it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2094/3257 [00:11<00:06, 189.87it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2114/3257 [00:11<00:06, 188.63it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2134/3257 [00:11<00:06, 174.37it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2152/3257 [00:11<00:06, 172.90it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2172/3257 [00:11<00:06, 178.98it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2191/3257 [00:11<00:05, 182.04it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2210/3257 [00:11<00:05, 178.75it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2233/3257 [00:11<00:05, 190.84it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2253/3257 [00:11<00:05, 188.29it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2272/3257 [00:12<00:05, 182.94it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2296/3257 [00:12<00:04, 197.02it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2316/3257 [00:12<00:04, 197.63it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2342/3257 [00:12<00:04, 214.16it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2367/3257 [00:12<00:04, 218.53it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2392/3257 [00:12<00:03, 226.70it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2415/3257 [00:12<00:04, 207.52it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2437/3257 [00:12<00:04, 199.80it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2459/3257 [00:12<00:03, 204.24it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2482/3257 [00:13<00:03, 209.73it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2508/3257 [00:13<00:03, 221.38it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2533/3257 [00:13<00:03, 227.92it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2556/3257 [00:13<00:03, 209.14it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2578/3257 [00:13<00:03, 198.47it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2599/3257 [00:13<00:03, 199.44it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2629/3257 [00:13<00:02, 226.38it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2653/3257 [00:13<00:02, 219.89it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2676/3257 [00:13<00:02, 221.94it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2699/3257 [00:14<00:02, 207.39it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2721/3257 [00:14<00:02, 205.59it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2749/3257 [00:14<00:02, 226.02it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2772/3257 [00:14<00:02, 208.57it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2799/3257 [00:14<00:02, 224.10it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2822/3257 [00:14<00:01, 219.76it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2845/3257 [00:14<00:01, 212.94it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2873/3257 [00:14<00:01, 230.62it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2897/3257 [00:14<00:01, 220.66it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2920/3257 [00:15<00:01, 222.86it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2943/3257 [00:15<00:01, 210.20it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2967/3257 [00:15<00:01, 215.06it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2989/3257 [00:15<00:01, 202.10it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3014/3257 [00:15<00:01, 212.90it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3036/3257 [00:15<00:01, 129.30it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3063/3257 [00:15<00:01, 155.16it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3086/3257 [00:16<00:01, 170.72it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3112/3257 [00:16<00:00, 190.89it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3135/3257 [00:16<00:00, 198.38it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3158/3257 [00:16<00:00, 202.89it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3180/3257 [00:16<00:00, 198.16it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3206/3257 [00:16<00:00, 213.02it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3229/3257 [00:16<00:00, 210.69it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3254/3257 [00:16<00:00, 219.95it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 193.98it/s]
2023-02-07 20:13:52.212 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:13:52,213][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d342,n5,mc3,s0.580005,t4>', 'datetime': '2023-02-07T20:13:52.213278', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:13:52,213][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:13:52,213][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:13:52,647][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 20:13:52,647][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:13:52,703][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 22723 unique words (71.45% of original 31803, drops 9080)', 'datetime': '2023-02-07T20:13:52.703733', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:13:52,705][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 5081575 word corpus (99.73% of original 5095118, drops 13543)', 'datetime': '2023-02-07T20:13:52.705169', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:13:52,776][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 20:13:52,777][gensim.models.word2vec][INFO] - sample=0.580005 downsamples 0 most-common words
[2023-02-07 20:13:52,778][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5081575 word corpus (100.0%% of prior 5081575)', 'datetime': '2023-02-07T20:13:52.778050', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:13:52,897][gensim.models.word2vec][INFO] - estimated required memory for 22723 words and 342 dimensions: 78638604 bytes
[2023-02-07 20:13:52,898][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:13:52,933][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 22723 vocabulary and 342 features, using sg=1 hs=0 sample=0.5800048670498017 negative=5 window=12 shrink_windows=True', 'datetime': '2023-02-07T20:13:52.933322', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:13:53,941][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 33.77% examples, 1730955 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:13:54,943][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 67.55% examples, 1742158 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:13:55,860][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5073452 effective words) took 2.9s, 1735742 effective words/s
[2023-02-07 20:13:56,873][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 48.36% examples, 2503839 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:13:57,875][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 97.36% examples, 2466293 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:13:57,920][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5073452 effective words) took 2.0s, 2474887 effective words/s
[2023-02-07 20:13:58,931][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 48.36% examples, 2489503 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:13:59,936][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 98.89% examples, 2495632 words/s, in_qsize 5, out_qsize 1
[2023-02-07 20:13:59,949][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5073452 effective words) took 2.0s, 2504336 effective words/s
[2023-02-07 20:14:00,959][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 38.19% examples, 1963500 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:01,961][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 76.54% examples, 1957458 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:02,544][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5073452 effective words) took 2.6s, 1956033 effective words/s
[2023-02-07 20:14:03,552][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 37.37% examples, 1938754 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:04,554][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 74.64% examples, 1916741 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:05,191][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5073452 effective words) took 2.6s, 1919151 effective words/s
[2023-02-07 20:14:06,198][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 36.66% examples, 1887843 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:14:07,206][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 74.03% examples, 1893797 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:07,866][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5073452 effective words) took 2.7s, 1897827 effective words/s
[2023-02-07 20:14:08,873][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 36.91% examples, 1917259 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:09,871][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 74.27% examples, 1907573 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:10,516][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5073452 effective words) took 2.6s, 1916047 effective words/s
[2023-02-07 20:14:11,528][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 37.06% examples, 1909857 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:14:12,529][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 74.85% examples, 1917414 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:13,155][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5073452 effective words) took 2.6s, 1924045 effective words/s
[2023-02-07 20:14:14,159][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 47.71% examples, 2471152 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:15,162][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 89.22% examples, 2273131 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:15,410][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5073452 effective words) took 2.3s, 2251718 effective words/s
[2023-02-07 20:14:16,413][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 39.02% examples, 2019032 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:17,415][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 78.69% examples, 2014180 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:14:17,942][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5073452 effective words) took 2.5s, 2005696 effective words/s
[2023-02-07 20:14:18,952][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 37.86% examples, 1948081 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:19,966][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 76.79% examples, 1949794 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:14:20,532][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5073452 effective words) took 2.6s, 1959635 effective words/s
[2023-02-07 20:14:21,535][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 37.21% examples, 1937758 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:22,538][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 75.44% examples, 1936541 words/s, in_qsize 6, out_qsize 1
[2023-02-07 20:14:23,144][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5073452 effective words) took 2.6s, 1943798 effective words/s
[2023-02-07 20:14:24,151][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 37.86% examples, 1958584 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:25,158][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 76.79% examples, 1961291 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:14:25,713][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5073452 effective words) took 2.6s, 1977987 effective words/s
[2023-02-07 20:14:26,719][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 38.75% examples, 2002085 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:27,725][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 78.69% examples, 2006326 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:28,263][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5073452 effective words) took 2.5s, 1990902 effective words/s
[2023-02-07 20:14:29,268][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 37.06% examples, 1923734 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:14:30,273][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 72.52% examples, 1861543 words/s, in_qsize 6, out_qsize 0
[2023-02-07 20:14:30,985][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5073452 effective words) took 2.7s, 1865453 effective words/s
[2023-02-07 20:14:30,986][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (76101780 effective words) took 38.1s, 2000000 effective words/s', 'datetime': '2023-02-07T20:14:30.986002', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:14:30.986 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:14:33,982][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201326-f9gidpyb/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:14:33.982638', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:14:33,983][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:14:34,112][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201326-f9gidpyb/files/../tmp/embedding_model.pt
2023-02-07 20:14:34.112 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:14:36.021 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:14:36.680 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:14:38.809 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1839395130201904, 'test_mae': 1.0963150054584914, 'test_r2': -2.466870420929943}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.98
wandb: percentage 0.28551
wandb:   test_mae 1.09632
wandb:   test_mse 2.18394
wandb:    test_r2 -2.46687
wandb: 
wandb: üöÄ View run hopeful-sweep-76 at: https://wandb.ai/xiaoqiz/mof2vec/runs/f9gidpyb
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_201326-f9gidpyb/logs
wandb: Agent Starting Run: hbeyq6uo with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 972
wandb: 	model.gensim.alpha: 0.009930157623111871
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.7405052920106263
wandb: 	model.gensim.vector_size: 395
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.7382693434308485
wandb: 	model.sklearn.max_depth: 33
wandb: 	model.sklearn.min_child_weight: 0.02054461368054457
wandb: 	model.sklearn.n_estimators: 1677
wandb: 	model.sklearn.num_leaves: 215
wandb: 	model.sklearn.reg_alpha: 0.08553128323596233
wandb: 	model.sklearn.reg_lambda: 0.032045463711235005
wandb: 	model.sklearn.subsample: 0.6995896372059303
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201451-hbeyq6uo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-77
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/hbeyq6uo
2023-02-07 20:15:02.191 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 20:15:02.192 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 972 for sweep.
2023-02-07 20:15:02.192 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.009930157623111871 for sweep.
2023-02-07 20:15:02.192 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:15:02.193 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 20:15:02.193 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7405052920106263 for sweep.
2023-02-07 20:15:02.193 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 395 for sweep.
2023-02-07 20:15:02.193 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 20:15:02.194 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.7382693434308485 for sweep.
2023-02-07 20:15:02.194 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 33 for sweep.
2023-02-07 20:15:02.194 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.02054461368054457 for sweep.
2023-02-07 20:15:02.195 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1677 for sweep.
2023-02-07 20:15:02.195 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 215 for sweep.
2023-02-07 20:15:02.195 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.08553128323596233 for sweep.
2023-02-07 20:15:02.195 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.032045463711235005 for sweep.
2023-02-07 20:15:02.195 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6995896372059303 for sweep.
2023-02-07 20:15:02.196 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:15:02.201 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201451-hbeyq6uo/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 972, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 395, 'window': 3, 'min_count': 3, 'dm': 0, 'sample': 0.7405052920106263, 'workers': 4, 'alpha': 0.009930157623111871, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1677, 'max_depth': 33, 'num_leaves': 215, 'reg_alpha': 0.08553128323596233, 'reg_lambda': 0.032045463711235005, 'subsample': 0.6995896372059303, 'min_child_weight': 0.02054461368054457, 'n_jobs': 4, 'learning_rate': 0.7382693434308485}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:18, 170.65it/s]  1%|          | 38/3257 [00:00<00:17, 185.34it/s]  2%|‚ñè         | 57/3257 [00:00<00:17, 179.68it/s]  2%|‚ñè         | 79/3257 [00:00<00:16, 192.97it/s]  3%|‚ñé         | 99/3257 [00:00<00:16, 188.52it/s]  4%|‚ñé         | 118/3257 [00:00<00:16, 188.48it/s]  4%|‚ñç         | 141/3257 [00:00<00:15, 200.43it/s]  5%|‚ñç         | 162/3257 [00:00<00:15, 199.85it/s]  6%|‚ñå         | 183/3257 [00:00<00:15, 198.22it/s]  6%|‚ñã         | 204/3257 [00:01<00:15, 199.07it/s]  7%|‚ñã         | 231/3257 [00:01<00:13, 219.29it/s]  8%|‚ñä         | 253/3257 [00:01<00:13, 216.75it/s]  9%|‚ñä         | 279/3257 [00:01<00:12, 229.30it/s]  9%|‚ñâ         | 302/3257 [00:01<00:13, 225.55it/s] 10%|‚ñà         | 326/3257 [00:01<00:12, 228.69it/s] 11%|‚ñà         | 349/3257 [00:01<00:13, 211.48it/s] 11%|‚ñà‚ñè        | 373/3257 [00:01<00:13, 219.11it/s] 12%|‚ñà‚ñè        | 396/3257 [00:01<00:14, 202.50it/s] 13%|‚ñà‚ñé        | 419/3257 [00:02<00:13, 208.58it/s] 14%|‚ñà‚ñé        | 441/3257 [00:02<00:15, 184.79it/s] 14%|‚ñà‚ñç        | 463/3257 [00:02<00:14, 193.27it/s] 15%|‚ñà‚ñç        | 483/3257 [00:02<00:14, 189.30it/s] 16%|‚ñà‚ñå        | 510/3257 [00:02<00:13, 207.51it/s] 16%|‚ñà‚ñã        | 532/3257 [00:02<00:13, 203.30it/s] 17%|‚ñà‚ñã        | 555/3257 [00:02<00:12, 210.07it/s] 18%|‚ñà‚ñä        | 577/3257 [00:02<00:14, 186.60it/s] 18%|‚ñà‚ñä        | 601/3257 [00:02<00:13, 200.13it/s] 19%|‚ñà‚ñâ        | 622/3257 [00:03<00:13, 195.30it/s] 20%|‚ñà‚ñâ        | 644/3257 [00:03<00:12, 201.60it/s] 20%|‚ñà‚ñà        | 665/3257 [00:03<00:13, 187.72it/s] 21%|‚ñà‚ñà        | 685/3257 [00:03<00:13, 185.75it/s] 22%|‚ñà‚ñà‚ñè       | 707/3257 [00:03<00:13, 193.56it/s] 22%|‚ñà‚ñà‚ñè       | 727/3257 [00:03<00:13, 190.38it/s] 23%|‚ñà‚ñà‚ñé       | 747/3257 [00:03<00:13, 188.95it/s] 24%|‚ñà‚ñà‚ñé       | 770/3257 [00:03<00:12, 197.30it/s] 24%|‚ñà‚ñà‚ñç       | 790/3257 [00:03<00:12, 191.13it/s] 25%|‚ñà‚ñà‚ñç       | 811/3257 [00:04<00:12, 195.80it/s] 26%|‚ñà‚ñà‚ñå       | 831/3257 [00:04<00:12, 190.26it/s] 26%|‚ñà‚ñà‚ñå       | 851/3257 [00:04<00:13, 181.70it/s] 27%|‚ñà‚ñà‚ñã       | 872/3257 [00:04<00:12, 189.12it/s] 27%|‚ñà‚ñà‚ñã       | 892/3257 [00:04<00:12, 186.91it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:04<00:12, 190.59it/s] 29%|‚ñà‚ñà‚ñä       | 935/3257 [00:04<00:18, 128.72it/s] 29%|‚ñà‚ñà‚ñâ       | 957/3257 [00:05<00:15, 146.55it/s] 30%|‚ñà‚ñà‚ñà       | 978/3257 [00:05<00:14, 159.85it/s] 31%|‚ñà‚ñà‚ñà       | 997/3257 [00:05<00:13, 162.59it/s] 31%|‚ñà‚ñà‚ñà       | 1017/3257 [00:05<00:13, 171.29it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1036/3257 [00:05<00:12, 172.25it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1055/3257 [00:05<00:12, 176.10it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:05<00:11, 183.59it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1100/3257 [00:05<00:11, 186.98it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1120/3257 [00:05<00:11, 188.87it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1140/3257 [00:05<00:11, 187.52it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1162/3257 [00:06<00:10, 196.59it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1182/3257 [00:06<00:11, 188.29it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1201/3257 [00:06<00:11, 174.91it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1220/3257 [00:06<00:11, 178.50it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1244/3257 [00:06<00:10, 193.12it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1265/3257 [00:06<00:10, 197.43it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1285/3257 [00:06<00:11, 179.15it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1307/3257 [00:06<00:10, 188.93it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:06<00:10, 191.78it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:07<00:09, 193.09it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:07<00:09, 190.08it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1390/3257 [00:07<00:10, 186.41it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1417/3257 [00:07<00:08, 208.26it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1439/3257 [00:07<00:08, 203.16it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1465/3257 [00:07<00:08, 216.84it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1487/3257 [00:07<00:08, 212.06it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1512/3257 [00:07<00:07, 221.41it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1535/3257 [00:07<00:08, 201.30it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1556/3257 [00:08<00:08, 197.35it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1577/3257 [00:08<00:08, 198.44it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1600/3257 [00:08<00:08, 206.96it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1621/3257 [00:08<00:07, 206.68it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1642/3257 [00:08<00:08, 201.21it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1663/3257 [00:08<00:08, 195.04it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1683/3257 [00:08<00:08, 188.59it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1706/3257 [00:08<00:07, 197.98it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:08<00:07, 193.33it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1748/3257 [00:09<00:07, 193.16it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1771/3257 [00:09<00:07, 201.17it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1793/3257 [00:09<00:07, 205.18it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1814/3257 [00:09<00:07, 194.82it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1835/3257 [00:09<00:07, 197.41it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1856/3257 [00:09<00:07, 199.22it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1879/3257 [00:09<00:06, 205.78it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1900/3257 [00:09<00:06, 205.71it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1921/3257 [00:09<00:06, 202.39it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1948/3257 [00:10<00:05, 220.86it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1974/3257 [00:10<00:05, 225.83it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1997/3257 [00:10<00:05, 215.34it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2019/3257 [00:10<00:05, 214.92it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2041/3257 [00:10<00:05, 205.68it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2062/3257 [00:10<00:06, 195.78it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2083/3257 [00:10<00:05, 199.15it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2104/3257 [00:10<00:05, 196.73it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2124/3257 [00:10<00:05, 190.47it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2144/3257 [00:11<00:05, 189.28it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:11<00:05, 202.25it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2190/3257 [00:11<00:05, 204.08it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2211/3257 [00:11<00:09, 115.27it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:11<00:07, 138.44it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2256/3257 [00:11<00:06, 150.74it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2275/3257 [00:11<00:06, 154.78it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:12<00:05, 174.86it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2326/3257 [00:12<00:04, 198.44it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2355/3257 [00:12<00:04, 221.93it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2379/3257 [00:12<00:04, 218.16it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2403/3257 [00:12<00:03, 224.06it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2427/3257 [00:12<00:03, 213.38it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2450/3257 [00:12<00:04, 195.43it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2478/3257 [00:12<00:03, 214.33it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2504/3257 [00:12<00:03, 225.87it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2528/3257 [00:13<00:03, 226.06it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2552/3257 [00:13<00:03, 220.43it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2575/3257 [00:13<00:03, 201.72it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2596/3257 [00:13<00:03, 195.44it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2623/3257 [00:13<00:02, 214.79it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2645/3257 [00:13<00:02, 211.10it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2667/3257 [00:13<00:02, 205.04it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2691/3257 [00:13<00:02, 211.93it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2713/3257 [00:13<00:02, 188.26it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2736/3257 [00:14<00:02, 198.66it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2760/3257 [00:14<00:02, 207.60it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2782/3257 [00:14<00:02, 202.30it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2807/3257 [00:14<00:02, 211.01it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2829/3257 [00:14<00:02, 201.35it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2852/3257 [00:14<00:01, 208.22it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:14<00:01, 226.59it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2904/3257 [00:14<00:01, 204.50it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:14<00:01, 208.94it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:15<00:01, 196.89it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2972/3257 [00:15<00:01, 201.52it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2993/3257 [00:15<00:01, 193.65it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3017/3257 [00:15<00:01, 205.70it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3041/3257 [00:15<00:01, 213.86it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3066/3257 [00:15<00:00, 223.92it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3089/3257 [00:15<00:00, 213.68it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3115/3257 [00:15<00:00, 224.53it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3138/3257 [00:15<00:00, 218.52it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3161/3257 [00:16<00:00, 213.91it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3183/3257 [00:16<00:00, 202.33it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:16<00:00, 209.37it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3229/3257 [00:16<00:00, 207.86it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:16<00:00, 215.77it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 197.01it/s]
2023-02-07 20:15:19.372 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:15:19,373][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d395,n5,mc3,s0.740505,t4>', 'datetime': '2023-02-07T20:15:19.373616', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:15:19,373][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:15:19,374][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:15:19,812][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 20:15:19,813][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:15:19,871][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 22723 unique words (71.45% of original 31803, drops 9080)', 'datetime': '2023-02-07T20:15:19.871874', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:15:19,872][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 5081575 word corpus (99.73% of original 5095118, drops 13543)', 'datetime': '2023-02-07T20:15:19.872240', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:15:19,945][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 20:15:19,946][gensim.models.word2vec][INFO] - sample=0.740505 downsamples 0 most-common words
[2023-02-07 20:15:19,946][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5081575 word corpus (100.0%% of prior 5081575)', 'datetime': '2023-02-07T20:15:19.946848', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:15:20,068][gensim.models.word2vec][INFO] - estimated required memory for 22723 words and 395 dimensions: 88963640 bytes
[2023-02-07 20:15:20,068][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:15:20,106][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 22723 vocabulary and 395 features, using sg=1 hs=0 sample=0.7405052920106263 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T20:15:20.106151', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:15:21,111][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 33.13% examples, 1693359 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:22,112][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 62.33% examples, 1601809 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:23,119][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 92.42% examples, 1569325 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:15:23,336][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5073452 effective words) took 3.2s, 1572522 effective words/s
[2023-02-07 20:15:24,352][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 34.48% examples, 1748421 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:15:25,356][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 67.30% examples, 1729008 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:26,274][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5073452 effective words) took 2.9s, 1728939 effective words/s
[2023-02-07 20:15:27,279][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 35.06% examples, 1801230 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:15:28,281][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 70.22% examples, 1815992 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:29,058][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5073452 effective words) took 2.8s, 1823394 effective words/s
[2023-02-07 20:15:30,066][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 45.13% examples, 2319123 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:15:31,073][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 87.53% examples, 2224973 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:31,392][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5073452 effective words) took 2.3s, 2174890 effective words/s
[2023-02-07 20:15:32,396][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 35.06% examples, 1801937 words/s, in_qsize 4, out_qsize 3
[2023-02-07 20:15:33,398][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 74.24% examples, 1906421 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:33,924][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5073452 effective words) took 2.5s, 2004901 effective words/s
[2023-02-07 20:15:34,927][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 35.49% examples, 1834942 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:15:35,929][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 70.86% examples, 1836421 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:36,688][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5073452 effective words) took 2.8s, 1837360 effective words/s
[2023-02-07 20:15:37,693][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 35.98% examples, 1847489 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:38,697][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 71.66% examples, 1844550 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:15:39,446][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5073452 effective words) took 2.8s, 1840202 effective words/s
[2023-02-07 20:15:40,452][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 37.64% examples, 1950558 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:41,453][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 72.74% examples, 1871143 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:42,177][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5073452 effective words) took 2.7s, 1860242 effective words/s
[2023-02-07 20:15:43,179][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 36.23% examples, 1866476 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:44,188][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 73.20% examples, 1878643 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:44,870][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5073452 effective words) took 2.7s, 1884840 effective words/s
[2023-02-07 20:15:45,876][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 36.23% examples, 1864003 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:46,884][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 73.47% examples, 1882408 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:47,560][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5073452 effective words) took 2.7s, 1889044 effective words/s
[2023-02-07 20:15:48,563][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 46.21% examples, 2379352 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:49,567][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 94.44% examples, 2395244 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:49,674][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5073452 effective words) took 2.1s, 2401541 effective words/s
[2023-02-07 20:15:50,678][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 46.64% examples, 2403435 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:51,685][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 81.58% examples, 2076676 words/s, in_qsize 3, out_qsize 6
[2023-02-07 20:15:52,150][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5073452 effective words) took 2.5s, 2049813 effective words/s
[2023-02-07 20:15:53,154][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 35.74% examples, 1843644 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:54,155][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 72.09% examples, 1859356 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:15:54,881][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5073452 effective words) took 2.7s, 1860055 effective words/s
[2023-02-07 20:15:55,902][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 35.06% examples, 1773442 words/s, in_qsize 8, out_qsize 7
[2023-02-07 20:15:56,906][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 72.18% examples, 1843396 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:15:57,629][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5073452 effective words) took 2.7s, 1847323 effective words/s
[2023-02-07 20:15:58,643][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 34.76% examples, 1764121 words/s, in_qsize 3, out_qsize 10
[2023-02-07 20:15:59,648][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 72.43% examples, 1848227 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:16:00,381][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5073452 effective words) took 2.8s, 1844008 effective words/s
[2023-02-07 20:16:00,382][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (76101780 effective words) took 40.3s, 1889572 effective words/s', 'datetime': '2023-02-07T20:16:00.382676', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:16:00.382 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:16:05,302][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201451-hbeyq6uo/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:16:05.302432', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:16:05,303][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:16:05,415][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201451-hbeyq6uo/files/../tmp/embedding_model.pt
2023-02-07 20:16:05.415 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:16:07.494 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:16:08.181 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:16:19.733 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2175749143047008, 'test_mae': 1.069842573662397, 'test_r2': -1.636043711707769}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.82
wandb: percentage 0.28551
wandb:   test_mae 1.06984
wandb:   test_mse 2.21757
wandb:    test_r2 -1.63604
wandb: 
wandb: üöÄ View run ancient-sweep-77 at: https://wandb.ai/xiaoqiz/mof2vec/runs/hbeyq6uo
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_201451-hbeyq6uo/logs
wandb: Agent Starting Run: 4dpk414j with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 1006
wandb: 	model.gensim.alpha: 0.029169338911097496
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 8
wandb: 	model.gensim.sample: 0.800212994735988
wandb: 	model.gensim.vector_size: 299
wandb: 	model.gensim.window: 8
wandb: 	model.sklearn.learning_rate: 0.05601711044730949
wandb: 	model.sklearn.max_depth: 32
wandb: 	model.sklearn.min_child_weight: 0.08651342143530921
wandb: 	model.sklearn.n_estimators: 2307
wandb: 	model.sklearn.num_leaves: 412
wandb: 	model.sklearn.reg_alpha: 0.135172277973255
wandb: 	model.sklearn.reg_lambda: 0.4672706688621448
wandb: 	model.sklearn.subsample: 0.7951418253701974
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201631-4dpk414j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-78
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/4dpk414j
2023-02-07 20:16:39.673 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 20:16:39.674 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 1006 for sweep.
2023-02-07 20:16:39.674 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.029169338911097496 for sweep.
2023-02-07 20:16:39.674 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:16:39.675 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 8 for sweep.
2023-02-07 20:16:39.675 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.800212994735988 for sweep.
2023-02-07 20:16:39.675 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 299 for sweep.
2023-02-07 20:16:39.675 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 8 for sweep.
2023-02-07 20:16:39.675 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.05601711044730949 for sweep.
2023-02-07 20:16:39.676 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 32 for sweep.
2023-02-07 20:16:39.676 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.08651342143530921 for sweep.
2023-02-07 20:16:39.676 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2307 for sweep.
2023-02-07 20:16:39.676 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 412 for sweep.
2023-02-07 20:16:39.677 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.135172277973255 for sweep.
2023-02-07 20:16:39.677 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.4672706688621448 for sweep.
2023-02-07 20:16:39.678 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.7951418253701974 for sweep.
2023-02-07 20:16:39.678 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:16:39.684 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201631-4dpk414j/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 1006, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 299, 'window': 8, 'min_count': 8, 'dm': 0, 'sample': 0.800212994735988, 'workers': 4, 'alpha': 0.029169338911097496, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2307, 'max_depth': 32, 'num_leaves': 412, 'reg_alpha': 0.135172277973255, 'reg_lambda': 0.4672706688621448, 'subsample': 0.7951418253701974, 'min_child_weight': 0.08651342143530921, 'n_jobs': 4, 'learning_rate': 0.05601711044730949}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 16/3257 [00:00<00:21, 152.87it/s]  1%|          | 34/3257 [00:00<00:20, 160.63it/s]  2%|‚ñè         | 53/3257 [00:00<00:19, 162.07it/s]  2%|‚ñè         | 71/3257 [00:00<00:18, 167.86it/s]  3%|‚ñé         | 90/3257 [00:00<00:18, 173.05it/s]  3%|‚ñé         | 108/3257 [00:00<00:19, 161.10it/s]  4%|‚ñç         | 125/3257 [00:00<00:19, 159.81it/s]  4%|‚ñç         | 146/3257 [00:00<00:18, 172.41it/s]  5%|‚ñå         | 164/3257 [00:01<00:19, 159.55it/s]  6%|‚ñå         | 181/3257 [00:01<00:19, 157.45it/s]  6%|‚ñå         | 199/3257 [00:01<00:18, 161.67it/s]  7%|‚ñã         | 218/3257 [00:01<00:18, 167.19it/s]  7%|‚ñã         | 237/3257 [00:01<00:17, 171.36it/s]  8%|‚ñä         | 256/3257 [00:01<00:17, 175.00it/s]  8%|‚ñä         | 274/3257 [00:01<00:17, 173.45it/s]  9%|‚ñâ         | 295/3257 [00:01<00:16, 183.14it/s] 10%|‚ñâ         | 314/3257 [00:01<00:17, 168.74it/s] 10%|‚ñà         | 334/3257 [00:01<00:16, 174.07it/s] 11%|‚ñà         | 352/3257 [00:02<00:17, 169.86it/s] 11%|‚ñà‚ñè        | 371/3257 [00:02<00:16, 174.89it/s] 12%|‚ñà‚ñè        | 389/3257 [00:02<00:18, 154.46it/s] 13%|‚ñà‚ñé        | 408/3257 [00:02<00:17, 161.62it/s] 13%|‚ñà‚ñé        | 425/3257 [00:02<00:26, 108.54it/s] 13%|‚ñà‚ñé        | 439/3257 [00:02<00:24, 112.81it/s] 14%|‚ñà‚ñç        | 458/3257 [00:02<00:21, 129.58it/s] 15%|‚ñà‚ñç        | 476/3257 [00:03<00:19, 141.12it/s] 15%|‚ñà‚ñå        | 492/3257 [00:03<00:19, 144.96it/s] 16%|‚ñà‚ñå        | 512/3257 [00:03<00:17, 157.55it/s] 16%|‚ñà‚ñå        | 529/3257 [00:03<00:17, 158.32it/s] 17%|‚ñà‚ñã        | 547/3257 [00:03<00:16, 163.41it/s] 17%|‚ñà‚ñã        | 564/3257 [00:03<00:17, 155.71it/s] 18%|‚ñà‚ñä        | 580/3257 [00:03<00:17, 155.93it/s] 18%|‚ñà‚ñä        | 599/3257 [00:03<00:16, 163.59it/s] 19%|‚ñà‚ñâ        | 619/3257 [00:03<00:15, 173.83it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:04<00:15, 168.35it/s] 20%|‚ñà‚ñà        | 655/3257 [00:04<00:16, 155.58it/s] 21%|‚ñà‚ñà        | 673/3257 [00:04<00:16, 159.37it/s] 21%|‚ñà‚ñà        | 690/3257 [00:04<00:16, 155.23it/s] 22%|‚ñà‚ñà‚ñè       | 709/3257 [00:04<00:15, 162.41it/s] 22%|‚ñà‚ñà‚ñè       | 726/3257 [00:04<00:16, 152.52it/s] 23%|‚ñà‚ñà‚ñé       | 742/3257 [00:04<00:16, 151.48it/s] 23%|‚ñà‚ñà‚ñé       | 761/3257 [00:04<00:15, 161.98it/s] 24%|‚ñà‚ñà‚ñç       | 778/3257 [00:04<00:15, 155.52it/s] 24%|‚ñà‚ñà‚ñç       | 796/3257 [00:05<00:15, 159.67it/s] 25%|‚ñà‚ñà‚ñç       | 814/3257 [00:05<00:15, 161.43it/s] 26%|‚ñà‚ñà‚ñå       | 831/3257 [00:05<00:15, 157.34it/s] 26%|‚ñà‚ñà‚ñå       | 847/3257 [00:05<00:16, 147.86it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:05<00:15, 150.23it/s] 27%|‚ñà‚ñà‚ñã       | 880/3257 [00:05<00:15, 149.05it/s] 28%|‚ñà‚ñà‚ñä       | 898/3257 [00:05<00:15, 156.54it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:05<00:14, 157.47it/s] 29%|‚ñà‚ñà‚ñä       | 933/3257 [00:05<00:14, 164.52it/s] 29%|‚ñà‚ñà‚ñâ       | 953/3257 [00:06<00:13, 174.02it/s] 30%|‚ñà‚ñà‚ñâ       | 971/3257 [00:06<00:13, 170.27it/s] 30%|‚ñà‚ñà‚ñà       | 989/3257 [00:06<00:13, 162.38it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:06<00:14, 157.99it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1026/3257 [00:06<00:13, 164.58it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1043/3257 [00:06<00:14, 152.76it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1061/3257 [00:06<00:13, 158.92it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:06<00:13, 161.06it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1097/3257 [00:06<00:13, 161.90it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1116/3257 [00:07<00:12, 168.48it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1133/3257 [00:07<00:13, 160.15it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1150/3257 [00:07<00:13, 156.17it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:07<00:12, 165.00it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1187/3257 [00:07<00:13, 155.43it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1203/3257 [00:07<00:14, 145.56it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1219/3257 [00:07<00:13, 147.26it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1241/3257 [00:07<00:12, 164.45it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1258/3257 [00:07<00:12, 156.43it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:08<00:12, 153.32it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1291/3257 [00:08<00:13, 145.58it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1307/3257 [00:08<00:13, 148.78it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1323/3257 [00:08<00:12, 149.74it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1342/3257 [00:08<00:12, 159.06it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1359/3257 [00:08<00:12, 154.32it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1375/3257 [00:08<00:12, 151.44it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1391/3257 [00:08<00:12, 148.73it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1411/3257 [00:08<00:11, 161.15it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1430/3257 [00:09<00:10, 167.91it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1447/3257 [00:09<00:11, 163.99it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1467/3257 [00:09<00:10, 172.55it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1485/3257 [00:09<00:10, 169.86it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1507/3257 [00:09<00:09, 183.23it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1526/3257 [00:09<00:10, 165.09it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1543/3257 [00:09<00:10, 162.18it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1560/3257 [00:09<00:10, 157.27it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1576/3257 [00:09<00:10, 154.09it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1594/3257 [00:10<00:10, 160.04it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1612/3257 [00:10<00:09, 164.72it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1629/3257 [00:10<00:09, 165.74it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1646/3257 [00:10<00:10, 154.77it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1662/3257 [00:10<00:10, 145.09it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1677/3257 [00:10<00:10, 144.32it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1693/3257 [00:10<00:10, 147.67it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1711/3257 [00:10<00:09, 155.89it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:11<00:15, 96.91it/s]  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1741/3257 [00:11<00:14, 102.22it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1760/3257 [00:11<00:12, 119.81it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1777/3257 [00:11<00:11, 129.93it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1796/3257 [00:11<00:10, 141.73it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1812/3257 [00:11<00:10, 141.53it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1828/3257 [00:11<00:09, 143.60it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1847/3257 [00:11<00:09, 154.51it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1868/3257 [00:12<00:08, 167.58it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1886/3257 [00:12<00:08, 161.04it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1906/3257 [00:12<00:07, 169.44it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1924/3257 [00:12<00:07, 168.31it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1949/3257 [00:12<00:06, 190.82it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1971/3257 [00:12<00:06, 199.06it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1992/3257 [00:12<00:06, 186.86it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2012/3257 [00:12<00:06, 181.54it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2033/3257 [00:12<00:06, 189.05it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2053/3257 [00:13<00:06, 173.53it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2071/3257 [00:13<00:06, 169.90it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2089/3257 [00:13<00:07, 165.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2106/3257 [00:13<00:07, 162.58it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2123/3257 [00:13<00:07, 146.49it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2139/3257 [00:13<00:07, 149.18it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2155/3257 [00:13<00:07, 148.71it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2176/3257 [00:13<00:06, 165.23it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2193/3257 [00:13<00:06, 159.45it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2210/3257 [00:14<00:06, 153.23it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2229/3257 [00:14<00:06, 162.56it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2246/3257 [00:14<00:06, 161.03it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2264/3257 [00:14<00:06, 164.87it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2281/3257 [00:14<00:06, 159.30it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:14<00:05, 164.77it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2319/3257 [00:14<00:05, 174.31it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2342/3257 [00:14<00:04, 189.24it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2362/3257 [00:14<00:04, 191.24it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2382/3257 [00:15<00:04, 189.58it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2404/3257 [00:15<00:04, 194.81it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2424/3257 [00:15<00:04, 182.74it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2443/3257 [00:15<00:04, 169.68it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2464/3257 [00:15<00:04, 178.88it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2483/3257 [00:15<00:04, 178.82it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2504/3257 [00:15<00:04, 186.59it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2523/3257 [00:15<00:03, 184.74it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2542/3257 [00:15<00:03, 182.60it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2561/3257 [00:16<00:03, 176.01it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2579/3257 [00:16<00:04, 166.23it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2598/3257 [00:16<00:03, 171.69it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2624/3257 [00:16<00:03, 195.28it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2644/3257 [00:16<00:03, 185.93it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2663/3257 [00:16<00:03, 175.86it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2682/3257 [00:16<00:03, 179.12it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2701/3257 [00:16<00:03, 158.63it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2718/3257 [00:16<00:03, 157.00it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2741/3257 [00:17<00:02, 172.93it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2760/3257 [00:17<00:02, 176.47it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2778/3257 [00:17<00:02, 168.09it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2799/3257 [00:17<00:02, 177.56it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2817/3257 [00:17<00:02, 163.38it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2834/3257 [00:17<00:02, 158.77it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2852/3257 [00:17<00:02, 163.29it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2874/3257 [00:17<00:02, 178.79it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2893/3257 [00:17<00:02, 168.88it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2911/3257 [00:18<00:02, 171.78it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2929/3257 [00:18<00:01, 173.12it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2947/3257 [00:18<00:01, 167.76it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2966/3257 [00:18<00:01, 172.40it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2984/3257 [00:18<00:01, 161.91it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3005/3257 [00:18<00:01, 174.79it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3023/3257 [00:18<00:01, 170.78it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3046/3257 [00:18<00:01, 185.17it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3068/3257 [00:18<00:00, 191.61it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3088/3257 [00:19<00:00, 187.78it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3111/3257 [00:19<00:00, 197.82it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3131/3257 [00:19<00:00, 193.96it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3151/3257 [00:19<00:01, 94.61it/s]  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3169/3257 [00:19<00:00, 108.35it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3185/3257 [00:19<00:00, 116.32it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3205/3257 [00:20<00:00, 133.83it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3222/3257 [00:20<00:00, 138.80it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:20<00:00, 157.85it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:20<00:00, 160.24it/s]
2023-02-07 20:17:00.821 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:17:00,822][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d299,n5,mc8,s0.800213,t4>', 'datetime': '2023-02-07T20:17:00.822003', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:17:00,823][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:17:00,823][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:17:01,387][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 20:17:01,387][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:17:01,453][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 retains 22849 unique words (42.27% of original 54054, drops 31205)', 'datetime': '2023-02-07T20:17:01.453925', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:17:01,454][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 leaves 6458832 word corpus (98.60% of original 6550866, drops 92034)', 'datetime': '2023-02-07T20:17:01.454280', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:17:01,528][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 20:17:01,530][gensim.models.word2vec][INFO] - sample=0.800213 downsamples 0 most-common words
[2023-02-07 20:17:01,530][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6458832 word corpus (100.0%% of prior 6458832)', 'datetime': '2023-02-07T20:17:01.530432', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:17:01,653][gensim.models.word2vec][INFO] - estimated required memory for 22849 words and 299 dimensions: 70626080 bytes
[2023-02-07 20:17:01,653][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:17:01,682][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 22849 vocabulary and 299 features, using sg=1 hs=0 sample=0.800212994735988 negative=5 window=8 shrink_windows=True', 'datetime': '2023-02-07T20:17:01.682140', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:17:02,686][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 27.88% examples, 1794964 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:03,689][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 56.65% examples, 1851505 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:04,692][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 87.04% examples, 1871351 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:05,112][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6404777 effective words) took 3.4s, 1868291 effective words/s
[2023-02-07 20:17:06,113][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 41.30% examples, 2710963 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:07,116][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 81.76% examples, 2634525 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:07,545][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6404777 effective words) took 2.4s, 2633985 effective words/s
[2023-02-07 20:17:08,548][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 41.17% examples, 2699098 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:09,550][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 84.00% examples, 2711520 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:09,982][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6404777 effective words) took 2.4s, 2630234 effective words/s
[2023-02-07 20:17:10,990][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 33.16% examples, 2141218 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:11,991][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 65.06% examples, 2115629 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:12,993][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 98.50% examples, 2101891 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:13,030][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6404777 effective words) took 3.0s, 2105178 effective words/s
[2023-02-07 20:17:14,032][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 36.57% examples, 2380210 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:15,035][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 77.25% examples, 2495491 words/s, in_qsize 6, out_qsize 0
[2023-02-07 20:17:15,593][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6404777 effective words) took 2.6s, 2499758 effective words/s
[2023-02-07 20:17:16,604][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 32.27% examples, 2076385 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:17,608][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 64.20% examples, 2084947 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:18,615][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 98.80% examples, 2098315 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:18,640][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6404777 effective words) took 3.0s, 2104555 effective words/s
[2023-02-07 20:17:19,643][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 31.75% examples, 2050931 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:20,649][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 61.65% examples, 1995587 words/s, in_qsize 8, out_qsize 3
[2023-02-07 20:17:21,656][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 92.14% examples, 1969435 words/s, in_qsize 7, out_qsize 1
[2023-02-07 20:17:21,893][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6404777 effective words) took 3.3s, 1969728 effective words/s
[2023-02-07 20:17:22,906][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 31.69% examples, 2023473 words/s, in_qsize 5, out_qsize 4
[2023-02-07 20:17:23,914][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 63.22% examples, 2042164 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:24,915][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 96.81% examples, 2055528 words/s, in_qsize 6, out_qsize 0
[2023-02-07 20:17:25,013][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6404777 effective words) took 3.1s, 2054404 effective words/s
[2023-02-07 20:17:26,021][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 32.39% examples, 2088808 words/s, in_qsize 6, out_qsize 1
[2023-02-07 20:17:27,023][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 64.75% examples, 2103506 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:28,025][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 98.83% examples, 2108353 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:28,051][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6404777 effective words) took 3.0s, 2110448 effective words/s
[2023-02-07 20:17:29,056][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 40.74% examples, 2666203 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:17:30,057][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 83.02% examples, 2683627 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:30,443][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6404777 effective words) took 2.4s, 2678973 effective words/s
[2023-02-07 20:17:31,445][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 38.38% examples, 2512490 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:32,451][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 72.89% examples, 2363265 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:33,271][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6404777 effective words) took 2.8s, 2266438 effective words/s
[2023-02-07 20:17:34,275][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 31.87% examples, 2063790 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:35,275][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 63.49% examples, 2069184 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:36,280][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 96.78% examples, 2066047 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:36,379][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6404777 effective words) took 3.1s, 2062757 effective words/s
[2023-02-07 20:17:37,385][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 33.28% examples, 2145971 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:38,386][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 73.50% examples, 2379533 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:17:39,001][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6404777 effective words) took 2.6s, 2444269 effective words/s
[2023-02-07 20:17:40,010][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 40.16% examples, 2633123 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:41,009][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 71.91% examples, 2339203 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:41,862][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6404777 effective words) took 2.9s, 2241542 effective words/s
[2023-02-07 20:17:42,864][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 39.21% examples, 2572620 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:43,872][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 70.00% examples, 2277782 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:17:44,787][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6404777 effective words) took 2.9s, 2190937 effective words/s
[2023-02-07 20:17:44,788][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (96071655 effective words) took 43.1s, 2228720 effective words/s', 'datetime': '2023-02-07T20:17:44.788747', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:17:44.789 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:17:48,249][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201631-4dpk414j/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:17:48.249910', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:17:48,251][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:17:48,364][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201631-4dpk414j/files/../tmp/embedding_model.pt
2023-02-07 20:17:48.365 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:17:50.202 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:17:50.813 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:17:53.099 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.261397594223858, 'test_mae': 1.1157228707812978, 'test_r2': -2.380286349732892}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.9
wandb: percentage 0.57729
wandb:   test_mae 1.11572
wandb:   test_mse 2.2614
wandb:    test_r2 -2.38029
wandb: 
wandb: üöÄ View run graceful-sweep-78 at: https://wandb.ai/xiaoqiz/mof2vec/runs/4dpk414j
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_201631-4dpk414j/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: m0pconf5 with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 909
wandb: 	model.gensim.alpha: 0.027195522086051378
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.2041395907082234
wandb: 	model.gensim.vector_size: 347
wandb: 	model.gensim.window: 14
wandb: 	model.sklearn.learning_rate: 0.00370109999396068
wandb: 	model.sklearn.max_depth: 60
wandb: 	model.sklearn.min_child_weight: 0.04505083480618772
wandb: 	model.sklearn.n_estimators: 3358
wandb: 	model.sklearn.num_leaves: 432
wandb: 	model.sklearn.reg_alpha: 0.7445514607341575
wandb: 	model.sklearn.reg_lambda: 0.07246148055052837
wandb: 	model.sklearn.subsample: 0.6406864281619722
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201811-m0pconf5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-sweep-79
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/m0pconf5
2023-02-07 20:18:19.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 20:18:19.262 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 909 for sweep.
2023-02-07 20:18:19.263 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.027195522086051378 for sweep.
2023-02-07 20:18:19.263 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:18:19.263 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 20:18:19.263 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2041395907082234 for sweep.
2023-02-07 20:18:19.263 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 347 for sweep.
2023-02-07 20:18:19.264 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 14 for sweep.
2023-02-07 20:18:19.264 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.00370109999396068 for sweep.
2023-02-07 20:18:19.264 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 60 for sweep.
2023-02-07 20:18:19.264 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.04505083480618772 for sweep.
2023-02-07 20:18:19.265 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3358 for sweep.
2023-02-07 20:18:19.265 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 432 for sweep.
2023-02-07 20:18:19.265 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.7445514607341575 for sweep.
2023-02-07 20:18:19.265 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.07246148055052837 for sweep.
2023-02-07 20:18:19.265 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6406864281619722 for sweep.
2023-02-07 20:18:19.266 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:18:19.277 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201811-m0pconf5/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 909, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 347, 'window': 14, 'min_count': 3, 'dm': 0, 'sample': 0.2041395907082234, 'workers': 4, 'alpha': 0.027195522086051378, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3358, 'max_depth': 60, 'num_leaves': 432, 'reg_alpha': 0.7445514607341575, 'reg_lambda': 0.07246148055052837, 'subsample': 0.6406864281619722, 'min_child_weight': 0.04505083480618772, 'n_jobs': 4, 'learning_rate': 0.00370109999396068}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 24/3257 [00:00<00:13, 238.27it/s]  1%|‚ñè         | 48/3257 [00:00<00:13, 231.71it/s]  2%|‚ñè         | 72/3257 [00:00<00:14, 220.32it/s]  3%|‚ñé         | 97/3257 [00:00<00:13, 227.95it/s]  4%|‚ñé         | 120/3257 [00:00<00:14, 222.19it/s]  5%|‚ñç         | 147/3257 [00:00<00:13, 234.97it/s]  5%|‚ñå         | 171/3257 [00:00<00:13, 229.11it/s]  6%|‚ñå         | 197/3257 [00:00<00:13, 234.22it/s]  7%|‚ñã         | 226/3257 [00:00<00:12, 249.89it/s]  8%|‚ñä         | 253/3257 [00:01<00:11, 251.58it/s]  9%|‚ñä         | 283/3257 [00:01<00:11, 264.15it/s] 10%|‚ñâ         | 310/3257 [00:01<00:11, 258.97it/s] 10%|‚ñà         | 336/3257 [00:01<00:11, 256.09it/s] 11%|‚ñà         | 362/3257 [00:01<00:11, 254.19it/s] 12%|‚ñà‚ñè        | 388/3257 [00:01<00:12, 234.57it/s] 13%|‚ñà‚ñé        | 415/3257 [00:01<00:11, 243.96it/s] 14%|‚ñà‚ñé        | 440/3257 [00:01<00:13, 216.68it/s] 14%|‚ñà‚ñç        | 466/3257 [00:01<00:12, 226.09it/s] 15%|‚ñà‚ñå        | 490/3257 [00:02<00:12, 228.36it/s] 16%|‚ñà‚ñå        | 518/3257 [00:02<00:11, 242.12it/s] 17%|‚ñà‚ñã        | 544/3257 [00:02<00:11, 245.45it/s] 17%|‚ñà‚ñã        | 569/3257 [00:02<00:11, 233.48it/s] 18%|‚ñà‚ñä        | 593/3257 [00:02<00:11, 223.78it/s] 19%|‚ñà‚ñâ        | 619/3257 [00:02<00:11, 231.46it/s] 20%|‚ñà‚ñâ        | 643/3257 [00:02<00:11, 225.76it/s] 20%|‚ñà‚ñà        | 666/3257 [00:02<00:11, 220.44it/s] 21%|‚ñà‚ñà        | 689/3257 [00:02<00:11, 219.86it/s] 22%|‚ñà‚ñà‚ñè       | 713/3257 [00:03<00:11, 223.60it/s] 23%|‚ñà‚ñà‚ñé       | 736/3257 [00:03<00:11, 212.30it/s] 23%|‚ñà‚ñà‚ñé       | 760/3257 [00:03<00:11, 218.99it/s] 24%|‚ñà‚ñà‚ñç       | 783/3257 [00:03<00:11, 219.14it/s] 25%|‚ñà‚ñà‚ñç       | 807/3257 [00:03<00:10, 223.73it/s] 26%|‚ñà‚ñà‚ñå       | 831/3257 [00:03<00:10, 228.01it/s] 26%|‚ñà‚ñà‚ñå       | 854/3257 [00:03<00:10, 219.54it/s] 27%|‚ñà‚ñà‚ñã       | 877/3257 [00:03<00:10, 217.26it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:03<00:10, 233.93it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:04<00:09, 239.10it/s] 29%|‚ñà‚ñà‚ñâ       | 959/3257 [00:04<00:09, 249.47it/s] 30%|‚ñà‚ñà‚ñà       | 985/3257 [00:04<00:09, 250.42it/s] 31%|‚ñà‚ñà‚ñà       | 1011/3257 [00:04<00:09, 241.10it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1036/3257 [00:04<00:09, 238.81it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1060/3257 [00:04<00:09, 235.33it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1084/3257 [00:04<00:13, 160.13it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1111/3257 [00:04<00:11, 183.00it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1133/3257 [00:05<00:11, 189.16it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1155/3257 [00:05<00:10, 195.65it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1178/3257 [00:05<00:10, 198.26it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1200/3257 [00:05<00:10, 201.32it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1227/3257 [00:05<00:09, 218.40it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1252/3257 [00:05<00:08, 226.92it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1276/3257 [00:05<00:08, 227.98it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1300/3257 [00:05<00:08, 225.51it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1325/3257 [00:05<00:08, 229.92it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1349/3257 [00:05<00:08, 224.68it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1372/3257 [00:06<00:08, 216.87it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1397/3257 [00:06<00:08, 224.57it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1425/3257 [00:06<00:07, 239.92it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1453/3257 [00:06<00:07, 249.85it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1482/3257 [00:06<00:06, 260.38it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:06<00:06, 266.20it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1538/3257 [00:06<00:06, 245.75it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1563/3257 [00:06<00:06, 246.67it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1588/3257 [00:06<00:06, 244.64it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1617/3257 [00:07<00:06, 253.87it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1643/3257 [00:07<00:06, 245.13it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1668/3257 [00:07<00:06, 236.91it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1692/3257 [00:07<00:06, 232.66it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1718/3257 [00:07<00:06, 237.73it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1742/3257 [00:07<00:06, 225.46it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1773/3257 [00:07<00:06, 247.28it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1799/3257 [00:07<00:05, 248.81it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1825/3257 [00:07<00:05, 245.29it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1850/3257 [00:08<00:05, 240.67it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1879/3257 [00:08<00:05, 250.98it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1905/3257 [00:08<00:05, 251.48it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1931/3257 [00:08<00:05, 251.36it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:08<00:04, 273.56it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1993/3257 [00:08<00:04, 273.01it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2021/3257 [00:08<00:04, 270.24it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2049/3257 [00:08<00:04, 252.91it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2075/3257 [00:08<00:04, 244.23it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2100/3257 [00:08<00:04, 236.36it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2124/3257 [00:09<00:04, 235.80it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2148/3257 [00:09<00:04, 234.90it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2177/3257 [00:09<00:04, 247.97it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2203/3257 [00:09<00:04, 249.15it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2228/3257 [00:09<00:04, 241.96it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2253/3257 [00:09<00:04, 244.24it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2278/3257 [00:09<00:06, 154.65it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2307/3257 [00:10<00:05, 182.16it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2337/3257 [00:10<00:04, 208.07it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2367/3257 [00:10<00:03, 228.10it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2397/3257 [00:10<00:03, 245.18it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2424/3257 [00:10<00:03, 238.48it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2450/3257 [00:10<00:03, 228.64it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2482/3257 [00:10<00:03, 251.58it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2514/3257 [00:10<00:02, 266.02it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2542/3257 [00:10<00:02, 266.07it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2570/3257 [00:11<00:02, 251.84it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2596/3257 [00:11<00:02, 251.42it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:11<00:02, 275.94it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2659/3257 [00:11<00:02, 261.41it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2689/3257 [00:11<00:02, 271.46it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2717/3257 [00:11<00:02, 242.29it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2749/3257 [00:11<00:01, 261.73it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2776/3257 [00:11<00:01, 252.90it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:11<00:01, 265.62it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2834/3257 [00:12<00:01, 243.22it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2864/3257 [00:12<00:01, 257.20it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2891/3257 [00:12<00:01, 259.52it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2918/3257 [00:12<00:01, 253.79it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2944/3257 [00:12<00:01, 245.22it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2974/3257 [00:12<00:01, 257.53it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3001/3257 [00:12<00:00, 260.22it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3028/3257 [00:12<00:00, 257.17it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3060/3257 [00:12<00:00, 274.77it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3088/3257 [00:12<00:00, 273.62it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3120/3257 [00:13<00:00, 284.56it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3149/3257 [00:13<00:00, 267.84it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3177/3257 [00:13<00:00, 258.38it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3206/3257 [00:13<00:00, 266.89it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3233/3257 [00:13<00:00, 265.71it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 238.82it/s]
2023-02-07 20:18:33.371 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:18:33,372][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d347,n5,mc3,s0.20414,t4>', 'datetime': '2023-02-07T20:18:33.372282', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:18:33,372][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:18:33,372][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:18:33,669][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 20:18:33,669][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:18:33,692][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 9372 unique words (71.76% of original 13061, drops 3689)', 'datetime': '2023-02-07T20:18:33.692092', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:18:33,693][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 3633898 word corpus (99.85% of original 3639370, drops 5472)', 'datetime': '2023-02-07T20:18:33.693691', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:18:33,723][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 20:18:33,724][gensim.models.word2vec][INFO] - sample=0.20414 downsamples 0 most-common words
[2023-02-07 20:18:33,724][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3633898 word corpus (100.0%% of prior 3633898)', 'datetime': '2023-02-07T20:18:33.724425', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:18:33,774][gensim.models.word2vec][INFO] - estimated required memory for 9372 words and 347 dimensions: 35874788 bytes
[2023-02-07 20:18:33,774][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:18:33,790][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 9372 vocabulary and 347 features, using sg=1 hs=0 sample=0.2041395907082234 negative=5 window=14 shrink_windows=True', 'datetime': '2023-02-07T20:18:33.790195', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:18:34,795][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 62.27% examples, 2291567 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:18:35,358][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3637155 effective words) took 1.6s, 2323764 effective words/s
[2023-02-07 20:18:36,362][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 65.34% examples, 2420505 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:18:36,853][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3637155 effective words) took 1.5s, 2436540 effective words/s
[2023-02-07 20:18:37,858][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 51.89% examples, 1926790 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:18:38,762][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3637155 effective words) took 1.9s, 1907836 effective words/s
[2023-02-07 20:18:39,765][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 50.94% examples, 1892052 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:18:40,669][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3637155 effective words) took 1.9s, 1909853 effective words/s
[2023-02-07 20:18:41,675][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 50.41% examples, 1866509 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:18:42,611][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3637155 effective words) took 1.9s, 1875586 effective words/s
[2023-02-07 20:18:43,615][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 50.66% examples, 1879644 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:18:44,566][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3637155 effective words) took 2.0s, 1861376 effective words/s
[2023-02-07 20:18:45,577][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 50.41% examples, 1858636 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:18:46,513][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3637155 effective words) took 1.9s, 1870781 effective words/s
[2023-02-07 20:18:47,517][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 50.69% examples, 1878603 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:18:48,445][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3637155 effective words) took 1.9s, 1884237 effective words/s
[2023-02-07 20:18:49,449][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 50.41% examples, 1873397 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:18:50,365][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3637155 effective words) took 1.9s, 1898515 effective words/s
[2023-02-07 20:18:51,370][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 67.12% examples, 2485740 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:18:51,886][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3637155 effective words) took 1.5s, 2392604 effective words/s
[2023-02-07 20:18:52,889][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 55.51% examples, 2070406 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:18:53,662][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3637155 effective words) took 1.8s, 2049856 effective words/s
[2023-02-07 20:18:54,664][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 53.05% examples, 1979680 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:18:55,500][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3637155 effective words) took 1.8s, 1980325 effective words/s
[2023-02-07 20:18:56,503][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 54.93% examples, 2043236 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:18:57,134][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3637155 effective words) took 1.6s, 2228258 effective words/s
[2023-02-07 20:18:58,144][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 52.29% examples, 1927527 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:18:59,008][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3637155 effective words) took 1.9s, 1942190 effective words/s
[2023-02-07 20:19:00,014][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 53.21% examples, 1983678 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:19:00,845][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3637155 effective words) took 1.8s, 1984229 effective words/s
[2023-02-07 20:19:00,845][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54557325 effective words) took 27.1s, 2016510 effective words/s', 'datetime': '2023-02-07T20:19:00.845937', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:19:00.846 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:19:02,767][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201811-m0pconf5/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:19:02.767206', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:19:02,769][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:19:02,818][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201811-m0pconf5/files/../tmp/embedding_model.pt
2023-02-07 20:19:02.818 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:19:04.662 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:19:05.303 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:19:08.042 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9425201982153921, 'test_mae': 1.0316046151443974, 'test_r2': -1.6399778122830653}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.94
wandb: percentage 0.28244
wandb:   test_mae 1.0316
wandb:   test_mse 1.94252
wandb:    test_r2 -1.63998
wandb: 
wandb: üöÄ View run clear-sweep-79 at: https://wandb.ai/xiaoqiz/mof2vec/runs/m0pconf5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_201811-m0pconf5/logs
wandb: Agent Starting Run: xqeirggb with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 631
wandb: 	model.gensim.alpha: 0.005246098796171366
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.5301447312239413
wandb: 	model.gensim.vector_size: 442
wandb: 	model.gensim.window: 17
wandb: 	model.sklearn.learning_rate: 0.3193237331770617
wandb: 	model.sklearn.max_depth: 29
wandb: 	model.sklearn.min_child_weight: 0.08996707420232099
wandb: 	model.sklearn.n_estimators: 1327
wandb: 	model.sklearn.num_leaves: 422
wandb: 	model.sklearn.reg_alpha: 0.09021158289282943
wandb: 	model.sklearn.reg_lambda: 0.5647894383960235
wandb: 	model.sklearn.subsample: 0.3109160463378978
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201920-xqeirggb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-80
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/xqeirggb
2023-02-07 20:19:28.977 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 20:19:28.978 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 631 for sweep.
2023-02-07 20:19:28.978 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.005246098796171366 for sweep.
2023-02-07 20:19:28.978 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:19:28.979 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 20:19:28.979 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5301447312239413 for sweep.
2023-02-07 20:19:28.979 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 442 for sweep.
2023-02-07 20:19:28.979 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 17 for sweep.
2023-02-07 20:19:28.979 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.3193237331770617 for sweep.
2023-02-07 20:19:28.980 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 29 for sweep.
2023-02-07 20:19:28.980 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.08996707420232099 for sweep.
2023-02-07 20:19:28.980 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1327 for sweep.
2023-02-07 20:19:28.980 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 422 for sweep.
2023-02-07 20:19:28.981 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.09021158289282943 for sweep.
2023-02-07 20:19:28.981 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.5647894383960235 for sweep.
2023-02-07 20:19:28.981 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3109160463378978 for sweep.
2023-02-07 20:19:28.981 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:19:28.988 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201920-xqeirggb/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 631, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 442, 'window': 17, 'min_count': 2, 'dm': 0, 'sample': 0.5301447312239413, 'workers': 4, 'alpha': 0.005246098796171366, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1327, 'max_depth': 29, 'num_leaves': 422, 'reg_alpha': 0.09021158289282943, 'reg_lambda': 0.5647894383960235, 'subsample': 0.3109160463378978, 'min_child_weight': 0.08996707420232099, 'n_jobs': 4, 'learning_rate': 0.3193237331770617}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 24/3257 [00:00<00:14, 228.37it/s]  2%|‚ñè         | 50/3257 [00:00<00:13, 239.91it/s]  2%|‚ñè         | 75/3257 [00:00<00:13, 243.92it/s]  3%|‚ñé         | 100/3257 [00:00<00:13, 242.16it/s]  4%|‚ñç         | 125/3257 [00:00<00:13, 236.54it/s]  5%|‚ñç         | 154/3257 [00:00<00:12, 253.95it/s]  6%|‚ñå         | 180/3257 [00:00<00:12, 253.36it/s]  6%|‚ñã         | 210/3257 [00:00<00:11, 267.21it/s]  7%|‚ñã         | 240/3257 [00:00<00:10, 275.54it/s]  8%|‚ñä         | 268/3257 [00:01<00:11, 268.40it/s]  9%|‚ñâ         | 299/3257 [00:01<00:10, 279.85it/s] 10%|‚ñà         | 328/3257 [00:01<00:10, 281.57it/s] 11%|‚ñà         | 357/3257 [00:01<00:10, 267.93it/s] 12%|‚ñà‚ñè        | 384/3257 [00:01<00:11, 256.86it/s] 13%|‚ñà‚ñé        | 411/3257 [00:01<00:10, 259.40it/s] 13%|‚ñà‚ñé        | 438/3257 [00:01<00:16, 172.22it/s] 14%|‚ñà‚ñç        | 467/3257 [00:01<00:14, 195.43it/s] 15%|‚ñà‚ñå        | 493/3257 [00:02<00:13, 209.76it/s] 16%|‚ñà‚ñå        | 519/3257 [00:02<00:12, 219.86it/s] 17%|‚ñà‚ñã        | 544/3257 [00:02<00:12, 225.41it/s] 17%|‚ñà‚ñã        | 569/3257 [00:02<00:12, 223.11it/s] 18%|‚ñà‚ñä        | 593/3257 [00:02<00:11, 227.54it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:02<00:11, 233.74it/s] 20%|‚ñà‚ñâ        | 646/3257 [00:02<00:10, 239.94it/s] 21%|‚ñà‚ñà        | 673/3257 [00:02<00:10, 244.91it/s] 21%|‚ñà‚ñà‚ñè       | 698/3257 [00:02<00:10, 237.02it/s] 22%|‚ñà‚ñà‚ñè       | 724/3257 [00:03<00:10, 242.46it/s] 23%|‚ñà‚ñà‚ñé       | 749/3257 [00:03<00:10, 241.75it/s] 24%|‚ñà‚ñà‚ñç       | 776/3257 [00:03<00:10, 247.91it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:03<00:09, 254.74it/s] 25%|‚ñà‚ñà‚ñå       | 830/3257 [00:03<00:09, 245.91it/s] 26%|‚ñà‚ñà‚ñã       | 855/3257 [00:03<00:10, 235.60it/s] 27%|‚ñà‚ñà‚ñã       | 881/3257 [00:03<00:09, 240.12it/s] 28%|‚ñà‚ñà‚ñä       | 908/3257 [00:03<00:09, 248.10it/s] 29%|‚ñà‚ñà‚ñä       | 934/3257 [00:03<00:09, 250.09it/s] 30%|‚ñà‚ñà‚ñâ       | 962/3257 [00:03<00:08, 258.18it/s] 30%|‚ñà‚ñà‚ñà       | 988/3257 [00:04<00:09, 247.91it/s] 31%|‚ñà‚ñà‚ñà       | 1013/3257 [00:04<00:09, 246.26it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1038/3257 [00:04<00:09, 240.82it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1063/3257 [00:04<00:09, 242.92it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1088/3257 [00:04<00:08, 244.90it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1115/3257 [00:04<00:08, 248.80it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1140/3257 [00:04<00:08, 241.21it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1166/3257 [00:04<00:08, 243.32it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:04<00:09, 224.72it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1214/3257 [00:05<00:09, 223.60it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1244/3257 [00:05<00:08, 243.80it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:05<00:08, 246.23it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1296/3257 [00:05<00:08, 226.19it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1321/3257 [00:05<00:08, 231.30it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1349/3257 [00:05<00:07, 242.37it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1374/3257 [00:05<00:07, 235.47it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1399/3257 [00:05<00:07, 239.45it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1427/3257 [00:05<00:07, 250.30it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1453/3257 [00:06<00:07, 246.17it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1479/3257 [00:06<00:07, 249.85it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1506/3257 [00:06<00:06, 254.92it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1532/3257 [00:06<00:07, 237.92it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1557/3257 [00:06<00:07, 226.46it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1580/3257 [00:06<00:07, 227.42it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1605/3257 [00:06<00:07, 233.44it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1632/3257 [00:06<00:06, 243.77it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1657/3257 [00:06<00:07, 222.69it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1680/3257 [00:07<00:07, 220.06it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1704/3257 [00:07<00:06, 225.11it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:07<00:10, 150.10it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1749/3257 [00:07<00:09, 162.59it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1774/3257 [00:07<00:08, 180.87it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1800/3257 [00:07<00:07, 199.20it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1824/3257 [00:07<00:06, 208.89it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1847/3257 [00:07<00:06, 214.27it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1873/3257 [00:08<00:06, 224.20it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1898/3257 [00:08<00:05, 229.40it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1922/3257 [00:08<00:05, 230.76it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1954/3257 [00:08<00:05, 255.36it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1980/3257 [00:08<00:05, 249.40it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2006/3257 [00:08<00:04, 251.10it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2033/3257 [00:08<00:04, 252.98it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2059/3257 [00:08<00:05, 232.17it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2084/3257 [00:08<00:04, 236.79it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2109/3257 [00:08<00:04, 235.47it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2133/3257 [00:09<00:04, 227.73it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2156/3257 [00:09<00:04, 226.48it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:09<00:04, 232.89it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2207/3257 [00:09<00:04, 239.97it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2233/3257 [00:09<00:04, 243.33it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2258/3257 [00:09<00:04, 237.45it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2282/3257 [00:09<00:04, 225.46it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2306/3257 [00:09<00:04, 226.64it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2336/3257 [00:09<00:03, 244.29it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2365/3257 [00:10<00:03, 256.03it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2393/3257 [00:10<00:03, 262.79it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2420/3257 [00:10<00:03, 251.62it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2446/3257 [00:10<00:03, 246.61it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2474/3257 [00:10<00:03, 254.74it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2503/3257 [00:10<00:02, 264.24it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2530/3257 [00:10<00:02, 265.53it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2557/3257 [00:10<00:02, 256.04it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2583/3257 [00:10<00:02, 245.62it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2612/3257 [00:11<00:02, 257.57it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:11<00:02, 260.07it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2667/3257 [00:11<00:02, 250.80it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2694/3257 [00:11<00:02, 252.07it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2720/3257 [00:11<00:02, 237.16it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2752/3257 [00:11<00:01, 257.31it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2779/3257 [00:11<00:01, 251.21it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2807/3257 [00:11<00:01, 258.64it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2834/3257 [00:11<00:01, 243.75it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2866/3257 [00:12<00:01, 260.89it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2893/3257 [00:12<00:01, 259.09it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2920/3257 [00:12<00:01, 259.31it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2947/3257 [00:12<00:01, 247.14it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2972/3257 [00:12<00:01, 247.38it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2997/3257 [00:12<00:01, 242.81it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3022/3257 [00:12<00:00, 242.80it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3053/3257 [00:12<00:00, 259.95it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3084/3257 [00:12<00:00, 272.27it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3115/3257 [00:12<00:00, 280.71it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3144/3257 [00:13<00:00, 267.67it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3171/3257 [00:13<00:00, 267.91it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3198/3257 [00:13<00:00, 266.58it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3225/3257 [00:13<00:00, 256.79it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3254/3257 [00:13<00:00, 263.63it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 240.82it/s]
2023-02-07 20:19:42.954 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:19:42,955][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d442,n5,mc2,s0.530145,t4>', 'datetime': '2023-02-07T20:19:42.955313', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:19:42,955][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:19:42,955][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:19:43,276][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 20:19:43,276][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:19:43,303][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 11155 unique words (85.41% of original 13061, drops 1906)', 'datetime': '2023-02-07T20:19:43.303611', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:19:43,304][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 3637464 word corpus (99.95% of original 3639370, drops 1906)', 'datetime': '2023-02-07T20:19:43.304011', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:19:43,339][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 20:19:43,339][gensim.models.word2vec][INFO] - sample=0.530145 downsamples 0 most-common words
[2023-02-07 20:19:43,339][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3637464 word corpus (100.0%% of prior 3637464)', 'datetime': '2023-02-07T20:19:43.339842', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:19:43,399][gensim.models.word2vec][INFO] - estimated required memory for 11155 words and 442 dimensions: 51431356 bytes
[2023-02-07 20:19:43,399][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:19:43,422][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 11155 vocabulary and 442 features, using sg=1 hs=0 sample=0.5301447312239413 negative=5 window=17 shrink_windows=True', 'datetime': '2023-02-07T20:19:43.422174', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:19:44,433][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 35.31% examples, 1301441 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:19:45,436][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 72.18% examples, 1329843 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:19:46,123][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3640721 effective words) took 2.7s, 1350459 effective words/s
[2023-02-07 20:19:47,133][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 42.25% examples, 1574806 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:19:48,137][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 86.98% examples, 1589848 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:19:48,409][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3640721 effective words) took 2.3s, 1594928 effective words/s
[2023-02-07 20:19:49,413][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 43.35% examples, 1617462 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:19:50,416][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 87.01% examples, 1595037 words/s, in_qsize 6, out_qsize 1
[2023-02-07 20:19:50,680][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3640721 effective words) took 2.3s, 1604537 effective words/s
[2023-02-07 20:19:51,690][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 42.25% examples, 1581668 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:19:52,688][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 85.66% examples, 1568896 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:19:52,997][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3640721 effective words) took 2.3s, 1573348 effective words/s
[2023-02-07 20:19:54,003][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 40.71% examples, 1517153 words/s, in_qsize 6, out_qsize 0
[2023-02-07 20:19:55,006][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 85.66% examples, 1566355 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:19:55,311][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3640721 effective words) took 2.3s, 1574617 effective words/s
[2023-02-07 20:19:56,324][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 41.85% examples, 1548860 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:19:57,326][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 86.77% examples, 1584521 words/s, in_qsize 5, out_qsize 2
[2023-02-07 20:19:57,588][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3640721 effective words) took 2.3s, 1600148 effective words/s
[2023-02-07 20:19:58,600][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 43.51% examples, 1612535 words/s, in_qsize 6, out_qsize 1
[2023-02-07 20:19:59,608][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 88.92% examples, 1616581 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:19:59,845][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3640721 effective words) took 2.3s, 1614799 effective words/s
[2023-02-07 20:20:00,851][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 43.02% examples, 1606492 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:20:01,853][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 86.98% examples, 1594533 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:20:02,135][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3640721 effective words) took 2.3s, 1591778 effective words/s
[2023-02-07 20:20:03,143][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 43.66% examples, 1620208 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:20:04,152][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 88.21% examples, 1606860 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:20:04,400][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3640721 effective words) took 2.3s, 1609112 effective words/s
[2023-02-07 20:20:05,406][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 45.32% examples, 1679567 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:20:06,416][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 89.65% examples, 1633146 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:20:06,635][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3640721 effective words) took 2.2s, 1631031 effective words/s
[2023-02-07 20:20:07,644][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 42.46% examples, 1583398 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:20:08,654][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 88.21% examples, 1604577 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:20:08,905][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3640721 effective words) took 2.3s, 1605864 effective words/s
[2023-02-07 20:20:09,911][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 42.62% examples, 1585741 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:20:10,914][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 87.78% examples, 1601831 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:20:11,162][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3640721 effective words) took 2.3s, 1613979 effective words/s
[2023-02-07 20:20:12,172][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 45.32% examples, 1680096 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:20:13,173][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 90.88% examples, 1663526 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:20:13,358][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3640721 effective words) took 2.2s, 1661952 effective words/s
[2023-02-07 20:20:14,372][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 45.32% examples, 1663529 words/s, in_qsize 6, out_qsize 1
[2023-02-07 20:20:15,373][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 90.42% examples, 1646681 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:20:15,584][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3640721 effective words) took 2.2s, 1636632 effective words/s
[2023-02-07 20:20:16,590][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 45.96% examples, 1695467 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:20:17,503][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3640721 effective words) took 1.9s, 1899084 effective words/s
[2023-02-07 20:20:17,506][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54610815 effective words) took 34.1s, 1602273 effective words/s', 'datetime': '2023-02-07T20:20:17.505985', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:20:17.506 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:20:19,836][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201920-xqeirggb/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:20:19.836215', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:20:19,837][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:20:19,907][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_201920-xqeirggb/files/../tmp/embedding_model.pt
2023-02-07 20:20:19.908 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:20:22.256 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:20:22.974 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:20:26.519 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1381880045485704, 'test_mae': 1.0994900496450875, 'test_r2': -1.6359731174180352}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.71
wandb: percentage 0.14593
wandb:   test_mae 1.09949
wandb:   test_mse 2.13819
wandb:    test_r2 -1.63597
wandb: 
wandb: üöÄ View run zesty-sweep-80 at: https://wandb.ai/xiaoqiz/mof2vec/runs/xqeirggb
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_201920-xqeirggb/logs
wandb: Agent Starting Run: 8cj685cl with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 952
wandb: 	model.gensim.alpha: 0.0019297380208745537
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.3068698448515564
wandb: 	model.gensim.vector_size: 357
wandb: 	model.gensim.window: 9
wandb: 	model.sklearn.learning_rate: 0.0027108023587945913
wandb: 	model.sklearn.max_depth: 34
wandb: 	model.sklearn.min_child_weight: 0.08962198355826691
wandb: 	model.sklearn.n_estimators: 1541
wandb: 	model.sklearn.num_leaves: 418
wandb: 	model.sklearn.reg_alpha: 0.06211801996862615
wandb: 	model.sklearn.reg_lambda: 0.49800835305234664
wandb: 	model.sklearn.subsample: 0.8933436259958285
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202041-8cj685cl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-81
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/8cj685cl
2023-02-07 20:20:48.578 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 20:20:48.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 952 for sweep.
2023-02-07 20:20:48.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0019297380208745537 for sweep.
2023-02-07 20:20:48.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:20:48.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 20:20:48.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3068698448515564 for sweep.
2023-02-07 20:20:48.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 357 for sweep.
2023-02-07 20:20:48.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 9 for sweep.
2023-02-07 20:20:48.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0027108023587945913 for sweep.
2023-02-07 20:20:48.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 34 for sweep.
2023-02-07 20:20:48.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.08962198355826691 for sweep.
2023-02-07 20:20:48.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1541 for sweep.
2023-02-07 20:20:48.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 418 for sweep.
2023-02-07 20:20:48.582 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.06211801996862615 for sweep.
2023-02-07 20:20:48.582 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.49800835305234664 for sweep.
2023-02-07 20:20:48.582 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8933436259958285 for sweep.
2023-02-07 20:20:48.583 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:20:48.589 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202041-8cj685cl/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 952, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 357, 'window': 9, 'min_count': 2, 'dm': 0, 'sample': 0.3068698448515564, 'workers': 4, 'alpha': 0.0019297380208745537, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1541, 'max_depth': 34, 'num_leaves': 418, 'reg_alpha': 0.06211801996862615, 'reg_lambda': 0.49800835305234664, 'subsample': 0.8933436259958285, 'min_child_weight': 0.08962198355826691, 'n_jobs': 4, 'learning_rate': 0.0027108023587945913}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:20, 160.56it/s]  1%|          | 35/3257 [00:00<00:19, 168.13it/s]  2%|‚ñè         | 53/3257 [00:00<00:19, 168.21it/s]  2%|‚ñè         | 73/3257 [00:00<00:17, 177.88it/s]  3%|‚ñé         | 93/3257 [00:00<00:17, 184.49it/s]  3%|‚ñé         | 112/3257 [00:00<00:18, 173.30it/s]  4%|‚ñç         | 131/3257 [00:00<00:17, 177.89it/s]  5%|‚ñç         | 151/3257 [00:00<00:16, 184.00it/s]  5%|‚ñå         | 170/3257 [00:00<00:17, 178.78it/s]  6%|‚ñå         | 190/3257 [00:01<00:16, 181.99it/s]  6%|‚ñã         | 210/3257 [00:01<00:16, 185.07it/s]  7%|‚ñã         | 232/3257 [00:01<00:15, 195.17it/s]  8%|‚ñä         | 252/3257 [00:01<00:15, 194.67it/s]  8%|‚ñä         | 272/3257 [00:01<00:15, 189.87it/s]  9%|‚ñâ         | 296/3257 [00:01<00:14, 201.37it/s] 10%|‚ñâ         | 317/3257 [00:01<00:15, 190.01it/s] 10%|‚ñà         | 337/3257 [00:01<00:15, 190.67it/s] 11%|‚ñà         | 358/3257 [00:01<00:14, 194.37it/s] 12%|‚ñà‚ñè        | 378/3257 [00:02<00:16, 179.75it/s] 12%|‚ñà‚ñè        | 397/3257 [00:02<00:16, 178.38it/s] 13%|‚ñà‚ñé        | 416/3257 [00:02<00:15, 181.26it/s] 13%|‚ñà‚ñé        | 435/3257 [00:02<00:18, 155.96it/s] 14%|‚ñà‚ñç        | 455/3257 [00:02<00:16, 166.05it/s] 15%|‚ñà‚ñç        | 475/3257 [00:02<00:16, 172.01it/s] 15%|‚ñà‚ñå        | 494/3257 [00:02<00:15, 176.36it/s] 16%|‚ñà‚ñå        | 515/3257 [00:02<00:14, 183.51it/s] 16%|‚ñà‚ñã        | 534/3257 [00:02<00:15, 177.52it/s] 17%|‚ñà‚ñã        | 555/3257 [00:03<00:14, 185.13it/s] 18%|‚ñà‚ñä        | 574/3257 [00:03<00:16, 163.99it/s] 18%|‚ñà‚ñä        | 595/3257 [00:03<00:15, 175.68it/s] 19%|‚ñà‚ñâ        | 616/3257 [00:03<00:14, 183.82it/s] 19%|‚ñà‚ñâ        | 635/3257 [00:03<00:14, 184.88it/s] 20%|‚ñà‚ñà        | 654/3257 [00:03<00:15, 172.74it/s] 21%|‚ñà‚ñà        | 673/3257 [00:03<00:14, 174.56it/s] 21%|‚ñà‚ñà        | 691/3257 [00:03<00:14, 171.23it/s] 22%|‚ñà‚ñà‚ñè       | 711/3257 [00:03<00:14, 177.34it/s] 22%|‚ñà‚ñà‚ñè       | 729/3257 [00:04<00:15, 168.31it/s] 23%|‚ñà‚ñà‚ñé       | 746/3257 [00:04<00:15, 167.34it/s] 24%|‚ñà‚ñà‚ñé       | 767/3257 [00:04<00:13, 178.42it/s] 24%|‚ñà‚ñà‚ñç       | 785/3257 [00:04<00:14, 167.71it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:04<00:14, 172.17it/s] 25%|‚ñà‚ñà‚ñå       | 825/3257 [00:04<00:13, 174.78it/s] 26%|‚ñà‚ñà‚ñå       | 843/3257 [00:04<00:14, 168.86it/s] 26%|‚ñà‚ñà‚ñã       | 862/3257 [00:04<00:13, 173.45it/s] 27%|‚ñà‚ñà‚ñã       | 880/3257 [00:04<00:14, 169.50it/s] 28%|‚ñà‚ñà‚ñä       | 901/3257 [00:05<00:13, 179.97it/s] 28%|‚ñà‚ñà‚ñä       | 920/3257 [00:05<00:13, 179.18it/s] 29%|‚ñà‚ñà‚ñâ       | 938/3257 [00:05<00:13, 177.85it/s] 29%|‚ñà‚ñà‚ñâ       | 959/3257 [00:05<00:12, 182.96it/s] 30%|‚ñà‚ñà‚ñà       | 979/3257 [00:05<00:12, 180.52it/s] 31%|‚ñà‚ñà‚ñà       | 998/3257 [00:05<00:12, 178.01it/s] 31%|‚ñà‚ñà‚ñà       | 1016/3257 [00:05<00:12, 177.46it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1034/3257 [00:05<00:13, 166.11it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1051/3257 [00:05<00:13, 165.14it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1071/3257 [00:06<00:12, 173.62it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1089/3257 [00:06<00:12, 173.68it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1107/3257 [00:06<00:12, 174.82it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1125/3257 [00:06<00:12, 173.42it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1143/3257 [00:06<00:12, 169.89it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1161/3257 [00:06<00:12, 172.45it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1179/3257 [00:06<00:12, 163.62it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1196/3257 [00:06<00:13, 155.08it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1212/3257 [00:06<00:13, 148.97it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1233/3257 [00:07<00:12, 163.39it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1250/3257 [00:07<00:12, 162.56it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1270/3257 [00:07<00:11, 170.38it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1288/3257 [00:07<00:20, 97.20it/s]  40%|‚ñà‚ñà‚ñà‚ñà      | 1306/3257 [00:07<00:17, 112.17it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1324/3257 [00:07<00:15, 126.04it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1343/3257 [00:07<00:13, 140.58it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1360/3257 [00:08<00:13, 145.23it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1377/3257 [00:08<00:12, 148.80it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1394/3257 [00:08<00:12, 153.31it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1416/3257 [00:08<00:10, 171.36it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1435/3257 [00:08<00:10, 169.71it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1458/3257 [00:08<00:09, 185.78it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1478/3257 [00:08<00:09, 187.00it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1500/3257 [00:08<00:09, 195.03it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1520/3257 [00:08<00:09, 182.89it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1539/3257 [00:09<00:09, 174.80it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1557/3257 [00:09<00:10, 168.94it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1575/3257 [00:09<00:10, 167.85it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1595/3257 [00:09<00:09, 175.98it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:09<00:08, 182.73it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:09<00:09, 174.21it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1652/3257 [00:09<00:09, 172.03it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1670/3257 [00:09<00:09, 160.66it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1687/3257 [00:09<00:09, 159.42it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1706/3257 [00:10<00:09, 167.01it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1724/3257 [00:10<00:09, 170.19it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1742/3257 [00:10<00:09, 152.06it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:10<00:09, 163.22it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1783/3257 [00:10<00:08, 172.75it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1801/3257 [00:10<00:08, 171.43it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1819/3257 [00:10<00:08, 169.67it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1837/3257 [00:10<00:08, 165.86it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1854/3257 [00:10<00:08, 165.97it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1874/3257 [00:11<00:07, 174.29it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1892/3257 [00:11<00:07, 170.93it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:11<00:07, 180.72it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1939/3257 [00:11<00:06, 193.72it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:11<00:06, 208.26it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1986/3257 [00:11<00:06, 200.90it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2007/3257 [00:11<00:06, 202.55it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2030/3257 [00:11<00:05, 208.44it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2051/3257 [00:11<00:06, 187.68it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2071/3257 [00:12<00:06, 185.73it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2091/3257 [00:12<00:06, 187.22it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2110/3257 [00:12<00:06, 185.51it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2129/3257 [00:12<00:06, 174.87it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2147/3257 [00:12<00:06, 174.44it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:12<00:05, 184.32it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2188/3257 [00:12<00:05, 183.84it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2209/3257 [00:12<00:05, 184.95it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2230/3257 [00:12<00:05, 191.30it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2250/3257 [00:12<00:05, 187.73it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2269/3257 [00:13<00:05, 188.34it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2288/3257 [00:13<00:05, 188.31it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2307/3257 [00:13<00:05, 186.58it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2331/3257 [00:13<00:04, 201.29it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2359/3257 [00:13<00:04, 223.68it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2382/3257 [00:13<00:04, 214.56it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:13<00:04, 207.83it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2427/3257 [00:13<00:03, 209.34it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2449/3257 [00:13<00:04, 186.35it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2475/3257 [00:14<00:03, 201.69it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2498/3257 [00:14<00:03, 208.65it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2521/3257 [00:14<00:03, 212.18it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2543/3257 [00:14<00:03, 212.44it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2565/3257 [00:14<00:03, 202.55it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2586/3257 [00:14<00:03, 193.36it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2610/3257 [00:14<00:03, 205.73it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2634/3257 [00:14<00:02, 215.00it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2656/3257 [00:14<00:02, 205.79it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2677/3257 [00:15<00:02, 200.84it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2698/3257 [00:15<00:02, 190.26it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2718/3257 [00:15<00:02, 185.67it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2740/3257 [00:15<00:02, 193.54it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2760/3257 [00:15<00:02, 192.39it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2780/3257 [00:15<00:04, 104.56it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2805/3257 [00:16<00:03, 130.07it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2823/3257 [00:16<00:03, 139.21it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2841/3257 [00:16<00:02, 145.35it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2870/3257 [00:16<00:02, 178.79it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2891/3257 [00:16<00:01, 185.99it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2912/3257 [00:16<00:01, 185.73it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2933/3257 [00:16<00:01, 189.08it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2953/3257 [00:16<00:01, 184.82it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:16<00:01, 188.99it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2996/3257 [00:16<00:01, 191.94it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3018/3257 [00:17<00:01, 196.22it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3044/3257 [00:17<00:00, 213.44it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3069/3257 [00:17<00:00, 223.74it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3092/3257 [00:17<00:00, 220.36it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3118/3257 [00:17<00:00, 230.20it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3142/3257 [00:17<00:00, 215.50it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3164/3257 [00:17<00:00, 210.16it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3186/3257 [00:17<00:00, 205.81it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3208/3257 [00:17<00:00, 207.51it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3229/3257 [00:18<00:00, 206.90it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:18<00:00, 207.07it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 179.03it/s]
2023-02-07 20:21:07.481 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:21:07,482][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d357,n5,mc2,s0.30687,t4>', 'datetime': '2023-02-07T20:21:07.482727', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:21:07,483][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:21:07,483][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:21:07,985][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 20:21:07,985][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:21:08,079][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 36619 unique words (85.76% of original 42701, drops 6082)', 'datetime': '2023-02-07T20:21:08.079567', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:21:08,079][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 5816910 word corpus (99.90% of original 5822992, drops 6082)', 'datetime': '2023-02-07T20:21:08.079983', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:21:08,201][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 20:21:08,202][gensim.models.word2vec][INFO] - sample=0.30687 downsamples 0 most-common words
[2023-02-07 20:21:08,202][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5816910 word corpus (100.0%% of prior 5816910)', 'datetime': '2023-02-07T20:21:08.202536', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:21:08,403][gensim.models.word2vec][INFO] - estimated required memory for 36619 words and 357 dimensions: 128195760 bytes
[2023-02-07 20:21:08,404][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:21:08,456][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 36619 vocabulary and 357 features, using sg=1 hs=0 sample=0.3068698448515564 negative=5 window=9 shrink_windows=True', 'datetime': '2023-02-07T20:21:08.456535', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:21:09,459][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 22.90% examples, 1316741 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:10,467][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 45.23% examples, 1329851 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:11,471][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 68.84% examples, 1351814 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:12,483][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 93.77% examples, 1354430 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:21:12,715][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5790320 effective words) took 4.3s, 1360384 effective words/s
[2023-02-07 20:21:13,722][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 24.26% examples, 1390589 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:14,729][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 48.20% examples, 1421369 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:21:15,734][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 73.81% examples, 1435123 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:16,734][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5790320 effective words) took 4.0s, 1442065 effective words/s
[2023-02-07 20:21:17,737][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 31.81% examples, 1854443 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:18,746][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 63.49% examples, 1859971 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:19,755][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 96.56% examples, 1853771 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:19,855][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5790320 effective words) took 3.1s, 1855888 effective words/s
[2023-02-07 20:21:20,867][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 29.81% examples, 1710559 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:21:21,870][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 61.13% examples, 1787132 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:22,874][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 87.14% examples, 1687401 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:23,347][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5790320 effective words) took 3.5s, 1658410 effective words/s
[2023-02-07 20:21:24,353][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 24.96% examples, 1434108 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:25,353][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 48.39% examples, 1432009 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:21:26,358][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 72.98% examples, 1425446 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:21:27,363][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 98.59% examples, 1424682 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:27,400][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5790320 effective words) took 4.1s, 1429604 effective words/s
[2023-02-07 20:21:28,407][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 30.03% examples, 1730994 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:29,410][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 60.58% examples, 1772380 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:30,411][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 86.95% examples, 1688836 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:30,894][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5790320 effective words) took 3.5s, 1658301 effective words/s
[2023-02-07 20:21:31,906][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 25.24% examples, 1440893 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:21:32,917][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 49.65% examples, 1449153 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:33,917][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 74.64% examples, 1453886 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:34,869][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5790320 effective words) took 4.0s, 1457845 effective words/s
[2023-02-07 20:21:35,875][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 25.27% examples, 1446638 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:21:36,879][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 49.68% examples, 1459084 words/s, in_qsize 5, out_qsize 2
[2023-02-07 20:21:37,880][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 74.55% examples, 1455738 words/s, in_qsize 7, out_qsize 4
[2023-02-07 20:21:38,800][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5790320 effective words) took 3.9s, 1473634 effective words/s
[2023-02-07 20:21:39,802][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 32.21% examples, 1885935 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:40,803][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 64.23% examples, 1891726 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:41,808][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 98.25% examples, 1894111 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:41,853][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5790320 effective words) took 3.1s, 1897365 effective words/s
[2023-02-07 20:21:42,856][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 24.81% examples, 1427701 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:43,858][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 48.63% examples, 1436843 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:44,867][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 73.56% examples, 1434658 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:21:45,873][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 98.93% examples, 1427848 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:21:45,897][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5790320 effective words) took 4.0s, 1432941 effective words/s
[2023-02-07 20:21:46,902][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 24.65% examples, 1410258 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:47,907][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 47.93% examples, 1417557 words/s, in_qsize 6, out_qsize 0
[2023-02-07 20:21:48,906][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 72.37% examples, 1415391 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:49,911][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 98.22% examples, 1419235 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:49,967][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5790320 effective words) took 4.1s, 1423922 effective words/s
[2023-02-07 20:21:50,971][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 28.71% examples, 1667104 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:51,975][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 60.30% examples, 1766811 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:52,978][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 92.94% examples, 1800181 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:21:53,182][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5790320 effective words) took 3.2s, 1802231 effective words/s
[2023-02-07 20:21:54,187][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 25.08% examples, 1437803 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:55,188][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 48.79% examples, 1440149 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:56,189][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 74.64% examples, 1461122 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:56,916][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5790320 effective words) took 3.7s, 1551515 effective words/s
[2023-02-07 20:21:57,927][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 28.34% examples, 1629282 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:58,931][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 54.04% examples, 1590717 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:21:59,933][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 81.12% examples, 1570070 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:22:00,622][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5790320 effective words) took 3.7s, 1562833 effective words/s
[2023-02-07 20:22:01,639][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 26.59% examples, 1524751 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:22:02,639][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 52.29% examples, 1534679 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:22:03,640][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 78.05% examples, 1513017 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:22:04,483][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5790320 effective words) took 3.9s, 1500440 effective words/s
[2023-02-07 20:22:04,484][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86854800 effective words) took 56.0s, 1550224 effective words/s', 'datetime': '2023-02-07T20:22:04.484185', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:22:04.484 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:22:09,223][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202041-8cj685cl/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:22:09.223880', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:22:09,224][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202041-8cj685cl/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 20:22:09,278][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202041-8cj685cl/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 20:22:09,326][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:22:09,363][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202041-8cj685cl/files/../tmp/embedding_model.pt
2023-02-07 20:22:09.363 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:22:11.308 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:22:11.967 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:22:28.833 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.228056734434275, 'test_mae': 1.1311433589488749, 'test_r2': -1.524301264750434}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.033 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.033 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.48
wandb: percentage 0.14243
wandb:   test_mae 1.13114
wandb:   test_mse 2.22806
wandb:    test_r2 -1.5243
wandb: 
wandb: üöÄ View run playful-sweep-81 at: https://wandb.ai/xiaoqiz/mof2vec/runs/8cj685cl
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_202041-8cj685cl/logs
wandb: Agent Starting Run: jmfe1078 with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 886
wandb: 	model.gensim.alpha: 0.0917985266331025
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.2616678054777727
wandb: 	model.gensim.vector_size: 420
wandb: 	model.gensim.window: 17
wandb: 	model.sklearn.learning_rate: 0.2941332350616907
wandb: 	model.sklearn.max_depth: 56
wandb: 	model.sklearn.min_child_weight: 0.05024585050132213
wandb: 	model.sklearn.n_estimators: 1574
wandb: 	model.sklearn.num_leaves: 256
wandb: 	model.sklearn.reg_alpha: 0.2556211451834434
wandb: 	model.sklearn.reg_lambda: 0.018979854321627077
wandb: 	model.sklearn.subsample: 0.9591638704347438
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202237-jmfe1078
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-82
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/jmfe1078
2023-02-07 20:22:45.960 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 20:22:45.961 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 886 for sweep.
2023-02-07 20:22:45.961 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0917985266331025 for sweep.
2023-02-07 20:22:45.961 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:22:45.962 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 20:22:45.962 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2616678054777727 for sweep.
2023-02-07 20:22:45.962 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 420 for sweep.
2023-02-07 20:22:45.962 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 17 for sweep.
2023-02-07 20:22:45.962 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.2941332350616907 for sweep.
2023-02-07 20:22:45.963 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 56 for sweep.
2023-02-07 20:22:45.963 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05024585050132213 for sweep.
2023-02-07 20:22:45.963 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1574 for sweep.
2023-02-07 20:22:45.963 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 256 for sweep.
2023-02-07 20:22:45.963 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.2556211451834434 for sweep.
2023-02-07 20:22:45.964 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.018979854321627077 for sweep.
2023-02-07 20:22:45.964 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.9591638704347438 for sweep.
2023-02-07 20:22:45.964 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:22:45.971 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202237-jmfe1078/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 886, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 420, 'window': 17, 'min_count': 4, 'dm': 0, 'sample': 0.2616678054777727, 'workers': 4, 'alpha': 0.0917985266331025, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1574, 'max_depth': 56, 'num_leaves': 256, 'reg_alpha': 0.2556211451834434, 'reg_lambda': 0.018979854321627077, 'subsample': 0.9591638704347438, 'min_child_weight': 0.05024585050132213, 'n_jobs': 4, 'learning_rate': 0.2941332350616907}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 16/3257 [00:00<00:21, 153.40it/s]  1%|          | 34/3257 [00:00<00:20, 156.10it/s]  2%|‚ñè         | 53/3257 [00:00<00:20, 159.89it/s]  2%|‚ñè         | 71/3257 [00:00<00:19, 165.27it/s]  3%|‚ñé         | 90/3257 [00:00<00:18, 172.93it/s]  3%|‚ñé         | 108/3257 [00:00<00:19, 160.00it/s]  4%|‚ñç         | 125/3257 [00:00<00:19, 158.57it/s]  4%|‚ñç         | 146/3257 [00:00<00:18, 171.52it/s]  5%|‚ñå         | 164/3257 [00:01<00:19, 162.69it/s]  6%|‚ñå         | 181/3257 [00:01<00:18, 162.94it/s]  6%|‚ñå         | 201/3257 [00:01<00:18, 166.06it/s]  7%|‚ñã         | 223/3257 [00:01<00:16, 179.80it/s]  7%|‚ñã         | 243/3257 [00:01<00:16, 181.50it/s]  8%|‚ñä         | 262/3257 [00:01<00:17, 168.98it/s]  9%|‚ñâ         | 285/3257 [00:01<00:16, 181.82it/s]  9%|‚ñâ         | 304/3257 [00:01<00:16, 175.17it/s] 10%|‚ñâ         | 324/3257 [00:01<00:16, 180.99it/s] 11%|‚ñà         | 343/3257 [00:02<00:17, 167.50it/s] 11%|‚ñà         | 362/3257 [00:02<00:16, 170.82it/s] 12%|‚ñà‚ñè        | 380/3257 [00:02<00:18, 157.05it/s] 12%|‚ñà‚ñè        | 397/3257 [00:02<00:18, 156.23it/s] 13%|‚ñà‚ñé        | 415/3257 [00:02<00:17, 162.05it/s] 13%|‚ñà‚ñé        | 432/3257 [00:02<00:19, 144.39it/s] 14%|‚ñà‚ñé        | 447/3257 [00:02<00:19, 141.66it/s] 14%|‚ñà‚ñç        | 466/3257 [00:02<00:18, 152.55it/s] 15%|‚ñà‚ñç        | 482/3257 [00:02<00:18, 151.30it/s] 15%|‚ñà‚ñå        | 502/3257 [00:03<00:16, 162.24it/s] 16%|‚ñà‚ñå        | 519/3257 [00:03<00:16, 163.61it/s] 16%|‚ñà‚ñã        | 536/3257 [00:03<00:17, 158.72it/s] 17%|‚ñà‚ñã        | 553/3257 [00:03<00:16, 160.40it/s] 18%|‚ñà‚ñä        | 570/3257 [00:03<00:17, 151.76it/s] 18%|‚ñà‚ñä        | 586/3257 [00:03<00:18, 147.07it/s] 19%|‚ñà‚ñä        | 603/3257 [00:03<00:17, 152.67it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:17, 148.79it/s] 20%|‚ñà‚ñâ        | 639/3257 [00:03<00:16, 159.28it/s] 20%|‚ñà‚ñà        | 656/3257 [00:04<00:17, 150.09it/s] 21%|‚ñà‚ñà        | 674/3257 [00:04<00:16, 156.49it/s] 21%|‚ñà‚ñà        | 690/3257 [00:04<00:17, 149.85it/s] 22%|‚ñà‚ñà‚ñè       | 708/3257 [00:04<00:16, 157.46it/s] 22%|‚ñà‚ñà‚ñè       | 724/3257 [00:04<00:16, 151.79it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:04<00:16, 149.35it/s] 23%|‚ñà‚ñà‚ñé       | 759/3257 [00:04<00:15, 159.26it/s] 24%|‚ñà‚ñà‚ñç       | 776/3257 [00:04<00:16, 153.32it/s] 24%|‚ñà‚ñà‚ñç       | 793/3257 [00:04<00:15, 156.13it/s] 25%|‚ñà‚ñà‚ñç       | 809/3257 [00:05<00:15, 155.40it/s] 25%|‚ñà‚ñà‚ñå       | 825/3257 [00:05<00:16, 149.60it/s] 26%|‚ñà‚ñà‚ñå       | 841/3257 [00:05<00:16, 143.78it/s] 26%|‚ñà‚ñà‚ñã       | 858/3257 [00:05<00:16, 149.93it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:05<00:15, 150.89it/s] 27%|‚ñà‚ñà‚ñã       | 890/3257 [00:05<00:15, 148.75it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:05<00:23, 101.68it/s] 28%|‚ñà‚ñà‚ñä       | 924/3257 [00:05<00:19, 119.55it/s] 29%|‚ñà‚ñà‚ñâ       | 940/3257 [00:06<00:17, 128.97it/s] 29%|‚ñà‚ñà‚ñâ       | 959/3257 [00:06<00:15, 143.72it/s] 30%|‚ñà‚ñà‚ñà       | 978/3257 [00:06<00:14, 154.95it/s] 31%|‚ñà‚ñà‚ñà       | 995/3257 [00:06<00:15, 149.90it/s] 31%|‚ñà‚ñà‚ñà       | 1011/3257 [00:06<00:14, 152.27it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:06<00:14, 149.92it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1043/3257 [00:06<00:14, 150.26it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1062/3257 [00:06<00:13, 159.49it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:06<00:13, 162.72it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1098/3257 [00:07<00:12, 166.90it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:07<00:12, 167.54it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1134/3257 [00:07<00:13, 160.79it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1151/3257 [00:07<00:13, 156.05it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:07<00:12, 164.68it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1188/3257 [00:07<00:13, 148.46it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1204/3257 [00:07<00:13, 149.03it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1220/3257 [00:07<00:13, 151.03it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:07<00:12, 165.27it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1259/3257 [00:08<00:12, 163.12it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1276/3257 [00:08<00:12, 159.48it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1293/3257 [00:08<00:12, 153.56it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1310/3257 [00:08<00:12, 158.02it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:08<00:11, 161.62it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1348/3257 [00:08<00:11, 170.87it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1366/3257 [00:08<00:11, 163.83it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1383/3257 [00:08<00:11, 158.42it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1402/3257 [00:08<00:11, 164.75it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1424/3257 [00:09<00:10, 176.80it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1443/3257 [00:09<00:10, 178.44it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1465/3257 [00:09<00:09, 189.34it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1485/3257 [00:09<00:09, 184.33it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1507/3257 [00:09<00:09, 192.74it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1527/3257 [00:09<00:10, 172.29it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1545/3257 [00:09<00:10, 162.45it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1565/3257 [00:09<00:10, 167.80it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1583/3257 [00:09<00:10, 166.31it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1602/3257 [00:10<00:09, 170.75it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1620/3257 [00:10<00:09, 171.22it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1638/3257 [00:10<00:09, 165.55it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1655/3257 [00:10<00:09, 163.63it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1672/3257 [00:10<00:10, 155.43it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1690/3257 [00:10<00:09, 158.25it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1708/3257 [00:10<00:09, 163.83it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:10<00:09, 159.46it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1745/3257 [00:10<00:09, 158.44it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1765/3257 [00:11<00:08, 167.29it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1785/3257 [00:11<00:08, 175.46it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1803/3257 [00:11<00:08, 175.16it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1821/3257 [00:11<00:08, 173.01it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1839/3257 [00:11<00:08, 169.06it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1857/3257 [00:11<00:08, 170.72it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1877/3257 [00:11<00:07, 177.43it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1895/3257 [00:11<00:08, 170.21it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:11<00:07, 173.87it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1937/3257 [00:12<00:07, 179.87it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1963/3257 [00:12<00:06, 199.72it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1984/3257 [00:12<00:06, 183.28it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2005/3257 [00:12<00:06, 189.04it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2025/3257 [00:12<00:06, 190.73it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2045/3257 [00:12<00:06, 182.63it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2064/3257 [00:12<00:07, 165.90it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2083/3257 [00:12<00:06, 169.88it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2101/3257 [00:12<00:07, 162.45it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:13<00:10, 110.38it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2137/3257 [00:13<00:09, 121.51it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2154/3257 [00:13<00:08, 130.02it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2175/3257 [00:13<00:07, 148.40it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2192/3257 [00:13<00:06, 152.81it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2209/3257 [00:13<00:06, 151.15it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2228/3257 [00:13<00:06, 159.91it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2245/3257 [00:14<00:06, 159.99it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2264/3257 [00:14<00:05, 166.70it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2282/3257 [00:14<00:05, 164.06it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2301/3257 [00:14<00:05, 169.35it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2326/3257 [00:14<00:04, 190.85it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2350/3257 [00:14<00:04, 204.83it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2371/3257 [00:14<00:04, 199.34it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2394/3257 [00:14<00:04, 205.30it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2415/3257 [00:14<00:04, 189.64it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2435/3257 [00:15<00:04, 175.16it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2453/3257 [00:15<00:04, 172.25it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2475/3257 [00:15<00:04, 182.91it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2495/3257 [00:15<00:04, 186.95it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2516/3257 [00:15<00:03, 192.98it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2537/3257 [00:15<00:03, 195.10it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2557/3257 [00:15<00:03, 181.14it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2576/3257 [00:15<00:04, 169.29it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2594/3257 [00:15<00:04, 165.45it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2619/3257 [00:16<00:03, 186.32it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:16<00:03, 190.15it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2660/3257 [00:16<00:03, 172.82it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2680/3257 [00:16<00:03, 179.88it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2699/3257 [00:16<00:03, 164.12it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2716/3257 [00:16<00:03, 157.67it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2739/3257 [00:16<00:02, 175.94it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2758/3257 [00:16<00:02, 176.63it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2777/3257 [00:16<00:02, 171.33it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2800/3257 [00:17<00:02, 187.21it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2820/3257 [00:17<00:02, 178.09it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2839/3257 [00:17<00:02, 168.72it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2863/3257 [00:17<00:02, 186.76it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:17<00:01, 191.50it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2906/3257 [00:17<00:01, 176.23it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2927/3257 [00:17<00:01, 184.74it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2946/3257 [00:17<00:01, 164.51it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2964/3257 [00:18<00:01, 166.26it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2982/3257 [00:18<00:01, 153.67it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3002/3257 [00:18<00:01, 164.19it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3019/3257 [00:18<00:01, 161.62it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3039/3257 [00:18<00:01, 171.06it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3060/3257 [00:18<00:01, 180.76it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:18<00:00, 182.99it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3098/3257 [00:18<00:00, 176.95it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3120/3257 [00:18<00:00, 188.70it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3140/3257 [00:19<00:00, 180.13it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3159/3257 [00:19<00:00, 174.31it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3177/3257 [00:19<00:00, 169.71it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3197/3257 [00:19<00:00, 174.14it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3215/3257 [00:19<00:00, 164.83it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3237/3257 [00:19<00:00, 177.43it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 181.99it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 165.24it/s]
2023-02-07 20:23:06.691 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:23:06,692][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d420,n5,mc4,s0.261668,t4>', 'datetime': '2023-02-07T20:23:06.692564', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:23:06,693][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:23:06,693][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:23:07,256][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 20:23:07,257][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:23:07,354][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 36921 unique words (68.30% of original 54054, drops 17133)', 'datetime': '2023-02-07T20:23:07.354704', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:23:07,356][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 6522479 word corpus (99.57% of original 6550866, drops 28387)', 'datetime': '2023-02-07T20:23:07.356127', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:23:07,475][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 20:23:07,476][gensim.models.word2vec][INFO] - sample=0.261668 downsamples 0 most-common words
[2023-02-07 20:23:07,477][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6522479 word corpus (100.0%% of prior 6522479)', 'datetime': '2023-02-07T20:23:07.477152', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:23:07,677][gensim.models.word2vec][INFO] - estimated required memory for 36921 words and 420 dimensions: 148638220 bytes
[2023-02-07 20:23:07,677][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:23:07,738][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 36921 vocabulary and 420 features, using sg=1 hs=0 sample=0.2616678054777727 negative=5 window=17 shrink_windows=True', 'datetime': '2023-02-07T20:23:07.738957', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:23:08,745][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 24.13% examples, 1543367 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:23:09,746][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 55.20% examples, 1818879 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:10,750][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 88.09% examples, 1904849 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:11,093][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6467889 effective words) took 3.4s, 1929384 effective words/s
[2023-02-07 20:23:12,099][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 34.45% examples, 2242023 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:13,099][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 69.24% examples, 2283498 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:13,903][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6467889 effective words) took 2.8s, 2303125 effective words/s
[2023-02-07 20:23:14,908][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 35.34% examples, 2311368 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:15,908][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 70.19% examples, 2312715 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:16,714][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6467889 effective words) took 2.8s, 2301675 effective words/s
[2023-02-07 20:23:17,718][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 35.00% examples, 2287918 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:18,723][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 69.85% examples, 2305012 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:19,520][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6467889 effective words) took 2.8s, 2306141 effective words/s
[2023-02-07 20:23:20,522][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 35.12% examples, 2299377 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:21,524][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 69.85% examples, 2310590 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:22,312][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6467889 effective words) took 2.8s, 2317976 effective words/s
[2023-02-07 20:23:23,315][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 28.74% examples, 1863214 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:23:24,315][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 56.09% examples, 1855623 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:25,323][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 85.48% examples, 1853973 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:25,797][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6467889 effective words) took 3.5s, 1856597 effective words/s
[2023-02-07 20:23:26,805][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 30.55% examples, 1972808 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:27,807][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 59.53% examples, 1953862 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:23:28,820][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 89.16% examples, 1923106 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:23:29,165][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6467889 effective words) took 3.4s, 1921608 effective words/s
[2023-02-07 20:23:30,173][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 28.77% examples, 1853297 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:31,182][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 57.51% examples, 1883635 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:32,184][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 86.89% examples, 1877478 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:32,609][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6467889 effective words) took 3.4s, 1879057 effective words/s
[2023-02-07 20:23:33,614][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 35.95% examples, 2350719 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:34,617][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 72.77% examples, 2381916 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:35,327][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6467889 effective words) took 2.7s, 2381443 effective words/s
[2023-02-07 20:23:36,331][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 30.06% examples, 1947125 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:37,332][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 60.18% examples, 1971928 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:38,333][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 98.43% examples, 2120773 words/s, in_qsize 7, out_qsize 1
[2023-02-07 20:23:38,367][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6467889 effective words) took 3.0s, 2129062 effective words/s
[2023-02-07 20:23:39,370][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 30.12% examples, 1958187 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:40,373][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 59.07% examples, 1949577 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:41,376][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 90.36% examples, 1957928 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:41,673][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6467889 effective words) took 3.3s, 1958030 effective words/s
[2023-02-07 20:23:42,677][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 35.92% examples, 2343645 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:43,679][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 73.10% examples, 2393871 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:44,388][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6467889 effective words) took 2.7s, 2383928 effective words/s
[2023-02-07 20:23:45,394][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 36.94% examples, 2433786 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:23:46,398][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 74.92% examples, 2449187 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:47,155][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6467889 effective words) took 2.8s, 2339460 effective words/s
[2023-02-07 20:23:48,163][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 28.71% examples, 1845026 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:49,164][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 56.25% examples, 1850654 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:23:50,165][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 84.99% examples, 1845170 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:50,673][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6467889 effective words) took 3.5s, 1839565 effective words/s
[2023-02-07 20:23:51,680][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 28.03% examples, 1809329 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:52,682][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 54.53% examples, 1798383 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:23:53,691][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 82.93% examples, 1798438 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:23:54,274][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6467889 effective words) took 3.6s, 1798169 effective words/s
[2023-02-07 20:23:54,274][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (97018335 effective words) took 46.5s, 2084838 effective words/s', 'datetime': '2023-02-07T20:23:54.274546', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:23:54.274 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:23:58,126][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202237-jmfe1078/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:23:58.126325', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:23:58,127][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202237-jmfe1078/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 20:23:58,183][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202237-jmfe1078/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 20:23:58,236][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:23:58,272][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202237-jmfe1078/files/../tmp/embedding_model.pt
2023-02-07 20:23:58.272 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:24:00.382 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:24:01.131 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:24:03.803 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.152205462644095, 'test_mae': 1.061212132652033, 'test_r2': -2.583435124072595}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.94
wandb: percentage 0.31696
wandb:   test_mae 1.06121
wandb:   test_mse 2.15221
wandb:    test_r2 -2.58344
wandb: 
wandb: üöÄ View run avid-sweep-82 at: https://wandb.ai/xiaoqiz/mof2vec/runs/jmfe1078
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_202237-jmfe1078/logs
wandb: Agent Starting Run: 4tfaw2rf with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 989
wandb: 	model.gensim.alpha: 0.06357182872893935
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.30171179907731666
wandb: 	model.gensim.vector_size: 412
wandb: 	model.gensim.window: 14
wandb: 	model.sklearn.learning_rate: 0.0004689145831227651
wandb: 	model.sklearn.max_depth: 60
wandb: 	model.sklearn.min_child_weight: 0.07555664107782531
wandb: 	model.sklearn.n_estimators: 3839
wandb: 	model.sklearn.num_leaves: 408
wandb: 	model.sklearn.reg_alpha: 0.3366372787014214
wandb: 	model.sklearn.reg_lambda: 0.4724653529485415
wandb: 	model.sklearn.subsample: 0.5113771139032927
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202414-4tfaw2rf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-83
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/4tfaw2rf
2023-02-07 20:24:23.162 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 20:24:23.163 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 989 for sweep.
2023-02-07 20:24:23.164 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.06357182872893935 for sweep.
2023-02-07 20:24:23.164 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:24:23.164 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 20:24:23.165 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.30171179907731666 for sweep.
2023-02-07 20:24:23.165 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 412 for sweep.
2023-02-07 20:24:23.165 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 14 for sweep.
2023-02-07 20:24:23.165 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0004689145831227651 for sweep.
2023-02-07 20:24:23.165 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 60 for sweep.
2023-02-07 20:24:23.166 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.07555664107782531 for sweep.
2023-02-07 20:24:23.166 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3839 for sweep.
2023-02-07 20:24:23.166 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 408 for sweep.
2023-02-07 20:24:23.166 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.3366372787014214 for sweep.
2023-02-07 20:24:23.167 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.4724653529485415 for sweep.
2023-02-07 20:24:23.167 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5113771139032927 for sweep.
2023-02-07 20:24:23.167 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:24:23.173 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202414-4tfaw2rf/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 989, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 412, 'window': 14, 'min_count': 4, 'dm': 0, 'sample': 0.30171179907731666, 'workers': 4, 'alpha': 0.06357182872893935, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3839, 'max_depth': 60, 'num_leaves': 408, 'reg_alpha': 0.3366372787014214, 'reg_lambda': 0.4724653529485415, 'subsample': 0.5113771139032927, 'min_child_weight': 0.07555664107782531, 'n_jobs': 4, 'learning_rate': 0.0004689145831227651}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 15/3257 [00:00<00:21, 149.77it/s]  1%|          | 33/3257 [00:00<00:19, 164.05it/s]  2%|‚ñè         | 50/3257 [00:00<00:19, 162.50it/s]  2%|‚ñè         | 67/3257 [00:00<00:20, 158.28it/s]  3%|‚ñé         | 89/3257 [00:00<00:18, 175.78it/s]  3%|‚ñé         | 107/3257 [00:00<00:20, 154.04it/s]  4%|‚ñç         | 124/3257 [00:00<00:19, 157.81it/s]  4%|‚ñç         | 145/3257 [00:00<00:18, 171.36it/s]  5%|‚ñå         | 163/3257 [00:01<00:19, 157.62it/s]  6%|‚ñå         | 180/3257 [00:01<00:19, 159.53it/s]  6%|‚ñå         | 199/3257 [00:01<00:18, 167.52it/s]  7%|‚ñã         | 218/3257 [00:01<00:17, 173.47it/s]  7%|‚ñã         | 237/3257 [00:01<00:17, 175.13it/s]  8%|‚ñä         | 255/3257 [00:01<00:17, 176.51it/s]  8%|‚ñä         | 273/3257 [00:01<00:17, 174.99it/s]  9%|‚ñâ         | 295/3257 [00:01<00:15, 187.08it/s] 10%|‚ñâ         | 314/3257 [00:01<00:17, 172.93it/s] 10%|‚ñà         | 334/3257 [00:01<00:16, 179.59it/s] 11%|‚ñà         | 353/3257 [00:02<00:16, 172.93it/s] 11%|‚ñà‚ñè        | 371/3257 [00:02<00:24, 119.59it/s] 12%|‚ñà‚ñè        | 386/3257 [00:02<00:24, 118.68it/s] 12%|‚ñà‚ñè        | 404/3257 [00:02<00:21, 131.63it/s] 13%|‚ñà‚ñé        | 421/3257 [00:02<00:20, 137.87it/s] 13%|‚ñà‚ñé        | 436/3257 [00:02<00:22, 126.17it/s] 14%|‚ñà‚ñç        | 454/3257 [00:02<00:20, 138.92it/s] 15%|‚ñà‚ñç        | 474/3257 [00:03<00:18, 151.58it/s] 15%|‚ñà‚ñå        | 493/3257 [00:03<00:17, 160.80it/s] 16%|‚ñà‚ñå        | 514/3257 [00:03<00:15, 173.66it/s] 16%|‚ñà‚ñã        | 532/3257 [00:03<00:16, 169.77it/s] 17%|‚ñà‚ñã        | 552/3257 [00:03<00:15, 177.81it/s] 18%|‚ñà‚ñä        | 571/3257 [00:03<00:17, 155.39it/s] 18%|‚ñà‚ñä        | 589/3257 [00:03<00:16, 161.51it/s] 19%|‚ñà‚ñä        | 610/3257 [00:03<00:15, 173.08it/s] 19%|‚ñà‚ñâ        | 628/3257 [00:03<00:15, 174.16it/s] 20%|‚ñà‚ñâ        | 646/3257 [00:04<00:15, 170.14it/s] 20%|‚ñà‚ñà        | 664/3257 [00:04<00:16, 161.94it/s] 21%|‚ñà‚ñà        | 683/3257 [00:04<00:15, 161.91it/s] 22%|‚ñà‚ñà‚ñè       | 701/3257 [00:04<00:15, 166.34it/s] 22%|‚ñà‚ñà‚ñè       | 720/3257 [00:04<00:14, 171.41it/s] 23%|‚ñà‚ñà‚ñé       | 738/3257 [00:04<00:15, 158.14it/s] 23%|‚ñà‚ñà‚ñé       | 757/3257 [00:04<00:15, 166.00it/s] 24%|‚ñà‚ñà‚ñç       | 775/3257 [00:04<00:14, 167.36it/s] 24%|‚ñà‚ñà‚ñç       | 792/3257 [00:04<00:14, 167.26it/s] 25%|‚ñà‚ñà‚ñç       | 809/3257 [00:05<00:14, 165.95it/s] 25%|‚ñà‚ñà‚ñå       | 826/3257 [00:05<00:15, 161.47it/s] 26%|‚ñà‚ñà‚ñå       | 843/3257 [00:05<00:15, 156.52it/s] 26%|‚ñà‚ñà‚ñã       | 861/3257 [00:05<00:14, 160.33it/s] 27%|‚ñà‚ñà‚ñã       | 878/3257 [00:05<00:15, 154.79it/s] 28%|‚ñà‚ñà‚ñä       | 897/3257 [00:05<00:14, 162.64it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:05<00:14, 163.37it/s] 29%|‚ñà‚ñà‚ñä       | 932/3257 [00:05<00:13, 167.65it/s] 29%|‚ñà‚ñà‚ñâ       | 949/3257 [00:05<00:13, 167.35it/s] 30%|‚ñà‚ñà‚ñâ       | 968/3257 [00:06<00:13, 171.04it/s] 30%|‚ñà‚ñà‚ñà       | 986/3257 [00:06<00:13, 162.47it/s] 31%|‚ñà‚ñà‚ñà       | 1004/3257 [00:06<00:13, 165.48it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1021/3257 [00:06<00:13, 164.69it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1038/3257 [00:06<00:14, 155.18it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1054/3257 [00:06<00:14, 155.02it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1073/3257 [00:06<00:13, 163.85it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1090/3257 [00:06<00:13, 162.17it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1108/3257 [00:06<00:12, 165.95it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1125/3257 [00:07<00:14, 151.73it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1141/3257 [00:07<00:14, 148.56it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1160/3257 [00:07<00:13, 157.81it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1176/3257 [00:07<00:13, 156.87it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1192/3257 [00:07<00:14, 146.97it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1207/3257 [00:07<00:13, 146.55it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1227/3257 [00:07<00:12, 160.87it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1245/3257 [00:07<00:12, 162.31it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1262/3257 [00:07<00:12, 163.59it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1279/3257 [00:08<00:12, 153.39it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1295/3257 [00:08<00:12, 151.01it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1313/3257 [00:08<00:12, 156.70it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1331/3257 [00:08<00:11, 160.81it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1349/3257 [00:08<00:11, 164.31it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1366/3257 [00:08<00:12, 155.49it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1382/3257 [00:08<00:12, 151.02it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1401/3257 [00:08<00:11, 159.68it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:08<00:10, 170.58it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1441/3257 [00:08<00:10, 171.34it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1463/3257 [00:09<00:09, 183.82it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1482/3257 [00:09<00:09, 183.96it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1503/3257 [00:09<00:09, 188.61it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1522/3257 [00:09<00:10, 173.11it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1540/3257 [00:09<00:10, 170.47it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1558/3257 [00:09<00:10, 164.58it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1575/3257 [00:09<00:10, 161.74it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1595/3257 [00:09<00:09, 171.07it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1613/3257 [00:10<00:15, 108.78it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1633/3257 [00:10<00:12, 126.61it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1649/3257 [00:10<00:12, 125.70it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1666/3257 [00:10<00:11, 135.56it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1683/3257 [00:10<00:11, 140.34it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1702/3257 [00:10<00:10, 152.85it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1719/3257 [00:10<00:09, 156.73it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1736/3257 [00:10<00:10, 145.93it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1754/3257 [00:11<00:09, 154.77it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1774/3257 [00:11<00:09, 164.03it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1794/3257 [00:11<00:08, 173.04it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1812/3257 [00:11<00:08, 163.16it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1829/3257 [00:11<00:08, 163.84it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1849/3257 [00:11<00:08, 173.36it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:11<00:07, 178.57it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1888/3257 [00:11<00:07, 180.30it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1908/3257 [00:11<00:07, 182.60it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1927/3257 [00:12<00:07, 175.51it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1952/3257 [00:12<00:06, 195.86it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1974/3257 [00:12<00:06, 197.06it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1994/3257 [00:12<00:06, 194.00it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2014/3257 [00:12<00:06, 187.18it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2034/3257 [00:12<00:06, 187.15it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2053/3257 [00:12<00:07, 171.04it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2071/3257 [00:12<00:07, 167.53it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2091/3257 [00:12<00:06, 174.95it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2109/3257 [00:13<00:06, 173.06it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2127/3257 [00:13<00:06, 162.17it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2144/3257 [00:13<00:07, 158.31it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2164/3257 [00:13<00:06, 168.53it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2182/3257 [00:13<00:06, 167.15it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2204/3257 [00:13<00:05, 179.89it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2223/3257 [00:13<00:05, 176.19it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2241/3257 [00:13<00:06, 162.51it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2260/3257 [00:13<00:06, 165.61it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2277/3257 [00:14<00:06, 157.91it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2298/3257 [00:14<00:05, 170.68it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2316/3257 [00:14<00:05, 171.52it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2340/3257 [00:14<00:04, 188.36it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2362/3257 [00:14<00:04, 197.28it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2382/3257 [00:14<00:04, 197.01it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2404/3257 [00:14<00:04, 201.98it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2425/3257 [00:14<00:04, 187.11it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2444/3257 [00:14<00:04, 174.86it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2465/3257 [00:15<00:04, 184.08it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2484/3257 [00:15<00:04, 183.55it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2508/3257 [00:15<00:03, 196.73it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2528/3257 [00:15<00:03, 196.34it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2548/3257 [00:15<00:03, 194.81it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2568/3257 [00:15<00:03, 175.25it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2586/3257 [00:15<00:03, 176.49it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2607/3257 [00:15<00:03, 184.05it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:15<00:03, 196.07it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2651/3257 [00:16<00:03, 182.52it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:16<00:03, 181.93it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2689/3257 [00:16<00:03, 182.30it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2708/3257 [00:16<00:03, 160.60it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2726/3257 [00:16<00:03, 164.68it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2748/3257 [00:16<00:02, 178.51it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2767/3257 [00:16<00:02, 181.19it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2786/3257 [00:16<00:02, 179.30it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2805/3257 [00:16<00:02, 182.32it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2824/3257 [00:17<00:02, 166.13it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2842/3257 [00:17<00:02, 169.37it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2867/3257 [00:17<00:02, 191.62it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2887/3257 [00:17<00:01, 190.23it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2907/3257 [00:17<00:02, 171.67it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:17<00:01, 173.99it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2946/3257 [00:17<00:01, 164.85it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2963/3257 [00:17<00:01, 165.59it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2980/3257 [00:17<00:01, 161.07it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3000/3257 [00:18<00:01, 170.03it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3018/3257 [00:18<00:01, 166.07it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3039/3257 [00:18<00:01, 175.23it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3061/3257 [00:18<00:01, 187.74it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3080/3257 [00:18<00:00, 187.98it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3099/3257 [00:18<00:00, 184.77it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3122/3257 [00:18<00:00, 195.18it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3142/3257 [00:18<00:00, 186.28it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3161/3257 [00:18<00:00, 181.49it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3180/3257 [00:19<00:00, 95.34it/s]  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3201/3257 [00:19<00:00, 114.38it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3217/3257 [00:19<00:00, 119.79it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3239/3257 [00:19<00:00, 139.37it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 148.55it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 164.63it/s]
2023-02-07 20:24:43.826 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:24:43,827][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d412,n5,mc4,s0.301712,t4>', 'datetime': '2023-02-07T20:24:43.827462', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:24:43,828][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:24:43,828][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:24:44,396][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 20:24:44,396][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:24:44,494][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 36921 unique words (68.30% of original 54054, drops 17133)', 'datetime': '2023-02-07T20:24:44.494218', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:24:44,495][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 6522479 word corpus (99.57% of original 6550866, drops 28387)', 'datetime': '2023-02-07T20:24:44.495427', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:24:44,617][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 20:24:44,618][gensim.models.word2vec][INFO] - sample=0.301712 downsamples 0 most-common words
[2023-02-07 20:24:44,618][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6522479 word corpus (100.0%% of prior 6522479)', 'datetime': '2023-02-07T20:24:44.618682', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:24:44,819][gensim.models.word2vec][INFO] - estimated required memory for 36921 words and 412 dimensions: 146171052 bytes
[2023-02-07 20:24:44,820][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:24:44,882][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 36921 vocabulary and 412 features, using sg=1 hs=0 sample=0.30171179907731666 negative=5 window=14 shrink_windows=True', 'datetime': '2023-02-07T20:24:44.881979', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:24:45,900][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 20.97% examples, 1316712 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:24:46,906][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 41.45% examples, 1362841 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:24:47,908][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 62.94% examples, 1365598 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:24:48,914][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 84.00% examples, 1361463 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:24:49,613][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6467889 effective words) took 4.7s, 1367661 effective words/s
[2023-02-07 20:24:50,615][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 29.75% examples, 1925105 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:24:51,618][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 58.27% examples, 1922199 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:24:52,621][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 88.61% examples, 1915596 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:24:52,985][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6467889 effective words) took 3.4s, 1919343 effective words/s
[2023-02-07 20:24:53,994][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 30.67% examples, 1988512 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:24:54,995][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 61.74% examples, 2019672 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:24:56,000][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 93.46% examples, 2016520 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:24:56,192][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6467889 effective words) took 3.2s, 2018294 effective words/s
[2023-02-07 20:24:57,196][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 30.92% examples, 2011960 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:24:58,198][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 60.85% examples, 1995826 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:24:59,199][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 89.90% examples, 1948341 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:24:59,498][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6467889 effective words) took 3.3s, 1958080 effective words/s
[2023-02-07 20:25:00,504][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 25.18% examples, 1610511 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:01,515][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 48.57% examples, 1593101 words/s, in_qsize 6, out_qsize 1
[2023-02-07 20:25:02,527][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 73.96% examples, 1601417 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:03,525][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6467889 effective words) took 4.0s, 1607058 effective words/s
[2023-02-07 20:25:04,529][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 25.42% examples, 1638316 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:05,532][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 50.14% examples, 1647606 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:06,535][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 74.92% examples, 1634557 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:07,471][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6467889 effective words) took 3.9s, 1640007 effective words/s
[2023-02-07 20:25:08,476][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 31.53% examples, 2052666 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:09,481][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 57.84% examples, 1904491 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:10,482][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 82.96% examples, 1804142 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:25:11,001][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6467889 effective words) took 3.5s, 1834134 effective words/s
[2023-02-07 20:25:12,006][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 24.84% examples, 1589197 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:25:13,010][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 53.61% examples, 1771197 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:14,018][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 83.79% examples, 1817763 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:14,629][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6467889 effective words) took 3.6s, 1784479 effective words/s
[2023-02-07 20:25:15,633][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 24.96% examples, 1597566 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:16,635][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 48.42% examples, 1597017 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:17,635][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 73.47% examples, 1602937 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:18,642][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 99.08% examples, 1600387 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:25:18,664][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6467889 effective words) took 4.0s, 1603636 effective words/s
[2023-02-07 20:25:19,669][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 28.55% examples, 1842887 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:20,678][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 53.61% examples, 1765112 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:21,681][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 79.28% examples, 1720494 words/s, in_qsize 8, out_qsize 2
[2023-02-07 20:25:22,436][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6467889 effective words) took 3.8s, 1715616 effective words/s
[2023-02-07 20:25:23,443][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 26.04% examples, 1681345 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:24,450][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 51.40% examples, 1688586 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:25,452][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 79.46% examples, 1727673 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:26,106][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6467889 effective words) took 3.7s, 1763870 effective words/s
[2023-02-07 20:25:27,111][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 25.97% examples, 1682846 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:28,117][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 50.91% examples, 1673259 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:29,117][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 77.06% examples, 1675924 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:29,973][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6467889 effective words) took 3.9s, 1674404 effective words/s
[2023-02-07 20:25:30,976][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 30.12% examples, 1958051 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:31,978][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 60.30% examples, 1976846 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:32,991][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 87.38% examples, 1891774 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:25:33,442][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6467889 effective words) took 3.5s, 1865903 effective words/s
[2023-02-07 20:25:34,447][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 26.56% examples, 1718792 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:35,452][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 51.06% examples, 1676434 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:36,452][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 76.05% examples, 1657483 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:37,355][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6467889 effective words) took 3.9s, 1653731 effective words/s
[2023-02-07 20:25:38,360][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 30.89% examples, 1995972 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:39,369][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 60.98% examples, 1992855 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:25:40,374][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 91.99% examples, 1984520 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:25:40,611][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6467889 effective words) took 3.3s, 1987086 effective words/s
[2023-02-07 20:25:40,612][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (97018335 effective words) took 55.7s, 1740867 effective words/s', 'datetime': '2023-02-07T20:25:40.612301', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:25:40.612 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:25:45,990][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202414-4tfaw2rf/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:25:45.989943', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:25:45,993][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202414-4tfaw2rf/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 20:25:46,047][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202414-4tfaw2rf/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 20:25:46,101][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:25:46,163][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202414-4tfaw2rf/files/../tmp/embedding_model.pt
2023-02-07 20:25:46.164 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:25:48.418 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:25:49.183 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:25:52.078 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2507021915628873, 'test_mae': 1.0962902331464792, 'test_r2': -2.513051114076089}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.96
wandb: percentage 0.31696
wandb:   test_mae 1.09629
wandb:   test_mse 2.2507
wandb:    test_r2 -2.51305
wandb: 
wandb: üöÄ View run logical-sweep-83 at: https://wandb.ai/xiaoqiz/mof2vec/runs/4tfaw2rf
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_202414-4tfaw2rf/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: eh18slg6 with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 624
wandb: 	model.gensim.alpha: 0.034349323292907186
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.7075989910804046
wandb: 	model.gensim.vector_size: 397
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.45949283933397583
wandb: 	model.sklearn.max_depth: 5
wandb: 	model.sklearn.min_child_weight: 0.02827792529296212
wandb: 	model.sklearn.n_estimators: 2452
wandb: 	model.sklearn.num_leaves: 454
wandb: 	model.sklearn.reg_alpha: 0.1317574273356714
wandb: 	model.sklearn.reg_lambda: 0.0322529956276347
wandb: 	model.sklearn.subsample: 0.5511209115713249
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202610-eh18slg6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-84
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/eh18slg6
2023-02-07 20:26:18.355 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 20:26:18.356 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 624 for sweep.
2023-02-07 20:26:18.356 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.034349323292907186 for sweep.
2023-02-07 20:26:18.356 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:26:18.357 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 20:26:18.357 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7075989910804046 for sweep.
2023-02-07 20:26:18.357 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 397 for sweep.
2023-02-07 20:26:18.357 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 20:26:18.358 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.45949283933397583 for sweep.
2023-02-07 20:26:18.358 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 5 for sweep.
2023-02-07 20:26:18.358 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.02827792529296212 for sweep.
2023-02-07 20:26:18.358 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2452 for sweep.
2023-02-07 20:26:18.359 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 454 for sweep.
2023-02-07 20:26:18.359 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.1317574273356714 for sweep.
2023-02-07 20:26:18.359 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.0322529956276347 for sweep.
2023-02-07 20:26:18.359 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5511209115713249 for sweep.
2023-02-07 20:26:18.359 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:26:18.364 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202610-eh18slg6/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 624, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 397, 'window': 3, 'min_count': 5, 'dm': 0, 'sample': 0.7075989910804046, 'workers': 4, 'alpha': 0.034349323292907186, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2452, 'max_depth': 5, 'num_leaves': 454, 'reg_alpha': 0.1317574273356714, 'reg_lambda': 0.0322529956276347, 'subsample': 0.5511209115713249, 'min_child_weight': 0.02827792529296212, 'n_jobs': 4, 'learning_rate': 0.45949283933397583}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 15/3257 [00:00<00:21, 147.39it/s]  1%|          | 33/3257 [00:00<00:19, 164.57it/s]  2%|‚ñè         | 50/3257 [00:00<00:19, 162.07it/s]  2%|‚ñè         | 67/3257 [00:00<00:20, 155.93it/s]  3%|‚ñé         | 89/3257 [00:00<00:18, 174.29it/s]  3%|‚ñé         | 107/3257 [00:00<00:20, 155.88it/s]  4%|‚ñç         | 123/3257 [00:00<00:20, 155.68it/s]  4%|‚ñç         | 141/3257 [00:00<00:19, 159.75it/s]  5%|‚ñç         | 158/3257 [00:00<00:19, 157.96it/s]  5%|‚ñå         | 174/3257 [00:01<00:20, 154.03it/s]  6%|‚ñå         | 193/3257 [00:01<00:19, 160.11it/s]  7%|‚ñã         | 212/3257 [00:01<00:18, 167.68it/s]  7%|‚ñã         | 234/3257 [00:01<00:16, 181.09it/s]  8%|‚ñä         | 253/3257 [00:01<00:17, 174.50it/s]  8%|‚ñä         | 271/3257 [00:01<00:17, 171.94it/s]  9%|‚ñâ         | 295/3257 [00:01<00:15, 189.24it/s] 10%|‚ñâ         | 315/3257 [00:01<00:16, 177.07it/s] 10%|‚ñà         | 335/3257 [00:01<00:16, 180.90it/s] 11%|‚ñà         | 354/3257 [00:02<00:16, 181.01it/s] 11%|‚ñà‚ñè        | 373/3257 [00:02<00:16, 179.17it/s] 12%|‚ñà‚ñè        | 392/3257 [00:02<00:17, 162.10it/s] 13%|‚ñà‚ñé        | 412/3257 [00:02<00:16, 170.97it/s] 13%|‚ñà‚ñé        | 430/3257 [00:02<00:18, 151.89it/s] 14%|‚ñà‚ñé        | 447/3257 [00:02<00:18, 154.44it/s] 14%|‚ñà‚ñç        | 467/3257 [00:02<00:16, 164.24it/s] 15%|‚ñà‚ñç        | 484/3257 [00:02<00:17, 162.82it/s] 15%|‚ñà‚ñå        | 504/3257 [00:03<00:16, 170.61it/s] 16%|‚ñà‚ñå        | 522/3257 [00:03<00:15, 173.17it/s] 17%|‚ñà‚ñã        | 540/3257 [00:03<00:15, 172.22it/s] 17%|‚ñà‚ñã        | 558/3257 [00:03<00:16, 161.86it/s] 18%|‚ñà‚ñä        | 575/3257 [00:03<00:17, 150.79it/s] 18%|‚ñà‚ñä        | 595/3257 [00:03<00:16, 163.35it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:03<00:15, 170.89it/s] 19%|‚ñà‚ñâ        | 633/3257 [00:03<00:15, 173.01it/s] 20%|‚ñà‚ñâ        | 651/3257 [00:03<00:16, 162.50it/s] 21%|‚ñà‚ñà        | 668/3257 [00:04<00:16, 156.48it/s] 21%|‚ñà‚ñà        | 684/3257 [00:04<00:16, 155.30it/s] 21%|‚ñà‚ñà‚ñè       | 700/3257 [00:04<00:16, 155.33it/s] 22%|‚ñà‚ñà‚ñè       | 720/3257 [00:04<00:15, 166.04it/s] 23%|‚ñà‚ñà‚ñé       | 737/3257 [00:04<00:16, 157.43it/s] 23%|‚ñà‚ñà‚ñé       | 755/3257 [00:04<00:15, 163.10it/s] 24%|‚ñà‚ñà‚ñé       | 773/3257 [00:04<00:15, 164.20it/s] 24%|‚ñà‚ñà‚ñç       | 790/3257 [00:04<00:15, 162.95it/s] 25%|‚ñà‚ñà‚ñç       | 807/3257 [00:04<00:15, 155.68it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:05<00:15, 153.08it/s] 26%|‚ñà‚ñà‚ñå       | 839/3257 [00:05<00:15, 151.37it/s] 26%|‚ñà‚ñà‚ñã       | 855/3257 [00:05<00:16, 145.72it/s] 27%|‚ñà‚ñà‚ñã       | 872/3257 [00:05<00:15, 151.67it/s] 27%|‚ñà‚ñà‚ñã       | 888/3257 [00:05<00:15, 148.36it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:05<00:15, 154.23it/s] 28%|‚ñà‚ñà‚ñä       | 921/3257 [00:05<00:15, 153.69it/s] 29%|‚ñà‚ñà‚ñâ       | 937/3257 [00:05<00:15, 152.94it/s] 29%|‚ñà‚ñà‚ñâ       | 956/3257 [00:05<00:14, 163.34it/s] 30%|‚ñà‚ñà‚ñâ       | 973/3257 [00:05<00:14, 158.35it/s] 30%|‚ñà‚ñà‚ñà       | 989/3257 [00:06<00:14, 156.27it/s] 31%|‚ñà‚ñà‚ñà       | 1006/3257 [00:06<00:14, 159.47it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1022/3257 [00:06<00:14, 159.09it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1038/3257 [00:06<00:15, 145.98it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1053/3257 [00:06<00:15, 144.31it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1071/3257 [00:06<00:14, 154.01it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1087/3257 [00:06<00:14, 153.94it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1103/3257 [00:06<00:14, 151.36it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1119/3257 [00:06<00:14, 147.64it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1136/3257 [00:07<00:13, 152.16it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1152/3257 [00:07<00:14, 147.99it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:07<00:13, 154.91it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1186/3257 [00:07<00:14, 146.04it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1201/3257 [00:07<00:23, 86.08it/s]  37%|‚ñà‚ñà‚ñà‚ñã      | 1217/3257 [00:07<00:20, 98.75it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1238/3257 [00:07<00:16, 121.71it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1254/3257 [00:08<00:15, 128.40it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:08<00:14, 134.01it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1287/3257 [00:08<00:15, 127.44it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1303/3257 [00:08<00:14, 134.12it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1321/3257 [00:08<00:13, 143.51it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1340/3257 [00:08<00:12, 155.61it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1357/3257 [00:08<00:12, 151.68it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1373/3257 [00:08<00:12, 146.69it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1389/3257 [00:08<00:12, 144.67it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1410/3257 [00:09<00:11, 160.63it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1430/3257 [00:09<00:10, 169.23it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1448/3257 [00:09<00:11, 163.92it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1469/3257 [00:09<00:10, 175.32it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1487/3257 [00:09<00:10, 171.08it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1509/3257 [00:09<00:09, 183.77it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1528/3257 [00:09<00:10, 160.31it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1545/3257 [00:09<00:11, 151.11it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1561/3257 [00:10<00:11, 152.73it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1577/3257 [00:10<00:11, 150.22it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1594/3257 [00:10<00:10, 154.38it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1612/3257 [00:10<00:10, 160.59it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1629/3257 [00:10<00:09, 163.10it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1646/3257 [00:10<00:10, 150.49it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1662/3257 [00:10<00:11, 143.11it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1678/3257 [00:10<00:10, 144.94it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1694/3257 [00:10<00:10, 146.96it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1714/3257 [00:11<00:09, 161.08it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:11<00:10, 149.26it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1747/3257 [00:11<00:10, 150.99it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1765/3257 [00:11<00:09, 156.27it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1784/3257 [00:11<00:08, 163.99it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1801/3257 [00:11<00:08, 163.26it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1818/3257 [00:11<00:08, 160.17it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1835/3257 [00:11<00:09, 156.64it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1852/3257 [00:11<00:08, 157.90it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1870/3257 [00:12<00:08, 163.04it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1887/3257 [00:12<00:08, 160.18it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1904/3257 [00:12<00:08, 160.77it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1921/3257 [00:12<00:08, 159.36it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1944/3257 [00:12<00:07, 178.65it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:12<00:07, 182.62it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1984/3257 [00:12<00:07, 169.67it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:12<00:07, 173.76it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2023/3257 [00:12<00:06, 177.90it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2041/3257 [00:12<00:07, 169.19it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2059/3257 [00:13<00:07, 156.32it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2078/3257 [00:13<00:07, 160.17it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2095/3257 [00:13<00:07, 162.23it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2112/3257 [00:13<00:07, 159.02it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2129/3257 [00:13<00:07, 148.02it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2145/3257 [00:13<00:07, 144.87it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2163/3257 [00:13<00:07, 154.10it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2180/3257 [00:13<00:06, 157.69it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2198/3257 [00:14<00:06, 162.05it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2215/3257 [00:14<00:06, 153.53it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2233/3257 [00:14<00:06, 158.12it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2249/3257 [00:14<00:06, 153.77it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2265/3257 [00:14<00:06, 155.33it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2281/3257 [00:14<00:06, 152.07it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:14<00:06, 156.96it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2317/3257 [00:14<00:05, 163.43it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2339/3257 [00:14<00:05, 179.11it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:14<00:04, 183.36it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2379/3257 [00:15<00:04, 183.68it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2399/3257 [00:15<00:04, 187.94it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2418/3257 [00:15<00:04, 172.87it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2436/3257 [00:15<00:05, 162.25it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2453/3257 [00:15<00:05, 159.79it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2475/3257 [00:15<00:04, 173.48it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2495/3257 [00:15<00:04, 177.97it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2517/3257 [00:15<00:03, 188.50it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2538/3257 [00:15<00:03, 193.88it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2558/3257 [00:16<00:03, 180.93it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2577/3257 [00:16<00:03, 173.10it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2595/3257 [00:16<00:03, 170.78it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2621/3257 [00:16<00:03, 194.68it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2641/3257 [00:16<00:03, 195.57it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2661/3257 [00:16<00:03, 180.40it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2680/3257 [00:17<00:05, 96.65it/s]  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2696/3257 [00:17<00:05, 106.68it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2711/3257 [00:17<00:04, 112.90it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:17<00:04, 130.43it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2754/3257 [00:17<00:03, 150.01it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2772/3257 [00:17<00:03, 150.63it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2795/3257 [00:17<00:02, 168.26it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2814/3257 [00:17<00:02, 169.65it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2832/3257 [00:17<00:02, 165.71it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2852/3257 [00:18<00:02, 173.59it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2876/3257 [00:18<00:01, 190.89it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2896/3257 [00:18<00:02, 172.27it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2916/3257 [00:18<00:01, 177.42it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2936/3257 [00:18<00:01, 181.24it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2955/3257 [00:18<00:01, 171.54it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2974/3257 [00:18<00:01, 173.93it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2992/3257 [00:18<00:01, 163.84it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3013/3257 [00:18<00:01, 174.37it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3031/3257 [00:19<00:01, 175.86it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3052/3257 [00:19<00:01, 185.10it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3074/3257 [00:19<00:00, 194.27it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3094/3257 [00:19<00:00, 184.88it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3117/3257 [00:19<00:00, 196.82it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3137/3257 [00:19<00:00, 185.26it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3156/3257 [00:19<00:00, 177.41it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3175/3257 [00:19<00:00, 178.05it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3193/3257 [00:19<00:00, 175.93it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3211/3257 [00:20<00:00, 171.64it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3231/3257 [00:20<00:00, 179.03it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:20<00:00, 182.75it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:20<00:00, 160.29it/s]
2023-02-07 20:26:39.493 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:26:39,494][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d397,n5,mc5,s0.707599,t4>', 'datetime': '2023-02-07T20:26:39.494776', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:26:39,496][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:26:39,496][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:26:40,064][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 20:26:40,064][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:26:40,136][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 26649 unique words (49.30% of original 54054, drops 27405)', 'datetime': '2023-02-07T20:26:40.136169', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:26:40,136][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 6481391 word corpus (98.94% of original 6550866, drops 69475)', 'datetime': '2023-02-07T20:26:40.136527', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:26:40,224][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 20:26:40,225][gensim.models.word2vec][INFO] - sample=0.707599 downsamples 0 most-common words
[2023-02-07 20:26:40,226][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6481391 word corpus (100.0%% of prior 6481391)', 'datetime': '2023-02-07T20:26:40.226212', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:26:40,371][gensim.models.word2vec][INFO] - estimated required memory for 26649 words and 397 dimensions: 103785240 bytes
[2023-02-07 20:26:40,372][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:26:40,415][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 26649 vocabulary and 397 features, using sg=1 hs=0 sample=0.7075989910804046 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T20:26:40.415559', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:26:41,426][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 22.44% examples, 1410869 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:26:42,427][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 45.47% examples, 1482266 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:26:43,434][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 68.99% examples, 1501536 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:26:44,435][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 94.32% examples, 1512924 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:26:44,654][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6426944 effective words) took 4.2s, 1516900 effective words/s
[2023-02-07 20:26:45,660][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 33.13% examples, 2137232 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:26:46,660][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 57.84% examples, 1895807 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:26:47,661][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 83.21% examples, 1802355 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:26:48,299][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6426944 effective words) took 3.6s, 1764480 effective words/s
[2023-02-07 20:26:49,305][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 26.22% examples, 1686983 words/s, in_qsize 8, out_qsize 3
[2023-02-07 20:26:50,310][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 59.10% examples, 1930906 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:26:51,313][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 93.46% examples, 2003828 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:26:51,487][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6426944 effective words) took 3.2s, 2016881 effective words/s
[2023-02-07 20:26:52,494][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 33.68% examples, 2176502 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:26:53,494][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 66.84% examples, 2183119 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:26:54,433][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6426944 effective words) took 2.9s, 2182725 effective words/s
[2023-02-07 20:26:55,439][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 26.62% examples, 1712396 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:26:56,441][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 52.04% examples, 1703707 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:26:57,447][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 78.88% examples, 1702779 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:26:58,205][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6426944 effective words) took 3.8s, 1704569 effective words/s
[2023-02-07 20:26:59,209][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 33.16% examples, 2147618 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:00,215][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 66.63% examples, 2174867 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:01,163][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6426944 effective words) took 3.0s, 2173502 effective words/s
[2023-02-07 20:27:02,167][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 26.90% examples, 1737951 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:03,168][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 52.63% examples, 1725270 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:04,169][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 79.37% examples, 1718660 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:04,905][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6426944 effective words) took 3.7s, 1719011 effective words/s
[2023-02-07 20:27:05,907][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 27.63% examples, 1787201 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:27:06,915][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 54.47% examples, 1785531 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:07,915][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 81.67% examples, 1761789 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:08,555][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6426944 effective words) took 3.6s, 1761555 effective words/s
[2023-02-07 20:27:09,560][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 26.83% examples, 1727346 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:10,563][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 53.61% examples, 1760003 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:11,567][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 81.76% examples, 1759891 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:12,215][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6426944 effective words) took 3.7s, 1757510 effective words/s
[2023-02-07 20:27:13,221][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 34.82% examples, 2261860 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:14,222][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 68.84% examples, 2256409 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:15,080][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6426944 effective words) took 2.9s, 2246392 effective words/s
[2023-02-07 20:27:16,087][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 33.34% examples, 2155231 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:17,087][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 66.87% examples, 2187411 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:18,089][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 98.19% examples, 2100507 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:18,150][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6426944 effective words) took 3.1s, 2096017 effective words/s
[2023-02-07 20:27:19,170][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 28.55% examples, 1836618 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:27:20,174][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 55.48% examples, 1822616 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:21,178][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 83.21% examples, 1799277 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:27:21,754][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6426944 effective words) took 3.6s, 1792531 effective words/s
[2023-02-07 20:27:22,763][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 27.11% examples, 1738910 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:27:23,770][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 53.42% examples, 1747558 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:24,771][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 81.58% examples, 1754281 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:27:25,419][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6426944 effective words) took 3.7s, 1754690 effective words/s
[2023-02-07 20:27:26,426][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 27.39% examples, 1760375 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:27,428][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 53.30% examples, 1750982 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:28,431][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 81.30% examples, 1750717 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:29,096][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6426944 effective words) took 3.7s, 1749831 effective words/s
[2023-02-07 20:27:30,100][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 26.83% examples, 1729593 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:31,108][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 52.87% examples, 1727548 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:32,111][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 81.09% examples, 1744030 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:27:32,739][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6426944 effective words) took 3.6s, 1766045 effective words/s
[2023-02-07 20:27:32,739][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (96404160 effective words) took 52.3s, 1842450 effective words/s', 'datetime': '2023-02-07T20:27:32.739894', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:27:32.740 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:27:36,433][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202610-eh18slg6/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:27:36.433395', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:27:36,434][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202610-eh18slg6/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 20:27:36,506][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202610-eh18slg6/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 20:27:36,575][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:27:36,598][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202610-eh18slg6/files/../tmp/embedding_model.pt
2023-02-07 20:27:36.598 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:27:38.647 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:27:39.359 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:27:41.915 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.191374847502088, 'test_mae': 1.056066480900898, 'test_r2': -1.8183783363631743}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: | 0.032 MB of 0.032 MB uploaded (0.000 MB deduped)wandb: / 0.032 MB of 0.032 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.96
wandb: percentage 0.50699
wandb:   test_mae 1.05607
wandb:   test_mse 2.19137
wandb:    test_r2 -1.81838
wandb: 
wandb: üöÄ View run firm-sweep-84 at: https://wandb.ai/xiaoqiz/mof2vec/runs/eh18slg6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_202610-eh18slg6/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: r8bhaet9 with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 925
wandb: 	model.gensim.alpha: 0.005909208703923208
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.20578008243416737
wandb: 	model.gensim.vector_size: 84
wandb: 	model.gensim.window: 17
wandb: 	model.sklearn.learning_rate: 0.00909845388032582
wandb: 	model.sklearn.max_depth: 68
wandb: 	model.sklearn.min_child_weight: 0.02869784199853321
wandb: 	model.sklearn.n_estimators: 3521
wandb: 	model.sklearn.num_leaves: 170
wandb: 	model.sklearn.reg_alpha: 0.9082292396106728
wandb: 	model.sklearn.reg_lambda: 0.007547815865039661
wandb: 	model.sklearn.subsample: 0.7193792250644964
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202801-r8bhaet9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-85
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/r8bhaet9
2023-02-07 20:28:10.282 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 20:28:10.283 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 925 for sweep.
2023-02-07 20:28:10.283 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.005909208703923208 for sweep.
2023-02-07 20:28:10.283 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:28:10.284 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 20:28:10.284 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.20578008243416737 for sweep.
2023-02-07 20:28:10.284 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 84 for sweep.
2023-02-07 20:28:10.284 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 17 for sweep.
2023-02-07 20:28:10.285 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.00909845388032582 for sweep.
2023-02-07 20:28:10.285 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 68 for sweep.
2023-02-07 20:28:10.285 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.02869784199853321 for sweep.
2023-02-07 20:28:10.285 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3521 for sweep.
2023-02-07 20:28:10.286 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 170 for sweep.
2023-02-07 20:28:10.286 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.9082292396106728 for sweep.
2023-02-07 20:28:10.286 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.007547815865039661 for sweep.
2023-02-07 20:28:10.286 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.7193792250644964 for sweep.
2023-02-07 20:28:10.286 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:28:10.294 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202801-r8bhaet9/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 925, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 84, 'window': 17, 'min_count': 1, 'dm': 0, 'sample': 0.20578008243416737, 'workers': 4, 'alpha': 0.005909208703923208, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3521, 'max_depth': 68, 'num_leaves': 170, 'reg_alpha': 0.9082292396106728, 'reg_lambda': 0.007547815865039661, 'subsample': 0.7193792250644964, 'min_child_weight': 0.02869784199853321, 'n_jobs': 4, 'learning_rate': 0.00909845388032582}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 24/3257 [00:00<00:13, 236.17it/s]  2%|‚ñè         | 50/3257 [00:00<00:13, 242.01it/s]  2%|‚ñè         | 77/3257 [00:00<00:12, 254.39it/s]  3%|‚ñé         | 105/3257 [00:00<00:12, 258.15it/s]  4%|‚ñç         | 131/3257 [00:00<00:12, 253.26it/s]  5%|‚ñç         | 158/3257 [00:00<00:12, 250.78it/s]  6%|‚ñå         | 184/3257 [00:00<00:12, 251.10it/s]  7%|‚ñã         | 213/3257 [00:00<00:11, 261.84it/s]  7%|‚ñã         | 243/3257 [00:00<00:11, 269.68it/s]  8%|‚ñä         | 270/3257 [00:01<00:11, 263.81it/s]  9%|‚ñâ         | 300/3257 [00:01<00:10, 270.75it/s] 10%|‚ñà         | 329/3257 [00:01<00:10, 274.74it/s] 11%|‚ñà         | 357/3257 [00:01<00:10, 274.22it/s] 12%|‚ñà‚ñè        | 385/3257 [00:01<00:11, 256.91it/s] 13%|‚ñà‚ñé        | 411/3257 [00:01<00:11, 256.14it/s] 13%|‚ñà‚ñé        | 437/3257 [00:01<00:12, 230.55it/s] 14%|‚ñà‚ñç        | 465/3257 [00:01<00:11, 242.96it/s] 15%|‚ñà‚ñå        | 490/3257 [00:01<00:11, 243.65it/s] 16%|‚ñà‚ñå        | 518/3257 [00:02<00:10, 251.47it/s] 17%|‚ñà‚ñã        | 544/3257 [00:02<00:10, 249.12it/s] 18%|‚ñà‚ñä        | 570/3257 [00:02<00:11, 237.89it/s] 18%|‚ñà‚ñä        | 595/3257 [00:02<00:11, 233.04it/s] 19%|‚ñà‚ñâ        | 621/3257 [00:02<00:11, 238.60it/s] 20%|‚ñà‚ñâ        | 646/3257 [00:02<00:11, 237.16it/s] 21%|‚ñà‚ñà        | 670/3257 [00:02<00:16, 159.99it/s] 21%|‚ñà‚ñà‚ñè       | 693/3257 [00:02<00:14, 174.59it/s] 22%|‚ñà‚ñà‚ñè       | 718/3257 [00:03<00:13, 191.83it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:03<00:13, 190.07it/s] 24%|‚ñà‚ñà‚ñé       | 769/3257 [00:03<00:11, 214.52it/s] 24%|‚ñà‚ñà‚ñç       | 793/3257 [00:03<00:11, 216.54it/s] 25%|‚ñà‚ñà‚ñå       | 819/3257 [00:03<00:10, 226.78it/s] 26%|‚ñà‚ñà‚ñå       | 843/3257 [00:03<00:11, 217.31it/s] 27%|‚ñà‚ñà‚ñã       | 869/3257 [00:03<00:10, 227.10it/s] 27%|‚ñà‚ñà‚ñã       | 893/3257 [00:03<00:10, 224.79it/s] 28%|‚ñà‚ñà‚ñä       | 917/3257 [00:03<00:10, 228.78it/s] 29%|‚ñà‚ñà‚ñâ       | 943/3257 [00:04<00:09, 235.41it/s] 30%|‚ñà‚ñà‚ñâ       | 967/3257 [00:04<00:09, 234.76it/s] 30%|‚ñà‚ñà‚ñà       | 991/3257 [00:04<00:09, 228.13it/s] 31%|‚ñà‚ñà‚ñà       | 1014/3257 [00:04<00:10, 222.99it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1037/3257 [00:04<00:10, 218.51it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1059/3257 [00:04<00:10, 215.10it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1082/3257 [00:04<00:10, 217.41it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1106/3257 [00:04<00:09, 223.70it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1129/3257 [00:04<00:09, 217.73it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1151/3257 [00:04<00:09, 212.71it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1175/3257 [00:05<00:09, 218.40it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1197/3257 [00:05<00:09, 206.92it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1218/3257 [00:05<00:10, 201.78it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1245/3257 [00:05<00:09, 218.94it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:05<00:08, 224.91it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1294/3257 [00:05<00:09, 211.65it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1319/3257 [00:05<00:08, 221.59it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1345/3257 [00:05<00:08, 231.25it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1369/3257 [00:05<00:08, 222.79it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1392/3257 [00:06<00:08, 219.86it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1422/3257 [00:06<00:07, 241.17it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1447/3257 [00:06<00:07, 239.64it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1474/3257 [00:06<00:07, 247.24it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1500/3257 [00:06<00:07, 249.68it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1526/3257 [00:06<00:07, 227.95it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1550/3257 [00:06<00:07, 220.36it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1574/3257 [00:06<00:07, 222.74it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1598/3257 [00:06<00:07, 226.33it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1623/3257 [00:07<00:07, 229.52it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1647/3257 [00:07<00:07, 225.42it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1670/3257 [00:07<00:07, 224.22it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1693/3257 [00:07<00:07, 221.83it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1719/3257 [00:07<00:06, 231.80it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1743/3257 [00:07<00:07, 215.90it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1770/3257 [00:07<00:06, 229.96it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1796/3257 [00:07<00:06, 235.36it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1820/3257 [00:07<00:06, 236.17it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1844/3257 [00:08<00:05, 236.26it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1871/3257 [00:08<00:05, 245.88it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1898/3257 [00:08<00:05, 251.46it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1925/3257 [00:08<00:05, 255.56it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1951/3257 [00:08<00:06, 187.34it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1979/3257 [00:08<00:06, 208.54it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2009/3257 [00:08<00:05, 229.96it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2038/3257 [00:08<00:04, 244.41it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2065/3257 [00:08<00:05, 236.11it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2094/3257 [00:09<00:04, 249.73it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:09<00:04, 241.78it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2145/3257 [00:09<00:04, 234.03it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2173/3257 [00:09<00:04, 246.26it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2201/3257 [00:09<00:04, 253.36it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2227/3257 [00:09<00:04, 250.61it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2254/3257 [00:09<00:03, 254.87it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2280/3257 [00:09<00:03, 246.01it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2308/3257 [00:09<00:03, 253.05it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2341/3257 [00:10<00:03, 273.27it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2371/3257 [00:10<00:03, 277.17it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2403/3257 [00:10<00:02, 286.64it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2432/3257 [00:10<00:03, 264.42it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2461/3257 [00:10<00:02, 269.45it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2490/3257 [00:10<00:02, 274.60it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2522/3257 [00:10<00:02, 285.71it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:10<00:02, 283.82it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2580/3257 [00:10<00:02, 261.70it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2609/3257 [00:11<00:02, 269.43it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:11<00:02, 279.05it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2669/3257 [00:11<00:02, 271.64it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2697/3257 [00:11<00:02, 266.96it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2724/3257 [00:11<00:02, 253.53it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:11<00:01, 269.53it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2784/3257 [00:11<00:01, 268.96it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2812/3257 [00:11<00:01, 270.11it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2840/3257 [00:11<00:01, 253.23it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2873/3257 [00:12<00:01, 273.92it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2901/3257 [00:12<00:01, 257.65it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2929/3257 [00:12<00:01, 260.00it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2956/3257 [00:12<00:01, 246.45it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2981/3257 [00:12<00:01, 244.01it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3011/3257 [00:12<00:00, 256.34it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3038/3257 [00:12<00:00, 257.62it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3067/3257 [00:12<00:00, 266.01it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3094/3257 [00:12<00:00, 264.03it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3123/3257 [00:12<00:00, 270.65it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3151/3257 [00:13<00:00, 250.73it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3177/3257 [00:13<00:00, 246.64it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3205/3257 [00:13<00:00, 255.18it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3231/3257 [00:13<00:00, 252.83it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 240.68it/s]
2023-02-07 20:28:24.259 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:28:24,260][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d84,n5,s0.20578,t4>', 'datetime': '2023-02-07T20:28:24.260634', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:28:24,261][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:28:24,261][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:28:24,564][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 20:28:24,565][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:28:24,597][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 13061 unique words (100.00% of original 13061, drops 0)', 'datetime': '2023-02-07T20:28:24.597313', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:28:24,597][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 3639370 word corpus (100.00% of original 3639370, drops 0)', 'datetime': '2023-02-07T20:28:24.597719', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:28:24,639][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 20:28:24,639][gensim.models.word2vec][INFO] - sample=0.20578 downsamples 0 most-common words
[2023-02-07 20:28:24,640][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3639370 word corpus (100.0%% of prior 3639370)', 'datetime': '2023-02-07T20:28:24.640056', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:28:24,709][gensim.models.word2vec][INFO] - estimated required memory for 13061 words and 84 dimensions: 17053244 bytes
[2023-02-07 20:28:24,710][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:28:24,715][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 13061 vocabulary and 84 features, using sg=1 hs=0 sample=0.20578008243416737 negative=5 window=17 shrink_windows=True', 'datetime': '2023-02-07T20:28:24.715405', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:28:25,722][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 64.45% examples, 2390313 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:28:26,251][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3642627 effective words) took 1.5s, 2374961 effective words/s
[2023-02-07 20:28:27,256][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 76.94% examples, 2827288 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:28:27,536][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3642627 effective words) took 1.3s, 2840427 effective words/s
[2023-02-07 20:28:28,541][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 73.04% examples, 2688950 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:28:28,882][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3642627 effective words) took 1.3s, 2709937 effective words/s
[2023-02-07 20:28:29,888][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 92.75% examples, 3391011 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:28:29,956][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3642627 effective words) took 1.1s, 3397158 effective words/s
[2023-02-07 20:28:30,963][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 74.79% examples, 2756714 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:28:31,266][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3642627 effective words) took 1.3s, 2787463 effective words/s
[2023-02-07 20:28:32,271][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 77.53% examples, 2846132 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:28:32,546][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3642627 effective words) took 1.3s, 2848724 effective words/s
[2023-02-07 20:28:33,557][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 77.22% examples, 2843187 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:28:33,837][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3642627 effective words) took 1.3s, 2845100 effective words/s
[2023-02-07 20:28:34,844][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 76.85% examples, 2822025 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:28:35,122][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3642627 effective words) took 1.3s, 2840617 effective words/s
[2023-02-07 20:28:36,127][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 74.76% examples, 2760881 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:28:36,435][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3642627 effective words) took 1.3s, 2779223 effective words/s
[2023-02-07 20:28:37,439][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 98.62% examples, 3594040 words/s, in_qsize 5, out_qsize 0
[2023-02-07 20:28:37,453][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3642627 effective words) took 1.0s, 3588897 effective words/s
[2023-02-07 20:28:38,456][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 75.99% examples, 2802542 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:28:38,712][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3642627 effective words) took 1.3s, 2897301 effective words/s
[2023-02-07 20:28:39,719][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 77.22% examples, 2827733 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:28:40,001][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3642627 effective words) took 1.3s, 2830222 effective words/s
[2023-02-07 20:28:41,004][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 98.80% examples, 3600002 words/s, in_qsize 4, out_qsize 0
[2023-02-07 20:28:41,011][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3642627 effective words) took 1.0s, 3609489 effective words/s
[2023-02-07 20:28:42,015][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 76.94% examples, 2830110 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:28:42,297][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3642627 effective words) took 1.3s, 2838199 effective words/s
[2023-02-07 20:28:43,303][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 77.86% examples, 2860728 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:28:43,566][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3642627 effective words) took 1.3s, 2875994 effective words/s
[2023-02-07 20:28:43,567][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54639405 effective words) took 18.9s, 2898417 effective words/s', 'datetime': '2023-02-07T20:28:43.567299', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:28:43.568 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:28:44,927][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202801-r8bhaet9/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:28:44.927474', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:28:44,929][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:28:44,952][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202801-r8bhaet9/files/../tmp/embedding_model.pt
2023-02-07 20:28:44.953 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:28:46.023 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:28:46.442 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:28:47.708 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0475034126692946, 'test_mae': 1.049882505953232, 'test_r2': -1.8517977109082224}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.8
wandb: percentage 0.0
wandb:   test_mae 1.04988
wandb:   test_mse 2.0475
wandb:    test_r2 -1.8518
wandb: 
wandb: üöÄ View run dandy-sweep-85 at: https://wandb.ai/xiaoqiz/mof2vec/runs/r8bhaet9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_202801-r8bhaet9/logs
wandb: Agent Starting Run: nuusbwmc with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 882
wandb: 	model.gensim.alpha: 0.0025178437496302993
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.5156321055440423
wandb: 	model.gensim.vector_size: 342
wandb: 	model.gensim.window: 13
wandb: 	model.sklearn.learning_rate: 0.005275293279308162
wandb: 	model.sklearn.max_depth: 60
wandb: 	model.sklearn.min_child_weight: 0.03703323794086315
wandb: 	model.sklearn.n_estimators: 641
wandb: 	model.sklearn.num_leaves: 383
wandb: 	model.sklearn.reg_alpha: 0.11791647718955232
wandb: 	model.sklearn.reg_lambda: 0.005002777792165042
wandb: 	model.sklearn.subsample: 0.7580831055386497
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202900-nuusbwmc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-86
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/nuusbwmc
2023-02-07 20:29:07.635 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 20:29:07.636 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 882 for sweep.
2023-02-07 20:29:07.636 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0025178437496302993 for sweep.
2023-02-07 20:29:07.637 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:29:07.637 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 20:29:07.637 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5156321055440423 for sweep.
2023-02-07 20:29:07.637 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 342 for sweep.
2023-02-07 20:29:07.638 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 13 for sweep.
2023-02-07 20:29:07.638 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.005275293279308162 for sweep.
2023-02-07 20:29:07.638 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 60 for sweep.
2023-02-07 20:29:07.639 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03703323794086315 for sweep.
2023-02-07 20:29:07.639 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 641 for sweep.
2023-02-07 20:29:07.639 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 383 for sweep.
2023-02-07 20:29:07.639 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.11791647718955232 for sweep.
2023-02-07 20:29:07.640 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.005002777792165042 for sweep.
2023-02-07 20:29:07.640 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.7580831055386497 for sweep.
2023-02-07 20:29:07.640 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:29:07.649 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202900-nuusbwmc/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 882, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 342, 'window': 13, 'min_count': 2, 'dm': 0, 'sample': 0.5156321055440423, 'workers': 4, 'alpha': 0.0025178437496302993, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 641, 'max_depth': 60, 'num_leaves': 383, 'reg_alpha': 0.11791647718955232, 'reg_lambda': 0.005002777792165042, 'subsample': 0.7580831055386497, 'min_child_weight': 0.03703323794086315, 'n_jobs': 4, 'learning_rate': 0.005275293279308162}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 9/3257 [00:00<01:37, 33.16it/s]  1%|          | 31/3257 [00:00<00:33, 96.53it/s]  2%|‚ñè         | 52/3257 [00:00<00:24, 131.82it/s]  2%|‚ñè         | 73/3257 [00:00<00:20, 156.33it/s]  3%|‚ñé         | 98/3257 [00:00<00:17, 184.62it/s]  4%|‚ñé         | 121/3257 [00:00<00:15, 198.22it/s]  5%|‚ñç         | 148/3257 [00:00<00:14, 219.57it/s]  5%|‚ñå         | 172/3257 [00:00<00:14, 218.50it/s]  6%|‚ñå         | 197/3257 [00:01<00:13, 227.04it/s]  7%|‚ñã         | 226/3257 [00:01<00:12, 245.38it/s]  8%|‚ñä         | 253/3257 [00:01<00:12, 249.38it/s]  9%|‚ñä         | 283/3257 [00:01<00:11, 262.61it/s] 10%|‚ñâ         | 310/3257 [00:01<00:11, 258.82it/s] 10%|‚ñà         | 337/3257 [00:01<00:11, 257.53it/s] 11%|‚ñà         | 363/3257 [00:01<00:11, 252.08it/s] 12%|‚ñà‚ñè        | 389/3257 [00:01<00:12, 235.29it/s] 13%|‚ñà‚ñé        | 417/3257 [00:01<00:11, 245.78it/s] 14%|‚ñà‚ñé        | 442/3257 [00:02<00:12, 221.81it/s] 14%|‚ñà‚ñç        | 470/3257 [00:02<00:11, 236.08it/s] 15%|‚ñà‚ñå        | 495/3257 [00:02<00:11, 236.96it/s] 16%|‚ñà‚ñå        | 522/3257 [00:02<00:11, 244.18it/s] 17%|‚ñà‚ñã        | 547/3257 [00:02<00:11, 241.21it/s] 18%|‚ñà‚ñä        | 572/3257 [00:02<00:12, 223.48it/s] 18%|‚ñà‚ñä        | 600/3257 [00:02<00:11, 236.04it/s] 19%|‚ñà‚ñâ        | 624/3257 [00:02<00:11, 237.01it/s] 20%|‚ñà‚ñâ        | 651/3257 [00:02<00:10, 245.20it/s] 21%|‚ñà‚ñà        | 676/3257 [00:03<00:10, 243.15it/s] 22%|‚ñà‚ñà‚ñè       | 701/3257 [00:03<00:10, 237.82it/s] 22%|‚ñà‚ñà‚ñè       | 725/3257 [00:03<00:10, 237.67it/s] 23%|‚ñà‚ñà‚ñé       | 749/3257 [00:03<00:10, 234.64it/s] 24%|‚ñà‚ñà‚ñç       | 776/3257 [00:03<00:10, 239.66it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:03<00:09, 247.93it/s] 25%|‚ñà‚ñà‚ñå       | 829/3257 [00:03<00:10, 236.91it/s] 26%|‚ñà‚ñà‚ñå       | 853/3257 [00:03<00:10, 224.71it/s] 27%|‚ñà‚ñà‚ñã       | 877/3257 [00:03<00:10, 227.11it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:04<00:09, 238.29it/s] 29%|‚ñà‚ñà‚ñä       | 930/3257 [00:04<00:09, 238.38it/s] 29%|‚ñà‚ñà‚ñâ       | 954/3257 [00:04<00:10, 229.33it/s] 30%|‚ñà‚ñà‚ñà       | 979/3257 [00:04<00:09, 228.49it/s] 31%|‚ñà‚ñà‚ñà       | 1002/3257 [00:04<00:09, 227.84it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:04<00:09, 224.93it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1050/3257 [00:04<00:10, 220.07it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1077/3257 [00:04<00:09, 231.89it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1101/3257 [00:04<00:09, 225.95it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1124/3257 [00:05<00:09, 226.54it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:05<00:09, 221.11it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1172/3257 [00:05<00:09, 228.64it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1195/3257 [00:05<00:09, 213.96it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1217/3257 [00:05<00:14, 139.16it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1245/3257 [00:05<00:12, 165.81it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:05<00:10, 182.55it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1293/3257 [00:05<00:10, 185.17it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1319/3257 [00:06<00:09, 202.70it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1345/3257 [00:06<00:08, 216.69it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1369/3257 [00:06<00:08, 217.86it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1392/3257 [00:06<00:08, 215.61it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:06<00:07, 235.40it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1449/3257 [00:06<00:07, 242.11it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1477/3257 [00:06<00:07, 252.77it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1507/3257 [00:06<00:06, 264.87it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1534/3257 [00:06<00:07, 242.21it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1559/3257 [00:07<00:07, 235.99it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1583/3257 [00:07<00:07, 235.23it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1610/3257 [00:07<00:06, 243.83it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1635/3257 [00:07<00:06, 236.91it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:07<00:06, 233.76it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1683/3257 [00:07<00:06, 227.65it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1710/3257 [00:07<00:06, 238.53it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1734/3257 [00:07<00:06, 225.11it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:07<00:06, 237.19it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1791/3257 [00:08<00:05, 248.73it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1817/3257 [00:08<00:05, 244.66it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1843/3257 [00:08<00:05, 247.02it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:08<00:05, 248.23it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1894/3257 [00:08<00:05, 244.38it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1919/3257 [00:08<00:05, 245.38it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1953/3257 [00:08<00:04, 271.74it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1981/3257 [00:08<00:04, 269.24it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2008/3257 [00:08<00:04, 266.14it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2036/3257 [00:08<00:04, 269.74it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2064/3257 [00:09<00:04, 245.90it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2091/3257 [00:09<00:04, 250.50it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2117/3257 [00:09<00:04, 244.34it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2142/3257 [00:09<00:04, 231.93it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2168/3257 [00:09<00:04, 236.79it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2194/3257 [00:09<00:04, 242.76it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2219/3257 [00:09<00:04, 241.70it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2244/3257 [00:09<00:04, 240.48it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2269/3257 [00:09<00:04, 242.74it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2294/3257 [00:10<00:03, 240.93it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2319/3257 [00:10<00:03, 243.53it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2350/3257 [00:10<00:03, 262.99it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2377/3257 [00:10<00:03, 261.89it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:10<00:03, 263.46it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2432/3257 [00:10<00:03, 251.52it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2459/3257 [00:10<00:03, 255.84it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2486/3257 [00:10<00:02, 259.03it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2517/3257 [00:10<00:02, 271.74it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2545/3257 [00:10<00:02, 269.65it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2573/3257 [00:11<00:02, 251.20it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2599/3257 [00:11<00:02, 252.20it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2632/3257 [00:11<00:02, 271.85it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2660/3257 [00:11<00:03, 165.83it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2690/3257 [00:11<00:02, 192.22it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2715/3257 [00:11<00:02, 190.12it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2748/3257 [00:11<00:02, 221.48it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2774/3257 [00:12<00:02, 225.89it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2805/3257 [00:12<00:01, 246.57it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2832/3257 [00:12<00:01, 238.32it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2863/3257 [00:12<00:01, 255.70it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2892/3257 [00:12<00:01, 262.88it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2920/3257 [00:12<00:01, 257.86it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2947/3257 [00:12<00:01, 250.88it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2974/3257 [00:12<00:01, 253.06it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3000/3257 [00:12<00:01, 251.59it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3026/3257 [00:13<00:00, 253.78it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3058/3257 [00:13<00:00, 271.75it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3087/3257 [00:13<00:00, 275.00it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3120/3257 [00:13<00:00, 288.51it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3149/3257 [00:13<00:00, 266.98it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3177/3257 [00:13<00:00, 249.73it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3204/3257 [00:13<00:00, 254.69it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3230/3257 [00:13<00:00, 248.43it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 253.47it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 233.62it/s]
2023-02-07 20:29:22.022 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:29:22,023][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d342,n5,mc2,s0.515632,t4>', 'datetime': '2023-02-07T20:29:22.023623', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:29:22,023][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:29:22,024][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:29:22,329][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 20:29:22,330][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:29:22,358][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 11155 unique words (85.41% of original 13061, drops 1906)', 'datetime': '2023-02-07T20:29:22.358133', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:29:22,358][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 3637464 word corpus (99.95% of original 3639370, drops 1906)', 'datetime': '2023-02-07T20:29:22.358523', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:29:22,394][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 20:29:22,395][gensim.models.word2vec][INFO] - sample=0.515632 downsamples 0 most-common words
[2023-02-07 20:29:22,395][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3637464 word corpus (100.0%% of prior 3637464)', 'datetime': '2023-02-07T20:29:22.395716', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:29:22,456][gensim.models.word2vec][INFO] - estimated required memory for 11155 words and 342 dimensions: 41204556 bytes
[2023-02-07 20:29:22,456][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:29:22,475][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 11155 vocabulary and 342 features, using sg=1 hs=0 sample=0.5156321055440423 negative=5 window=13 shrink_windows=True', 'datetime': '2023-02-07T20:29:22.475013', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:29:23,482][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 35.95% examples, 1323010 words/s, in_qsize 7, out_qsize 1
[2023-02-07 20:29:24,484][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 74.92% examples, 1379129 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:25,064][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3640721 effective words) took 2.6s, 1407547 effective words/s
[2023-02-07 20:29:26,071][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 42.43% examples, 1584103 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:27,080][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 86.98% examples, 1587390 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:27,356][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3640721 effective words) took 2.3s, 1589709 effective words/s
[2023-02-07 20:29:28,361][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 56.03% examples, 2083282 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:29,098][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3640721 effective words) took 1.7s, 2090743 effective words/s
[2023-02-07 20:29:30,105][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 41.26% examples, 1540444 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:31,115][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 85.66% examples, 1560685 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:31,426][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3640721 effective words) took 2.3s, 1566098 effective words/s
[2023-02-07 20:29:32,429][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 42.65% examples, 1602361 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:29:33,438][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 87.63% examples, 1599379 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:33,705][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3640721 effective words) took 2.3s, 1598961 effective words/s
[2023-02-07 20:29:34,707][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 41.73% examples, 1563926 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:35,710][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 86.15% examples, 1577621 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:36,016][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3640721 effective words) took 2.3s, 1576374 effective words/s
[2023-02-07 20:29:37,027][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 42.22% examples, 1571173 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:38,035][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 86.98% examples, 1586467 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:38,311][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3640721 effective words) took 2.3s, 1588303 effective words/s
[2023-02-07 20:29:39,315][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 42.83% examples, 1602035 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:40,315][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 87.23% examples, 1602543 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:40,586][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3640721 effective words) took 2.3s, 1602561 effective words/s
[2023-02-07 20:29:41,593][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 58.21% examples, 2158220 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:42,499][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3640721 effective words) took 1.9s, 1906281 effective words/s
[2023-02-07 20:29:43,502][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 42.80% examples, 1599883 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:29:44,505][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 88.03% examples, 1609601 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:44,761][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3640721 effective words) took 2.3s, 1610629 effective words/s
[2023-02-07 20:29:45,777][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 43.35% examples, 1599958 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:29:46,785][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 89.22% examples, 1619282 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:47,010][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3640721 effective words) took 2.2s, 1620782 effective words/s
[2023-02-07 20:29:48,024][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 43.35% examples, 1601839 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:29:49,028][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 88.61% examples, 1608001 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:49,274][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3640721 effective words) took 2.3s, 1609168 effective words/s
[2023-02-07 20:29:50,278][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 42.25% examples, 1582378 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:51,279][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 85.88% examples, 1574393 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:51,588][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3640721 effective words) took 2.3s, 1574837 effective words/s
[2023-02-07 20:29:52,595][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 41.54% examples, 1549213 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:53,600][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 85.66% examples, 1564283 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:29:53,902][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3640721 effective words) took 2.3s, 1575341 effective words/s
[2023-02-07 20:29:54,904][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 42.65% examples, 1601863 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:55,910][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 86.52% examples, 1584526 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:29:56,194][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3640721 effective words) took 2.3s, 1589486 effective words/s
[2023-02-07 20:29:56,195][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54610815 effective words) took 33.7s, 1619556 effective words/s', 'datetime': '2023-02-07T20:29:56.195069', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:29:56.195 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:29:58,344][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202900-nuusbwmc/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:29:58.344632', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:29:58,345][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:29:58,432][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_202900-nuusbwmc/files/../tmp/embedding_model.pt
2023-02-07 20:29:58.433 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:30:00.317 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:30:00.981 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:30:03.354 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0576116986875195, 'test_mae': 1.0725493871798293, 'test_r2': -1.2123845658253605}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.51
wandb: percentage 0.14593
wandb:   test_mae 1.07255
wandb:   test_mse 2.05761
wandb:    test_r2 -1.21238
wandb: 
wandb: üöÄ View run azure-sweep-86 at: https://wandb.ai/xiaoqiz/mof2vec/runs/nuusbwmc
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_202900-nuusbwmc/logs
wandb: Agent Starting Run: p7zraczx with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 1023
wandb: 	model.gensim.alpha: 0.005751095835286076
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.551026659980461
wandb: 	model.gensim.vector_size: 330
wandb: 	model.gensim.window: 11
wandb: 	model.sklearn.learning_rate: 0.05854132457484743
wandb: 	model.sklearn.max_depth: 33
wandb: 	model.sklearn.min_child_weight: 0.05844805374844768
wandb: 	model.sklearn.n_estimators: 699
wandb: 	model.sklearn.num_leaves: 391
wandb: 	model.sklearn.reg_alpha: 0.18201142418844257
wandb: 	model.sklearn.reg_lambda: 0.8184421673251934
wandb: 	model.sklearn.subsample: 0.9412359223238188
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203013-p7zraczx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-87
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/p7zraczx
2023-02-07 20:30:21.576 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 20:30:21.576 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 1023 for sweep.
2023-02-07 20:30:21.577 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.005751095835286076 for sweep.
2023-02-07 20:30:21.577 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:30:21.577 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 20:30:21.578 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.551026659980461 for sweep.
2023-02-07 20:30:21.578 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 330 for sweep.
2023-02-07 20:30:21.578 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 11 for sweep.
2023-02-07 20:30:21.578 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.05854132457484743 for sweep.
2023-02-07 20:30:21.578 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 33 for sweep.
2023-02-07 20:30:21.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05844805374844768 for sweep.
2023-02-07 20:30:21.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 699 for sweep.
2023-02-07 20:30:21.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 391 for sweep.
2023-02-07 20:30:21.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.18201142418844257 for sweep.
2023-02-07 20:30:21.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.8184421673251934 for sweep.
2023-02-07 20:30:21.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.9412359223238188 for sweep.
2023-02-07 20:30:21.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:30:21.591 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203013-p7zraczx/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 1023, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 330, 'window': 11, 'min_count': 5, 'dm': 0, 'sample': 0.551026659980461, 'workers': 4, 'alpha': 0.005751095835286076, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 699, 'max_depth': 33, 'num_leaves': 391, 'reg_alpha': 0.18201142418844257, 'reg_lambda': 0.8184421673251934, 'subsample': 0.9412359223238188, 'min_child_weight': 0.05844805374844768, 'n_jobs': 4, 'learning_rate': 0.05854132457484743}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 12/3257 [00:00<00:27, 119.71it/s]  1%|          | 27/3257 [00:00<00:23, 135.69it/s]  1%|‚ñè         | 41/3257 [00:00<00:24, 131.08it/s]  2%|‚ñè         | 55/3257 [00:00<00:24, 128.67it/s]  2%|‚ñè         | 71/3257 [00:00<00:22, 138.73it/s]  3%|‚ñé         | 89/3257 [00:00<00:21, 148.41it/s]  3%|‚ñé         | 104/3257 [00:00<00:22, 138.36it/s]  4%|‚ñé         | 118/3257 [00:00<00:23, 133.04it/s]  4%|‚ñç         | 136/3257 [00:00<00:21, 144.39it/s]  5%|‚ñç         | 157/3257 [00:01<00:19, 160.82it/s]  5%|‚ñå         | 174/3257 [00:01<00:19, 158.97it/s]  6%|‚ñå         | 195/3257 [00:01<00:17, 171.48it/s]  7%|‚ñã         | 215/3257 [00:01<00:17, 178.88it/s]  7%|‚ñã         | 237/3257 [00:01<00:16, 184.19it/s]  8%|‚ñä         | 257/3257 [00:01<00:16, 186.93it/s]  9%|‚ñä         | 278/3257 [00:01<00:15, 192.37it/s]  9%|‚ñâ         | 299/3257 [00:01<00:15, 195.74it/s] 10%|‚ñâ         | 319/3257 [00:01<00:14, 196.14it/s] 10%|‚ñà         | 339/3257 [00:02<00:15, 194.14it/s] 11%|‚ñà         | 360/3257 [00:02<00:14, 198.31it/s] 12%|‚ñà‚ñè        | 380/3257 [00:02<00:15, 182.79it/s] 12%|‚ñà‚ñè        | 399/3257 [00:02<00:15, 181.55it/s] 13%|‚ñà‚ñé        | 420/3257 [00:02<00:15, 187.99it/s] 13%|‚ñà‚ñé        | 439/3257 [00:02<00:16, 166.34it/s] 14%|‚ñà‚ñç        | 459/3257 [00:02<00:16, 172.68it/s] 15%|‚ñà‚ñç        | 478/3257 [00:02<00:15, 175.41it/s] 15%|‚ñà‚ñå        | 497/3257 [00:02<00:15, 178.42it/s] 16%|‚ñà‚ñå        | 517/3257 [00:03<00:15, 181.76it/s] 16%|‚ñà‚ñã        | 536/3257 [00:03<00:15, 178.46it/s] 17%|‚ñà‚ñã        | 556/3257 [00:03<00:14, 183.83it/s] 18%|‚ñà‚ñä        | 575/3257 [00:03<00:16, 158.34it/s] 18%|‚ñà‚ñä        | 599/3257 [00:03<00:15, 174.76it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:15, 173.83it/s] 20%|‚ñà‚ñâ        | 641/3257 [00:03<00:14, 178.55it/s] 20%|‚ñà‚ñà        | 660/3257 [00:03<00:15, 163.56it/s] 21%|‚ñà‚ñà        | 682/3257 [00:03<00:14, 176.57it/s] 22%|‚ñà‚ñà‚ñè       | 701/3257 [00:04<00:14, 171.82it/s] 22%|‚ñà‚ñà‚ñè       | 721/3257 [00:04<00:14, 176.40it/s] 23%|‚ñà‚ñà‚ñé       | 739/3257 [00:04<00:15, 167.42it/s] 23%|‚ñà‚ñà‚ñé       | 756/3257 [00:04<00:21, 116.33it/s] 24%|‚ñà‚ñà‚ñç       | 774/3257 [00:04<00:19, 129.36it/s] 24%|‚ñà‚ñà‚ñç       | 792/3257 [00:04<00:17, 139.05it/s] 25%|‚ñà‚ñà‚ñç       | 812/3257 [00:04<00:16, 152.51it/s] 25%|‚ñà‚ñà‚ñå       | 829/3257 [00:05<00:15, 154.00it/s] 26%|‚ñà‚ñà‚ñå       | 846/3257 [00:05<00:15, 154.00it/s] 27%|‚ñà‚ñà‚ñã       | 866/3257 [00:05<00:14, 162.95it/s] 27%|‚ñà‚ñà‚ñã       | 884/3257 [00:05<00:14, 161.47it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:05<00:13, 174.79it/s] 28%|‚ñà‚ñà‚ñä       | 925/3257 [00:05<00:13, 177.42it/s] 29%|‚ñà‚ñà‚ñâ       | 943/3257 [00:05<00:13, 171.19it/s] 30%|‚ñà‚ñà‚ñâ       | 963/3257 [00:05<00:12, 178.46it/s] 30%|‚ñà‚ñà‚ñà       | 982/3257 [00:05<00:13, 174.48it/s] 31%|‚ñà‚ñà‚ñà       | 1000/3257 [00:06<00:13, 171.91it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1018/3257 [00:06<00:12, 172.83it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1036/3257 [00:06<00:13, 166.18it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1053/3257 [00:06<00:13, 164.33it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1074/3257 [00:06<00:12, 176.90it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1092/3257 [00:06<00:12, 170.75it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1115/3257 [00:06<00:11, 185.23it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1134/3257 [00:06<00:11, 177.40it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1152/3257 [00:06<00:11, 177.46it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:06<00:11, 185.19it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1192/3257 [00:07<00:12, 167.09it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1210/3257 [00:07<00:12, 162.20it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1231/3257 [00:07<00:11, 174.33it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:07<00:12, 164.53it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:07<00:11, 173.72it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1289/3257 [00:07<00:12, 162.50it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1308/3257 [00:07<00:11, 168.06it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:07<00:11, 172.22it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1348/3257 [00:08<00:10, 178.68it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1367/3257 [00:08<00:11, 171.33it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1385/3257 [00:08<00:11, 167.64it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1403/3257 [00:08<00:10, 169.27it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1424/3257 [00:08<00:10, 179.40it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1446/3257 [00:08<00:09, 189.11it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1471/3257 [00:08<00:08, 203.59it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1492/3257 [00:08<00:08, 203.39it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1514/3257 [00:08<00:08, 205.81it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1535/3257 [00:09<00:09, 184.55it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1554/3257 [00:09<00:09, 179.17it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1574/3257 [00:09<00:09, 182.25it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1593/3257 [00:09<00:09, 183.18it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:09<00:08, 191.39it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1635/3257 [00:09<00:09, 177.21it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1654/3257 [00:09<00:09, 174.06it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1672/3257 [00:09<00:09, 167.46it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1690/3257 [00:09<00:09, 168.50it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1709/3257 [00:10<00:08, 173.67it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:10<00:09, 167.79it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1745/3257 [00:10<00:09, 163.18it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1765/3257 [00:10<00:08, 171.28it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1787/3257 [00:10<00:08, 182.94it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1806/3257 [00:10<00:08, 174.58it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1825/3257 [00:10<00:08, 177.23it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1843/3257 [00:10<00:07, 177.32it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1863/3257 [00:10<00:07, 182.48it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1882/3257 [00:10<00:07, 180.99it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1901/3257 [00:11<00:07, 178.03it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1919/3257 [00:11<00:07, 173.20it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1940/3257 [00:11<00:07, 182.98it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1963/3257 [00:11<00:06, 196.25it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1983/3257 [00:11<00:11, 113.57it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2005/3257 [00:11<00:09, 132.75it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2026/3257 [00:11<00:08, 148.52it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2045/3257 [00:12<00:07, 153.69it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2063/3257 [00:12<00:07, 150.09it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2083/3257 [00:12<00:07, 159.95it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2101/3257 [00:12<00:07, 158.31it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:12<00:07, 162.37it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2137/3257 [00:12<00:06, 164.06it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2154/3257 [00:12<00:06, 163.85it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2177/3257 [00:12<00:06, 178.20it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2196/3257 [00:12<00:05, 178.27it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2215/3257 [00:13<00:05, 174.00it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2235/3257 [00:13<00:05, 179.84it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2254/3257 [00:13<00:05, 173.39it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2272/3257 [00:13<00:06, 163.72it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2292/3257 [00:13<00:05, 173.41it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2310/3257 [00:13<00:05, 172.79it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2335/3257 [00:13<00:04, 194.13it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2359/3257 [00:13<00:04, 206.98it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2380/3257 [00:13<00:04, 196.17it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2401/3257 [00:14<00:04, 199.99it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2422/3257 [00:14<00:04, 188.06it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2442/3257 [00:14<00:04, 174.26it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2462/3257 [00:14<00:04, 178.96it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2481/3257 [00:14<00:04, 179.90it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2503/3257 [00:14<00:03, 190.54it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2523/3257 [00:14<00:03, 187.93it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2542/3257 [00:14<00:03, 179.28it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2561/3257 [00:14<00:04, 164.88it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2578/3257 [00:15<00:04, 162.60it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2595/3257 [00:15<00:04, 158.81it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2619/3257 [00:15<00:03, 178.23it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2639/3257 [00:15<00:03, 184.23it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2658/3257 [00:15<00:03, 173.39it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2677/3257 [00:15<00:03, 177.53it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2695/3257 [00:15<00:03, 173.06it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2713/3257 [00:15<00:03, 163.04it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:15<00:03, 168.86it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2752/3257 [00:16<00:02, 176.91it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2770/3257 [00:16<00:02, 166.19it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2791/3257 [00:16<00:02, 175.19it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2809/3257 [00:16<00:02, 176.18it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2827/3257 [00:16<00:02, 166.54it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:16<00:02, 164.48it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2868/3257 [00:16<00:02, 183.81it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2887/3257 [00:16<00:02, 184.84it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2906/3257 [00:16<00:02, 173.87it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:17<00:01, 180.83it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2947/3257 [00:17<00:01, 174.92it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2967/3257 [00:17<00:01, 177.39it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:17<00:01, 172.27it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3008/3257 [00:17<00:01, 186.59it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3027/3257 [00:17<00:01, 182.68it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:17<00:01, 191.83it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3073/3257 [00:17<00:00, 205.20it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3094/3257 [00:17<00:00, 200.84it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3118/3257 [00:18<00:00, 211.28it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3140/3257 [00:18<00:00, 198.08it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3161/3257 [00:18<00:00, 194.78it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3181/3257 [00:18<00:00, 182.95it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3203/3257 [00:18<00:00, 192.22it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3223/3257 [00:18<00:00, 182.86it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:18<00:00, 187.82it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 173.00it/s]
2023-02-07 20:30:41.239 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:30:41,240][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d330,n5,mc5,s0.551027,t4>', 'datetime': '2023-02-07T20:30:41.240644', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:30:41,241][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:30:41,241][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:30:41,750][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 20:30:41,751][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:30:41,810][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 21312 unique words (49.91% of original 42701, drops 21389)', 'datetime': '2023-02-07T20:30:41.810417', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:30:41,811][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5769229 word corpus (99.08% of original 5822992, drops 53763)', 'datetime': '2023-02-07T20:30:41.811696', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:30:41,882][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 20:30:41,883][gensim.models.word2vec][INFO] - sample=0.551027 downsamples 0 most-common words
[2023-02-07 20:30:41,883][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5769229 word corpus (100.0%% of prior 5769229)', 'datetime': '2023-02-07T20:30:41.883600', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:30:42,001][gensim.models.word2vec][INFO] - estimated required memory for 21312 words and 330 dimensions: 71870320 bytes
[2023-02-07 20:30:42,001][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:30:42,038][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 21312 vocabulary and 330 features, using sg=1 hs=0 sample=0.551026659980461 negative=5 window=11 shrink_windows=True', 'datetime': '2023-02-07T20:30:42.038315', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:30:43,044][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 33.53% examples, 1944423 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:30:44,046][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 61.53% examples, 1789765 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:30:45,048][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 89.90% examples, 1728490 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:30:45,393][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5743174 effective words) took 3.4s, 1712938 effective words/s
[2023-02-07 20:30:46,403][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 30.55% examples, 1752922 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:30:47,404][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 60.06% examples, 1745915 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:30:48,404][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 91.10% examples, 1749766 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:30:48,670][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5743174 effective words) took 3.3s, 1754127 effective words/s
[2023-02-07 20:30:49,675][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 30.89% examples, 1774569 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:30:50,676][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 60.85% examples, 1771744 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:30:51,688][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 92.42% examples, 1770260 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:30:51,904][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5743174 effective words) took 3.2s, 1776641 effective words/s
[2023-02-07 20:30:52,909][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 31.53% examples, 1824887 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:30:53,918][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 62.97% examples, 1823620 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:30:54,920][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 96.47% examples, 1840422 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:30:55,022][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5743174 effective words) took 3.1s, 1843164 effective words/s
[2023-02-07 20:30:56,030][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 32.21% examples, 1863628 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:30:57,033][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 63.86% examples, 1857998 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:30:58,035][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 96.47% examples, 1842159 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:30:58,130][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5743174 effective words) took 3.1s, 1849317 effective words/s
[2023-02-07 20:30:59,132][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 31.81% examples, 1845189 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:00,132][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 63.37% examples, 1851725 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:01,135][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 97.39% examples, 1865471 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:01,207][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5743174 effective words) took 3.1s, 1867239 effective words/s
[2023-02-07 20:31:02,215][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 32.51% examples, 1881326 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:03,223][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 65.64% examples, 1912820 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:04,222][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 100.00% examples, 1906849 words/s, in_qsize 0, out_qsize 1
[2023-02-07 20:31:04,222][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5743174 effective words) took 3.0s, 1906492 effective words/s
[2023-02-07 20:31:05,226][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 31.69% examples, 1832879 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:06,227][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 64.17% examples, 1871408 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:07,238][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 97.88% examples, 1867959 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:07,289][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5743174 effective words) took 3.1s, 1873195 effective words/s
[2023-02-07 20:31:08,293][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 41.26% examples, 2426142 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:09,294][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 74.64% examples, 2174502 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:31:10,033][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5743174 effective words) took 2.7s, 2094928 effective words/s
[2023-02-07 20:31:11,040][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 41.97% examples, 2463682 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:12,045][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 84.83% examples, 2448263 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:31:12,379][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5743174 effective words) took 2.3s, 2451761 effective words/s
[2023-02-07 20:31:13,387][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 31.81% examples, 1840618 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:14,391][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 63.25% examples, 1841658 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:15,391][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 96.56% examples, 1847501 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:15,494][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5743174 effective words) took 3.1s, 1847088 effective words/s
[2023-02-07 20:31:16,500][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 41.08% examples, 2412311 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:17,501][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 76.14% examples, 2211883 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:18,216][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5743174 effective words) took 2.7s, 2112179 effective words/s
[2023-02-07 20:31:19,220][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 40.90% examples, 2408162 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:20,224][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 84.43% examples, 2438149 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:20,561][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5743174 effective words) took 2.3s, 2451138 effective words/s
[2023-02-07 20:31:21,568][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 43.35% examples, 2547056 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:31:22,568][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 77.96% examples, 2254181 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:31:23,201][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5743174 effective words) took 2.6s, 2178847 effective words/s
[2023-02-07 20:31:24,210][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 33.13% examples, 1906279 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:31:25,214][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 66.04% examples, 1926615 words/s, in_qsize 6, out_qsize 1
[2023-02-07 20:31:26,165][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5743174 effective words) took 3.0s, 1939703 effective words/s
[2023-02-07 20:31:26,165][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86147610 effective words) took 44.1s, 1952260 effective words/s', 'datetime': '2023-02-07T20:31:26.165832', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:31:26.166 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:31:29,349][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203013-p7zraczx/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:31:29.349610', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:31:29,351][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:31:29,447][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203013-p7zraczx/files/../tmp/embedding_model.pt
2023-02-07 20:31:29.447 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:31:31.233 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:31:31.861 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:31:34.006 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.059965871010577, 'test_mae': 1.0520486341849358, 'test_r2': -1.6627927270083736}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.79
wandb: percentage 0.5009
wandb:   test_mae 1.05205
wandb:   test_mse 2.05997
wandb:    test_r2 -1.66279
wandb: 
wandb: üöÄ View run glowing-sweep-87 at: https://wandb.ai/xiaoqiz/mof2vec/runs/p7zraczx
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_203013-p7zraczx/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: txo7y8l2 with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 989
wandb: 	model.gensim.alpha: 0.01404559333520037
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.4347823385340844
wandb: 	model.gensim.vector_size: 275
wandb: 	model.gensim.window: 18
wandb: 	model.sklearn.learning_rate: 0.04570237512164455
wandb: 	model.sklearn.max_depth: 94
wandb: 	model.sklearn.min_child_weight: 0.05010778192944221
wandb: 	model.sklearn.n_estimators: 1536
wandb: 	model.sklearn.num_leaves: 303
wandb: 	model.sklearn.reg_alpha: 0.27513890391948337
wandb: 	model.sklearn.reg_lambda: 0.21646907689245615
wandb: 	model.sklearn.subsample: 0.48476873156466693
wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203239-txo7y8l2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-sweep-88
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/txo7y8l2
2023-02-07 20:32:47.699 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 20:32:47.700 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 989 for sweep.
2023-02-07 20:32:47.700 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.01404559333520037 for sweep.
2023-02-07 20:32:47.700 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:32:47.700 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 20:32:47.701 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4347823385340844 for sweep.
2023-02-07 20:32:47.701 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 275 for sweep.
2023-02-07 20:32:47.701 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 18 for sweep.
2023-02-07 20:32:47.701 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.04570237512164455 for sweep.
2023-02-07 20:32:47.702 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 94 for sweep.
2023-02-07 20:32:47.702 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05010778192944221 for sweep.
2023-02-07 20:32:47.702 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1536 for sweep.
2023-02-07 20:32:47.702 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 303 for sweep.
2023-02-07 20:32:47.703 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.27513890391948337 for sweep.
2023-02-07 20:32:47.703 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.21646907689245615 for sweep.
2023-02-07 20:32:47.703 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.48476873156466693 for sweep.
2023-02-07 20:32:47.703 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:32:47.709 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203239-txo7y8l2/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 989, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 275, 'window': 18, 'min_count': 1, 'dm': 0, 'sample': 0.4347823385340844, 'workers': 4, 'alpha': 0.01404559333520037, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1536, 'max_depth': 94, 'num_leaves': 303, 'reg_alpha': 0.27513890391948337, 'reg_lambda': 0.21646907689245615, 'subsample': 0.48476873156466693, 'min_child_weight': 0.05010778192944221, 'n_jobs': 4, 'learning_rate': 0.04570237512164455}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 22/3257 [00:00<00:14, 218.88it/s]  1%|‚ñè         | 45/3257 [00:00<00:14, 225.15it/s]  2%|‚ñè         | 68/3257 [00:00<00:14, 220.30it/s]  3%|‚ñé         | 94/3257 [00:00<00:13, 230.90it/s]  4%|‚ñé         | 118/3257 [00:00<00:13, 227.59it/s]  4%|‚ñç         | 141/3257 [00:00<00:20, 155.54it/s]  5%|‚ñå         | 164/3257 [00:00<00:17, 172.75it/s]  6%|‚ñå         | 190/3257 [00:00<00:15, 194.77it/s]  7%|‚ñã         | 216/3257 [00:01<00:14, 208.36it/s]  7%|‚ñã         | 243/3257 [00:01<00:13, 224.43it/s]  8%|‚ñä         | 267/3257 [00:01<00:13, 221.72it/s]  9%|‚ñâ         | 297/3257 [00:01<00:12, 242.13it/s] 10%|‚ñâ         | 323/3257 [00:01<00:11, 245.91it/s] 11%|‚ñà         | 349/3257 [00:01<00:12, 235.82it/s] 12%|‚ñà‚ñè        | 375/3257 [00:01<00:12, 237.76it/s] 12%|‚ñà‚ñè        | 400/3257 [00:01<00:12, 228.20it/s] 13%|‚ñà‚ñé        | 424/3257 [00:01<00:12, 230.19it/s] 14%|‚ñà‚ñç        | 448/3257 [00:02<00:13, 214.39it/s] 15%|‚ñà‚ñç        | 475/3257 [00:02<00:12, 226.72it/s] 15%|‚ñà‚ñå        | 501/3257 [00:02<00:11, 232.87it/s] 16%|‚ñà‚ñå        | 526/3257 [00:02<00:11, 233.58it/s] 17%|‚ñà‚ñã        | 552/3257 [00:02<00:11, 238.85it/s] 18%|‚ñà‚ñä        | 577/3257 [00:02<00:12, 219.32it/s] 19%|‚ñà‚ñä        | 605/3257 [00:02<00:11, 234.72it/s] 19%|‚ñà‚ñâ        | 632/3257 [00:02<00:10, 243.87it/s] 20%|‚ñà‚ñà        | 657/3257 [00:02<00:11, 228.69it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:11, 230.11it/s] 22%|‚ñà‚ñà‚ñè       | 710/3257 [00:03<00:10, 238.25it/s] 23%|‚ñà‚ñà‚ñé       | 735/3257 [00:03<00:10, 234.24it/s] 23%|‚ñà‚ñà‚ñé       | 759/3257 [00:03<00:10, 230.07it/s] 24%|‚ñà‚ñà‚ñç       | 783/3257 [00:03<00:10, 229.84it/s] 25%|‚ñà‚ñà‚ñç       | 809/3257 [00:03<00:10, 237.63it/s] 26%|‚ñà‚ñà‚ñå       | 833/3257 [00:03<00:10, 236.45it/s] 26%|‚ñà‚ñà‚ñã       | 857/3257 [00:03<00:10, 230.56it/s] 27%|‚ñà‚ñà‚ñã       | 881/3257 [00:03<00:10, 229.95it/s] 28%|‚ñà‚ñà‚ñä       | 908/3257 [00:04<00:09, 238.70it/s] 29%|‚ñà‚ñà‚ñä       | 932/3257 [00:04<00:09, 235.62it/s] 29%|‚ñà‚ñà‚ñâ       | 958/3257 [00:04<00:09, 242.31it/s] 30%|‚ñà‚ñà‚ñà       | 983/3257 [00:04<00:09, 239.57it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:04<00:09, 234.04it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1032/3257 [00:04<00:09, 237.42it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1056/3257 [00:04<00:09, 232.36it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:04<00:09, 231.79it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1105/3257 [00:04<00:09, 235.67it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1129/3257 [00:04<00:09, 235.09it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1154/3257 [00:05<00:08, 238.30it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1178/3257 [00:05<00:09, 228.14it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1201/3257 [00:05<00:09, 220.45it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1226/3257 [00:05<00:08, 227.71it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:05<00:08, 226.19it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1274/3257 [00:05<00:08, 232.81it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1298/3257 [00:05<00:08, 218.84it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1325/3257 [00:05<00:08, 229.91it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:05<00:08, 230.94it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1374/3257 [00:06<00:12, 154.04it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1401/3257 [00:06<00:10, 177.24it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1429/3257 [00:06<00:09, 200.64it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1458/3257 [00:06<00:08, 221.76it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1485/3257 [00:06<00:07, 231.79it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:06<00:07, 241.43it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1539/3257 [00:06<00:07, 220.93it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1563/3257 [00:06<00:07, 225.41it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1587/3257 [00:07<00:07, 222.65it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:07<00:06, 236.27it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1640/3257 [00:07<00:07, 227.61it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1664/3257 [00:07<00:07, 218.31it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1687/3257 [00:07<00:07, 214.64it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1711/3257 [00:07<00:06, 221.02it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1734/3257 [00:07<00:07, 207.39it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1759/3257 [00:07<00:06, 217.93it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1783/3257 [00:07<00:06, 223.37it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1806/3257 [00:08<00:06, 219.78it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1829/3257 [00:08<00:06, 218.26it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1856/3257 [00:08<00:06, 230.15it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1883/3257 [00:08<00:05, 237.69it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1908/3257 [00:08<00:05, 238.93it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1932/3257 [00:08<00:05, 237.72it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:08<00:04, 259.07it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1991/3257 [00:08<00:05, 252.41it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2017/3257 [00:08<00:04, 254.37it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2043/3257 [00:09<00:04, 246.68it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2068/3257 [00:09<00:05, 232.06it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2093/3257 [00:09<00:04, 235.85it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2117/3257 [00:09<00:04, 235.87it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2141/3257 [00:09<00:05, 222.73it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2164/3257 [00:09<00:04, 223.78it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2189/3257 [00:09<00:04, 229.18it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2213/3257 [00:09<00:04, 224.02it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2238/3257 [00:09<00:04, 229.05it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2262/3257 [00:10<00:04, 230.71it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2288/3257 [00:10<00:04, 238.26it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2312/3257 [00:10<00:04, 234.91it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2342/3257 [00:10<00:03, 253.28it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2370/3257 [00:10<00:03, 258.55it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2398/3257 [00:10<00:03, 264.41it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2425/3257 [00:10<00:03, 250.24it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2451/3257 [00:10<00:03, 232.07it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2481/3257 [00:10<00:03, 248.07it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2509/3257 [00:10<00:02, 254.55it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2535/3257 [00:11<00:02, 253.66it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2561/3257 [00:11<00:02, 241.03it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2586/3257 [00:11<00:02, 228.94it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2617/3257 [00:11<00:02, 249.68it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2643/3257 [00:11<00:02, 251.85it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2669/3257 [00:11<00:02, 249.39it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2695/3257 [00:11<00:02, 247.21it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2720/3257 [00:11<00:02, 233.72it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2751/3257 [00:11<00:02, 250.25it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2777/3257 [00:12<00:02, 168.16it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:12<00:02, 192.40it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2829/3257 [00:12<00:02, 193.53it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2859/3257 [00:12<00:01, 217.86it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2888/3257 [00:12<00:01, 235.37it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2914/3257 [00:12<00:01, 240.03it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2940/3257 [00:12<00:01, 239.16it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2965/3257 [00:12<00:01, 237.72it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2990/3257 [00:13<00:01, 229.56it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3018/3257 [00:13<00:00, 239.56it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3047/3257 [00:13<00:00, 251.99it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3075/3257 [00:13<00:00, 258.24it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3102/3257 [00:13<00:00, 260.76it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3130/3257 [00:13<00:00, 266.03it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3157/3257 [00:13<00:00, 254.07it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3183/3257 [00:13<00:00, 244.57it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3209/3257 [00:13<00:00, 246.59it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3234/3257 [00:14<00:00, 244.06it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:14<00:00, 230.06it/s]
2023-02-07 20:33:02.297 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:33:02,299][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d275,n5,s0.434782,t4>', 'datetime': '2023-02-07T20:33:02.299211', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:33:02,299][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:33:02,299][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:33:02,606][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 20:33:02,606][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:33:02,636][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 13061 unique words (100.00% of original 13061, drops 0)', 'datetime': '2023-02-07T20:33:02.636957', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:33:02,637][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 3639370 word corpus (100.00% of original 3639370, drops 0)', 'datetime': '2023-02-07T20:33:02.637340', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:33:02,680][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 20:33:02,681][gensim.models.word2vec][INFO] - sample=0.434782 downsamples 0 most-common words
[2023-02-07 20:33:02,681][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3639370 word corpus (100.0%% of prior 3639370)', 'datetime': '2023-02-07T20:33:02.681703', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:33:02,754][gensim.models.word2vec][INFO] - estimated required memory for 13061 words and 275 dimensions: 39498800 bytes
[2023-02-07 20:33:02,754][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:33:02,772][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 13061 vocabulary and 275 features, using sg=1 hs=0 sample=0.4347823385340844 negative=5 window=18 shrink_windows=True', 'datetime': '2023-02-07T20:33:02.772256', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:33:03,781][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 48.76% examples, 1805336 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:33:04,757][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3642627 effective words) took 2.0s, 1838364 effective words/s
[2023-02-07 20:33:05,763][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 56.03% examples, 2088401 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:33:06,538][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3642627 effective words) took 1.8s, 2048532 effective words/s
[2023-02-07 20:33:07,548][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 52.13% examples, 1927529 words/s, in_qsize 7, out_qsize 5
[2023-02-07 20:33:08,403][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3642627 effective words) took 1.9s, 1956715 effective words/s
[2023-02-07 20:33:09,408][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 46.64% examples, 1725586 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:33:10,107][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3642627 effective words) took 1.7s, 2139589 effective words/s
[2023-02-07 20:33:11,111][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 54.93% examples, 2045637 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:33:11,911][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3642627 effective words) took 1.8s, 2022366 effective words/s
[2023-02-07 20:33:12,913][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 53.42% examples, 1999565 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:33:13,730][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3642627 effective words) took 1.8s, 2004239 effective words/s
[2023-02-07 20:33:14,738][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 56.19% examples, 2090585 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:33:15,447][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3642627 effective words) took 1.7s, 2124869 effective words/s
[2023-02-07 20:33:16,454][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 58.03% examples, 2151124 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:33:17,155][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3642627 effective words) took 1.7s, 2134575 effective words/s
[2023-02-07 20:33:18,168][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 55.51% examples, 2051744 words/s, in_qsize 8, out_qsize 11
[2023-02-07 20:33:18,883][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3642627 effective words) took 1.7s, 2109082 effective words/s
[2023-02-07 20:33:19,889][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 54.59% examples, 2030133 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:33:20,653][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3642627 effective words) took 1.8s, 2061133 effective words/s
[2023-02-07 20:33:21,658][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 72.64% examples, 2677597 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:33:22,010][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3642627 effective words) took 1.4s, 2686568 effective words/s
[2023-02-07 20:33:23,012][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 72.86% examples, 2697033 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:33:23,360][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3642627 effective words) took 1.3s, 2701216 effective words/s
[2023-02-07 20:33:24,363][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 74.18% examples, 2738225 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:33:24,797][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3642627 effective words) took 1.4s, 2537927 effective words/s
[2023-02-07 20:33:25,804][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 54.31% examples, 2019801 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:33:26,545][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3642627 effective words) took 1.7s, 2085834 effective words/s
[2023-02-07 20:33:27,552][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 74.06% examples, 2727663 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:33:27,900][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3642627 effective words) took 1.4s, 2691390 effective words/s
[2023-02-07 20:33:27,901][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54639405 effective words) took 25.1s, 2174377 effective words/s', 'datetime': '2023-02-07T20:33:27.901415', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:33:27.901 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:33:30,356][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203239-txo7y8l2/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:33:30.356507', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:33:30,358][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:33:30,413][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203239-txo7y8l2/files/../tmp/embedding_model.pt
2023-02-07 20:33:30.413 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:33:32.125 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:33:32.724 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:33:34.902 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1926382758431746, 'test_mae': 1.0855470390867534, 'test_r2': -2.2567667203403343}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.83
wandb: percentage 0.0
wandb:   test_mae 1.08555
wandb:   test_mse 2.19264
wandb:    test_r2 -2.25677
wandb: 
wandb: üöÄ View run golden-sweep-88 at: https://wandb.ai/xiaoqiz/mof2vec/runs/txo7y8l2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_203239-txo7y8l2/logs
wandb: Agent Starting Run: pbwyvss1 with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 1020
wandb: 	model.gensim.alpha: 0.006885892608589227
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.32148958116095355
wandb: 	model.gensim.vector_size: 117
wandb: 	model.gensim.window: 12
wandb: 	model.sklearn.learning_rate: 0.0013819368036044977
wandb: 	model.sklearn.max_depth: 42
wandb: 	model.sklearn.min_child_weight: 0.010186163507956248
wandb: 	model.sklearn.n_estimators: 3196
wandb: 	model.sklearn.num_leaves: 332
wandb: 	model.sklearn.reg_alpha: 0.04127267987570485
wandb: 	model.sklearn.reg_lambda: 0.003268565356094836
wandb: 	model.sklearn.subsample: 0.4735481504917826
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203345-pbwyvss1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-89
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/pbwyvss1
2023-02-07 20:33:54.656 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 20:33:54.657 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 1020 for sweep.
2023-02-07 20:33:54.657 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.006885892608589227 for sweep.
2023-02-07 20:33:54.658 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:33:54.658 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 20:33:54.658 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.32148958116095355 for sweep.
2023-02-07 20:33:54.658 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 117 for sweep.
2023-02-07 20:33:54.659 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 12 for sweep.
2023-02-07 20:33:54.659 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0013819368036044977 for sweep.
2023-02-07 20:33:54.659 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 42 for sweep.
2023-02-07 20:33:54.659 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.010186163507956248 for sweep.
2023-02-07 20:33:54.660 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3196 for sweep.
2023-02-07 20:33:54.660 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 332 for sweep.
2023-02-07 20:33:54.660 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.04127267987570485 for sweep.
2023-02-07 20:33:54.660 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.003268565356094836 for sweep.
2023-02-07 20:33:54.661 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.4735481504917826 for sweep.
2023-02-07 20:33:54.661 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:33:54.667 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203345-pbwyvss1/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 1020, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 117, 'window': 12, 'min_count': 3, 'dm': 0, 'sample': 0.32148958116095355, 'workers': 4, 'alpha': 0.006885892608589227, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3196, 'max_depth': 42, 'num_leaves': 332, 'reg_alpha': 0.04127267987570485, 'reg_lambda': 0.003268565356094836, 'subsample': 0.4735481504917826, 'min_child_weight': 0.010186163507956248, 'n_jobs': 4, 'learning_rate': 0.0013819368036044977}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 24/3257 [00:00<00:14, 230.34it/s]  1%|‚ñè         | 48/3257 [00:00<00:14, 225.00it/s]  2%|‚ñè         | 71/3257 [00:00<00:14, 223.83it/s]  3%|‚ñé         | 96/3257 [00:00<00:13, 232.84it/s]  4%|‚ñé         | 120/3257 [00:00<00:13, 228.76it/s]  5%|‚ñç         | 147/3257 [00:00<00:12, 240.08it/s]  5%|‚ñå         | 172/3257 [00:00<00:13, 236.74it/s]  6%|‚ñå         | 197/3257 [00:00<00:12, 238.81it/s]  7%|‚ñã         | 227/3257 [00:00<00:11, 256.37it/s]  8%|‚ñä         | 253/3257 [00:01<00:11, 255.42it/s]  9%|‚ñä         | 284/3257 [00:01<00:11, 269.29it/s] 10%|‚ñâ         | 311/3257 [00:01<00:11, 267.09it/s] 10%|‚ñà         | 338/3257 [00:01<00:11, 263.00it/s] 11%|‚ñà         | 366/3257 [00:01<00:10, 264.43it/s] 12%|‚ñà‚ñè        | 393/3257 [00:01<00:11, 243.71it/s] 13%|‚ñà‚ñé        | 420/3257 [00:01<00:11, 246.85it/s] 14%|‚ñà‚ñé        | 445/3257 [00:01<00:12, 219.75it/s] 14%|‚ñà‚ñç        | 471/3257 [00:01<00:12, 229.98it/s] 15%|‚ñà‚ñå        | 496/3257 [00:02<00:11, 234.46it/s] 16%|‚ñà‚ñå        | 522/3257 [00:02<00:11, 237.70it/s] 17%|‚ñà‚ñã        | 547/3257 [00:02<00:11, 230.75it/s] 18%|‚ñà‚ñä        | 571/3257 [00:02<00:12, 217.32it/s] 18%|‚ñà‚ñä        | 599/3257 [00:02<00:11, 229.55it/s] 19%|‚ñà‚ñâ        | 623/3257 [00:02<00:11, 228.56it/s] 20%|‚ñà‚ñâ        | 649/3257 [00:02<00:11, 233.64it/s] 21%|‚ñà‚ñà        | 673/3257 [00:02<00:11, 230.41it/s] 21%|‚ñà‚ñà‚ñè       | 697/3257 [00:02<00:11, 224.30it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:03<00:11, 227.32it/s] 23%|‚ñà‚ñà‚ñé       | 745/3257 [00:03<00:11, 222.10it/s] 24%|‚ñà‚ñà‚ñé       | 771/3257 [00:03<00:10, 230.36it/s] 24%|‚ñà‚ñà‚ñç       | 795/3257 [00:03<00:10, 230.48it/s] 25%|‚ñà‚ñà‚ñå       | 820/3257 [00:03<00:10, 233.61it/s] 26%|‚ñà‚ñà‚ñå       | 844/3257 [00:03<00:10, 220.61it/s] 27%|‚ñà‚ñà‚ñã       | 867/3257 [00:03<00:10, 220.44it/s] 27%|‚ñà‚ñà‚ñã       | 890/3257 [00:03<00:10, 215.44it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:03<00:10, 219.93it/s] 29%|‚ñà‚ñà‚ñâ       | 937/3257 [00:04<00:14, 158.11it/s] 30%|‚ñà‚ñà‚ñâ       | 962/3257 [00:04<00:12, 178.48it/s] 30%|‚ñà‚ñà‚ñà       | 985/3257 [00:04<00:12, 189.04it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:04<00:11, 191.26it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1029/3257 [00:04<00:11, 198.57it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1050/3257 [00:04<00:11, 194.23it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1075/3257 [00:04<00:10, 208.53it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1097/3257 [00:04<00:10, 202.86it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1121/3257 [00:05<00:10, 211.33it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1143/3257 [00:05<00:10, 208.74it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1168/3257 [00:05<00:09, 217.92it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:05<00:10, 200.55it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1212/3257 [00:05<00:10, 200.30it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1240/3257 [00:05<00:09, 221.52it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1263/3257 [00:05<00:09, 221.23it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1286/3257 [00:05<00:09, 211.47it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1309/3257 [00:05<00:09, 216.16it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1337/3257 [00:05<00:08, 233.13it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1361/3257 [00:06<00:08, 233.51it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1385/3257 [00:06<00:08, 229.75it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1414/3257 [00:06<00:07, 246.99it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1440/3257 [00:06<00:07, 247.66it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1471/3257 [00:06<00:06, 263.48it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1500/3257 [00:06<00:06, 269.34it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1528/3257 [00:06<00:06, 252.79it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1554/3257 [00:06<00:07, 242.70it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1579/3257 [00:06<00:07, 238.57it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1606/3257 [00:07<00:06, 245.88it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1633/3257 [00:07<00:06, 251.13it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:07<00:06, 232.28it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1683/3257 [00:07<00:07, 222.78it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1708/3257 [00:07<00:06, 227.15it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:07<00:06, 223.41it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1755/3257 [00:07<00:06, 226.93it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1783/3257 [00:07<00:06, 241.41it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1808/3257 [00:07<00:06, 230.99it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1835/3257 [00:08<00:05, 240.21it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1861/3257 [00:08<00:05, 245.53it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1887/3257 [00:08<00:05, 246.75it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:08<00:05, 248.14it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1947/3257 [00:08<00:04, 263.75it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1976/3257 [00:08<00:04, 271.15it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:08<00:04, 259.37it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2033/3257 [00:08<00:04, 266.19it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2060/3257 [00:08<00:04, 239.51it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2089/3257 [00:09<00:04, 251.05it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2116/3257 [00:09<00:04, 254.66it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2142/3257 [00:09<00:04, 235.40it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2167/3257 [00:09<00:06, 170.81it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2189/3257 [00:09<00:05, 179.81it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2211/3257 [00:09<00:05, 187.97it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2237/3257 [00:09<00:05, 202.78it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2259/3257 [00:09<00:04, 206.33it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2281/3257 [00:10<00:04, 206.20it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2305/3257 [00:10<00:04, 214.60it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2333/3257 [00:10<00:03, 231.30it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2361/3257 [00:10<00:03, 244.57it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2390/3257 [00:10<00:03, 254.41it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2416/3257 [00:10<00:03, 248.75it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2442/3257 [00:10<00:03, 242.58it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2470/3257 [00:10<00:03, 252.43it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2496/3257 [00:10<00:03, 250.23it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2524/3257 [00:10<00:02, 256.61it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2550/3257 [00:11<00:02, 253.57it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2576/3257 [00:11<00:02, 233.83it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2600/3257 [00:11<00:02, 232.79it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2632/3257 [00:11<00:02, 257.14it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2659/3257 [00:11<00:02, 244.59it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2687/3257 [00:11<00:02, 252.25it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2713/3257 [00:11<00:02, 229.68it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2741/3257 [00:11<00:02, 240.72it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:12<00:02, 241.23it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2795/3257 [00:12<00:01, 248.32it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2821/3257 [00:12<00:01, 247.26it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2846/3257 [00:12<00:01, 238.86it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2880/3257 [00:12<00:01, 267.13it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2908/3257 [00:12<00:01, 246.86it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2934/3257 [00:12<00:01, 247.51it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2960/3257 [00:12<00:01, 239.34it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:12<00:01, 235.94it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3014/3257 [00:12<00:00, 248.83it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3041/3257 [00:13<00:00, 254.36it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3072/3257 [00:13<00:00, 269.11it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3100/3257 [00:13<00:00, 269.51it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3129/3257 [00:13<00:00, 273.62it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3157/3257 [00:13<00:00, 253.71it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3183/3257 [00:13<00:00, 247.68it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3212/3257 [00:13<00:00, 258.17it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3243/3257 [00:13<00:00, 272.37it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 234.25it/s]
2023-02-07 20:34:08.991 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:34:08,991][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d117,n5,mc3,s0.32149,t4>', 'datetime': '2023-02-07T20:34:08.991939', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:34:08,993][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:34:08,993][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:34:09,290][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 20:34:09,290][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:34:09,313][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 9372 unique words (71.76% of original 13061, drops 3689)', 'datetime': '2023-02-07T20:34:09.313887', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:34:09,314][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 3633898 word corpus (99.85% of original 3639370, drops 5472)', 'datetime': '2023-02-07T20:34:09.314697', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:34:09,344][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 20:34:09,344][gensim.models.word2vec][INFO] - sample=0.32149 downsamples 0 most-common words
[2023-02-07 20:34:09,345][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3633898 word corpus (100.0%% of prior 3633898)', 'datetime': '2023-02-07T20:34:09.345136', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:34:09,398][gensim.models.word2vec][INFO] - estimated required memory for 9372 words and 117 dimensions: 15633868 bytes
[2023-02-07 20:34:09,398][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:34:09,404][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 9372 vocabulary and 117 features, using sg=1 hs=0 sample=0.32148958116095355 negative=5 window=12 shrink_windows=True', 'datetime': '2023-02-07T20:34:09.404551', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:34:10,410][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 61.25% examples, 2259251 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:34:10,967][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3637155 effective words) took 1.6s, 2330199 effective words/s
[2023-02-07 20:34:11,969][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 93.40% examples, 3412494 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:34:12,033][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3637155 effective words) took 1.1s, 3414251 effective words/s
[2023-02-07 20:34:13,039][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 60.06% examples, 2213817 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:34:13,741][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3637155 effective words) took 1.7s, 2131450 effective words/s
[2023-02-07 20:34:14,750][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 66.81% examples, 2469376 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:34:15,218][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3637155 effective words) took 1.5s, 2467033 effective words/s
[2023-02-07 20:34:16,220][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 69.85% examples, 2603938 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:34:16,595][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3637155 effective words) took 1.4s, 2645232 effective words/s
[2023-02-07 20:34:17,611][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 74.79% examples, 2745737 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:34:17,926][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3637155 effective words) took 1.3s, 2751612 effective words/s
[2023-02-07 20:34:18,931][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 72.64% examples, 2674719 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:34:19,286][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3637155 effective words) took 1.4s, 2677378 effective words/s
[2023-02-07 20:34:20,296][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 71.57% examples, 2649189 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:34:20,656][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3637155 effective words) took 1.4s, 2669120 effective words/s
[2023-02-07 20:34:21,663][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 72.86% examples, 2682857 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:34:22,011][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3637155 effective words) took 1.4s, 2690559 effective words/s
[2023-02-07 20:34:23,020][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 73.04% examples, 2674852 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:34:23,369][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3637155 effective words) took 1.4s, 2682008 effective words/s
[2023-02-07 20:34:24,374][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 72.64% examples, 2678599 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:34:24,726][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3637155 effective words) took 1.4s, 2683126 effective words/s
[2023-02-07 20:34:25,733][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 72.86% examples, 2681409 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:34:26,081][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3637155 effective words) took 1.4s, 2689575 effective words/s
[2023-02-07 20:34:27,085][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 94.11% examples, 3425842 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:34:27,138][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3637155 effective words) took 1.1s, 3444538 effective words/s
[2023-02-07 20:34:28,142][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 72.86% examples, 2688250 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:34:28,492][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3637155 effective words) took 1.4s, 2689391 effective words/s
[2023-02-07 20:34:29,497][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 75.10% examples, 2769431 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:34:29,746][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3637155 effective words) took 1.3s, 2906398 effective words/s
[2023-02-07 20:34:29,747][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54557325 effective words) took 20.3s, 2681953 effective words/s', 'datetime': '2023-02-07T20:34:29.747314', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:34:29.747 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:34:31,215][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203345-pbwyvss1/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:34:31.215671', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:34:31,217][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:34:31,251][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203345-pbwyvss1/files/../tmp/embedding_model.pt
2023-02-07 20:34:31.251 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:34:32.459 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:34:32.880 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:34:34.217 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.10496387390543, 'test_mae': 1.0676412733423128, 'test_r2': -1.6474431782076255}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.028 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.028 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.74
wandb: percentage 0.28244
wandb:   test_mae 1.06764
wandb:   test_mse 2.10496
wandb:    test_r2 -1.64744
wandb: 
wandb: üöÄ View run wandering-sweep-89 at: https://wandb.ai/xiaoqiz/mof2vec/runs/pbwyvss1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_203345-pbwyvss1/logs
wandb: Agent Starting Run: xspmjxlh with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 973
wandb: 	model.gensim.alpha: 0.1081871131603536
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.2550021821514724
wandb: 	model.gensim.vector_size: 215
wandb: 	model.gensim.window: 18
wandb: 	model.sklearn.learning_rate: 0.009369504813136472
wandb: 	model.sklearn.max_depth: 28
wandb: 	model.sklearn.min_child_weight: 0.07497070073170904
wandb: 	model.sklearn.n_estimators: 3692
wandb: 	model.sklearn.num_leaves: 470
wandb: 	model.sklearn.reg_alpha: 0.07638145013460389
wandb: 	model.sklearn.reg_lambda: 0.5724200353080635
wandb: 	model.sklearn.subsample: 0.5594605455286686
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203445-xspmjxlh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-sweep-90
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/xspmjxlh
2023-02-07 20:34:53.632 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 20:34:53.633 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 973 for sweep.
2023-02-07 20:34:53.633 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.1081871131603536 for sweep.
2023-02-07 20:34:53.634 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:34:53.634 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 20:34:53.634 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2550021821514724 for sweep.
2023-02-07 20:34:53.634 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 215 for sweep.
2023-02-07 20:34:53.635 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 18 for sweep.
2023-02-07 20:34:53.635 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.009369504813136472 for sweep.
2023-02-07 20:34:53.635 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 28 for sweep.
2023-02-07 20:34:53.635 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.07497070073170904 for sweep.
2023-02-07 20:34:53.636 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3692 for sweep.
2023-02-07 20:34:53.636 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 470 for sweep.
2023-02-07 20:34:53.636 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.07638145013460389 for sweep.
2023-02-07 20:34:53.636 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.5724200353080635 for sweep.
2023-02-07 20:34:53.636 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5594605455286686 for sweep.
2023-02-07 20:34:53.637 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:34:53.644 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203445-xspmjxlh/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 973, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 215, 'window': 18, 'min_count': 3, 'dm': 0, 'sample': 0.2550021821514724, 'workers': 4, 'alpha': 0.1081871131603536, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3692, 'max_depth': 28, 'num_leaves': 470, 'reg_alpha': 0.07638145013460389, 'reg_lambda': 0.5724200353080635, 'subsample': 0.5594605455286686, 'min_child_weight': 0.07497070073170904, 'n_jobs': 4, 'learning_rate': 0.009369504813136472}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:18, 173.11it/s]  1%|          | 38/3257 [00:00<00:17, 187.95it/s]  2%|‚ñè         | 57/3257 [00:00<00:17, 178.04it/s]  2%|‚ñè         | 79/3257 [00:00<00:16, 190.48it/s]  3%|‚ñé         | 99/3257 [00:00<00:17, 184.62it/s]  4%|‚ñé         | 118/3257 [00:00<00:17, 179.94it/s]  4%|‚ñç         | 137/3257 [00:00<00:17, 179.73it/s]  5%|‚ñç         | 158/3257 [00:00<00:17, 181.53it/s]  5%|‚ñå         | 177/3257 [00:00<00:17, 179.62it/s]  6%|‚ñå         | 197/3257 [00:01<00:16, 184.46it/s]  7%|‚ñã         | 219/3257 [00:01<00:15, 191.66it/s]  7%|‚ñã         | 242/3257 [00:01<00:14, 202.79it/s]  8%|‚ñä         | 263/3257 [00:01<00:15, 196.54it/s]  9%|‚ñâ         | 287/3257 [00:01<00:14, 207.87it/s]  9%|‚ñâ         | 308/3257 [00:01<00:14, 203.36it/s] 10%|‚ñà         | 329/3257 [00:01<00:14, 204.87it/s] 11%|‚ñà         | 350/3257 [00:01<00:14, 194.31it/s] 11%|‚ñà‚ñè        | 370/3257 [00:02<00:20, 141.45it/s] 12%|‚ñà‚ñè        | 387/3257 [00:02<00:20, 141.84it/s] 13%|‚ñà‚ñé        | 409/3257 [00:02<00:17, 158.67it/s] 13%|‚ñà‚ñé        | 427/3257 [00:02<00:19, 148.22it/s] 14%|‚ñà‚ñé        | 445/3257 [00:02<00:18, 155.93it/s] 14%|‚ñà‚ñç        | 464/3257 [00:02<00:17, 163.64it/s] 15%|‚ñà‚ñç        | 482/3257 [00:02<00:17, 159.25it/s] 15%|‚ñà‚ñå        | 504/3257 [00:02<00:15, 172.87it/s] 16%|‚ñà‚ñå        | 524/3257 [00:02<00:15, 180.25it/s] 17%|‚ñà‚ñã        | 546/3257 [00:03<00:14, 182.82it/s] 17%|‚ñà‚ñã        | 565/3257 [00:03<00:15, 174.82it/s] 18%|‚ñà‚ñä        | 583/3257 [00:03<00:15, 171.81it/s] 19%|‚ñà‚ñä        | 604/3257 [00:03<00:14, 178.47it/s] 19%|‚ñà‚ñâ        | 622/3257 [00:03<00:14, 177.34it/s] 20%|‚ñà‚ñâ        | 644/3257 [00:03<00:14, 186.10it/s] 20%|‚ñà‚ñà        | 663/3257 [00:03<00:15, 170.13it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:15, 171.59it/s] 22%|‚ñà‚ñà‚ñè       | 703/3257 [00:03<00:14, 177.56it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:04<00:14, 178.17it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:04<00:14, 170.90it/s] 23%|‚ñà‚ñà‚ñé       | 762/3257 [00:04<00:13, 182.43it/s] 24%|‚ñà‚ñà‚ñç       | 781/3257 [00:04<00:14, 175.99it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:04<00:13, 184.00it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:04<00:13, 176.10it/s] 26%|‚ñà‚ñà‚ñå       | 841/3257 [00:04<00:14, 168.48it/s] 26%|‚ñà‚ñà‚ñã       | 861/3257 [00:04<00:13, 174.84it/s] 27%|‚ñà‚ñà‚ñã       | 879/3257 [00:04<00:13, 171.42it/s] 28%|‚ñà‚ñà‚ñä       | 901/3257 [00:05<00:12, 183.64it/s] 28%|‚ñà‚ñà‚ñä       | 921/3257 [00:05<00:12, 188.13it/s] 29%|‚ñà‚ñà‚ñâ       | 940/3257 [00:05<00:12, 185.11it/s] 30%|‚ñà‚ñà‚ñâ       | 961/3257 [00:05<00:12, 189.37it/s] 30%|‚ñà‚ñà‚ñà       | 980/3257 [00:05<00:12, 182.57it/s] 31%|‚ñà‚ñà‚ñà       | 999/3257 [00:05<00:12, 180.99it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1018/3257 [00:05<00:12, 183.49it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1037/3257 [00:05<00:12, 174.41it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1055/3257 [00:05<00:12, 172.19it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1078/3257 [00:06<00:11, 186.71it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1097/3257 [00:06<00:12, 172.57it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:06<00:12, 175.62it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1135/3257 [00:06<00:12, 171.75it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1153/3257 [00:06<00:12, 169.02it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:06<00:11, 176.19it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:06<00:12, 161.91it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1208/3257 [00:06<00:13, 157.58it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1231/3257 [00:06<00:11, 176.84it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1250/3257 [00:07<00:11, 171.31it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:07<00:11, 177.14it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1289/3257 [00:07<00:11, 164.01it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1308/3257 [00:07<00:11, 170.88it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:07<00:10, 175.41it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:07<00:10, 178.74it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:07<00:10, 179.37it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1389/3257 [00:07<00:10, 174.80it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1407/3257 [00:08<00:14, 131.90it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1429/3257 [00:08<00:12, 150.50it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1448/3257 [00:08<00:11, 159.07it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1471/3257 [00:08<00:10, 176.98it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1492/3257 [00:08<00:09, 185.79it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:08<00:09, 191.66it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1533/3257 [00:08<00:09, 178.15it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1552/3257 [00:08<00:09, 172.50it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1573/3257 [00:08<00:09, 180.93it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1592/3257 [00:09<00:09, 177.16it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1613/3257 [00:09<00:08, 184.94it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1632/3257 [00:09<00:08, 184.99it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1651/3257 [00:09<00:09, 168.79it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1669/3257 [00:09<00:09, 168.77it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1687/3257 [00:09<00:09, 168.56it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1707/3257 [00:09<00:08, 175.01it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:09<00:08, 171.92it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1746/3257 [00:09<00:08, 170.72it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1765/3257 [00:10<00:08, 174.99it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1786/3257 [00:10<00:07, 184.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:10<00:08, 179.12it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1825/3257 [00:10<00:07, 181.59it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1844/3257 [00:10<00:07, 182.12it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1864/3257 [00:10<00:07, 187.04it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1883/3257 [00:10<00:07, 186.89it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1902/3257 [00:10<00:07, 186.57it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1921/3257 [00:10<00:07, 184.06it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1946/3257 [00:11<00:06, 203.16it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1969/3257 [00:11<00:06, 209.63it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1991/3257 [00:11<00:06, 197.57it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2011/3257 [00:11<00:06, 196.84it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2033/3257 [00:11<00:06, 201.73it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2054/3257 [00:11<00:06, 185.99it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2073/3257 [00:11<00:06, 182.75it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2093/3257 [00:11<00:06, 185.59it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2112/3257 [00:11<00:06, 182.31it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2131/3257 [00:12<00:06, 171.28it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2149/3257 [00:12<00:06, 171.56it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:12<00:06, 178.49it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2188/3257 [00:12<00:05, 180.95it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2208/3257 [00:12<00:05, 186.39it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2227/3257 [00:12<00:05, 184.34it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2246/3257 [00:12<00:05, 180.15it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2266/3257 [00:12<00:05, 183.33it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2285/3257 [00:12<00:05, 184.52it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2304/3257 [00:12<00:05, 183.16it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2330/3257 [00:13<00:04, 203.63it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2357/3257 [00:13<00:04, 220.81it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2380/3257 [00:13<00:04, 209.05it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2402/3257 [00:13<00:04, 210.07it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2424/3257 [00:13<00:04, 197.20it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2444/3257 [00:13<00:04, 184.37it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2466/3257 [00:13<00:04, 193.81it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2486/3257 [00:13<00:03, 193.42it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2511/3257 [00:13<00:03, 208.46it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2533/3257 [00:14<00:03, 208.59it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2555/3257 [00:14<00:03, 193.18it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2575/3257 [00:14<00:03, 185.76it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2594/3257 [00:14<00:03, 184.55it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2621/3257 [00:14<00:03, 206.25it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2642/3257 [00:14<00:02, 205.22it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2663/3257 [00:14<00:03, 194.90it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2686/3257 [00:14<00:02, 203.72it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2707/3257 [00:14<00:03, 182.21it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2727/3257 [00:15<00:02, 185.82it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2751/3257 [00:15<00:02, 195.93it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2771/3257 [00:15<00:02, 187.77it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2795/3257 [00:15<00:02, 199.33it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2816/3257 [00:15<00:03, 129.42it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2833/3257 [00:15<00:03, 137.02it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2855/3257 [00:15<00:02, 154.64it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2884/3257 [00:16<00:02, 186.35it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2906/3257 [00:16<00:01, 176.08it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:16<00:01, 182.77it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2948/3257 [00:16<00:01, 177.13it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2969/3257 [00:16<00:01, 182.48it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2988/3257 [00:16<00:01, 177.75it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:16<00:01, 190.52it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3034/3257 [00:16<00:01, 197.46it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3057/3257 [00:16<00:00, 204.88it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3080/3257 [00:17<00:00, 210.46it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3102/3257 [00:17<00:00, 212.69it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3127/3257 [00:17<00:00, 218.56it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3149/3257 [00:17<00:00, 203.08it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3170/3257 [00:17<00:00, 202.19it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3191/3257 [00:17<00:00, 196.04it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3211/3257 [00:17<00:00, 192.55it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3232/3257 [00:17<00:00, 197.23it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:17<00:00, 196.96it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:17<00:00, 181.52it/s]
2023-02-07 20:35:12.498 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:35:12,499][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d215,n5,mc3,s0.255002,t4>', 'datetime': '2023-02-07T20:35:12.499735', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:35:12,500][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:35:12,500][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:35:13,019][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 20:35:13,020][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:35:13,102][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 30516 unique words (71.46% of original 42701, drops 12185)', 'datetime': '2023-02-07T20:35:13.102702', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:35:13,103][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 5804704 word corpus (99.69% of original 5822992, drops 18288)', 'datetime': '2023-02-07T20:35:13.103146', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:35:13,208][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 20:35:13,210][gensim.models.word2vec][INFO] - sample=0.255002 downsamples 0 most-common words
[2023-02-07 20:35:13,211][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5804704 word corpus (100.0%% of prior 5804704)', 'datetime': '2023-02-07T20:35:13.211271', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:35:13,380][gensim.models.word2vec][INFO] - estimated required memory for 30516 words and 215 dimensions: 71197940 bytes
[2023-02-07 20:35:13,381][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:35:13,415][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 30516 vocabulary and 215 features, using sg=1 hs=0 sample=0.2550021821514724 negative=5 window=18 shrink_windows=True', 'datetime': '2023-02-07T20:35:13.415284', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:35:14,424][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 35.71% examples, 2075713 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:15,425][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 70.99% examples, 2087719 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:16,196][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5778362 effective words) took 2.8s, 2079936 effective words/s
[2023-02-07 20:35:17,199][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 37.24% examples, 2206319 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:18,201][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 74.82% examples, 2191977 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:18,834][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5778362 effective words) took 2.6s, 2192716 effective words/s
[2023-02-07 20:35:19,849][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 38.16% examples, 2228319 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:35:20,851][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 77.46% examples, 2247157 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:21,415][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5778362 effective words) took 2.6s, 2242266 effective words/s
[2023-02-07 20:35:22,419][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 49.77% examples, 2926642 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:23,402][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5778362 effective words) took 2.0s, 2913121 effective words/s
[2023-02-07 20:35:24,404][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 41.66% examples, 2470627 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:25,406][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 83.27% examples, 2430011 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:25,791][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5778362 effective words) took 2.4s, 2419390 effective words/s
[2023-02-07 20:35:26,797][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 38.81% examples, 2289024 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:27,801][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 78.26% examples, 2277178 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:28,332][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5778362 effective words) took 2.5s, 2278627 effective words/s
[2023-02-07 20:35:29,335][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 40.41% examples, 2393439 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:30,335][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 82.16% examples, 2391674 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:30,755][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5778362 effective words) took 2.4s, 2387375 effective words/s
[2023-02-07 20:35:31,761][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 38.19% examples, 2248562 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:32,770][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 80.66% examples, 2335122 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:33,219][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5778362 effective words) took 2.5s, 2348087 effective words/s
[2023-02-07 20:35:34,226][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 40.22% examples, 2374837 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:35,226][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 81.76% examples, 2373859 words/s, in_qsize 6, out_qsize 1
[2023-02-07 20:35:35,651][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5778362 effective words) took 2.4s, 2377622 effective words/s
[2023-02-07 20:35:36,657][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 39.36% examples, 2327282 words/s, in_qsize 1, out_qsize 3
[2023-02-07 20:35:37,659][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 78.45% examples, 2280428 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:38,170][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5778362 effective words) took 2.5s, 2294700 effective words/s
[2023-02-07 20:35:39,179][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 39.15% examples, 2312201 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:40,185][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 80.66% examples, 2337432 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:40,653][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5778362 effective words) took 2.5s, 2332565 effective words/s
[2023-02-07 20:35:41,662][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 39.15% examples, 2303543 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:42,663][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 77.31% examples, 2248020 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:43,215][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5778362 effective words) took 2.6s, 2257827 effective words/s
[2023-02-07 20:35:44,220][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 42.43% examples, 2515062 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:45,223][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 91.77% examples, 2662864 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:45,366][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5778362 effective words) took 2.2s, 2687169 effective words/s
[2023-02-07 20:35:46,378][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 48.45% examples, 2834247 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:35:47,382][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 98.93% examples, 2842645 words/s, in_qsize 6, out_qsize 1
[2023-02-07 20:35:47,394][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5778362 effective words) took 2.0s, 2852692 effective words/s
[2023-02-07 20:35:48,400][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 38.19% examples, 2243895 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:35:49,401][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 77.86% examples, 2265170 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:35:49,958][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5778362 effective words) took 2.6s, 2255665 effective words/s
[2023-02-07 20:35:49,959][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86675430 effective words) took 36.5s, 2371811 effective words/s', 'datetime': '2023-02-07T20:35:49.959733', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:35:49.960 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:35:52,828][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203445-xspmjxlh/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:35:52.828453', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:35:52,829][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:35:52,928][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203445-xspmjxlh/files/../tmp/embedding_model.pt
2023-02-07 20:35:52.929 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:35:54.435 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:35:54.973 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:35:56.380 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1217493745435623, 'test_mae': 1.0811267316618511, 'test_r2': -2.5221974939664706}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.93
wandb: percentage 0.28536
wandb:   test_mae 1.08113
wandb:   test_mse 2.12175
wandb:    test_r2 -2.5222
wandb: 
wandb: üöÄ View run drawn-sweep-90 at: https://wandb.ai/xiaoqiz/mof2vec/runs/xspmjxlh
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_203445-xspmjxlh/logs
wandb: Agent Starting Run: tnt54osj with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 897
wandb: 	model.gensim.alpha: 0.050725148646255304
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.2539198523471939
wandb: 	model.gensim.vector_size: 153
wandb: 	model.gensim.window: 16
wandb: 	model.sklearn.learning_rate: 0.023975182468873816
wandb: 	model.sklearn.max_depth: 30
wandb: 	model.sklearn.min_child_weight: 0.07508065885920112
wandb: 	model.sklearn.n_estimators: 509
wandb: 	model.sklearn.num_leaves: 413
wandb: 	model.sklearn.reg_alpha: 0.1135406167153706
wandb: 	model.sklearn.reg_lambda: 0.023548550184461843
wandb: 	model.sklearn.subsample: 0.8191007986029881
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203608-tnt54osj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-sweep-91
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/tnt54osj
2023-02-07 20:36:16.412 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 20:36:16.413 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 897 for sweep.
2023-02-07 20:36:16.413 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.050725148646255304 for sweep.
2023-02-07 20:36:16.414 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:36:16.414 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 20:36:16.414 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2539198523471939 for sweep.
2023-02-07 20:36:16.415 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 153 for sweep.
2023-02-07 20:36:16.415 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 16 for sweep.
2023-02-07 20:36:16.415 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.023975182468873816 for sweep.
2023-02-07 20:36:16.416 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 30 for sweep.
2023-02-07 20:36:16.416 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.07508065885920112 for sweep.
2023-02-07 20:36:16.416 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 509 for sweep.
2023-02-07 20:36:16.416 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 413 for sweep.
2023-02-07 20:36:16.417 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.1135406167153706 for sweep.
2023-02-07 20:36:16.417 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.023548550184461843 for sweep.
2023-02-07 20:36:16.417 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8191007986029881 for sweep.
2023-02-07 20:36:16.417 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:36:16.425 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203608-tnt54osj/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 897, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 153, 'window': 16, 'min_count': 2, 'dm': 0, 'sample': 0.2539198523471939, 'workers': 4, 'alpha': 0.050725148646255304, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 509, 'max_depth': 30, 'num_leaves': 413, 'reg_alpha': 0.1135406167153706, 'reg_lambda': 0.023548550184461843, 'subsample': 0.8191007986029881, 'min_child_weight': 0.07508065885920112, 'n_jobs': 4, 'learning_rate': 0.023975182468873816}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 15/3257 [00:00<00:22, 144.89it/s]  1%|          | 34/3257 [00:00<00:19, 161.72it/s]  2%|‚ñè         | 53/3257 [00:00<00:19, 165.04it/s]  2%|‚ñè         | 72/3257 [00:00<00:18, 172.87it/s]  3%|‚ñé         | 90/3257 [00:00<00:19, 162.13it/s]  3%|‚ñé         | 107/3257 [00:00<00:20, 155.22it/s]  4%|‚ñç         | 124/3257 [00:00<00:19, 158.41it/s]  4%|‚ñç         | 145/3257 [00:00<00:18, 172.29it/s]  5%|‚ñå         | 163/3257 [00:00<00:17, 172.59it/s]  6%|‚ñå         | 189/3257 [00:01<00:15, 196.09it/s]  6%|‚ñã         | 209/3257 [00:01<00:15, 195.10it/s]  7%|‚ñã         | 232/3257 [00:01<00:14, 203.26it/s]  8%|‚ñä         | 256/3257 [00:01<00:14, 210.78it/s]  9%|‚ñä         | 283/3257 [00:01<00:13, 227.28it/s]  9%|‚ñâ         | 307/3257 [00:01<00:12, 229.39it/s] 10%|‚ñà         | 331/3257 [00:01<00:12, 232.18it/s] 11%|‚ñà         | 355/3257 [00:01<00:12, 231.51it/s] 12%|‚ñà‚ñè        | 379/3257 [00:01<00:13, 217.49it/s] 12%|‚ñà‚ñè        | 402/3257 [00:02<00:12, 220.44it/s] 13%|‚ñà‚ñé        | 425/3257 [00:02<00:13, 210.21it/s] 14%|‚ñà‚ñé        | 447/3257 [00:02<00:13, 201.89it/s] 14%|‚ñà‚ñç        | 470/3257 [00:02<00:13, 208.46it/s] 15%|‚ñà‚ñå        | 492/3257 [00:02<00:13, 205.57it/s] 16%|‚ñà‚ñå        | 517/3257 [00:02<00:12, 216.58it/s] 17%|‚ñà‚ñã        | 542/3257 [00:02<00:12, 225.34it/s] 17%|‚ñà‚ñã        | 565/3257 [00:02<00:12, 207.86it/s] 18%|‚ñà‚ñä        | 587/3257 [00:02<00:12, 208.49it/s] 19%|‚ñà‚ñâ        | 611/3257 [00:03<00:12, 215.28it/s] 19%|‚ñà‚ñâ        | 635/3257 [00:03<00:11, 222.04it/s] 20%|‚ñà‚ñà        | 658/3257 [00:03<00:12, 205.01it/s] 21%|‚ñà‚ñà        | 682/3257 [00:03<00:12, 214.29it/s] 22%|‚ñà‚ñà‚ñè       | 704/3257 [00:03<00:12, 209.09it/s] 22%|‚ñà‚ñà‚ñè       | 728/3257 [00:03<00:11, 216.73it/s] 23%|‚ñà‚ñà‚ñé       | 753/3257 [00:03<00:11, 224.50it/s] 24%|‚ñà‚ñà‚ñç       | 778/3257 [00:03<00:10, 230.34it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:03<00:10, 236.40it/s] 25%|‚ñà‚ñà‚ñå       | 828/3257 [00:03<00:10, 229.93it/s] 26%|‚ñà‚ñà‚ñå       | 852/3257 [00:04<00:10, 219.52it/s] 27%|‚ñà‚ñà‚ñã       | 877/3257 [00:04<00:10, 226.83it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:04<00:09, 242.15it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:04<00:09, 243.59it/s] 29%|‚ñà‚ñà‚ñâ       | 956/3257 [00:04<00:13, 167.76it/s] 30%|‚ñà‚ñà‚ñà       | 979/3257 [00:04<00:12, 180.40it/s] 31%|‚ñà‚ñà‚ñà       | 1002/3257 [00:04<00:11, 191.70it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1025/3257 [00:04<00:11, 200.45it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:05<00:11, 200.72it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1075/3257 [00:05<00:09, 220.76it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1099/3257 [00:05<00:09, 220.87it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1123/3257 [00:05<00:09, 225.79it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:05<00:09, 223.49it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1175/3257 [00:05<00:08, 234.35it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1199/3257 [00:05<00:09, 218.45it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1222/3257 [00:05<00:09, 220.78it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:05<00:08, 232.51it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:06<00:08, 234.57it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1299/3257 [00:06<00:08, 225.11it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1325/3257 [00:06<00:08, 234.17it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:06<00:08, 236.80it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1374/3257 [00:06<00:08, 235.12it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1398/3257 [00:06<00:07, 234.60it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1426/3257 [00:06<00:07, 245.80it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1453/3257 [00:06<00:07, 252.29it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1483/3257 [00:06<00:06, 266.23it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1512/3257 [00:07<00:06, 272.26it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1540/3257 [00:07<00:06, 254.49it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1566/3257 [00:07<00:06, 247.54it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1591/3257 [00:07<00:06, 247.17it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:07<00:06, 255.68it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1645/3257 [00:07<00:06, 249.88it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1671/3257 [00:07<00:06, 238.54it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1698/3257 [00:07<00:06, 246.63it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1724/3257 [00:07<00:06, 248.96it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1750/3257 [00:07<00:06, 236.50it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1777/3257 [00:08<00:06, 244.27it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1804/3257 [00:08<00:05, 250.20it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1830/3257 [00:08<00:05, 238.15it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1858/3257 [00:08<00:05, 245.38it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1888/3257 [00:08<00:05, 259.19it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:08<00:05, 263.63it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1948/3257 [00:08<00:04, 279.11it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:08<00:04, 280.05it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2006/3257 [00:08<00:04, 276.83it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2034/3257 [00:09<00:04, 272.64it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2062/3257 [00:09<00:04, 248.82it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2088/3257 [00:09<00:04, 248.30it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2114/3257 [00:09<00:04, 249.84it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2140/3257 [00:09<00:04, 241.35it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2167/3257 [00:09<00:04, 249.18it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2193/3257 [00:09<00:04, 251.29it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2219/3257 [00:10<00:06, 158.89it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2243/3257 [00:10<00:05, 173.24it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2270/3257 [00:10<00:05, 192.19it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:10<00:04, 211.46it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2331/3257 [00:10<00:03, 237.31it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2361/3257 [00:10<00:03, 253.76it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2390/3257 [00:10<00:03, 262.52it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2418/3257 [00:10<00:03, 254.45it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2445/3257 [00:10<00:03, 251.10it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2475/3257 [00:10<00:02, 262.13it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2507/3257 [00:11<00:02, 273.29it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2537/3257 [00:11<00:02, 280.71it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2566/3257 [00:11<00:02, 266.80it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2594/3257 [00:11<00:02, 253.76it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:11<00:02, 273.86it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2656/3257 [00:11<00:02, 265.14it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2684/3257 [00:11<00:02, 267.15it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2711/3257 [00:11<00:02, 247.49it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2742/3257 [00:11<00:01, 262.66it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2769/3257 [00:12<00:01, 259.83it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2801/3257 [00:12<00:01, 275.02it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2829/3257 [00:12<00:01, 256.12it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2861/3257 [00:12<00:01, 272.34it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2892/3257 [00:12<00:01, 282.84it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2921/3257 [00:12<00:01, 278.09it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:12<00:01, 266.31it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2978/3257 [00:12<00:01, 266.37it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3009/3257 [00:12<00:00, 277.19it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3037/3257 [00:13<00:00, 275.09it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3069/3257 [00:13<00:00, 286.81it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3098/3257 [00:13<00:00, 287.25it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3129/3257 [00:13<00:00, 292.68it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3159/3257 [00:13<00:00, 276.17it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3187/3257 [00:13<00:00, 265.76it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3214/3257 [00:13<00:00, 265.06it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:13<00:00, 280.27it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 235.12it/s]
2023-02-07 20:36:30.752 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:36:30,754][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d153,n5,mc2,s0.25392,t4>', 'datetime': '2023-02-07T20:36:30.754122', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:36:30,754][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:36:30,754][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:36:31,059][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 20:36:31,060][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:36:31,087][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 11155 unique words (85.41% of original 13061, drops 1906)', 'datetime': '2023-02-07T20:36:31.087353', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:36:31,087][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 3637464 word corpus (99.95% of original 3639370, drops 1906)', 'datetime': '2023-02-07T20:36:31.087702', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:36:31,122][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 20:36:31,123][gensim.models.word2vec][INFO] - sample=0.25392 downsamples 0 most-common words
[2023-02-07 20:36:31,123][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3637464 word corpus (100.0%% of prior 3637464)', 'datetime': '2023-02-07T20:36:31.123332', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:36:31,182][gensim.models.word2vec][INFO] - estimated required memory for 11155 words and 153 dimensions: 21875904 bytes
[2023-02-07 20:36:31,182][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:36:31,191][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 11155 vocabulary and 153 features, using sg=1 hs=0 sample=0.2539198523471939 negative=5 window=16 shrink_windows=True', 'datetime': '2023-02-07T20:36:31.191227', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:36:32,195][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 65.09% examples, 2416433 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:36:32,697][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3640721 effective words) took 1.5s, 2422478 effective words/s
[2023-02-07 20:36:33,699][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 69.67% examples, 2596878 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:36:34,096][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3640721 effective words) took 1.4s, 2607158 effective words/s
[2023-02-07 20:36:35,102][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 91.65% examples, 3346488 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:36:35,181][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3640721 effective words) took 1.1s, 3359904 effective words/s
[2023-02-07 20:36:36,182][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 71.05% examples, 2642044 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:36:36,570][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3640721 effective words) took 1.4s, 2622809 effective words/s
[2023-02-07 20:36:37,578][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 74.76% examples, 2747733 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:36:37,926][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3640721 effective words) took 1.4s, 2685750 effective words/s
[2023-02-07 20:36:38,935][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 67.12% examples, 2487629 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:36:39,391][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3640721 effective words) took 1.5s, 2493851 effective words/s
[2023-02-07 20:36:40,394][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 88.42% examples, 3230551 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:36:40,523][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3640721 effective words) took 1.1s, 3220108 effective words/s
[2023-02-07 20:36:41,528][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 61.59% examples, 2274535 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:36:42,074][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3640721 effective words) took 1.5s, 2350206 effective words/s
[2023-02-07 20:36:43,083][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 67.58% examples, 2501163 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:36:43,531][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3640721 effective words) took 1.5s, 2504597 effective words/s
[2023-02-07 20:36:44,541][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 87.96% examples, 3222770 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:36:44,695][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3640721 effective words) took 1.2s, 3153412 effective words/s
[2023-02-07 20:36:45,697][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 68.65% examples, 2551087 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:36:46,129][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3640721 effective words) took 1.4s, 2541415 effective words/s
[2023-02-07 20:36:47,134][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 70.77% examples, 2628382 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:36:47,514][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3640721 effective words) took 1.4s, 2633731 effective words/s
[2023-02-07 20:36:48,518][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 70.28% examples, 2614751 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:36:48,910][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3640721 effective words) took 1.4s, 2614883 effective words/s
[2023-02-07 20:36:49,914][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 90.42% examples, 3309626 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:36:50,015][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3640721 effective words) took 1.1s, 3301698 effective words/s
[2023-02-07 20:36:51,022][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 88.61% examples, 3222510 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:36:51,144][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3640721 effective words) took 1.1s, 3229521 effective words/s
[2023-02-07 20:36:51,145][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54610815 effective words) took 20.0s, 2737106 effective words/s', 'datetime': '2023-02-07T20:36:51.145009', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:36:51.145 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:36:52,674][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203608-tnt54osj/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:36:52.674383', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:36:52,675][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:36:52,708][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203608-tnt54osj/files/../tmp/embedding_model.pt
2023-02-07 20:36:52.708 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:36:53.965 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:36:54.436 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:36:55.713 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2812454705576726, 'test_mae': 1.1001718122172142, 'test_r2': -2.3570492620541272}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.98
wandb: percentage 0.14593
wandb:   test_mae 1.10017
wandb:   test_mse 2.28125
wandb:    test_r2 -2.35705
wandb: 
wandb: üöÄ View run major-sweep-91 at: https://wandb.ai/xiaoqiz/mof2vec/runs/tnt54osj
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_203608-tnt54osj/logs
wandb: Agent Starting Run: 504f8sbo with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 823
wandb: 	model.gensim.alpha: 0.02411753135590634
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.35512607202556334
wandb: 	model.gensim.vector_size: 329
wandb: 	model.gensim.window: 19
wandb: 	model.sklearn.learning_rate: 0.06421151220035343
wandb: 	model.sklearn.max_depth: 17
wandb: 	model.sklearn.min_child_weight: 0.08766064485926615
wandb: 	model.sklearn.n_estimators: 1887
wandb: 	model.sklearn.num_leaves: 351
wandb: 	model.sklearn.reg_alpha: 0.18324732745037645
wandb: 	model.sklearn.reg_lambda: 0.10585882370126098
wandb: 	model.sklearn.subsample: 0.4530796885779507
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203707-504f8sbo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-92
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/504f8sbo
2023-02-07 20:37:16.538 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 20:37:16.538 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 823 for sweep.
2023-02-07 20:37:16.539 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.02411753135590634 for sweep.
2023-02-07 20:37:16.539 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:37:16.539 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 20:37:16.540 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.35512607202556334 for sweep.
2023-02-07 20:37:16.540 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 329 for sweep.
2023-02-07 20:37:16.540 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 19 for sweep.
2023-02-07 20:37:16.540 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.06421151220035343 for sweep.
2023-02-07 20:37:16.541 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 17 for sweep.
2023-02-07 20:37:16.541 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.08766064485926615 for sweep.
2023-02-07 20:37:16.541 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1887 for sweep.
2023-02-07 20:37:16.541 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 351 for sweep.
2023-02-07 20:37:16.542 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.18324732745037645 for sweep.
2023-02-07 20:37:16.542 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.10585882370126098 for sweep.
2023-02-07 20:37:16.542 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.4530796885779507 for sweep.
2023-02-07 20:37:16.542 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:37:16.551 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203707-504f8sbo/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 823, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 329, 'window': 19, 'min_count': 2, 'dm': 0, 'sample': 0.35512607202556334, 'workers': 4, 'alpha': 0.02411753135590634, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1887, 'max_depth': 17, 'num_leaves': 351, 'reg_alpha': 0.18324732745037645, 'reg_lambda': 0.10585882370126098, 'subsample': 0.4530796885779507, 'min_child_weight': 0.08766064485926615, 'n_jobs': 4, 'learning_rate': 0.06421151220035343}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:19, 162.76it/s]  1%|          | 36/3257 [00:00<00:18, 178.34it/s]  2%|‚ñè         | 54/3257 [00:00<00:18, 174.49it/s]  2%|‚ñè         | 73/3257 [00:00<00:17, 178.74it/s]  3%|‚ñé         | 92/3257 [00:00<00:17, 180.48it/s]  3%|‚ñé         | 111/3257 [00:00<00:18, 168.60it/s]  4%|‚ñç         | 130/3257 [00:00<00:17, 174.18it/s]  5%|‚ñç         | 150/3257 [00:00<00:17, 181.81it/s]  5%|‚ñå         | 169/3257 [00:00<00:17, 176.66it/s]  6%|‚ñå         | 187/3257 [00:01<00:17, 177.25it/s]  6%|‚ñã         | 205/3257 [00:01<00:17, 175.55it/s]  7%|‚ñã         | 230/3257 [00:01<00:15, 194.89it/s]  8%|‚ñä         | 250/3257 [00:01<00:15, 192.75it/s]  8%|‚ñä         | 270/3257 [00:01<00:16, 186.46it/s]  9%|‚ñâ         | 295/3257 [00:01<00:14, 202.08it/s] 10%|‚ñâ         | 316/3257 [00:01<00:15, 189.16it/s] 10%|‚ñà         | 336/3257 [00:01<00:15, 191.80it/s] 11%|‚ñà         | 357/3257 [00:01<00:14, 196.50it/s] 12%|‚ñà‚ñè        | 377/3257 [00:02<00:16, 177.40it/s] 12%|‚ñà‚ñè        | 396/3257 [00:02<00:16, 178.62it/s] 13%|‚ñà‚ñé        | 417/3257 [00:02<00:15, 187.18it/s] 13%|‚ñà‚ñé        | 437/3257 [00:02<00:17, 161.21it/s] 14%|‚ñà‚ñç        | 457/3257 [00:02<00:16, 171.04it/s] 15%|‚ñà‚ñç        | 478/3257 [00:02<00:15, 179.07it/s] 15%|‚ñà‚ñå        | 497/3257 [00:02<00:22, 124.25it/s] 16%|‚ñà‚ñå        | 518/3257 [00:03<00:19, 140.73it/s] 17%|‚ñà‚ñã        | 538/3257 [00:03<00:17, 152.60it/s] 17%|‚ñà‚ñã        | 557/3257 [00:03<00:16, 161.53it/s] 18%|‚ñà‚ñä        | 575/3257 [00:03<00:18, 147.16it/s] 18%|‚ñà‚ñä        | 595/3257 [00:03<00:16, 159.48it/s] 19%|‚ñà‚ñâ        | 616/3257 [00:03<00:15, 172.45it/s] 20%|‚ñà‚ñâ        | 636/3257 [00:03<00:14, 177.62it/s] 20%|‚ñà‚ñà        | 655/3257 [00:03<00:16, 162.24it/s] 21%|‚ñà‚ñà        | 673/3257 [00:03<00:15, 164.97it/s] 21%|‚ñà‚ñà        | 691/3257 [00:04<00:15, 162.23it/s] 22%|‚ñà‚ñà‚ñè       | 711/3257 [00:04<00:14, 171.90it/s] 22%|‚ñà‚ñà‚ñè       | 729/3257 [00:04<00:15, 165.29it/s] 23%|‚ñà‚ñà‚ñé       | 746/3257 [00:04<00:15, 162.21it/s] 24%|‚ñà‚ñà‚ñé       | 766/3257 [00:04<00:14, 171.73it/s] 24%|‚ñà‚ñà‚ñç       | 784/3257 [00:04<00:15, 161.91it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:04<00:14, 170.03it/s] 25%|‚ñà‚ñà‚ñå       | 822/3257 [00:04<00:14, 169.90it/s] 26%|‚ñà‚ñà‚ñå       | 840/3257 [00:04<00:14, 165.22it/s] 26%|‚ñà‚ñà‚ñã       | 857/3257 [00:05<00:14, 162.43it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:05<00:14, 163.38it/s] 27%|‚ñà‚ñà‚ñã       | 892/3257 [00:05<00:14, 165.22it/s] 28%|‚ñà‚ñà‚ñä       | 913/3257 [00:05<00:13, 177.56it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:05<00:13, 176.40it/s] 29%|‚ñà‚ñà‚ñâ       | 950/3257 [00:05<00:12, 178.15it/s] 30%|‚ñà‚ñà‚ñâ       | 970/3257 [00:05<00:12, 181.20it/s] 30%|‚ñà‚ñà‚ñà       | 989/3257 [00:05<00:12, 179.89it/s] 31%|‚ñà‚ñà‚ñà       | 1008/3257 [00:05<00:13, 172.75it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:06<00:13, 169.83it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1045/3257 [00:06<00:13, 167.82it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1063/3257 [00:06<00:12, 170.64it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1081/3257 [00:06<00:12, 170.91it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1100/3257 [00:06<00:12, 174.15it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1119/3257 [00:06<00:12, 173.01it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1137/3257 [00:06<00:12, 171.22it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1155/3257 [00:06<00:12, 165.77it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1172/3257 [00:06<00:12, 166.22it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1189/3257 [00:07<00:14, 147.04it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1206/3257 [00:07<00:13, 152.66it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1225/3257 [00:07<00:12, 162.77it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1244/3257 [00:07<00:11, 170.30it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1262/3257 [00:07<00:11, 171.20it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1280/3257 [00:07<00:12, 162.70it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1297/3257 [00:07<00:12, 154.34it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1316/3257 [00:07<00:11, 163.65it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1336/3257 [00:07<00:11, 172.57it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1354/3257 [00:07<00:11, 164.79it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1371/3257 [00:08<00:11, 163.23it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1388/3257 [00:08<00:11, 163.04it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1410/3257 [00:08<00:10, 178.31it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1431/3257 [00:08<00:09, 186.62it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1450/3257 [00:08<00:09, 183.25it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1472/3257 [00:08<00:09, 189.42it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1493/3257 [00:08<00:09, 195.05it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:08<00:08, 196.47it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1533/3257 [00:08<00:09, 174.63it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1551/3257 [00:09<00:10, 169.47it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1569/3257 [00:09<00:09, 169.10it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1587/3257 [00:09<00:10, 166.94it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1605/3257 [00:09<00:09, 168.60it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1622/3257 [00:09<00:09, 166.79it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1639/3257 [00:09<00:09, 164.18it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1656/3257 [00:09<00:09, 162.39it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1673/3257 [00:09<00:10, 153.93it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1690/3257 [00:09<00:10, 154.06it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1708/3257 [00:10<00:09, 158.86it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1727/3257 [00:10<00:09, 166.46it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1744/3257 [00:10<00:10, 149.08it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:10<00:09, 159.03it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1781/3257 [00:10<00:09, 163.56it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1798/3257 [00:10<00:08, 162.63it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1815/3257 [00:10<00:15, 94.24it/s]  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1830/3257 [00:11<00:13, 104.39it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1849/3257 [00:11<00:11, 121.30it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:11<00:10, 137.75it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1886/3257 [00:11<00:09, 143.88it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1903/3257 [00:11<00:09, 149.47it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1920/3257 [00:11<00:08, 151.41it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1943/3257 [00:11<00:07, 172.57it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1964/3257 [00:11<00:07, 182.35it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1983/3257 [00:11<00:07, 167.19it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:12<00:07, 172.04it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2023/3257 [00:12<00:07, 172.87it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2041/3257 [00:12<00:07, 163.25it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2058/3257 [00:12<00:07, 156.20it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2075/3257 [00:12<00:07, 159.61it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2093/3257 [00:12<00:07, 162.81it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2110/3257 [00:12<00:07, 162.35it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2127/3257 [00:12<00:07, 152.98it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2143/3257 [00:12<00:07, 147.41it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2162/3257 [00:13<00:06, 158.67it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:13<00:06, 161.68it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2198/3257 [00:13<00:06, 161.76it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2215/3257 [00:13<00:06, 153.98it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2234/3257 [00:13<00:06, 162.49it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2251/3257 [00:13<00:06, 159.37it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2268/3257 [00:13<00:06, 158.59it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2284/3257 [00:13<00:06, 158.35it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2300/3257 [00:13<00:06, 155.02it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2321/3257 [00:14<00:05, 168.59it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2343/3257 [00:14<00:05, 181.86it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2364/3257 [00:14<00:04, 189.68it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2384/3257 [00:14<00:04, 190.05it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2404/3257 [00:14<00:04, 192.04it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2424/3257 [00:14<00:04, 179.55it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2443/3257 [00:14<00:04, 168.25it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2463/3257 [00:14<00:04, 175.23it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2482/3257 [00:14<00:04, 177.68it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2504/3257 [00:15<00:03, 188.26it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2524/3257 [00:15<00:03, 186.96it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2543/3257 [00:15<00:03, 186.05it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2562/3257 [00:15<00:04, 173.57it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2580/3257 [00:15<00:04, 163.84it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2599/3257 [00:15<00:03, 169.62it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2626/3257 [00:15<00:03, 195.96it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2646/3257 [00:15<00:03, 185.36it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2665/3257 [00:15<00:03, 178.50it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2685/3257 [00:16<00:03, 182.57it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2704/3257 [00:16<00:03, 163.77it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2721/3257 [00:16<00:03, 160.90it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2744/3257 [00:16<00:02, 178.51it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2763/3257 [00:16<00:02, 177.63it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2782/3257 [00:16<00:02, 174.68it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2803/3257 [00:16<00:02, 183.38it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2822/3257 [00:16<00:02, 175.01it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2840/3257 [00:16<00:02, 167.71it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2863/3257 [00:17<00:02, 182.93it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:17<00:01, 189.35it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2906/3257 [00:17<00:01, 175.71it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:17<00:01, 181.40it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2947/3257 [00:17<00:01, 176.12it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2967/3257 [00:17<00:01, 179.70it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2986/3257 [00:17<00:01, 170.60it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3010/3257 [00:17<00:01, 187.69it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3030/3257 [00:17<00:01, 181.61it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3052/3257 [00:18<00:01, 190.51it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3075/3257 [00:18<00:00, 200.07it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3096/3257 [00:18<00:00, 189.29it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3119/3257 [00:18<00:00, 199.48it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3140/3257 [00:18<00:00, 188.11it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3160/3257 [00:18<00:00, 182.80it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3179/3257 [00:18<00:00, 171.35it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3201/3257 [00:18<00:00, 182.13it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3220/3257 [00:19<00:00, 171.77it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3238/3257 [00:19<00:00, 100.06it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 115.98it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 166.91it/s]
2023-02-07 20:37:36.772 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:37:36,773][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d329,n5,mc2,s0.355126,t4>', 'datetime': '2023-02-07T20:37:36.773761', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:37:36,774][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:37:36,774][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:37:37,266][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 20:37:37,266][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:37:37,360][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 36619 unique words (85.76% of original 42701, drops 6082)', 'datetime': '2023-02-07T20:37:37.360718', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:37:37,361][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 5816910 word corpus (99.90% of original 5822992, drops 6082)', 'datetime': '2023-02-07T20:37:37.361095', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:37:37,478][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 20:37:37,479][gensim.models.word2vec][INFO] - sample=0.355126 downsamples 0 most-common words
[2023-02-07 20:37:37,479][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5816910 word corpus (100.0%% of prior 5816910)', 'datetime': '2023-02-07T20:37:37.479521', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:37:37,678][gensim.models.word2vec][INFO] - estimated required memory for 36619 words and 329 dimensions: 119628320 bytes
[2023-02-07 20:37:37,678][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:37:37,728][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 36619 vocabulary and 329 features, using sg=1 hs=0 sample=0.35512607202556334 negative=5 window=19 shrink_windows=True', 'datetime': '2023-02-07T20:37:37.728762', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:37:38,736][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 25.79% examples, 1485865 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:37:39,737][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 59.13% examples, 1744195 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:37:40,737][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 96.07% examples, 1851318 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:37:40,840][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5790320 effective words) took 3.1s, 1862023 effective words/s
[2023-02-07 20:37:41,854][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 37.70% examples, 2205444 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:37:42,854][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 68.99% examples, 2028521 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:37:43,773][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5790320 effective words) took 2.9s, 1975649 effective words/s
[2023-02-07 20:37:44,793][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 31.10% examples, 1773646 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:37:45,800][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 60.24% examples, 1750393 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:37:46,802][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 91.50% examples, 1763760 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:37:47,058][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5790320 effective words) took 3.3s, 1764123 effective words/s
[2023-02-07 20:37:48,063][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 38.81% examples, 2293962 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:37:49,063][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 78.78% examples, 2302192 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:37:49,563][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5790320 effective words) took 2.5s, 2314827 effective words/s
[2023-02-07 20:37:50,581][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 31.10% examples, 1778131 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:37:51,585][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 62.11% examples, 1807023 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:37:52,595][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 93.83% examples, 1799231 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:37:52,768][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5790320 effective words) took 3.2s, 1807904 effective words/s
[2023-02-07 20:37:53,771][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 39.48% examples, 2348975 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:37:54,776][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 80.81% examples, 2351022 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:37:55,231][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5790320 effective words) took 2.5s, 2353384 effective words/s
[2023-02-07 20:37:56,236][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 40.37% examples, 2392245 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:37:57,238][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 82.68% examples, 2405191 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:37:57,635][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5790320 effective words) took 2.4s, 2409781 effective words/s
[2023-02-07 20:37:58,640][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 32.51% examples, 1904879 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:37:59,644][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 65.21% examples, 1913845 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:38:00,654][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 98.93% examples, 1902844 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:38:00,671][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5790320 effective words) took 3.0s, 1910025 effective words/s
[2023-02-07 20:38:01,678][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 33.71% examples, 1968096 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:38:02,680][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 68.22% examples, 2010135 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:38:03,531][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5790320 effective words) took 2.9s, 2026854 effective words/s
[2023-02-07 20:38:04,537][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 42.98% examples, 2553473 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:38:05,539][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 79.49% examples, 2325321 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:38:06,079][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5790320 effective words) took 2.5s, 2275864 effective words/s
[2023-02-07 20:38:07,081][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 36.05% examples, 2112699 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:38:08,086][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 70.43% examples, 2079612 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:38:08,883][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5790320 effective words) took 2.8s, 2065934 effective words/s
[2023-02-07 20:38:09,885][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 41.91% examples, 2493660 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:38:10,890][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 77.46% examples, 2259458 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:38:11,510][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5790320 effective words) took 2.6s, 2205435 effective words/s
[2023-02-07 20:38:12,516][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 33.71% examples, 1966957 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:38:13,521][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 66.04% examples, 1944143 words/s, in_qsize 7, out_qsize 2
[2023-02-07 20:38:14,517][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5790320 effective words) took 3.0s, 1927262 effective words/s
[2023-02-07 20:38:15,524][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 32.12% examples, 1872333 words/s, in_qsize 7, out_qsize 3
[2023-02-07 20:38:16,530][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 65.83% examples, 1938728 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:38:17,533][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 99.11% examples, 1908269 words/s, in_qsize 2, out_qsize 6
[2023-02-07 20:38:17,535][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5790320 effective words) took 3.0s, 1921366 effective words/s
[2023-02-07 20:38:18,538][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 32.98% examples, 1922825 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:38:19,546][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 65.70% examples, 1934210 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:38:20,546][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 99.36% examples, 1912688 words/s, in_qsize 5, out_qsize 0
[2023-02-07 20:38:20,561][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5790320 effective words) took 3.0s, 1914572 effective words/s
[2023-02-07 20:38:20,562][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86854800 effective words) took 42.8s, 2027755 effective words/s', 'datetime': '2023-02-07T20:38:20.562150', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:38:20.563 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:38:26,590][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203707-504f8sbo/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:38:26.590786', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:38:26,591][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203707-504f8sbo/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 20:38:26,632][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203707-504f8sbo/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 20:38:26,666][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:38:26,702][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203707-504f8sbo/files/../tmp/embedding_model.pt
2023-02-07 20:38:26.702 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:38:28.520 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:38:29.123 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:38:31.789 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.338840594176576, 'test_mae': 1.1531840714074648, 'test_r2': -2.2824705062304154}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.94
wandb: percentage 0.14243
wandb:   test_mae 1.15318
wandb:   test_mse 2.33884
wandb:    test_r2 -2.28247
wandb: 
wandb: üöÄ View run floral-sweep-92 at: https://wandb.ai/xiaoqiz/mof2vec/runs/504f8sbo
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_203707-504f8sbo/logs
wandb: Agent Starting Run: fnns7xbf with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 1004
wandb: 	model.gensim.alpha: 0.004162143632361503
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.257483978135439
wandb: 	model.gensim.vector_size: 61
wandb: 	model.gensim.window: 17
wandb: 	model.sklearn.learning_rate: 0.0708938259413883
wandb: 	model.sklearn.max_depth: 17
wandb: 	model.sklearn.min_child_weight: 0.03783853490260245
wandb: 	model.sklearn.n_estimators: 2896
wandb: 	model.sklearn.num_leaves: 322
wandb: 	model.sklearn.reg_alpha: 0.09107663831920482
wandb: 	model.sklearn.reg_lambda: 0.061193402001007625
wandb: 	model.sklearn.subsample: 0.8214944244265749
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203842-fnns7xbf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-93
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/fnns7xbf
2023-02-07 20:38:51.073 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 20:38:51.073 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 1004 for sweep.
2023-02-07 20:38:51.074 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.004162143632361503 for sweep.
2023-02-07 20:38:51.074 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:38:51.074 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 20:38:51.074 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.257483978135439 for sweep.
2023-02-07 20:38:51.075 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 61 for sweep.
2023-02-07 20:38:51.075 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 17 for sweep.
2023-02-07 20:38:51.075 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0708938259413883 for sweep.
2023-02-07 20:38:51.075 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 17 for sweep.
2023-02-07 20:38:51.076 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03783853490260245 for sweep.
2023-02-07 20:38:51.076 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2896 for sweep.
2023-02-07 20:38:51.076 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 322 for sweep.
2023-02-07 20:38:51.076 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.09107663831920482 for sweep.
2023-02-07 20:38:51.076 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.061193402001007625 for sweep.
2023-02-07 20:38:51.077 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8214944244265749 for sweep.
2023-02-07 20:38:51.077 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:38:51.090 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203842-fnns7xbf/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 1004, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 61, 'window': 17, 'min_count': 5, 'dm': 0, 'sample': 0.257483978135439, 'workers': 4, 'alpha': 0.004162143632361503, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2896, 'max_depth': 17, 'num_leaves': 322, 'reg_alpha': 0.09107663831920482, 'reg_lambda': 0.061193402001007625, 'subsample': 0.8214944244265749, 'min_child_weight': 0.03783853490260245, 'n_jobs': 4, 'learning_rate': 0.0708938259413883}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 16/3257 [00:00<00:20, 158.24it/s]  1%|          | 34/3257 [00:00<00:19, 163.71it/s]  2%|‚ñè         | 51/3257 [00:00<00:19, 161.88it/s]  2%|‚ñè         | 68/3257 [00:00<00:19, 162.60it/s]  3%|‚ñé         | 90/3257 [00:00<00:18, 173.25it/s]  3%|‚ñé         | 108/3257 [00:00<00:19, 164.54it/s]  4%|‚ñç         | 125/3257 [00:00<00:18, 165.60it/s]  4%|‚ñç         | 145/3257 [00:00<00:17, 175.81it/s]  5%|‚ñå         | 163/3257 [00:00<00:18, 165.55it/s]  6%|‚ñå         | 180/3257 [00:01<00:18, 165.48it/s]  6%|‚ñå         | 201/3257 [00:01<00:17, 171.01it/s]  7%|‚ñã         | 222/3257 [00:01<00:16, 181.17it/s]  7%|‚ñã         | 243/3257 [00:01<00:16, 187.59it/s]  8%|‚ñä         | 262/3257 [00:01<00:17, 173.31it/s]  9%|‚ñä         | 284/3257 [00:01<00:16, 184.38it/s]  9%|‚ñâ         | 303/3257 [00:01<00:16, 181.92it/s] 10%|‚ñâ         | 324/3257 [00:01<00:15, 186.42it/s] 11%|‚ñà         | 343/3257 [00:01<00:16, 179.06it/s] 11%|‚ñà         | 362/3257 [00:02<00:16, 179.32it/s] 12%|‚ñà‚ñè        | 381/3257 [00:02<00:16, 170.10it/s] 12%|‚ñà‚ñè        | 399/3257 [00:02<00:17, 167.09it/s] 13%|‚ñà‚ñé        | 420/3257 [00:02<00:16, 173.01it/s] 13%|‚ñà‚ñé        | 438/3257 [00:02<00:18, 153.39it/s] 14%|‚ñà‚ñç        | 458/3257 [00:02<00:16, 164.96it/s] 15%|‚ñà‚ñç        | 476/3257 [00:02<00:16, 168.10it/s] 15%|‚ñà‚ñå        | 497/3257 [00:02<00:15, 178.81it/s] 16%|‚ñà‚ñå        | 517/3257 [00:02<00:14, 182.67it/s] 16%|‚ñà‚ñã        | 536/3257 [00:03<00:15, 180.07it/s] 17%|‚ñà‚ñã        | 556/3257 [00:03<00:14, 184.88it/s] 18%|‚ñà‚ñä        | 575/3257 [00:03<00:16, 160.53it/s] 18%|‚ñà‚ñä        | 595/3257 [00:03<00:15, 170.59it/s] 19%|‚ñà‚ñâ        | 616/3257 [00:03<00:14, 180.90it/s] 19%|‚ñà‚ñâ        | 635/3257 [00:03<00:14, 180.79it/s] 20%|‚ñà‚ñà        | 654/3257 [00:03<00:15, 169.52it/s] 21%|‚ñà‚ñà        | 672/3257 [00:03<00:15, 169.14it/s] 21%|‚ñà‚ñà        | 690/3257 [00:04<00:15, 163.63it/s] 22%|‚ñà‚ñà‚ñè       | 709/3257 [00:04<00:14, 170.24it/s] 22%|‚ñà‚ñà‚ñè       | 727/3257 [00:04<00:15, 160.52it/s] 23%|‚ñà‚ñà‚ñé       | 744/3257 [00:04<00:15, 158.21it/s] 23%|‚ñà‚ñà‚ñé       | 764/3257 [00:04<00:14, 168.67it/s] 24%|‚ñà‚ñà‚ñç       | 782/3257 [00:04<00:15, 161.53it/s] 25%|‚ñà‚ñà‚ñç       | 802/3257 [00:04<00:14, 171.21it/s] 25%|‚ñà‚ñà‚ñå       | 820/3257 [00:04<00:14, 165.74it/s] 26%|‚ñà‚ñà‚ñå       | 837/3257 [00:04<00:15, 159.65it/s] 26%|‚ñà‚ñà‚ñå       | 854/3257 [00:05<00:15, 154.60it/s] 27%|‚ñà‚ñà‚ñã       | 873/3257 [00:05<00:14, 160.77it/s] 27%|‚ñà‚ñà‚ñã       | 890/3257 [00:05<00:14, 162.35it/s] 28%|‚ñà‚ñà‚ñä       | 909/3257 [00:05<00:13, 168.46it/s] 28%|‚ñà‚ñà‚ñä       | 928/3257 [00:05<00:13, 172.17it/s] 29%|‚ñà‚ñà‚ñâ       | 946/3257 [00:05<00:14, 163.46it/s] 30%|‚ñà‚ñà‚ñâ       | 964/3257 [00:05<00:13, 167.01it/s] 30%|‚ñà‚ñà‚ñà       | 981/3257 [00:05<00:13, 165.94it/s] 31%|‚ñà‚ñà‚ñà       | 998/3257 [00:05<00:13, 161.48it/s] 31%|‚ñà‚ñà‚ñà       | 1015/3257 [00:05<00:13, 162.17it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1032/3257 [00:06<00:13, 161.41it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1049/3257 [00:06<00:13, 159.05it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:06<00:13, 164.21it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1084/3257 [00:06<00:13, 164.48it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1102/3257 [00:06<00:12, 168.04it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1119/3257 [00:06<00:12, 164.66it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1136/3257 [00:06<00:12, 165.47it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1153/3257 [00:07<00:20, 104.27it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:07<00:17, 119.19it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1186/3257 [00:07<00:16, 123.80it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1201/3257 [00:07<00:16, 124.67it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1218/3257 [00:07<00:15, 135.59it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1241/3257 [00:07<00:12, 156.54it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1258/3257 [00:07<00:12, 154.62it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:07<00:12, 155.97it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1292/3257 [00:07<00:13, 148.78it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1309/3257 [00:08<00:12, 150.24it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:08<00:12, 155.61it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1348/3257 [00:08<00:11, 165.81it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1365/3257 [00:08<00:11, 160.69it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1382/3257 [00:08<00:12, 154.26it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1401/3257 [00:08<00:11, 162.96it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1420/3257 [00:08<00:10, 169.69it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1438/3257 [00:08<00:11, 163.03it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1462/3257 [00:08<00:09, 182.42it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1481/3257 [00:09<00:09, 183.52it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1501/3257 [00:09<00:09, 186.19it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1520/3257 [00:09<00:10, 171.91it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1538/3257 [00:09<00:09, 172.75it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1556/3257 [00:09<00:09, 173.77it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1575/3257 [00:09<00:09, 173.82it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1594/3257 [00:09<00:09, 176.22it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:09<00:08, 185.63it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:09<00:09, 178.39it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1652/3257 [00:09<00:09, 177.75it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1670/3257 [00:10<00:08, 177.90it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1688/3257 [00:10<00:08, 177.38it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1707/3257 [00:10<00:08, 179.59it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1727/3257 [00:10<00:08, 184.64it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1746/3257 [00:10<00:08, 169.86it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1767/3257 [00:10<00:08, 179.32it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1790/3257 [00:10<00:07, 192.78it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1810/3257 [00:10<00:08, 180.58it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1829/3257 [00:10<00:07, 178.54it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1851/3257 [00:11<00:07, 189.20it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1873/3257 [00:11<00:07, 196.80it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1893/3257 [00:11<00:07, 190.46it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:11<00:06, 194.80it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1939/3257 [00:11<00:06, 204.18it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:11<00:05, 216.92it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1987/3257 [00:11<00:06, 207.02it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2008/3257 [00:11<00:06, 206.18it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2033/3257 [00:11<00:05, 212.08it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2055/3257 [00:12<00:06, 193.13it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2075/3257 [00:12<00:06, 191.57it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2096/3257 [00:12<00:06, 188.85it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2119/3257 [00:12<00:05, 197.47it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2139/3257 [00:12<00:06, 185.02it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2158/3257 [00:12<00:05, 185.27it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:12<00:05, 185.14it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2199/3257 [00:12<00:05, 191.97it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2219/3257 [00:12<00:05, 189.72it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2239/3257 [00:13<00:05, 185.35it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2260/3257 [00:13<00:05, 189.18it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2279/3257 [00:13<00:08, 111.09it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2300/3257 [00:13<00:07, 129.52it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2326/3257 [00:13<00:05, 156.47it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2353/3257 [00:13<00:04, 182.08it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2375/3257 [00:13<00:04, 187.93it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2400/3257 [00:14<00:04, 201.87it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2422/3257 [00:14<00:04, 195.80it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2443/3257 [00:14<00:04, 187.80it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2467/3257 [00:14<00:03, 200.90it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2489/3257 [00:14<00:03, 203.68it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2514/3257 [00:14<00:03, 215.51it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2539/3257 [00:14<00:03, 223.68it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2562/3257 [00:14<00:03, 205.49it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2584/3257 [00:14<00:03, 195.26it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2606/3257 [00:15<00:03, 200.87it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:15<00:02, 212.06it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2653/3257 [00:15<00:03, 198.66it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2674/3257 [00:15<00:02, 195.81it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2694/3257 [00:15<00:02, 188.43it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2714/3257 [00:15<00:03, 172.77it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2735/3257 [00:15<00:02, 181.58it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:15<00:02, 186.64it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2775/3257 [00:15<00:02, 180.03it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2799/3257 [00:16<00:02, 195.68it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2819/3257 [00:16<00:02, 188.75it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2839/3257 [00:16<00:02, 181.27it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2864/3257 [00:16<00:01, 198.94it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:16<00:01, 203.57it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2907/3257 [00:16<00:01, 187.91it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2929/3257 [00:16<00:01, 192.34it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2949/3257 [00:16<00:01, 183.50it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2969/3257 [00:16<00:01, 185.04it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2988/3257 [00:17<00:01, 176.45it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:17<00:01, 190.52it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3034/3257 [00:17<00:01, 197.18it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3057/3257 [00:17<00:00, 205.60it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3080/3257 [00:17<00:00, 212.54it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3102/3257 [00:17<00:00, 213.04it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3125/3257 [00:17<00:00, 217.54it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3147/3257 [00:17<00:00, 198.84it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3168/3257 [00:17<00:00, 199.49it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3189/3257 [00:18<00:00, 191.18it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3209/3257 [00:18<00:00, 188.00it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3228/3257 [00:18<00:00, 182.69it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:18<00:00, 190.05it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 176.67it/s]
2023-02-07 20:39:10.244 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:39:10,245][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d61,n5,mc5,s0.257484,t4>', 'datetime': '2023-02-07T20:39:10.245412', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:39:10,245][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:39:10,245][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:39:10,766][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 20:39:10,766][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:39:10,822][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 21312 unique words (49.91% of original 42701, drops 21389)', 'datetime': '2023-02-07T20:39:10.822592', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:39:10,822][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5769229 word corpus (99.08% of original 5822992, drops 53763)', 'datetime': '2023-02-07T20:39:10.822975', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:39:10,891][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 20:39:10,892][gensim.models.word2vec][INFO] - sample=0.257484 downsamples 0 most-common words
[2023-02-07 20:39:10,893][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5769229 word corpus (100.0%% of prior 5769229)', 'datetime': '2023-02-07T20:39:10.893000', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:39:11,009][gensim.models.word2vec][INFO] - estimated required memory for 21312 words and 61 dimensions: 22502364 bytes
[2023-02-07 20:39:11,009][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:39:11,015][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 21312 vocabulary and 61 features, using sg=1 hs=0 sample=0.257483978135439 negative=5 window=17 shrink_windows=True', 'datetime': '2023-02-07T20:39:11.015135', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:39:12,020][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 40.90% examples, 2404367 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:13,041][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 82.38% examples, 2354889 words/s, in_qsize 0, out_qsize 5
[2023-02-07 20:39:13,492][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5743174 effective words) took 2.5s, 2320444 effective words/s
[2023-02-07 20:39:14,524][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 39.30% examples, 2256863 words/s, in_qsize 6, out_qsize 11
[2023-02-07 20:39:15,542][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 82.81% examples, 2343178 words/s, in_qsize 8, out_qsize 4
[2023-02-07 20:39:15,899][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5743174 effective words) took 2.4s, 2388636 effective words/s
[2023-02-07 20:39:16,901][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 51.49% examples, 3020739 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:17,905][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 92.48% examples, 2668103 words/s, in_qsize 2, out_qsize 1
[2023-02-07 20:39:18,064][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5743174 effective words) took 2.2s, 2654304 effective words/s
[2023-02-07 20:39:19,066][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 52.29% examples, 3068001 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:19,932][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5743174 effective words) took 1.9s, 3077275 effective words/s
[2023-02-07 20:39:20,935][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 42.43% examples, 2508857 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:21,936][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 89.01% examples, 2570492 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:22,163][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5743174 effective words) took 2.2s, 2577415 effective words/s
[2023-02-07 20:39:23,174][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 41.30% examples, 2415893 words/s, in_qsize 3, out_qsize 5
[2023-02-07 20:39:24,177][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 84.25% examples, 2424949 words/s, in_qsize 4, out_qsize 3
[2023-02-07 20:39:24,510][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5743174 effective words) took 2.3s, 2447672 effective words/s
[2023-02-07 20:39:25,529][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 40.13% examples, 2329444 words/s, in_qsize 0, out_qsize 4
[2023-02-07 20:39:26,535][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 85.20% examples, 2441081 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:26,843][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5743174 effective words) took 2.3s, 2463929 effective words/s
[2023-02-07 20:39:27,850][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 44.03% examples, 2583398 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:28,850][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 89.71% examples, 2584658 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:29,060][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5743174 effective words) took 2.2s, 2594981 effective words/s
[2023-02-07 20:39:30,063][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 54.77% examples, 3209574 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:30,850][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5743174 effective words) took 1.8s, 3210774 effective words/s
[2023-02-07 20:39:31,853][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 46.73% examples, 2726503 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:32,859][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 93.83% examples, 2694023 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:32,979][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5743174 effective words) took 2.1s, 2701147 effective words/s
[2023-02-07 20:39:33,986][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 45.90% examples, 2668329 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:34,995][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 92.94% examples, 2665520 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:35,129][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5743174 effective words) took 2.1s, 2672549 effective words/s
[2023-02-07 20:39:36,135][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 43.69% examples, 2564660 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:37,138][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 89.65% examples, 2582171 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:37,353][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5743174 effective words) took 2.2s, 2584965 effective words/s
[2023-02-07 20:39:38,356][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 51.49% examples, 3017932 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:39,194][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5743174 effective words) took 1.8s, 3121827 effective words/s
[2023-02-07 20:39:40,198][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 46.24% examples, 2699494 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:41,199][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 92.91% examples, 2684188 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:39:41,334][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5743174 effective words) took 2.1s, 2688649 effective words/s
[2023-02-07 20:39:42,341][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 55.27% examples, 3237495 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:39:43,114][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5743174 effective words) took 1.8s, 3233060 effective words/s
[2023-02-07 20:39:43,114][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86147610 effective words) took 32.1s, 2683819 effective words/s', 'datetime': '2023-02-07T20:39:43.114806', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:39:43.115 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:39:45,591][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203842-fnns7xbf/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:39:45.591605', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:39:45,592][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:39:45,632][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_203842-fnns7xbf/files/../tmp/embedding_model.pt
2023-02-07 20:39:45.632 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:39:46.676 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:39:47.048 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:39:47.548 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.30610094674376, 'test_mae': 1.1040005122091439, 'test_r2': -1.7884336406422183}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.78
wandb: percentage 0.5009
wandb:   test_mae 1.104
wandb:   test_mse 2.3061
wandb:    test_r2 -1.78843
wandb: 
wandb: üöÄ View run magic-sweep-93 at: https://wandb.ai/xiaoqiz/mof2vec/runs/fnns7xbf
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_203842-fnns7xbf/logs
wandb: Agent Starting Run: 24pun3p4 with config:
wandb: 	data.data.wl_step: 5
wandb: 	data.nn.batch_size: 1001
wandb: 	model.gensim.alpha: 0.009403746685402597
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.5748963116021106
wandb: 	model.gensim.vector_size: 286
wandb: 	model.gensim.window: 18
wandb: 	model.sklearn.learning_rate: 0.0018156299561936265
wandb: 	model.sklearn.max_depth: 49
wandb: 	model.sklearn.min_child_weight: 0.01773253227976109
wandb: 	model.sklearn.n_estimators: 3205
wandb: 	model.sklearn.num_leaves: 469
wandb: 	model.sklearn.reg_alpha: 0.0674218426615827
wandb: 	model.sklearn.reg_lambda: 0.005085326775375813
wandb: 	model.sklearn.subsample: 0.26556075022272463
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204001-24pun3p4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-94
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/24pun3p4
2023-02-07 20:40:08.739 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 5 for sweep.
2023-02-07 20:40:08.740 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 1001 for sweep.
2023-02-07 20:40:08.740 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.009403746685402597 for sweep.
2023-02-07 20:40:08.740 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:40:08.740 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 20:40:08.741 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5748963116021106 for sweep.
2023-02-07 20:40:08.741 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 286 for sweep.
2023-02-07 20:40:08.741 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 18 for sweep.
2023-02-07 20:40:08.741 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0018156299561936265 for sweep.
2023-02-07 20:40:08.741 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 49 for sweep.
2023-02-07 20:40:08.742 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.01773253227976109 for sweep.
2023-02-07 20:40:08.742 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3205 for sweep.
2023-02-07 20:40:08.742 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 469 for sweep.
2023-02-07 20:40:08.742 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.0674218426615827 for sweep.
2023-02-07 20:40:08.743 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.005085326775375813 for sweep.
2023-02-07 20:40:08.743 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.26556075022272463 for sweep.
2023-02-07 20:40:08.743 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:40:08.749 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 5}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204001-24pun3p4/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 1001, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 286, 'window': 18, 'min_count': 3, 'dm': 0, 'sample': 0.5748963116021106, 'workers': 4, 'alpha': 0.009403746685402597, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3205, 'max_depth': 49, 'num_leaves': 469, 'reg_alpha': 0.0674218426615827, 'reg_lambda': 0.005085326775375813, 'subsample': 0.26556075022272463, 'min_child_weight': 0.01773253227976109, 'n_jobs': 4, 'learning_rate': 0.0018156299561936265}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 21/3257 [00:00<00:15, 206.55it/s]  1%|‚ñè         | 44/3257 [00:00<00:14, 218.88it/s]  2%|‚ñè         | 67/3257 [00:00<00:14, 216.75it/s]  3%|‚ñé         | 91/3257 [00:00<00:14, 222.01it/s]  4%|‚ñé         | 114/3257 [00:00<00:14, 214.30it/s]  4%|‚ñç         | 139/3257 [00:00<00:13, 223.22it/s]  5%|‚ñç         | 162/3257 [00:00<00:13, 225.34it/s]  6%|‚ñå         | 186/3257 [00:00<00:13, 227.61it/s]  7%|‚ñã         | 214/3257 [00:00<00:12, 240.89it/s]  7%|‚ñã         | 242/3257 [00:01<00:12, 250.52it/s]  8%|‚ñä         | 268/3257 [00:01<00:12, 238.15it/s]  9%|‚ñâ         | 298/3257 [00:01<00:11, 254.05it/s] 10%|‚ñâ         | 324/3257 [00:01<00:11, 252.23it/s] 11%|‚ñà         | 350/3257 [00:01<00:12, 241.77it/s] 12%|‚ñà‚ñè        | 375/3257 [00:01<00:11, 242.33it/s] 12%|‚ñà‚ñè        | 400/3257 [00:01<00:12, 234.86it/s] 13%|‚ñà‚ñé        | 424/3257 [00:01<00:12, 232.83it/s] 14%|‚ñà‚ñç        | 448/3257 [00:01<00:13, 205.27it/s] 15%|‚ñà‚ñç        | 474/3257 [00:02<00:12, 216.16it/s] 15%|‚ñà‚ñå        | 500/3257 [00:02<00:12, 226.06it/s] 16%|‚ñà‚ñå        | 524/3257 [00:02<00:12, 222.10it/s] 17%|‚ñà‚ñã        | 547/3257 [00:02<00:17, 153.63it/s] 17%|‚ñà‚ñã        | 568/3257 [00:02<00:16, 165.32it/s] 18%|‚ñà‚ñä        | 589/3257 [00:02<00:15, 174.59it/s] 19%|‚ñà‚ñâ        | 617/3257 [00:02<00:13, 199.92it/s] 20%|‚ñà‚ñâ        | 639/3257 [00:02<00:12, 203.94it/s] 20%|‚ñà‚ñà        | 661/3257 [00:03<00:13, 196.47it/s] 21%|‚ñà‚ñà        | 684/3257 [00:03<00:12, 204.71it/s] 22%|‚ñà‚ñà‚ñè       | 709/3257 [00:03<00:11, 216.10it/s] 22%|‚ñà‚ñà‚ñè       | 732/3257 [00:03<00:12, 209.84it/s] 23%|‚ñà‚ñà‚ñé       | 754/3257 [00:03<00:11, 210.51it/s] 24%|‚ñà‚ñà‚ñç       | 777/3257 [00:03<00:11, 214.93it/s] 25%|‚ñà‚ñà‚ñç       | 803/3257 [00:03<00:10, 226.47it/s] 25%|‚ñà‚ñà‚ñå       | 826/3257 [00:03<00:11, 211.83it/s] 26%|‚ñà‚ñà‚ñå       | 848/3257 [00:03<00:11, 208.10it/s] 27%|‚ñà‚ñà‚ñã       | 870/3257 [00:04<00:11, 210.88it/s] 27%|‚ñà‚ñà‚ñã       | 892/3257 [00:04<00:11, 211.81it/s] 28%|‚ñà‚ñà‚ñä       | 916/3257 [00:04<00:10, 218.62it/s] 29%|‚ñà‚ñà‚ñâ       | 941/3257 [00:04<00:10, 225.72it/s] 30%|‚ñà‚ñà‚ñâ       | 966/3257 [00:04<00:09, 231.35it/s] 30%|‚ñà‚ñà‚ñà       | 990/3257 [00:04<00:10, 219.97it/s] 31%|‚ñà‚ñà‚ñà       | 1013/3257 [00:04<00:10, 218.49it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1035/3257 [00:04<00:10, 212.71it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1057/3257 [00:04<00:10, 205.43it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:05<00:10, 211.69it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1103/3257 [00:05<00:09, 216.12it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1125/3257 [00:05<00:09, 217.17it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:05<00:09, 212.36it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1172/3257 [00:05<00:09, 222.72it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1195/3257 [00:05<00:09, 207.88it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1217/3257 [00:05<00:10, 202.90it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1243/3257 [00:05<00:09, 218.43it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1268/3257 [00:05<00:08, 226.31it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1291/3257 [00:05<00:09, 207.90it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1313/3257 [00:06<00:09, 210.86it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:06<00:08, 221.73it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1362/3257 [00:06<00:08, 221.11it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1385/3257 [00:06<00:08, 211.44it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1412/3257 [00:06<00:08, 226.41it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1436/3257 [00:06<00:07, 229.17it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1464/3257 [00:06<00:07, 242.83it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1489/3257 [00:06<00:07, 244.81it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1516/3257 [00:06<00:06, 249.87it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1542/3257 [00:07<00:07, 231.43it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1566/3257 [00:07<00:07, 225.04it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1589/3257 [00:07<00:07, 225.05it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1612/3257 [00:07<00:11, 148.92it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:07<00:10, 161.88it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1657/3257 [00:07<00:09, 173.21it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1678/3257 [00:07<00:08, 181.46it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1703/3257 [00:07<00:07, 197.12it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:08<00:07, 200.89it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1750/3257 [00:08<00:07, 205.86it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1775/3257 [00:08<00:06, 216.89it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1800/3257 [00:08<00:06, 224.45it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1823/3257 [00:08<00:06, 225.15it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1846/3257 [00:08<00:06, 222.21it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1874/3257 [00:08<00:05, 237.83it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1898/3257 [00:08<00:05, 232.42it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1922/3257 [00:08<00:05, 232.89it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1953/3257 [00:09<00:05, 255.01it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1979/3257 [00:09<00:05, 246.16it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:09<00:05, 244.95it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2032/3257 [00:09<00:04, 252.28it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2058/3257 [00:09<00:05, 232.47it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2082/3257 [00:09<00:05, 231.17it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2106/3257 [00:09<00:05, 228.83it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2130/3257 [00:09<00:05, 215.73it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2152/3257 [00:09<00:05, 216.22it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:10<00:04, 224.52it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2203/3257 [00:10<00:04, 231.18it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2227/3257 [00:10<00:04, 223.58it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2250/3257 [00:10<00:04, 219.24it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2273/3257 [00:10<00:04, 208.74it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:10<00:04, 218.05it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2326/3257 [00:10<00:04, 231.56it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2353/3257 [00:10<00:03, 241.56it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2378/3257 [00:10<00:03, 237.49it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:11<00:03, 237.11it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2429/3257 [00:11<00:03, 235.07it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2453/3257 [00:11<00:03, 220.41it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2481/3257 [00:11<00:03, 232.62it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2508/3257 [00:11<00:03, 241.34it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2535/3257 [00:11<00:02, 246.37it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2560/3257 [00:11<00:02, 233.77it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2584/3257 [00:11<00:03, 221.01it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2610/3257 [00:11<00:02, 229.10it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2637/3257 [00:12<00:02, 238.25it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2662/3257 [00:12<00:02, 221.89it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2688/3257 [00:12<00:02, 231.40it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2712/3257 [00:12<00:02, 209.76it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2738/3257 [00:12<00:02, 220.00it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:12<00:02, 216.07it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2783/3257 [00:12<00:02, 216.72it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2807/3257 [00:12<00:02, 223.18it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2830/3257 [00:12<00:02, 211.73it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2855/3257 [00:13<00:01, 221.02it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:13<00:01, 237.90it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2910/3257 [00:13<00:01, 226.42it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2933/3257 [00:13<00:01, 224.71it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2956/3257 [00:13<00:01, 211.25it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2978/3257 [00:13<00:01, 211.30it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3004/3257 [00:13<00:01, 222.50it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3028/3257 [00:13<00:01, 225.15it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3055/3257 [00:14<00:01, 137.09it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3081/3257 [00:14<00:01, 159.31it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3106/3257 [00:14<00:00, 178.04it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3130/3257 [00:14<00:00, 191.90it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3153/3257 [00:14<00:00, 194.70it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3176/3257 [00:14<00:00, 202.17it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3199/3257 [00:14<00:00, 207.98it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3221/3257 [00:14<00:00, 205.25it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3247/3257 [00:15<00:00, 219.90it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:15<00:00, 216.39it/s]
2023-02-07 20:40:24.348 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:40:24,350][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d286,n5,mc3,s0.574896,t4>', 'datetime': '2023-02-07T20:40:24.350021', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:40:24,350][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:40:24,350][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:40:24,715][gensim.models.doc2vec][INFO] - collected 21699 word types and 3257 unique tags from a corpus of 3257 examples and 4367244 words
[2023-02-07 20:40:24,715][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:40:24,755][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 15507 unique words (71.46% of original 21699, drops 6192)', 'datetime': '2023-02-07T20:40:24.755333', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:40:24,756][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 4358064 word corpus (99.79% of original 4367244, drops 9180)', 'datetime': '2023-02-07T20:40:24.756424', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:40:24,806][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 21699 items
[2023-02-07 20:40:24,807][gensim.models.word2vec][INFO] - sample=0.574896 downsamples 0 most-common words
[2023-02-07 20:40:24,807][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4358064 word corpus (100.0%% of prior 4358064)', 'datetime': '2023-02-07T20:40:24.807787', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:40:24,893][gensim.models.word2vec][INFO] - estimated required memory for 15507 words and 286 dimensions: 47610924 bytes
[2023-02-07 20:40:24,893][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:40:24,916][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 15507 vocabulary and 286 features, using sg=1 hs=0 sample=0.5748963116021106 negative=5 window=18 shrink_windows=True', 'datetime': '2023-02-07T20:40:24.916664', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:40:25,921][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 35.77% examples, 1577795 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:26,930][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 74.58% examples, 1640074 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:27,522][gensim.models.word2vec][INFO] - EPOCH 0: training on 4367244 raw words (4359560 effective words) took 2.6s, 1674399 effective words/s
[2023-02-07 20:40:28,530][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 56.34% examples, 2505310 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:29,479][gensim.models.word2vec][INFO] - EPOCH 1: training on 4367244 raw words (4359560 effective words) took 2.0s, 2229086 effective words/s
[2023-02-07 20:40:30,489][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 42.22% examples, 1876744 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:31,491][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 87.69% examples, 1919219 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:31,750][gensim.models.word2vec][INFO] - EPOCH 2: training on 4367244 raw words (4359560 effective words) took 2.3s, 1920860 effective words/s
[2023-02-07 20:40:32,756][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 42.22% examples, 1888234 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:40:33,758][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 85.45% examples, 1877361 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:40:34,077][gensim.models.word2vec][INFO] - EPOCH 3: training on 4367244 raw words (4359560 effective words) took 2.3s, 1877165 effective words/s
[2023-02-07 20:40:35,084][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 43.14% examples, 1930061 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:36,093][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 87.38% examples, 1909607 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:36,351][gensim.models.word2vec][INFO] - EPOCH 4: training on 4367244 raw words (4359560 effective words) took 2.3s, 1919223 effective words/s
[2023-02-07 20:40:37,356][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 42.49% examples, 1907970 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:38,364][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 88.42% examples, 1926447 words/s, in_qsize 6, out_qsize 1
[2023-02-07 20:40:38,606][gensim.models.word2vec][INFO] - EPOCH 5: training on 4367244 raw words (4359560 effective words) took 2.3s, 1935288 effective words/s
[2023-02-07 20:40:39,614][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 44.24% examples, 1968586 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:40,619][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 91.25% examples, 1992584 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:40,797][gensim.models.word2vec][INFO] - EPOCH 6: training on 4367244 raw words (4359560 effective words) took 2.2s, 1992285 effective words/s
[2023-02-07 20:40:41,802][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 45.59% examples, 2019755 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:42,802][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 91.37% examples, 2004223 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:42,975][gensim.models.word2vec][INFO] - EPOCH 7: training on 4367244 raw words (4359560 effective words) took 2.2s, 2003446 effective words/s
[2023-02-07 20:40:43,983][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 57.32% examples, 2545233 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:44,672][gensim.models.word2vec][INFO] - EPOCH 8: training on 4367244 raw words (4359560 effective words) took 1.7s, 2573993 effective words/s
[2023-02-07 20:40:45,680][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 45.32% examples, 2007220 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:46,682][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 91.93% examples, 2009739 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:46,833][gensim.models.word2vec][INFO] - EPOCH 9: training on 4367244 raw words (4359560 effective words) took 2.2s, 2020262 effective words/s
[2023-02-07 20:40:47,835][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 57.51% examples, 2564204 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:48,667][gensim.models.word2vec][INFO] - EPOCH 10: training on 4367244 raw words (4359560 effective words) took 1.8s, 2380347 effective words/s
[2023-02-07 20:40:49,672][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 47.68% examples, 2122723 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:50,559][gensim.models.word2vec][INFO] - EPOCH 11: training on 4367244 raw words (4359560 effective words) took 1.9s, 2306119 effective words/s
[2023-02-07 20:40:51,573][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 45.59% examples, 2006392 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:52,580][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 91.07% examples, 1980823 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:52,764][gensim.models.word2vec][INFO] - EPOCH 12: training on 4367244 raw words (4359560 effective words) took 2.2s, 1980728 effective words/s
[2023-02-07 20:40:53,769][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 43.02% examples, 1927914 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:54,770][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 88.79% examples, 1942328 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:40:55,009][gensim.models.word2vec][INFO] - EPOCH 13: training on 4367244 raw words (4359560 effective words) took 2.2s, 1944096 effective words/s
[2023-02-07 20:40:56,018][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 55.54% examples, 2464674 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:40:56,759][gensim.models.word2vec][INFO] - EPOCH 14: training on 4367244 raw words (4359560 effective words) took 1.7s, 2492059 effective words/s
[2023-02-07 20:40:56,760][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 65508660 raw words (65393400 effective words) took 31.8s, 2053582 effective words/s', 'datetime': '2023-02-07T20:40:56.760684', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:40:56.761 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:40:59,364][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204001-24pun3p4/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:40:59.363970', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:40:59,365][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:40:59,434][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204001-24pun3p4/files/../tmp/embedding_model.pt
2023-02-07 20:40:59.434 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:41:01.064 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:41:01.681 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:41:03.650 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0654201995275385, 'test_mae': 1.0363420782556494, 'test_r2': -1.5233931280027315}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.8
wandb: percentage 0.28536
wandb:   test_mae 1.03634
wandb:   test_mse 2.06542
wandb:    test_r2 -1.52339
wandb: 
wandb: üöÄ View run ruby-sweep-94 at: https://wandb.ai/xiaoqiz/mof2vec/runs/24pun3p4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_204001-24pun3p4/logs
wandb: Agent Starting Run: 5ppwyjdn with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 875
wandb: 	model.gensim.alpha: 0.020156688489127987
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.24305506907797236
wandb: 	model.gensim.vector_size: 357
wandb: 	model.gensim.window: 7
wandb: 	model.sklearn.learning_rate: 0.038593282844209424
wandb: 	model.sklearn.max_depth: 31
wandb: 	model.sklearn.min_child_weight: 0.055656704417814194
wandb: 	model.sklearn.n_estimators: 2918
wandb: 	model.sklearn.num_leaves: 389
wandb: 	model.sklearn.reg_alpha: 0.3608865671233621
wandb: 	model.sklearn.reg_lambda: 0.7654774956084204
wandb: 	model.sklearn.subsample: 0.9099414950543484
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204115-5ppwyjdn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-95
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/5ppwyjdn
2023-02-07 20:41:23.320 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 20:41:23.321 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 875 for sweep.
2023-02-07 20:41:23.321 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.020156688489127987 for sweep.
2023-02-07 20:41:23.321 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:41:23.321 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 20:41:23.322 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.24305506907797236 for sweep.
2023-02-07 20:41:23.322 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 357 for sweep.
2023-02-07 20:41:23.323 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 7 for sweep.
2023-02-07 20:41:23.323 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.038593282844209424 for sweep.
2023-02-07 20:41:23.323 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 31 for sweep.
2023-02-07 20:41:23.323 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.055656704417814194 for sweep.
2023-02-07 20:41:23.323 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2918 for sweep.
2023-02-07 20:41:23.324 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 389 for sweep.
2023-02-07 20:41:23.324 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.3608865671233621 for sweep.
2023-02-07 20:41:23.324 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.7654774956084204 for sweep.
2023-02-07 20:41:23.324 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.9099414950543484 for sweep.
2023-02-07 20:41:23.324 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:41:23.334 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204115-5ppwyjdn/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 875, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 357, 'window': 7, 'min_count': 3, 'dm': 0, 'sample': 0.24305506907797236, 'workers': 4, 'alpha': 0.020156688489127987, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2918, 'max_depth': 31, 'num_leaves': 389, 'reg_alpha': 0.3608865671233621, 'reg_lambda': 0.7654774956084204, 'subsample': 0.9099414950543484, 'min_child_weight': 0.055656704417814194, 'n_jobs': 4, 'learning_rate': 0.038593282844209424}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 12/3257 [00:00<00:27, 116.97it/s]  1%|          | 31/3257 [00:00<00:20, 158.14it/s]  2%|‚ñè         | 50/3257 [00:00<00:18, 171.65it/s]  2%|‚ñè         | 68/3257 [00:00<00:18, 171.57it/s]  3%|‚ñé         | 90/3257 [00:00<00:16, 188.51it/s]  3%|‚ñé         | 109/3257 [00:00<00:17, 179.01it/s]  4%|‚ñç         | 129/3257 [00:00<00:16, 184.94it/s]  5%|‚ñç         | 151/3257 [00:00<00:15, 195.06it/s]  5%|‚ñå         | 171/3257 [00:00<00:16, 191.30it/s]  6%|‚ñå         | 192/3257 [00:01<00:15, 193.79it/s]  7%|‚ñã         | 214/3257 [00:01<00:15, 199.87it/s]  7%|‚ñã         | 237/3257 [00:01<00:14, 203.05it/s]  8%|‚ñä         | 258/3257 [00:01<00:14, 203.51it/s]  9%|‚ñä         | 282/3257 [00:01<00:14, 211.55it/s]  9%|‚ñâ         | 304/3257 [00:01<00:14, 203.31it/s] 10%|‚ñà         | 328/3257 [00:01<00:14, 208.77it/s] 11%|‚ñà         | 349/3257 [00:01<00:14, 201.50it/s] 11%|‚ñà‚ñè        | 371/3257 [00:01<00:13, 206.20it/s] 12%|‚ñà‚ñè        | 392/3257 [00:02<00:14, 191.82it/s] 13%|‚ñà‚ñé        | 414/3257 [00:02<00:14, 198.64it/s] 13%|‚ñà‚ñé        | 435/3257 [00:02<00:16, 171.30it/s] 14%|‚ñà‚ñç        | 458/3257 [00:02<00:15, 184.29it/s] 15%|‚ñà‚ñç        | 479/3257 [00:02<00:14, 187.61it/s] 15%|‚ñà‚ñå        | 502/3257 [00:02<00:13, 199.01it/s] 16%|‚ñà‚ñå        | 523/3257 [00:02<00:13, 200.03it/s] 17%|‚ñà‚ñã        | 545/3257 [00:02<00:13, 204.25it/s] 17%|‚ñà‚ñã        | 566/3257 [00:02<00:14, 189.68it/s] 18%|‚ñà‚ñä        | 586/3257 [00:03<00:14, 185.55it/s] 19%|‚ñà‚ñä        | 610/3257 [00:03<00:13, 198.97it/s] 19%|‚ñà‚ñâ        | 631/3257 [00:03<00:13, 201.52it/s] 20%|‚ñà‚ñà        | 652/3257 [00:03<00:13, 194.64it/s] 21%|‚ñà‚ñà        | 672/3257 [00:03<00:13, 191.65it/s] 21%|‚ñà‚ñà        | 692/3257 [00:03<00:13, 185.25it/s] 22%|‚ñà‚ñà‚ñè       | 714/3257 [00:03<00:13, 194.69it/s] 23%|‚ñà‚ñà‚ñé       | 734/3257 [00:03<00:13, 186.65it/s] 23%|‚ñà‚ñà‚ñé       | 753/3257 [00:03<00:13, 184.43it/s] 24%|‚ñà‚ñà‚ñç       | 774/3257 [00:04<00:13, 189.99it/s] 24%|‚ñà‚ñà‚ñç       | 794/3257 [00:04<00:13, 189.02it/s] 25%|‚ñà‚ñà‚ñå       | 815/3257 [00:04<00:12, 192.69it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:04<00:12, 187.74it/s] 26%|‚ñà‚ñà‚ñå       | 854/3257 [00:04<00:13, 178.53it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:04<00:13, 183.17it/s] 27%|‚ñà‚ñà‚ñã       | 893/3257 [00:04<00:12, 183.76it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:04<00:12, 187.58it/s] 29%|‚ñà‚ñà‚ñä       | 936/3257 [00:04<00:11, 195.71it/s] 29%|‚ñà‚ñà‚ñâ       | 957/3257 [00:04<00:11, 197.66it/s] 30%|‚ñà‚ñà‚ñâ       | 977/3257 [00:05<00:17, 133.28it/s] 31%|‚ñà‚ñà‚ñà       | 994/3257 [00:05<00:16, 138.04it/s] 31%|‚ñà‚ñà‚ñà       | 1011/3257 [00:05<00:15, 144.76it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1028/3257 [00:05<00:14, 150.59it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:05<00:13, 158.19it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1068/3257 [00:05<00:12, 171.61it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1088/3257 [00:05<00:12, 176.14it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1108/3257 [00:05<00:11, 182.19it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1127/3257 [00:06<00:12, 175.72it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1145/3257 [00:06<00:12, 173.30it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1165/3257 [00:06<00:11, 180.10it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1184/3257 [00:06<00:12, 168.58it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1202/3257 [00:06<00:12, 164.40it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1222/3257 [00:06<00:11, 172.88it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1244/3257 [00:06<00:10, 185.60it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1264/3257 [00:06<00:10, 189.17it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1284/3257 [00:07<00:11, 173.22it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1303/3257 [00:07<00:11, 176.33it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1322/3257 [00:07<00:10, 179.67it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1343/3257 [00:07<00:10, 187.49it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1362/3257 [00:07<00:10, 184.67it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1381/3257 [00:07<00:10, 175.60it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1401/3257 [00:07<00:10, 181.22it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:07<00:09, 185.62it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1442/3257 [00:07<00:09, 186.65it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1466/3257 [00:07<00:08, 199.60it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1487/3257 [00:08<00:08, 201.21it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:08<00:08, 210.83it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1533/3257 [00:08<00:08, 193.21it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1553/3257 [00:08<00:09, 186.81it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1574/3257 [00:08<00:08, 193.06it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1596/3257 [00:08<00:08, 198.43it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:08<00:07, 206.52it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1640/3257 [00:08<00:07, 203.30it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1661/3257 [00:08<00:08, 192.23it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1681/3257 [00:09<00:08, 193.01it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1703/3257 [00:09<00:07, 197.33it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1724/3257 [00:09<00:07, 200.69it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1745/3257 [00:09<00:08, 186.42it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1768/3257 [00:09<00:07, 197.34it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1792/3257 [00:09<00:07, 207.88it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1814/3257 [00:09<00:07, 197.06it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1835/3257 [00:09<00:07, 199.36it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1858/3257 [00:09<00:06, 202.52it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1882/3257 [00:10<00:06, 212.83it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1904/3257 [00:10<00:06, 212.69it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1926/3257 [00:10<00:06, 210.10it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1955/3257 [00:10<00:05, 232.61it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1979/3257 [00:10<00:05, 222.15it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2002/3257 [00:10<00:05, 222.59it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2025/3257 [00:10<00:05, 220.02it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2048/3257 [00:10<00:05, 207.05it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2069/3257 [00:10<00:06, 197.09it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2092/3257 [00:11<00:05, 204.29it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2113/3257 [00:11<00:05, 203.74it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2134/3257 [00:11<00:05, 192.68it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2154/3257 [00:11<00:05, 192.79it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:11<00:05, 200.49it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2201/3257 [00:11<00:05, 207.02it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2222/3257 [00:11<00:05, 203.57it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2243/3257 [00:12<00:08, 118.19it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2266/3257 [00:12<00:07, 137.93it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2288/3257 [00:12<00:06, 154.38it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2308/3257 [00:12<00:05, 163.25it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2336/3257 [00:12<00:04, 190.28it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2362/3257 [00:12<00:04, 206.44it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2387/3257 [00:12<00:04, 217.26it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2411/3257 [00:12<00:03, 213.82it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2434/3257 [00:12<00:03, 206.55it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2456/3257 [00:12<00:03, 208.80it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2481/3257 [00:13<00:03, 220.04it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2508/3257 [00:13<00:03, 232.86it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2533/3257 [00:13<00:03, 236.87it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2557/3257 [00:13<00:03, 222.88it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2580/3257 [00:13<00:03, 210.04it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2605/3257 [00:13<00:02, 218.44it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2634/3257 [00:13<00:02, 234.41it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2658/3257 [00:13<00:02, 219.53it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2684/3257 [00:13<00:02, 227.55it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2708/3257 [00:14<00:02, 204.21it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2730/3257 [00:14<00:02, 207.48it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2754/3257 [00:14<00:02, 214.15it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2776/3257 [00:14<00:02, 204.57it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2803/3257 [00:14<00:02, 221.37it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2826/3257 [00:14<00:02, 209.94it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2848/3257 [00:14<00:01, 212.56it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2878/3257 [00:14<00:01, 236.33it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2902/3257 [00:15<00:01, 213.47it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:15<00:01, 220.10it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2951/3257 [00:15<00:01, 209.03it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2974/3257 [00:15<00:01, 214.02it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2996/3257 [00:15<00:01, 208.64it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3020/3257 [00:15<00:01, 215.08it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3046/3257 [00:15<00:00, 226.24it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3072/3257 [00:15<00:00, 234.03it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3096/3257 [00:15<00:00, 227.32it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3124/3257 [00:16<00:00, 240.12it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3149/3257 [00:16<00:00, 225.16it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3172/3257 [00:16<00:00, 225.05it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3196/3257 [00:16<00:00, 226.71it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3219/3257 [00:16<00:00, 212.51it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:16<00:00, 226.64it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 196.03it/s]
2023-02-07 20:41:40.545 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:41:40,546][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d357,n5,mc3,s0.243055,t4>', 'datetime': '2023-02-07T20:41:40.546922', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:41:40,547][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:41:40,547][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:41:40,975][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 20:41:40,976][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:41:41,031][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 22723 unique words (71.45% of original 31803, drops 9080)', 'datetime': '2023-02-07T20:41:41.031370', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:41:41,031][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 5081575 word corpus (99.73% of original 5095118, drops 13543)', 'datetime': '2023-02-07T20:41:41.031750', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:41:41,105][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 20:41:41,106][gensim.models.word2vec][INFO] - sample=0.243055 downsamples 0 most-common words
[2023-02-07 20:41:41,106][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5081575 word corpus (100.0%% of prior 5081575)', 'datetime': '2023-02-07T20:41:41.106953', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:41:41,228][gensim.models.word2vec][INFO] - estimated required memory for 22723 words and 357 dimensions: 81560784 bytes
[2023-02-07 20:41:41,228][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:41:41,266][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 22723 vocabulary and 357 features, using sg=1 hs=0 sample=0.24305506907797236 negative=5 window=7 shrink_windows=True', 'datetime': '2023-02-07T20:41:41.266809', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:41:42,276][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 30.76% examples, 1566854 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:41:43,276][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 64.29% examples, 1653417 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:41:44,278][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 100.00% examples, 1686676 words/s, in_qsize 0, out_qsize 1
[2023-02-07 20:41:44,278][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5073452 effective words) took 3.0s, 1686439 effective words/s
[2023-02-07 20:41:45,284][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 37.06% examples, 1922893 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:41:46,295][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 74.03% examples, 1891910 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:41:46,850][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5073452 effective words) took 2.6s, 1974334 effective words/s
[2023-02-07 20:41:47,855][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 36.81% examples, 1908264 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:41:48,873][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 74.03% examples, 1887786 words/s, in_qsize 8, out_qsize 2
[2023-02-07 20:41:49,545][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5073452 effective words) took 2.7s, 1884778 effective words/s
[2023-02-07 20:41:50,554][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 36.44% examples, 1873265 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:41:51,559][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 73.81% examples, 1885731 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:41:52,253][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5073452 effective words) took 2.7s, 1874643 effective words/s
[2023-02-07 20:41:53,262][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 36.91% examples, 1910806 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:41:54,264][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 74.85% examples, 1921371 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:41:54,899][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5073452 effective words) took 2.6s, 1920763 effective words/s
[2023-02-07 20:41:55,908][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 37.06% examples, 1918006 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:41:56,911][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 82.84% examples, 2114643 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:41:57,235][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5073452 effective words) took 2.3s, 2174776 effective words/s
[2023-02-07 20:41:58,239][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 48.57% examples, 2514380 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:41:59,227][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5073452 effective words) took 2.0s, 2548846 effective words/s
[2023-02-07 20:42:00,231][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 41.08% examples, 2135758 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:42:01,232][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 81.24% examples, 2076134 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:42:01,682][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5073452 effective words) took 2.5s, 2068848 effective words/s
[2023-02-07 20:42:02,687][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 49.25% examples, 2542129 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:42:03,666][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5073452 effective words) took 2.0s, 2560472 effective words/s
[2023-02-07 20:42:04,675][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 39.58% examples, 2053115 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:42:05,676][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 81.12% examples, 2066672 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:42:06,126][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5073452 effective words) took 2.5s, 2064938 effective words/s
[2023-02-07 20:42:07,128][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 39.30% examples, 2044465 words/s, in_qsize 7, out_qsize 1
[2023-02-07 20:42:08,130][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 78.32% examples, 2004615 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:42:08,651][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5073452 effective words) took 2.5s, 2010256 effective words/s
[2023-02-07 20:42:09,656][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 48.76% examples, 2522590 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:42:10,662][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 98.86% examples, 2501855 words/s, in_qsize 6, out_qsize 0
[2023-02-07 20:42:10,675][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5073452 effective words) took 2.0s, 2509770 effective words/s
[2023-02-07 20:42:11,678][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 38.19% examples, 1978170 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:42:12,679][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 75.96% examples, 1951565 words/s, in_qsize 8, out_qsize 2
[2023-02-07 20:42:13,258][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5073452 effective words) took 2.6s, 1965304 effective words/s
[2023-02-07 20:42:14,268][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 36.32% examples, 1865994 words/s, in_qsize 5, out_qsize 5
[2023-02-07 20:42:15,275][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 74.06% examples, 1893560 words/s, in_qsize 5, out_qsize 2
[2023-02-07 20:42:15,915][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5073452 effective words) took 2.7s, 1912221 effective words/s
[2023-02-07 20:42:16,926][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 37.06% examples, 1911031 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:42:17,932][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 75.13% examples, 1921150 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:42:18,569][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5073452 effective words) took 2.7s, 1912984 effective words/s
[2023-02-07 20:42:18,570][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (76101780 effective words) took 37.3s, 2040160 effective words/s', 'datetime': '2023-02-07T20:42:18.570196', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:42:18.570 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:42:21,391][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204115-5ppwyjdn/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:42:21.391625', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:42:21,392][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:42:21,507][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204115-5ppwyjdn/files/../tmp/embedding_model.pt
2023-02-07 20:42:21.507 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:42:23.489 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:42:24.159 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:42:26.443 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2669580798177633, 'test_mae': 1.0942011503402738, 'test_r2': -2.106884808176777}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.94
wandb: percentage 0.28551
wandb:   test_mae 1.0942
wandb:   test_mse 2.26696
wandb:    test_r2 -2.10688
wandb: 
wandb: üöÄ View run misty-sweep-95 at: https://wandb.ai/xiaoqiz/mof2vec/runs/5ppwyjdn
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_204115-5ppwyjdn/logs
wandb: Agent Starting Run: 7vc73dj7 with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 741
wandb: 	model.gensim.alpha: 0.07635188862617774
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.5212198135129072
wandb: 	model.gensim.vector_size: 273
wandb: 	model.gensim.window: 19
wandb: 	model.sklearn.learning_rate: 0.0004782523269975029
wandb: 	model.sklearn.max_depth: 67
wandb: 	model.sklearn.min_child_weight: 0.03183061737664211
wandb: 	model.sklearn.n_estimators: 4949
wandb: 	model.sklearn.num_leaves: 372
wandb: 	model.sklearn.reg_alpha: 0.4978781771146342
wandb: 	model.sklearn.reg_lambda: 0.007297052003095881
wandb: 	model.sklearn.subsample: 0.587795134264102
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204240-7vc73dj7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-96
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/7vc73dj7
2023-02-07 20:42:48.251 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 20:42:48.251 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 741 for sweep.
2023-02-07 20:42:48.252 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.07635188862617774 for sweep.
2023-02-07 20:42:48.252 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:42:48.252 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 20:42:48.252 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5212198135129072 for sweep.
2023-02-07 20:42:48.253 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 273 for sweep.
2023-02-07 20:42:48.253 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 19 for sweep.
2023-02-07 20:42:48.253 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0004782523269975029 for sweep.
2023-02-07 20:42:48.253 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 67 for sweep.
2023-02-07 20:42:48.254 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03183061737664211 for sweep.
2023-02-07 20:42:48.254 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4949 for sweep.
2023-02-07 20:42:48.255 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 372 for sweep.
2023-02-07 20:42:48.255 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.4978781771146342 for sweep.
2023-02-07 20:42:48.255 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.007297052003095881 for sweep.
2023-02-07 20:42:48.255 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.587795134264102 for sweep.
2023-02-07 20:42:48.255 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:42:48.260 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204240-7vc73dj7/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 741, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 273, 'window': 19, 'min_count': 5, 'dm': 0, 'sample': 0.5212198135129072, 'workers': 4, 'alpha': 0.07635188862617774, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4949, 'max_depth': 67, 'num_leaves': 372, 'reg_alpha': 0.4978781771146342, 'reg_lambda': 0.007297052003095881, 'subsample': 0.587795134264102, 'min_child_weight': 0.03183061737664211, 'n_jobs': 4, 'learning_rate': 0.0004782523269975029}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 15/3257 [00:00<00:22, 143.59it/s]  1%|          | 33/3257 [00:00<00:20, 160.05it/s]  2%|‚ñè         | 50/3257 [00:00<00:19, 161.96it/s]  2%|‚ñè         | 67/3257 [00:00<00:19, 160.96it/s]  3%|‚ñé         | 89/3257 [00:00<00:17, 176.62it/s]  3%|‚ñé         | 107/3257 [00:00<00:19, 158.53it/s]  4%|‚ñç         | 124/3257 [00:00<00:19, 161.08it/s]  4%|‚ñç         | 145/3257 [00:00<00:18, 171.28it/s]  5%|‚ñå         | 163/3257 [00:01<00:19, 156.85it/s]  5%|‚ñå         | 179/3257 [00:01<00:19, 155.86it/s]  6%|‚ñå         | 198/3257 [00:01<00:18, 164.79it/s]  7%|‚ñã         | 216/3257 [00:01<00:18, 168.80it/s]  7%|‚ñã         | 237/3257 [00:01<00:17, 173.33it/s]  8%|‚ñä         | 256/3257 [00:01<00:16, 177.81it/s]  8%|‚ñä         | 274/3257 [00:01<00:17, 175.42it/s]  9%|‚ñâ         | 295/3257 [00:01<00:16, 183.94it/s] 10%|‚ñâ         | 314/3257 [00:01<00:16, 173.36it/s] 10%|‚ñà         | 333/3257 [00:01<00:16, 177.72it/s] 11%|‚ñà         | 351/3257 [00:02<00:17, 170.57it/s] 11%|‚ñà‚ñè        | 369/3257 [00:02<00:16, 172.39it/s] 12%|‚ñà‚ñè        | 387/3257 [00:02<00:17, 161.05it/s] 12%|‚ñà‚ñè        | 407/3257 [00:02<00:16, 169.30it/s] 13%|‚ñà‚ñé        | 425/3257 [00:02<00:17, 158.74it/s] 14%|‚ñà‚ñé        | 442/3257 [00:02<00:18, 150.91it/s] 14%|‚ñà‚ñç        | 459/3257 [00:02<00:17, 155.82it/s] 15%|‚ñà‚ñç        | 477/3257 [00:02<00:17, 162.24it/s] 15%|‚ñà‚ñå        | 495/3257 [00:02<00:16, 166.86it/s] 16%|‚ñà‚ñå        | 514/3257 [00:03<00:15, 172.75it/s] 16%|‚ñà‚ñã        | 532/3257 [00:03<00:16, 168.00it/s] 17%|‚ñà‚ñã        | 549/3257 [00:03<00:23, 116.02it/s] 17%|‚ñà‚ñã        | 563/3257 [00:03<00:22, 117.43it/s] 18%|‚ñà‚ñä        | 577/3257 [00:03<00:22, 120.45it/s] 18%|‚ñà‚ñä        | 595/3257 [00:03<00:19, 134.57it/s] 19%|‚ñà‚ñâ        | 614/3257 [00:03<00:17, 147.19it/s] 19%|‚ñà‚ñâ        | 630/3257 [00:03<00:17, 148.50it/s] 20%|‚ñà‚ñâ        | 646/3257 [00:04<00:18, 143.52it/s] 20%|‚ñà‚ñà        | 661/3257 [00:04<00:18, 137.55it/s] 21%|‚ñà‚ñà        | 682/3257 [00:04<00:16, 155.19it/s] 21%|‚ñà‚ñà‚ñè       | 698/3257 [00:04<00:17, 149.51it/s] 22%|‚ñà‚ñà‚ñè       | 718/3257 [00:04<00:15, 161.30it/s] 23%|‚ñà‚ñà‚ñé       | 735/3257 [00:04<00:16, 153.65it/s] 23%|‚ñà‚ñà‚ñé       | 751/3257 [00:04<00:16, 149.10it/s] 24%|‚ñà‚ñà‚ñé       | 771/3257 [00:04<00:15, 162.10it/s] 24%|‚ñà‚ñà‚ñç       | 788/3257 [00:05<00:15, 159.45it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:05<00:15, 161.23it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:05<00:15, 160.72it/s] 26%|‚ñà‚ñà‚ñå       | 840/3257 [00:05<00:14, 162.53it/s] 26%|‚ñà‚ñà‚ñã       | 857/3257 [00:05<00:14, 160.46it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:05<00:14, 161.13it/s] 27%|‚ñà‚ñà‚ñã       | 892/3257 [00:05<00:14, 162.08it/s] 28%|‚ñà‚ñà‚ñä       | 913/3257 [00:05<00:13, 175.02it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:05<00:13, 174.15it/s] 29%|‚ñà‚ñà‚ñâ       | 950/3257 [00:05<00:13, 177.33it/s] 30%|‚ñà‚ñà‚ñâ       | 968/3257 [00:06<00:12, 177.54it/s] 30%|‚ñà‚ñà‚ñà       | 986/3257 [00:06<00:13, 167.66it/s] 31%|‚ñà‚ñà‚ñà       | 1003/3257 [00:06<00:13, 167.85it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1021/3257 [00:06<00:13, 170.14it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1039/3257 [00:06<00:14, 158.34it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1056/3257 [00:06<00:13, 158.44it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1077/3257 [00:06<00:12, 172.66it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1095/3257 [00:06<00:13, 161.02it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1115/3257 [00:06<00:12, 168.36it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1133/3257 [00:07<00:13, 161.31it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1150/3257 [00:07<00:13, 159.61it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:07<00:12, 168.75it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1188/3257 [00:07<00:13, 150.65it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1204/3257 [00:07<00:13, 151.37it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1220/3257 [00:07<00:13, 153.22it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:07<00:12, 166.97it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1259/3257 [00:07<00:12, 164.82it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1276/3257 [00:07<00:12, 160.70it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1293/3257 [00:08<00:12, 154.52it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1309/3257 [00:08<00:12, 154.31it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:08<00:11, 161.71it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1348/3257 [00:08<00:11, 172.25it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1366/3257 [00:08<00:11, 162.99it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1383/3257 [00:08<00:11, 157.78it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1402/3257 [00:08<00:11, 166.31it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1424/3257 [00:08<00:10, 179.30it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1443/3257 [00:08<00:10, 179.81it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1466/3257 [00:09<00:09, 190.85it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1486/3257 [00:09<00:09, 187.07it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1509/3257 [00:09<00:08, 199.15it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1530/3257 [00:09<00:09, 174.73it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1549/3257 [00:09<00:10, 167.57it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1567/3257 [00:09<00:10, 167.23it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1585/3257 [00:09<00:10, 166.34it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1605/3257 [00:09<00:09, 172.96it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1623/3257 [00:09<00:09, 174.75it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1641/3257 [00:10<00:09, 173.76it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:10<00:09, 167.69it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1676/3257 [00:10<00:09, 160.98it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1694/3257 [00:10<00:09, 162.44it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1714/3257 [00:10<00:08, 172.79it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1732/3257 [00:10<00:09, 158.58it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1750/3257 [00:10<00:09, 163.48it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1769/3257 [00:10<00:08, 170.52it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1788/3257 [00:10<00:08, 175.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1806/3257 [00:11<00:08, 170.16it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1824/3257 [00:11<00:12, 111.08it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1840/3257 [00:11<00:11, 121.04it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1858/3257 [00:11<00:10, 130.75it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1879/3257 [00:11<00:09, 147.99it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1896/3257 [00:11<00:08, 152.65it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:11<00:08, 161.13it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1937/3257 [00:12<00:07, 170.98it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1962/3257 [00:12<00:06, 191.38it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1982/3257 [00:12<00:07, 178.76it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2001/3257 [00:12<00:06, 181.24it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2020/3257 [00:12<00:06, 182.07it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2039/3257 [00:12<00:06, 179.29it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2058/3257 [00:12<00:07, 162.72it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2076/3257 [00:12<00:07, 166.22it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2094/3257 [00:12<00:06, 167.93it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2112/3257 [00:13<00:06, 166.99it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2129/3257 [00:13<00:07, 158.09it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2145/3257 [00:13<00:07, 154.86it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2164/3257 [00:13<00:06, 164.24it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:13<00:06, 163.00it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2201/3257 [00:13<00:06, 171.55it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2219/3257 [00:13<00:06, 167.96it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2237/3257 [00:13<00:06, 169.94it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2255/3257 [00:13<00:06, 166.89it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2272/3257 [00:14<00:06, 157.00it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2294/3257 [00:14<00:05, 172.44it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2312/3257 [00:14<00:05, 169.35it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2336/3257 [00:14<00:04, 186.64it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:14<00:04, 193.58it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2381/3257 [00:14<00:04, 196.47it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2403/3257 [00:14<00:04, 200.70it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2424/3257 [00:14<00:04, 182.92it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2443/3257 [00:14<00:04, 171.32it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2463/3257 [00:15<00:04, 177.66it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2482/3257 [00:15<00:04, 180.69it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2504/3257 [00:15<00:03, 190.54it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2524/3257 [00:15<00:03, 192.39it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2544/3257 [00:15<00:03, 192.64it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2564/3257 [00:15<00:03, 184.73it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2583/3257 [00:15<00:03, 169.75it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2602/3257 [00:15<00:03, 173.85it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:15<00:03, 196.28it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2648/3257 [00:16<00:03, 188.75it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2668/3257 [00:16<00:03, 187.68it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2688/3257 [00:16<00:02, 190.01it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2708/3257 [00:16<00:03, 169.19it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2726/3257 [00:16<00:03, 169.54it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2749/3257 [00:16<00:02, 184.49it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:16<00:02, 179.18it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2790/3257 [00:16<00:02, 189.81it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2810/3257 [00:16<00:02, 185.82it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2829/3257 [00:17<00:02, 173.77it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2851/3257 [00:17<00:02, 183.69it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2879/3257 [00:17<00:01, 209.23it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2901/3257 [00:17<00:01, 186.73it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2926/3257 [00:17<00:01, 201.15it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2947/3257 [00:17<00:01, 184.08it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2967/3257 [00:17<00:01, 183.26it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2986/3257 [00:17<00:01, 173.16it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3011/3257 [00:17<00:01, 190.19it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3031/3257 [00:18<00:01, 185.56it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3055/3257 [00:18<00:01, 198.67it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:18<00:00, 206.63it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3100/3257 [00:18<00:00, 201.85it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3124/3257 [00:18<00:00, 209.76it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3146/3257 [00:18<00:00, 192.57it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3166/3257 [00:18<00:00, 189.99it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3186/3257 [00:18<00:00, 181.24it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:18<00:00, 186.01it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3226/3257 [00:19<00:00, 181.49it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3250/3257 [00:19<00:00, 196.76it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 169.17it/s]
2023-02-07 20:43:08.373 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:43:08,374][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d273,n5,mc5,s0.52122,t4>', 'datetime': '2023-02-07T20:43:08.374303', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:43:08,374][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:43:08,374][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:43:08,947][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 20:43:08,947][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:43:09,016][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 26649 unique words (49.30% of original 54054, drops 27405)', 'datetime': '2023-02-07T20:43:09.016725', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:43:09,017][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 6481391 word corpus (98.94% of original 6550866, drops 69475)', 'datetime': '2023-02-07T20:43:09.017090', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:43:09,101][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 20:43:09,102][gensim.models.word2vec][INFO] - sample=0.52122 downsamples 0 most-common words
[2023-02-07 20:43:09,103][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6481391 word corpus (100.0%% of prior 6481391)', 'datetime': '2023-02-07T20:43:09.103591', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:43:09,244][gensim.models.word2vec][INFO] - estimated required memory for 26649 words and 273 dimensions: 75733960 bytes
[2023-02-07 20:43:09,244][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:43:09,285][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 26649 vocabulary and 273 features, using sg=1 hs=0 sample=0.5212198135129072 negative=5 window=19 shrink_windows=True', 'datetime': '2023-02-07T20:43:09.285337', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:43:10,289][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 38.19% examples, 2501817 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:11,308][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 69.97% examples, 2291188 words/s, in_qsize 8, out_qsize 2
[2023-02-07 20:43:12,265][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6426944 effective words) took 3.0s, 2158901 effective words/s
[2023-02-07 20:43:13,274][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 33.53% examples, 2172163 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:14,273][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 65.77% examples, 2151817 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:15,264][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6426944 effective words) took 3.0s, 2145729 effective words/s
[2023-02-07 20:43:16,266][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 33.53% examples, 2179144 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:43:17,269][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 67.45% examples, 2205876 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:43:18,173][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6426944 effective words) took 2.9s, 2210915 effective words/s
[2023-02-07 20:43:19,176][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 44.00% examples, 2895601 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:20,177][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 90.18% examples, 2912988 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:20,378][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6426944 effective words) took 2.2s, 2917890 effective words/s
[2023-02-07 20:43:21,382][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 44.21% examples, 2905946 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:22,383][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 89.90% examples, 2903676 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:22,591][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6426944 effective words) took 2.2s, 2906496 effective words/s
[2023-02-07 20:43:23,597][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 44.89% examples, 2932531 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:43:24,597][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 90.97% examples, 2933717 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:24,785][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6426944 effective words) took 2.2s, 2932507 effective words/s
[2023-02-07 20:43:25,791][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 35.31% examples, 2290705 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:26,793][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 70.16% examples, 2296078 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:27,513][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6426944 effective words) took 2.7s, 2356751 effective words/s
[2023-02-07 20:43:28,518][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 35.00% examples, 2277269 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:43:29,518][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 69.36% examples, 2274707 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:30,375][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6426944 effective words) took 2.9s, 2247962 effective words/s
[2023-02-07 20:43:31,380][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 42.37% examples, 2793017 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:32,383][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 85.97% examples, 2778472 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:32,691][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6426944 effective words) took 2.3s, 2779824 effective words/s
[2023-02-07 20:43:33,693][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 33.68% examples, 2187183 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:34,693][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 66.44% examples, 2178293 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:35,657][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6426944 effective words) took 3.0s, 2167878 effective words/s
[2023-02-07 20:43:36,662][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 33.34% examples, 2158692 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:37,664][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 68.19% examples, 2232680 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:38,381][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6426944 effective words) took 2.7s, 2362397 effective words/s
[2023-02-07 20:43:39,389][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 33.74% examples, 2179832 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:40,397][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 76.05% examples, 2460168 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:40,924][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6426944 effective words) took 2.5s, 2529371 effective words/s
[2023-02-07 20:43:41,937][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 33.34% examples, 2138753 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:43:42,938][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 65.77% examples, 2142476 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:43,896][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6426944 effective words) took 3.0s, 2163631 effective words/s
[2023-02-07 20:43:44,902][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 35.12% examples, 2280521 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:45,904][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 69.14% examples, 2265129 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:46,728][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6426944 effective words) took 2.8s, 2271637 effective words/s
[2023-02-07 20:43:47,736][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 33.71% examples, 2182849 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:43:48,742][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 66.38% examples, 2172737 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:43:49,698][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6426944 effective words) took 3.0s, 2167394 effective words/s
[2023-02-07 20:43:49,699][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (96404160 effective words) took 40.4s, 2385421 effective words/s', 'datetime': '2023-02-07T20:43:49.699673', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:43:49.700 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:43:52,986][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204240-7vc73dj7/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:43:52.986788', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:43:52,988][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:43:53,093][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204240-7vc73dj7/files/../tmp/embedding_model.pt
2023-02-07 20:43:53.094 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:43:54.775 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:43:55.347 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:43:57.360 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.12744309225313, 'test_mae': 1.0676224601692148, 'test_r2': -2.0965660296382}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.95
wandb: percentage 0.50699
wandb:   test_mae 1.06762
wandb:   test_mse 2.12744
wandb:    test_r2 -2.09657
wandb: 
wandb: üöÄ View run divine-sweep-96 at: https://wandb.ai/xiaoqiz/mof2vec/runs/7vc73dj7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_204240-7vc73dj7/logs
wandb: Agent Starting Run: o6tqdx52 with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 786
wandb: 	model.gensim.alpha: 0.0038998520781384615
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.2660881859845565
wandb: 	model.gensim.vector_size: 298
wandb: 	model.gensim.window: 10
wandb: 	model.sklearn.learning_rate: 0.0003399140569110316
wandb: 	model.sklearn.max_depth: 61
wandb: 	model.sklearn.min_child_weight: 0.03523699094093651
wandb: 	model.sklearn.n_estimators: 3284
wandb: 	model.sklearn.num_leaves: 304
wandb: 	model.sklearn.reg_alpha: 0.6086654991657086
wandb: 	model.sklearn.reg_lambda: 0.017028196791454103
wandb: 	model.sklearn.subsample: 0.6640012801698549
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204410-o6tqdx52
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-97
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/o6tqdx52
2023-02-07 20:44:18.479 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 20:44:18.480 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 786 for sweep.
2023-02-07 20:44:18.480 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0038998520781384615 for sweep.
2023-02-07 20:44:18.481 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:44:18.481 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 20:44:18.481 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2660881859845565 for sweep.
2023-02-07 20:44:18.481 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 298 for sweep.
2023-02-07 20:44:18.482 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 10 for sweep.
2023-02-07 20:44:18.482 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0003399140569110316 for sweep.
2023-02-07 20:44:18.482 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 61 for sweep.
2023-02-07 20:44:18.482 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03523699094093651 for sweep.
2023-02-07 20:44:18.483 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3284 for sweep.
2023-02-07 20:44:18.483 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 304 for sweep.
2023-02-07 20:44:18.483 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.6086654991657086 for sweep.
2023-02-07 20:44:18.483 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.017028196791454103 for sweep.
2023-02-07 20:44:18.484 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6640012801698549 for sweep.
2023-02-07 20:44:18.484 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:44:18.488 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204410-o6tqdx52/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 786, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 298, 'window': 10, 'min_count': 6, 'dm': 0, 'sample': 0.2660881859845565, 'workers': 4, 'alpha': 0.0038998520781384615, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3284, 'max_depth': 61, 'num_leaves': 304, 'reg_alpha': 0.6086654991657086, 'reg_lambda': 0.017028196791454103, 'subsample': 0.6640012801698549, 'min_child_weight': 0.03523699094093651, 'n_jobs': 4, 'learning_rate': 0.0003399140569110316}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:20, 160.78it/s]  1%|          | 34/3257 [00:00<00:19, 165.62it/s]  2%|‚ñè         | 53/3257 [00:00<00:19, 165.63it/s]  2%|‚ñè         | 72/3257 [00:00<00:18, 173.39it/s]  3%|‚ñé         | 91/3257 [00:00<00:17, 178.41it/s]  3%|‚ñé         | 109/3257 [00:00<00:18, 167.24it/s]  4%|‚ñç         | 128/3257 [00:00<00:18, 171.83it/s]  5%|‚ñç         | 149/3257 [00:00<00:17, 180.82it/s]  5%|‚ñå         | 168/3257 [00:00<00:17, 174.54it/s]  6%|‚ñå         | 187/3257 [00:01<00:17, 176.72it/s]  6%|‚ñã         | 205/3257 [00:01<00:17, 176.49it/s]  7%|‚ñã         | 230/3257 [00:01<00:15, 194.55it/s]  8%|‚ñä         | 250/3257 [00:01<00:15, 191.25it/s]  8%|‚ñä         | 270/3257 [00:01<00:15, 186.71it/s]  9%|‚ñâ         | 292/3257 [00:01<00:15, 195.62it/s] 10%|‚ñâ         | 312/3257 [00:01<00:16, 181.41it/s] 10%|‚ñà         | 334/3257 [00:01<00:15, 190.68it/s] 11%|‚ñà         | 354/3257 [00:01<00:15, 189.21it/s] 11%|‚ñà‚ñè        | 374/3257 [00:02<00:15, 188.92it/s] 12%|‚ñà‚ñè        | 394/3257 [00:02<00:16, 172.63it/s] 13%|‚ñà‚ñé        | 415/3257 [00:02<00:15, 182.39it/s] 13%|‚ñà‚ñé        | 434/3257 [00:02<00:17, 157.14it/s] 14%|‚ñà‚ñç        | 453/3257 [00:02<00:17, 164.36it/s] 15%|‚ñà‚ñç        | 474/3257 [00:02<00:16, 173.56it/s] 15%|‚ñà‚ñå        | 492/3257 [00:02<00:15, 175.01it/s] 16%|‚ñà‚ñå        | 513/3257 [00:02<00:14, 183.31it/s] 16%|‚ñà‚ñã        | 532/3257 [00:02<00:15, 179.64it/s] 17%|‚ñà‚ñã        | 552/3257 [00:03<00:14, 184.27it/s] 18%|‚ñà‚ñä        | 571/3257 [00:03<00:16, 160.49it/s] 18%|‚ñà‚ñä        | 591/3257 [00:03<00:15, 169.42it/s] 19%|‚ñà‚ñâ        | 612/3257 [00:03<00:14, 179.36it/s] 19%|‚ñà‚ñâ        | 631/3257 [00:03<00:14, 177.98it/s] 20%|‚ñà‚ñâ        | 650/3257 [00:03<00:15, 171.84it/s] 21%|‚ñà‚ñà        | 668/3257 [00:03<00:15, 163.57it/s] 21%|‚ñà‚ñà        | 685/3257 [00:03<00:16, 159.25it/s] 22%|‚ñà‚ñà‚ñè       | 703/3257 [00:04<00:15, 164.65it/s] 22%|‚ñà‚ñà‚ñè       | 721/3257 [00:04<00:15, 167.27it/s] 23%|‚ñà‚ñà‚ñé       | 738/3257 [00:04<00:15, 158.19it/s] 23%|‚ñà‚ñà‚ñé       | 759/3257 [00:04<00:14, 169.88it/s] 24%|‚ñà‚ñà‚ñç       | 777/3257 [00:04<00:14, 165.49it/s] 24%|‚ñà‚ñà‚ñç       | 796/3257 [00:04<00:14, 169.51it/s] 25%|‚ñà‚ñà‚ñç       | 814/3257 [00:04<00:14, 168.85it/s] 26%|‚ñà‚ñà‚ñå       | 831/3257 [00:04<00:14, 162.85it/s] 26%|‚ñà‚ñà‚ñå       | 848/3257 [00:04<00:15, 154.21it/s] 27%|‚ñà‚ñà‚ñã       | 866/3257 [00:05<00:15, 158.19it/s] 27%|‚ñà‚ñà‚ñã       | 884/3257 [00:05<00:15, 155.90it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:05<00:13, 170.85it/s] 28%|‚ñà‚ñà‚ñä       | 925/3257 [00:05<00:13, 174.87it/s] 29%|‚ñà‚ñà‚ñâ       | 943/3257 [00:05<00:13, 174.00it/s] 30%|‚ñà‚ñà‚ñâ       | 962/3257 [00:05<00:12, 177.50it/s] 30%|‚ñà‚ñà‚ñà       | 980/3257 [00:05<00:13, 167.94it/s] 31%|‚ñà‚ñà‚ñà       | 997/3257 [00:05<00:13, 167.86it/s] 31%|‚ñà‚ñà‚ñà       | 1014/3257 [00:05<00:13, 165.38it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1031/3257 [00:05<00:13, 161.53it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1048/3257 [00:06<00:14, 156.47it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:06<00:13, 160.79it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1084/3257 [00:06<00:13, 162.48it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1103/3257 [00:06<00:12, 167.57it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1120/3257 [00:06<00:12, 167.91it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1137/3257 [00:06<00:12, 168.00it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1154/3257 [00:06<00:12, 163.66it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:07<00:19, 104.79it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1185/3257 [00:07<00:18, 111.62it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1199/3257 [00:07<00:17, 117.47it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1213/3257 [00:07<00:16, 122.96it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1235/3257 [00:07<00:13, 147.56it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1252/3257 [00:07<00:13, 152.21it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:07<00:12, 160.10it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1288/3257 [00:07<00:13, 148.72it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1306/3257 [00:07<00:12, 156.25it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1325/3257 [00:07<00:11, 163.39it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1345/3257 [00:08<00:11, 172.12it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1363/3257 [00:08<00:11, 171.64it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1381/3257 [00:08<00:11, 160.80it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1401/3257 [00:08<00:10, 169.19it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:08<00:10, 177.49it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1441/3257 [00:08<00:10, 177.40it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1464/3257 [00:08<00:09, 191.69it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1484/3257 [00:08<00:09, 190.05it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1507/3257 [00:08<00:08, 199.92it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1528/3257 [00:09<00:09, 180.88it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1547/3257 [00:09<00:10, 170.97it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1566/3257 [00:09<00:09, 174.74it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1584/3257 [00:09<00:09, 174.87it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1603/3257 [00:09<00:09, 178.99it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1622/3257 [00:09<00:09, 178.04it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1640/3257 [00:09<00:09, 176.59it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1658/3257 [00:09<00:09, 168.66it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1675/3257 [00:09<00:09, 164.48it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1692/3257 [00:10<00:09, 164.12it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1713/3257 [00:10<00:08, 174.46it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:10<00:09, 159.27it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1749/3257 [00:10<00:09, 163.24it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1768/3257 [00:10<00:08, 170.36it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1789/3257 [00:10<00:08, 180.65it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1808/3257 [00:10<00:08, 168.10it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1827/3257 [00:10<00:08, 171.62it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1846/3257 [00:10<00:08, 174.06it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1868/3257 [00:11<00:07, 184.71it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1887/3257 [00:11<00:07, 173.90it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1905/3257 [00:11<00:07, 173.93it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1923/3257 [00:11<00:07, 167.87it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1947/3257 [00:11<00:07, 185.99it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1968/3257 [00:11<00:06, 192.34it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1988/3257 [00:11<00:07, 175.53it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2007/3257 [00:11<00:07, 176.78it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2029/3257 [00:11<00:06, 184.85it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2048/3257 [00:12<00:07, 164.26it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2065/3257 [00:12<00:07, 157.19it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2083/3257 [00:12<00:07, 161.53it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2100/3257 [00:12<00:07, 154.85it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2119/3257 [00:12<00:06, 164.13it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2136/3257 [00:12<00:07, 152.16it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2152/3257 [00:12<00:07, 148.95it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:12<00:07, 154.30it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2185/3257 [00:13<00:06, 153.42it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2204/3257 [00:13<00:06, 162.83it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2221/3257 [00:13<00:06, 161.25it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2238/3257 [00:13<00:06, 154.95it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2255/3257 [00:13<00:06, 157.62it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2271/3257 [00:13<00:06, 149.67it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2291/3257 [00:13<00:05, 161.69it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2308/3257 [00:13<00:05, 162.57it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2331/3257 [00:13<00:05, 180.34it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2353/3257 [00:13<00:04, 191.57it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2373/3257 [00:14<00:04, 182.67it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2395/3257 [00:14<00:04, 192.83it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2415/3257 [00:14<00:04, 180.96it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2434/3257 [00:14<00:04, 169.53it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2452/3257 [00:14<00:04, 164.13it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2475/3257 [00:14<00:04, 177.94it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2494/3257 [00:14<00:04, 179.28it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2514/3257 [00:14<00:04, 180.10it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2536/3257 [00:15<00:03, 187.47it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2555/3257 [00:15<00:07, 91.95it/s]  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2570/3257 [00:15<00:06, 100.33it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2587/3257 [00:15<00:05, 112.03it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2608/3257 [00:15<00:04, 131.43it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:15<00:04, 152.76it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2650/3257 [00:16<00:03, 152.90it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2668/3257 [00:16<00:03, 158.49it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2687/3257 [00:16<00:03, 164.29it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2705/3257 [00:16<00:03, 150.37it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2722/3257 [00:16<00:03, 147.30it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2744/3257 [00:16<00:03, 164.59it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2762/3257 [00:16<00:03, 164.44it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2779/3257 [00:16<00:02, 163.68it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2801/3257 [00:16<00:02, 178.23it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2820/3257 [00:17<00:02, 166.91it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2838/3257 [00:17<00:02, 160.86it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2859/3257 [00:17<00:02, 173.43it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2882/3257 [00:17<00:02, 187.21it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2902/3257 [00:17<00:02, 169.04it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2922/3257 [00:17<00:01, 176.12it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2941/3257 [00:17<00:01, 173.14it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2959/3257 [00:17<00:01, 157.37it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:17<00:01, 160.46it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2993/3257 [00:18<00:01, 159.39it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3014/3257 [00:18<00:01, 171.89it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3032/3257 [00:18<00:01, 172.91it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3052/3257 [00:18<00:01, 179.84it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3075/3257 [00:18<00:00, 190.57it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3095/3257 [00:18<00:00, 183.53it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3116/3257 [00:18<00:00, 189.98it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3136/3257 [00:18<00:00, 178.88it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3155/3257 [00:18<00:00, 171.66it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3173/3257 [00:19<00:00, 171.97it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3191/3257 [00:19<00:00, 168.88it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3208/3257 [00:19<00:00, 166.69it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3225/3257 [00:19<00:00, 166.17it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:19<00:00, 176.59it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 166.40it/s]
2023-02-07 20:44:38.889 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:44:38,890][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d298,n5,mc6,s0.266088,t4>', 'datetime': '2023-02-07T20:44:38.890861', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:44:38,891][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:44:38,891][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:44:39,491][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 20:44:39,491][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:44:39,563][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 25857 unique words (47.84% of original 54054, drops 28197)', 'datetime': '2023-02-07T20:44:39.563647', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:44:39,565][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 6477431 word corpus (98.88% of original 6550866, drops 73435)', 'datetime': '2023-02-07T20:44:39.565227', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:44:39,650][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 20:44:39,652][gensim.models.word2vec][INFO] - sample=0.266088 downsamples 0 most-common words
[2023-02-07 20:44:39,652][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6477431 word corpus (100.0%% of prior 6477431)', 'datetime': '2023-02-07T20:44:39.652336', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:44:39,794][gensim.models.word2vec][INFO] - estimated required memory for 25857 words and 298 dimensions: 79105332 bytes
[2023-02-07 20:44:39,794][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:44:39,833][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 25857 vocabulary and 298 features, using sg=1 hs=0 sample=0.2660881859845565 negative=5 window=10 shrink_windows=True', 'datetime': '2023-02-07T20:44:39.833107', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:44:40,837][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 25.51% examples, 1636872 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:44:41,845][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 51.15% examples, 1668823 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:44:42,853][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 78.29% examples, 1683408 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:44:43,619][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6423064 effective words) took 3.8s, 1697748 effective words/s
[2023-02-07 20:44:44,622][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 36.60% examples, 2394778 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:44:45,623][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 73.81% examples, 2398267 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:44:46,505][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6423064 effective words) took 2.9s, 2226946 effective words/s
[2023-02-07 20:44:47,511][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 27.79% examples, 1785769 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:44:48,512][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 54.47% examples, 1784705 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:44:49,537][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 82.62% examples, 1764202 words/s, in_qsize 7, out_qsize 5
[2023-02-07 20:44:50,135][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6423064 effective words) took 3.6s, 1770017 effective words/s
[2023-02-07 20:44:51,146][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 27.94% examples, 1779639 words/s, in_qsize 7, out_qsize 2
[2023-02-07 20:44:52,151][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 54.07% examples, 1764752 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:44:53,159][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 82.22% examples, 1761008 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:44:53,765][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6423064 effective words) took 3.6s, 1770714 effective words/s
[2023-02-07 20:44:54,773][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 27.36% examples, 1764976 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:44:55,780][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 54.99% examples, 1793928 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:44:56,786][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 83.33% examples, 1793827 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:44:57,360][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6423064 effective words) took 3.6s, 1788828 effective words/s
[2023-02-07 20:44:58,364][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 27.05% examples, 1737280 words/s, in_qsize 1, out_qsize 8
[2023-02-07 20:44:59,366][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 54.50% examples, 1785633 words/s, in_qsize 8, out_qsize 4
[2023-02-07 20:45:00,368][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 83.33% examples, 1799915 words/s, in_qsize 6, out_qsize 1
[2023-02-07 20:45:00,903][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6423064 effective words) took 3.5s, 1813919 effective words/s
[2023-02-07 20:45:01,918][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 30.15% examples, 1918599 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:45:02,918][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 56.95% examples, 1857744 words/s, in_qsize 3, out_qsize 4
[2023-02-07 20:45:03,926][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 84.43% examples, 1810821 words/s, in_qsize 7, out_qsize 5
[2023-02-07 20:45:04,480][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6423064 effective words) took 3.6s, 1796606 effective words/s
[2023-02-07 20:45:05,499][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 27.36% examples, 1736637 words/s, in_qsize 8, out_qsize 3
[2023-02-07 20:45:06,506][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 54.65% examples, 1772493 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:45:07,510][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 82.78% examples, 1768778 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:08,127][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6423064 effective words) took 3.6s, 1761844 effective words/s
[2023-02-07 20:45:09,134][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 27.11% examples, 1747125 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:10,133][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 53.91% examples, 1770099 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:45:11,136][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 83.57% examples, 1806151 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:11,665][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6423064 effective words) took 3.5s, 1816879 effective words/s
[2023-02-07 20:45:12,668][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 28.89% examples, 1859797 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:13,672][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 57.14% examples, 1870723 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:14,673][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 86.28% examples, 1855782 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:15,121][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6423064 effective words) took 3.5s, 1859840 effective words/s
[2023-02-07 20:45:16,125][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 27.88% examples, 1800380 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:17,130][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 55.08% examples, 1801227 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:18,131][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 84.99% examples, 1833084 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:18,644][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6423064 effective words) took 3.5s, 1824368 effective words/s
[2023-02-07 20:45:19,655][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 28.74% examples, 1844254 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:20,659][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 57.14% examples, 1867292 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:21,659][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 86.28% examples, 1853617 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:22,109][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6423064 effective words) took 3.5s, 1856923 effective words/s
[2023-02-07 20:45:23,114][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 28.89% examples, 1858310 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:24,115][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 56.49% examples, 1852954 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:45:25,119][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 86.18% examples, 1857436 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:25,578][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6423064 effective words) took 3.5s, 1852534 effective words/s
[2023-02-07 20:45:26,587][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 29.23% examples, 1867687 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:27,588][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 56.34% examples, 1846415 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:28,591][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 87.04% examples, 1875486 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:28,993][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6423064 effective words) took 3.4s, 1882349 effective words/s
[2023-02-07 20:45:29,998][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 28.28% examples, 1825251 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:30,998][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 56.00% examples, 1837389 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:31,997][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 85.05% examples, 1835821 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:45:32,499][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6423064 effective words) took 3.5s, 1832899 effective words/s
[2023-02-07 20:45:32,500][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (96345960 effective words) took 52.7s, 1829372 effective words/s', 'datetime': '2023-02-07T20:45:32.500500', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:45:32.500 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:45:36,192][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204410-o6tqdx52/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:45:36.192446', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:45:36,193][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:45:36,300][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204410-o6tqdx52/files/../tmp/embedding_model.pt
2023-02-07 20:45:36.301 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:45:38.086 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:45:38.682 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:45:41.215 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1315075806245782, 'test_mae': 1.0785938944464744, 'test_r2': -1.4693897786528445}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.78
wandb: percentage 0.52165
wandb:   test_mae 1.07859
wandb:   test_mse 2.13151
wandb:    test_r2 -1.46939
wandb: 
wandb: üöÄ View run stellar-sweep-97 at: https://wandb.ai/xiaoqiz/mof2vec/runs/o6tqdx52
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_204410-o6tqdx52/logs
wandb: Agent Starting Run: yqj6tqp9 with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 685
wandb: 	model.gensim.alpha: 0.0330641635414967
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.4889575206410361
wandb: 	model.gensim.vector_size: 268
wandb: 	model.gensim.window: 19
wandb: 	model.sklearn.learning_rate: 0.0027622138031668454
wandb: 	model.sklearn.max_depth: 45
wandb: 	model.sklearn.min_child_weight: 0.0653527747504569
wandb: 	model.sklearn.n_estimators: 1089
wandb: 	model.sklearn.num_leaves: 466
wandb: 	model.sklearn.reg_alpha: 0.1655936004298994
wandb: 	model.sklearn.reg_lambda: 0.4545768981919575
wandb: 	model.sklearn.subsample: 0.8144326236923287
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204551-yqj6tqp9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-98
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/yqj6tqp9
2023-02-07 20:45:58.729 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 20:45:58.730 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 685 for sweep.
2023-02-07 20:45:58.730 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0330641635414967 for sweep.
2023-02-07 20:45:58.731 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:45:58.731 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 20:45:58.731 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4889575206410361 for sweep.
2023-02-07 20:45:58.731 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 268 for sweep.
2023-02-07 20:45:58.732 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 19 for sweep.
2023-02-07 20:45:58.732 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0027622138031668454 for sweep.
2023-02-07 20:45:58.732 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 45 for sweep.
2023-02-07 20:45:58.732 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0653527747504569 for sweep.
2023-02-07 20:45:58.732 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1089 for sweep.
2023-02-07 20:45:58.733 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 466 for sweep.
2023-02-07 20:45:58.733 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.1655936004298994 for sweep.
2023-02-07 20:45:58.733 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.4545768981919575 for sweep.
2023-02-07 20:45:58.734 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8144326236923287 for sweep.
2023-02-07 20:45:58.734 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:45:58.740 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204551-yqj6tqp9/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 685, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 268, 'window': 19, 'min_count': 3, 'dm': 0, 'sample': 0.4889575206410361, 'workers': 4, 'alpha': 0.0330641635414967, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1089, 'max_depth': 45, 'num_leaves': 466, 'reg_alpha': 0.1655936004298994, 'reg_lambda': 0.4545768981919575, 'subsample': 0.8144326236923287, 'min_child_weight': 0.0653527747504569, 'n_jobs': 4, 'learning_rate': 0.0027622138031668454}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:18, 172.86it/s]  1%|‚ñè         | 41/3257 [00:00<00:16, 198.28it/s]  2%|‚ñè         | 65/3257 [00:00<00:14, 216.76it/s]  3%|‚ñé         | 89/3257 [00:00<00:14, 222.10it/s]  3%|‚ñé         | 112/3257 [00:00<00:15, 202.88it/s]  4%|‚ñç         | 135/3257 [00:00<00:14, 209.49it/s]  5%|‚ñç         | 158/3257 [00:00<00:14, 209.05it/s]  6%|‚ñå         | 180/3257 [00:00<00:16, 191.97it/s]  6%|‚ñå         | 201/3257 [00:00<00:15, 195.78it/s]  7%|‚ñã         | 228/3257 [00:01<00:14, 216.14it/s]  8%|‚ñä         | 250/3257 [00:01<00:14, 211.81it/s]  8%|‚ñä         | 272/3257 [00:01<00:14, 206.26it/s]  9%|‚ñâ         | 299/3257 [00:01<00:13, 221.99it/s] 10%|‚ñâ         | 322/3257 [00:01<00:13, 223.49it/s] 11%|‚ñà         | 345/3257 [00:01<00:13, 213.75it/s] 11%|‚ñà‚ñè        | 367/3257 [00:01<00:13, 211.28it/s] 12%|‚ñà‚ñè        | 389/3257 [00:01<00:14, 202.46it/s] 13%|‚ñà‚ñé        | 413/3257 [00:01<00:13, 211.96it/s] 13%|‚ñà‚ñé        | 435/3257 [00:02<00:15, 183.77it/s] 14%|‚ñà‚ñç        | 456/3257 [00:02<00:14, 188.48it/s] 15%|‚ñà‚ñç        | 476/3257 [00:02<00:15, 183.06it/s] 15%|‚ñà‚ñå        | 496/3257 [00:02<00:14, 187.20it/s] 16%|‚ñà‚ñå        | 517/3257 [00:02<00:14, 191.01it/s] 16%|‚ñà‚ñã        | 537/3257 [00:02<00:14, 192.58it/s] 17%|‚ñà‚ñã        | 557/3257 [00:02<00:19, 136.77it/s] 18%|‚ñà‚ñä        | 574/3257 [00:03<00:19, 135.31it/s] 18%|‚ñà‚ñä        | 599/3257 [00:03<00:16, 158.73it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:15, 165.55it/s] 20%|‚ñà‚ñâ        | 643/3257 [00:03<00:14, 180.10it/s] 20%|‚ñà‚ñà        | 663/3257 [00:03<00:15, 171.47it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:14, 178.10it/s] 22%|‚ñà‚ñà‚ñè       | 705/3257 [00:03<00:13, 188.52it/s] 22%|‚ñà‚ñà‚ñè       | 725/3257 [00:03<00:13, 188.85it/s] 23%|‚ñà‚ñà‚ñé       | 745/3257 [00:03<00:13, 183.91it/s] 24%|‚ñà‚ñà‚ñé       | 768/3257 [00:04<00:12, 196.07it/s] 24%|‚ñà‚ñà‚ñç       | 788/3257 [00:04<00:13, 186.90it/s] 25%|‚ñà‚ñà‚ñç       | 808/3257 [00:04<00:12, 189.89it/s] 25%|‚ñà‚ñà‚ñå       | 828/3257 [00:04<00:13, 185.98it/s] 26%|‚ñà‚ñà‚ñå       | 847/3257 [00:04<00:13, 177.39it/s] 27%|‚ñà‚ñà‚ñã       | 867/3257 [00:04<00:13, 182.67it/s] 27%|‚ñà‚ñà‚ñã       | 886/3257 [00:04<00:13, 179.73it/s] 28%|‚ñà‚ñà‚ñä       | 907/3257 [00:04<00:12, 185.25it/s] 28%|‚ñà‚ñà‚ñä       | 928/3257 [00:04<00:12, 192.17it/s] 29%|‚ñà‚ñà‚ñâ       | 948/3257 [00:04<00:11, 193.17it/s] 30%|‚ñà‚ñà‚ñâ       | 969/3257 [00:05<00:11, 197.33it/s] 30%|‚ñà‚ñà‚ñà       | 989/3257 [00:05<00:12, 185.73it/s] 31%|‚ñà‚ñà‚ñà       | 1008/3257 [00:05<00:12, 181.69it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:05<00:12, 181.94it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1046/3257 [00:05<00:12, 182.89it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1065/3257 [00:05<00:11, 183.28it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1084/3257 [00:05<00:11, 184.12it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1103/3257 [00:05<00:11, 185.28it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1122/3257 [00:05<00:11, 184.84it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1141/3257 [00:06<00:11, 177.88it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1162/3257 [00:06<00:11, 184.78it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1181/3257 [00:06<00:11, 175.40it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1199/3257 [00:06<00:12, 168.35it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1216/3257 [00:06<00:12, 165.87it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1239/3257 [00:06<00:11, 183.31it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1258/3257 [00:06<00:11, 180.21it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1277/3257 [00:06<00:11, 176.86it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1295/3257 [00:06<00:11, 167.00it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1315/3257 [00:07<00:11, 175.58it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1335/3257 [00:07<00:10, 182.34it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1354/3257 [00:07<00:10, 174.82it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1372/3257 [00:07<00:10, 173.84it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1390/3257 [00:07<00:10, 170.54it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1413/3257 [00:07<00:09, 186.26it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1435/3257 [00:07<00:09, 193.83it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1459/3257 [00:07<00:08, 205.05it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1480/3257 [00:07<00:08, 201.41it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1503/3257 [00:08<00:08, 209.07it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1524/3257 [00:08<00:08, 195.24it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1544/3257 [00:08<00:09, 185.73it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1565/3257 [00:08<00:08, 188.51it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1585/3257 [00:08<00:08, 186.69it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1606/3257 [00:08<00:08, 190.86it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1629/3257 [00:08<00:08, 201.30it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1650/3257 [00:08<00:08, 186.62it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1669/3257 [00:08<00:08, 186.16it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1688/3257 [00:09<00:08, 185.79it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1708/3257 [00:09<00:08, 187.82it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:09<00:08, 184.75it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1747/3257 [00:09<00:08, 182.16it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1768/3257 [00:09<00:07, 187.94it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1792/3257 [00:09<00:07, 199.83it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1813/3257 [00:09<00:07, 189.36it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1833/3257 [00:09<00:10, 131.97it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1855/3257 [00:10<00:09, 149.97it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1879/3257 [00:10<00:08, 167.98it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1899/3257 [00:10<00:07, 173.76it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1919/3257 [00:10<00:07, 178.21it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1945/3257 [00:10<00:06, 198.56it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1971/3257 [00:10<00:06, 212.10it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1993/3257 [00:10<00:06, 199.64it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2014/3257 [00:10<00:06, 197.96it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2036/3257 [00:10<00:06, 202.43it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2057/3257 [00:11<00:06, 186.75it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2077/3257 [00:11<00:06, 189.67it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2097/3257 [00:11<00:06, 182.56it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2119/3257 [00:11<00:05, 191.26it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2139/3257 [00:11<00:06, 179.20it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2158/3257 [00:11<00:06, 181.43it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:11<00:05, 184.32it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2199/3257 [00:11<00:05, 190.08it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2219/3257 [00:11<00:05, 188.78it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2238/3257 [00:12<00:05, 188.88it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2260/3257 [00:12<00:05, 193.84it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2280/3257 [00:12<00:05, 190.71it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2303/3257 [00:12<00:04, 200.71it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2330/3257 [00:12<00:04, 218.00it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2357/3257 [00:12<00:03, 232.88it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2381/3257 [00:12<00:03, 222.10it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:12<00:03, 218.53it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2428/3257 [00:12<00:03, 216.51it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2450/3257 [00:13<00:03, 202.17it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2478/3257 [00:13<00:03, 220.59it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2504/3257 [00:13<00:03, 230.24it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2528/3257 [00:13<00:03, 230.13it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2552/3257 [00:13<00:03, 230.75it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2576/3257 [00:13<00:03, 214.66it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2598/3257 [00:13<00:03, 214.01it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:13<00:02, 236.70it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2652/3257 [00:13<00:02, 223.21it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2675/3257 [00:13<00:02, 220.02it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2698/3257 [00:14<00:02, 205.65it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2719/3257 [00:14<00:02, 201.55it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2746/3257 [00:14<00:02, 219.67it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2769/3257 [00:14<00:02, 210.78it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2795/3257 [00:14<00:02, 221.66it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2818/3257 [00:14<00:02, 214.25it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2840/3257 [00:14<00:01, 209.77it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2867/3257 [00:14<00:01, 226.30it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2890/3257 [00:14<00:01, 225.06it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2913/3257 [00:15<00:01, 211.34it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2935/3257 [00:15<00:01, 206.70it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2956/3257 [00:15<00:01, 192.37it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2978/3257 [00:15<00:01, 197.32it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3003/3257 [00:15<00:01, 209.11it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3025/3257 [00:15<00:01, 206.22it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3050/3257 [00:15<00:00, 217.62it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:15<00:00, 233.69it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3104/3257 [00:15<00:00, 236.94it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3128/3257 [00:16<00:00, 237.57it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3152/3257 [00:16<00:00, 221.33it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3175/3257 [00:16<00:00, 222.78it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3198/3257 [00:16<00:00, 213.54it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3220/3257 [00:16<00:00, 204.75it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3247/3257 [00:16<00:00, 222.38it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 195.15it/s]
2023-02-07 20:46:16.105 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:46:16,106][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d268,n5,mc3,s0.488958,t4>', 'datetime': '2023-02-07T20:46:16.106773', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:46:16,107][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:46:16,107][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:46:16,753][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 20:46:16,754][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:46:16,810][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 22723 unique words (71.45% of original 31803, drops 9080)', 'datetime': '2023-02-07T20:46:16.810680', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:46:16,811][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 5081575 word corpus (99.73% of original 5095118, drops 13543)', 'datetime': '2023-02-07T20:46:16.811065', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:46:16,885][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 20:46:16,886][gensim.models.word2vec][INFO] - sample=0.488958 downsamples 0 most-common words
[2023-02-07 20:46:16,886][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5081575 word corpus (100.0%% of prior 5081575)', 'datetime': '2023-02-07T20:46:16.886286', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:46:17,008][gensim.models.word2vec][INFO] - estimated required memory for 22723 words and 268 dimensions: 64222516 bytes
[2023-02-07 20:46:17,008][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:46:17,035][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 22723 vocabulary and 268 features, using sg=1 hs=0 sample=0.4889575206410361 negative=5 window=19 shrink_windows=True', 'datetime': '2023-02-07T20:46:17.035118', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:46:18,039][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 37.49% examples, 1944439 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:19,046][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 77.65% examples, 1981646 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:19,566][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5073452 effective words) took 2.5s, 2006261 effective words/s
[2023-02-07 20:46:20,573][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 56.25% examples, 2913962 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:21,336][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5073452 effective words) took 1.8s, 2870853 effective words/s
[2023-02-07 20:46:22,344][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 42.55% examples, 2217347 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:23,352][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 87.29% examples, 2220526 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:23,615][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5073452 effective words) took 2.3s, 2229037 effective words/s
[2023-02-07 20:46:24,618][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 58.06% examples, 3011464 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:25,315][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5073452 effective words) took 1.7s, 2989144 effective words/s
[2023-02-07 20:46:26,322][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 43.23% examples, 2242692 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:27,324][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 89.87% examples, 2283010 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:46:27,535][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5073452 effective words) took 2.2s, 2286814 effective words/s
[2023-02-07 20:46:28,538][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 43.35% examples, 2259427 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:29,545][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 88.30% examples, 2243332 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:29,806][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5073452 effective words) took 2.3s, 2236202 effective words/s
[2023-02-07 20:46:30,811][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 43.54% examples, 2256701 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:46:31,814][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 89.62% examples, 2279893 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:46:32,027][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5073452 effective words) took 2.2s, 2286387 effective words/s
[2023-02-07 20:46:33,036][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 41.63% examples, 2157397 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:34,036][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 84.83% examples, 2164082 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:34,374][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5073452 effective words) took 2.3s, 2163661 effective words/s
[2023-02-07 20:46:35,378][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 42.28% examples, 2207312 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:36,381][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 86.58% examples, 2211394 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:36,621][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5073452 effective words) took 2.2s, 2259407 effective words/s
[2023-02-07 20:46:37,624][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 43.02% examples, 2243909 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:46:38,626][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 88.30% examples, 2249035 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:38,876][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5073452 effective words) took 2.3s, 2251956 effective words/s
[2023-02-07 20:46:39,884][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 42.55% examples, 2213750 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:46:40,885][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 87.10% examples, 2223246 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:41,163][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5073452 effective words) took 2.3s, 2220616 effective words/s
[2023-02-07 20:46:42,166][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 44.43% examples, 2306116 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:43,124][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5073452 effective words) took 2.0s, 2589145 effective words/s
[2023-02-07 20:46:44,131][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 45.32% examples, 2340674 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:45,066][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5073452 effective words) took 1.9s, 2618886 effective words/s
[2023-02-07 20:46:46,071][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 42.80% examples, 2230387 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:47,074][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 88.79% examples, 2258219 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:47,309][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5073452 effective words) took 2.2s, 2264160 effective words/s
[2023-02-07 20:46:48,312][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 43.35% examples, 2260438 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:49,315][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 88.15% examples, 2244478 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:46:49,573][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5073452 effective words) took 2.3s, 2243527 effective words/s
[2023-02-07 20:46:49,573][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 76426770 raw words (76101780 effective words) took 32.5s, 2338856 effective words/s', 'datetime': '2023-02-07T20:46:49.573560', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:46:49.573 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:46:51,795][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204551-yqj6tqp9/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:46:51.795063', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:46:51,796][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:46:51,884][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204551-yqj6tqp9/files/../tmp/embedding_model.pt
2023-02-07 20:46:51.885 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:46:53.503 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:46:54.052 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:46:56.198 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1565703898740303, 'test_mae': 1.1082588587987574, 'test_r2': -2.4914717269582436}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.93
wandb: percentage 0.28551
wandb:   test_mae 1.10826
wandb:   test_mse 2.15657
wandb:    test_r2 -2.49147
wandb: 
wandb: üöÄ View run distinctive-sweep-98 at: https://wandb.ai/xiaoqiz/mof2vec/runs/yqj6tqp9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_204551-yqj6tqp9/logs
wandb: Agent Starting Run: 4ovj0n0g with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 1007
wandb: 	model.gensim.alpha: 0.3893450976376648
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.2474114375365493
wandb: 	model.gensim.vector_size: 254
wandb: 	model.gensim.window: 19
wandb: 	model.sklearn.learning_rate: 0.0003625634172295209
wandb: 	model.sklearn.max_depth: 55
wandb: 	model.sklearn.min_child_weight: 0.018325506613029056
wandb: 	model.sklearn.n_estimators: 731
wandb: 	model.sklearn.num_leaves: 451
wandb: 	model.sklearn.reg_alpha: 0.5213084411927823
wandb: 	model.sklearn.reg_lambda: 0.0038785434711283337
wandb: 	model.sklearn.subsample: 0.39211353389201253
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204705-4ovj0n0g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-99
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/4ovj0n0g
2023-02-07 20:47:13.210 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 20:47:13.210 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 1007 for sweep.
2023-02-07 20:47:13.211 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.3893450976376648 for sweep.
2023-02-07 20:47:13.211 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:47:13.211 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 20:47:13.211 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2474114375365493 for sweep.
2023-02-07 20:47:13.212 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 254 for sweep.
2023-02-07 20:47:13.212 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 19 for sweep.
2023-02-07 20:47:13.212 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0003625634172295209 for sweep.
2023-02-07 20:47:13.212 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 55 for sweep.
2023-02-07 20:47:13.213 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.018325506613029056 for sweep.
2023-02-07 20:47:13.213 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 731 for sweep.
2023-02-07 20:47:13.213 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 451 for sweep.
2023-02-07 20:47:13.213 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.5213084411927823 for sweep.
2023-02-07 20:47:13.214 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.0038785434711283337 for sweep.
2023-02-07 20:47:13.214 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.39211353389201253 for sweep.
2023-02-07 20:47:13.214 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:47:13.222 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204705-4ovj0n0g/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 1007, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 254, 'window': 19, 'min_count': 5, 'dm': 0, 'sample': 0.2474114375365493, 'workers': 4, 'alpha': 0.3893450976376648, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 731, 'max_depth': 55, 'num_leaves': 451, 'reg_alpha': 0.5213084411927823, 'reg_lambda': 0.0038785434711283337, 'subsample': 0.39211353389201253, 'min_child_weight': 0.018325506613029056, 'n_jobs': 4, 'learning_rate': 0.0003625634172295209}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 27/3257 [00:00<00:12, 261.79it/s]  2%|‚ñè         | 54/3257 [00:00<00:12, 262.91it/s]  3%|‚ñé         | 83/3257 [00:00<00:11, 270.32it/s]  3%|‚ñé         | 111/3257 [00:00<00:11, 263.67it/s]  4%|‚ñç         | 140/3257 [00:00<00:11, 269.85it/s]  5%|‚ñå         | 168/3257 [00:00<00:11, 266.43it/s]  6%|‚ñå         | 197/3257 [00:00<00:11, 272.28it/s]  7%|‚ñã         | 230/3257 [00:00<00:10, 287.49it/s]  8%|‚ñä         | 259/3257 [00:00<00:10, 284.93it/s]  9%|‚ñâ         | 293/3257 [00:01<00:09, 298.99it/s] 10%|‚ñà         | 326/3257 [00:01<00:09, 307.14it/s] 11%|‚ñà         | 357/3257 [00:01<00:09, 300.42it/s] 12%|‚ñà‚ñè        | 388/3257 [00:01<00:10, 279.17it/s] 13%|‚ñà‚ñé        | 420/3257 [00:01<00:09, 284.90it/s] 14%|‚ñà‚ñç        | 449/3257 [00:01<00:10, 262.38it/s] 15%|‚ñà‚ñç        | 476/3257 [00:01<00:10, 263.64it/s] 16%|‚ñà‚ñå        | 507/3257 [00:01<00:09, 275.62it/s] 16%|‚ñà‚ñã        | 537/3257 [00:01<00:09, 282.37it/s] 17%|‚ñà‚ñã        | 566/3257 [00:02<00:09, 272.11it/s] 18%|‚ñà‚ñä        | 594/3257 [00:02<00:09, 271.08it/s] 19%|‚ñà‚ñâ        | 622/3257 [00:02<00:09, 272.30it/s] 20%|‚ñà‚ñà        | 653/3257 [00:02<00:09, 281.68it/s] 21%|‚ñà‚ñà        | 682/3257 [00:02<00:09, 277.85it/s] 22%|‚ñà‚ñà‚ñè       | 710/3257 [00:02<00:09, 275.04it/s] 23%|‚ñà‚ñà‚ñé       | 738/3257 [00:02<00:09, 264.91it/s] 23%|‚ñà‚ñà‚ñé       | 765/3257 [00:02<00:09, 264.19it/s] 24%|‚ñà‚ñà‚ñç       | 792/3257 [00:02<00:09, 254.52it/s] 25%|‚ñà‚ñà‚ñå       | 819/3257 [00:02<00:09, 258.86it/s] 26%|‚ñà‚ñà‚ñå       | 845/3257 [00:03<00:09, 246.46it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:03<00:09, 257.95it/s] 28%|‚ñà‚ñà‚ñä       | 901/3257 [00:03<00:09, 260.28it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:03<00:08, 266.98it/s] 30%|‚ñà‚ñà‚ñâ       | 962/3257 [00:03<00:08, 278.29it/s] 30%|‚ñà‚ñà‚ñà       | 990/3257 [00:03<00:08, 273.13it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1019/3257 [00:03<00:08, 274.39it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:03<00:08, 264.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:03<00:07, 273.36it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1111/3257 [00:04<00:07, 281.89it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1140/3257 [00:04<00:07, 271.46it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:04<00:07, 275.31it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1198/3257 [00:04<00:07, 262.95it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1225/3257 [00:04<00:07, 264.79it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1255/3257 [00:04<00:07, 271.27it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1283/3257 [00:04<00:07, 266.55it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1310/3257 [00:05<00:11, 170.64it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1341/3257 [00:05<00:09, 198.70it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1367/3257 [00:05<00:08, 211.27it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1396/3257 [00:05<00:08, 229.41it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1431/3257 [00:05<00:07, 260.27it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1463/3257 [00:05<00:06, 274.61it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1497/3257 [00:05<00:06, 291.80it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1528/3257 [00:05<00:06, 284.22it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1558/3257 [00:05<00:06, 276.77it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1587/3257 [00:05<00:05, 280.14it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:06<00:05, 287.67it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1649/3257 [00:06<00:05, 278.14it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1678/3257 [00:06<00:05, 275.05it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1709/3257 [00:06<00:05, 283.33it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1738/3257 [00:06<00:05, 274.39it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1770/3257 [00:06<00:05, 286.44it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1800/3257 [00:06<00:05, 289.25it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1830/3257 [00:06<00:05, 281.95it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1863/3257 [00:06<00:04, 293.68it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1893/3257 [00:07<00:04, 291.09it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1924/3257 [00:07<00:04, 293.80it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1963/3257 [00:07<00:04, 321.21it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1996/3257 [00:07<00:03, 315.59it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2029/3257 [00:07<00:03, 316.60it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2061/3257 [00:07<00:04, 280.32it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2093/3257 [00:07<00:04, 288.60it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2123/3257 [00:07<00:04, 275.30it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2152/3257 [00:07<00:03, 277.38it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2182/3257 [00:08<00:03, 283.02it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2211/3257 [00:08<00:03, 284.21it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2240/3257 [00:08<00:03, 284.01it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2270/3257 [00:08<00:03, 283.47it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2300/3257 [00:08<00:03, 287.21it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2335/3257 [00:08<00:03, 304.56it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2368/3257 [00:08<00:02, 311.63it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2404/3257 [00:08<00:02, 322.23it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2437/3257 [00:08<00:02, 297.90it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2470/3257 [00:08<00:02, 305.89it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2504/3257 [00:09<00:02, 312.67it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2538/3257 [00:09<00:02, 319.37it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2571/3257 [00:09<00:02, 302.51it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2602/3257 [00:09<00:02, 303.46it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:09<00:01, 320.27it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2673/3257 [00:09<00:01, 310.66it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2705/3257 [00:09<00:01, 294.39it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2735/3257 [00:09<00:01, 292.78it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:09<00:01, 299.32it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2803/3257 [00:10<00:01, 311.52it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2835/3257 [00:10<00:02, 208.41it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2872/3257 [00:10<00:01, 242.56it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2901/3257 [00:10<00:01, 251.96it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2933/3257 [00:10<00:01, 268.79it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2963/3257 [00:10<00:01, 275.43it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2993/3257 [00:10<00:00, 276.65it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3026/3257 [00:10<00:00, 291.23it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3063/3257 [00:11<00:00, 311.86it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3096/3257 [00:11<00:00, 311.61it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3132/3257 [00:11<00:00, 324.81it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3165/3257 [00:11<00:00, 315.33it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3197/3257 [00:11<00:00, 312.94it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3229/3257 [00:11<00:00, 300.16it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 279.30it/s]
2023-02-07 20:47:25.215 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:47:25,216][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d254,n5,mc5,s0.247411,t4>', 'datetime': '2023-02-07T20:47:25.216788', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:47:25,218][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:47:25,218][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:47:25,462][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 20:47:25,462][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:47:25,472][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 3705 unique words (55.61% of original 6662, drops 2957)', 'datetime': '2023-02-07T20:47:25.472175', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:47:25,472][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 2904334 word corpus (99.75% of original 2911496, drops 7162)', 'datetime': '2023-02-07T20:47:25.472527', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:47:25,484][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 20:47:25,485][gensim.models.word2vec][INFO] - sample=0.247411 downsamples 0 most-common words
[2023-02-07 20:47:25,485][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2904334 word corpus (100.0%% of prior 2904334)', 'datetime': '2023-02-07T20:47:25.485375', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:47:25,505][gensim.models.word2vec][INFO] - estimated required memory for 3705 words and 254 dimensions: 13341572 bytes
[2023-02-07 20:47:25,506][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:47:25,512][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 3705 vocabulary and 254 features, using sg=1 hs=0 sample=0.2474114375365493 negative=5 window=19 shrink_windows=True', 'datetime': '2023-02-07T20:47:25.512538', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:47:26,205][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2907591 effective words) took 0.7s, 4210314 effective words/s
[2023-02-07 20:47:26,790][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2907591 effective words) took 0.6s, 4978262 effective words/s
[2023-02-07 20:47:27,405][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2907591 effective words) took 0.6s, 4740820 effective words/s
[2023-02-07 20:47:27,984][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2907591 effective words) took 0.6s, 5039543 effective words/s
[2023-02-07 20:47:28,560][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2907591 effective words) took 0.6s, 5060828 effective words/s
[2023-02-07 20:47:29,163][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2907591 effective words) took 0.6s, 4831128 effective words/s
[2023-02-07 20:47:29,795][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2907591 effective words) took 0.6s, 4610843 effective words/s
[2023-02-07 20:47:30,426][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2907591 effective words) took 0.6s, 4619158 effective words/s
[2023-02-07 20:47:31,060][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2907591 effective words) took 0.6s, 4612024 effective words/s
[2023-02-07 20:47:31,691][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2907591 effective words) took 0.6s, 4612700 effective words/s
[2023-02-07 20:47:32,317][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2907591 effective words) took 0.6s, 4658736 effective words/s
[2023-02-07 20:47:32,977][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2907591 effective words) took 0.7s, 4416509 effective words/s
[2023-02-07 20:47:33,623][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2907591 effective words) took 0.6s, 4514592 effective words/s
[2023-02-07 20:47:34,308][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2907591 effective words) took 0.7s, 4254710 effective words/s
[2023-02-07 20:47:34,969][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2907591 effective words) took 0.7s, 4415177 effective words/s
[2023-02-07 20:47:34,970][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 43672440 raw words (43613865 effective words) took 9.5s, 4611699 effective words/s', 'datetime': '2023-02-07T20:47:34.970168', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:47:34.970 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:47:36,260][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204705-4ovj0n0g/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:47:36.260192', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:47:36,261][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:47:36,285][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204705-4ovj0n0g/files/../tmp/embedding_model.pt
2023-02-07 20:47:36.286 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:47:37.776 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:47:38.335 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:47:40.051 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.7615341308510715, 'test_mae': 1.2723318177683884, 'test_r2': -4.312957920756694}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.0
wandb: percentage 0.44386
wandb:   test_mae 1.27233
wandb:   test_mse 2.76153
wandb:    test_r2 -4.31296
wandb: 
wandb: üöÄ View run ancient-sweep-99 at: https://wandb.ai/xiaoqiz/mof2vec/runs/4ovj0n0g
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_204705-4ovj0n0g/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: jphfbaht with config:
wandb: 	data.data.wl_step: 7
wandb: 	data.nn.batch_size: 847
wandb: 	model.gensim.alpha: 0.007036787283794739
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.23803240940019343
wandb: 	model.gensim.vector_size: 238
wandb: 	model.gensim.window: 6
wandb: 	model.sklearn.learning_rate: 0.1762767730932213
wandb: 	model.sklearn.max_depth: 52
wandb: 	model.sklearn.min_child_weight: 0.05259393153793146
wandb: 	model.sklearn.n_estimators: 3632
wandb: 	model.sklearn.num_leaves: 255
wandb: 	model.sklearn.reg_alpha: 0.383360629540834
wandb: 	model.sklearn.reg_lambda: 0.02408954350994583
wandb: 	model.sklearn.subsample: 0.345783726916281
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: wandb version 0.13.10 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204759-jphfbaht
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-100
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/o7w8sqx8
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/jphfbaht
2023-02-07 20:48:07.463 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 7 for sweep.
2023-02-07 20:48:07.463 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 847 for sweep.
2023-02-07 20:48:07.463 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.007036787283794739 for sweep.
2023-02-07 20:48:07.464 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 20:48:07.464 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 20:48:07.464 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.23803240940019343 for sweep.
2023-02-07 20:48:07.464 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 238 for sweep.
2023-02-07 20:48:07.465 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 6 for sweep.
2023-02-07 20:48:07.465 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.1762767730932213 for sweep.
2023-02-07 20:48:07.465 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 52 for sweep.
2023-02-07 20:48:07.465 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05259393153793146 for sweep.
2023-02-07 20:48:07.466 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3632 for sweep.
2023-02-07 20:48:07.466 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 255 for sweep.
2023-02-07 20:48:07.466 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.383360629540834 for sweep.
2023-02-07 20:48:07.466 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.02408954350994583 for sweep.
2023-02-07 20:48:07.467 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.345783726916281 for sweep.
2023-02-07 20:48:07.467 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 20:48:07.473 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'rsm, uptake_co2, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 512, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 7}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204759-jphfbaht/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 847, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 238, 'window': 6, 'min_count': 3, 'dm': 0, 'sample': 0.23803240940019343, 'workers': 4, 'alpha': 0.007036787283794739, 'epochs': 15}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3632, 'max_depth': 52, 'num_leaves': 255, 'reg_alpha': 0.383360629540834, 'reg_lambda': 0.02408954350994583, 'subsample': 0.345783726916281, 'min_child_weight': 0.05259393153793146, 'n_jobs': 4, 'learning_rate': 0.1762767730932213}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:18, 173.50it/s]  1%|          | 40/3257 [00:00<00:16, 197.41it/s]  2%|‚ñè         | 60/3257 [00:00<00:17, 185.77it/s]  3%|‚ñé         | 82/3257 [00:00<00:16, 198.13it/s]  3%|‚ñé         | 102/3257 [00:00<00:15, 198.60it/s]  4%|‚ñé         | 122/3257 [00:00<00:17, 179.97it/s]  4%|‚ñç         | 141/3257 [00:00<00:17, 178.00it/s]  5%|‚ñç         | 160/3257 [00:00<00:17, 176.23it/s]  5%|‚ñå         | 178/3257 [00:00<00:17, 175.53it/s]  6%|‚ñå         | 199/3257 [00:01<00:16, 184.61it/s]  7%|‚ñã         | 222/3257 [00:01<00:15, 197.15it/s]  8%|‚ñä         | 245/3257 [00:01<00:14, 203.01it/s]  8%|‚ñä         | 266/3257 [00:01<00:15, 196.51it/s]  9%|‚ñâ         | 291/3257 [00:01<00:14, 209.75it/s] 10%|‚ñâ         | 313/3257 [00:01<00:15, 195.54it/s] 10%|‚ñà         | 336/3257 [00:01<00:14, 202.37it/s] 11%|‚ñà         | 359/3257 [00:01<00:14, 204.60it/s] 12%|‚ñà‚ñè        | 380/3257 [00:01<00:15, 189.54it/s] 12%|‚ñà‚ñè        | 400/3257 [00:02<00:15, 189.17it/s] 13%|‚ñà‚ñé        | 421/3257 [00:02<00:14, 191.99it/s] 14%|‚ñà‚ñé        | 441/3257 [00:02<00:16, 172.71it/s] 14%|‚ñà‚ñç        | 462/3257 [00:02<00:15, 181.89it/s] 15%|‚ñà‚ñç        | 481/3257 [00:02<00:15, 179.89it/s] 15%|‚ñà‚ñå        | 504/3257 [00:02<00:14, 192.32it/s] 16%|‚ñà‚ñå        | 526/3257 [00:02<00:14, 193.75it/s] 17%|‚ñà‚ñã        | 546/3257 [00:02<00:13, 193.67it/s] 17%|‚ñà‚ñã        | 566/3257 [00:02<00:14, 183.73it/s] 18%|‚ñà‚ñä        | 585/3257 [00:03<00:14, 184.80it/s] 19%|‚ñà‚ñä        | 606/3257 [00:03<00:13, 191.01it/s] 19%|‚ñà‚ñâ        | 626/3257 [00:03<00:13, 192.20it/s] 20%|‚ñà‚ñâ        | 646/3257 [00:03<00:13, 191.13it/s] 20%|‚ñà‚ñà        | 666/3257 [00:03<00:14, 184.80it/s] 21%|‚ñà‚ñà        | 685/3257 [00:03<00:14, 182.87it/s] 22%|‚ñà‚ñà‚ñè       | 706/3257 [00:03<00:13, 190.18it/s] 22%|‚ñà‚ñà‚ñè       | 726/3257 [00:03<00:13, 184.29it/s] 23%|‚ñà‚ñà‚ñé       | 745/3257 [00:03<00:13, 183.27it/s] 24%|‚ñà‚ñà‚ñé       | 768/3257 [00:04<00:12, 195.49it/s] 24%|‚ñà‚ñà‚ñç       | 788/3257 [00:04<00:13, 187.70it/s] 25%|‚ñà‚ñà‚ñç       | 807/3257 [00:04<00:13, 188.02it/s] 25%|‚ñà‚ñà‚ñå       | 827/3257 [00:04<00:12, 189.16it/s] 26%|‚ñà‚ñà‚ñå       | 846/3257 [00:04<00:13, 174.93it/s] 27%|‚ñà‚ñà‚ñã       | 866/3257 [00:04<00:13, 179.73it/s] 27%|‚ñà‚ñà‚ñã       | 885/3257 [00:04<00:13, 174.17it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:04<00:12, 183.52it/s] 28%|‚ñà‚ñà‚ñä       | 926/3257 [00:04<00:12, 186.59it/s] 29%|‚ñà‚ñà‚ñâ       | 945/3257 [00:05<00:20, 115.49it/s] 30%|‚ñà‚ñà‚ñâ       | 965/3257 [00:05<00:17, 132.21it/s] 30%|‚ñà‚ñà‚ñà       | 982/3257 [00:05<00:16, 140.29it/s] 31%|‚ñà‚ñà‚ñà       | 999/3257 [00:05<00:15, 147.26it/s] 31%|‚ñà‚ñà‚ñà       | 1017/3257 [00:05<00:14, 155.36it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1035/3257 [00:05<00:13, 158.92it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1052/3257 [00:05<00:14, 157.45it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1073/3257 [00:05<00:12, 171.55it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:06<00:12, 169.35it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1113/3257 [00:06<00:11, 183.35it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1132/3257 [00:06<00:12, 172.17it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1150/3257 [00:06<00:12, 169.91it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:06<00:11, 180.17it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1190/3257 [00:06<00:12, 165.14it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1207/3257 [00:06<00:12, 165.81it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1230/3257 [00:06<00:11, 182.32it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:06<00:11, 178.23it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:07<00:10, 185.51it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1290/3257 [00:07<00:11, 174.49it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1309/3257 [00:07<00:11, 175.81it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1331/3257 [00:07<00:10, 186.11it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:07<00:10, 183.65it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1369/3257 [00:07<00:10, 184.86it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1388/3257 [00:07<00:10, 175.68it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1412/3257 [00:07<00:09, 191.84it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:07<00:09, 193.69it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1454/3257 [00:08<00:08, 200.61it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1475/3257 [00:08<00:08, 200.14it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1496/3257 [00:08<00:08, 200.75it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1517/3257 [00:08<00:08, 202.09it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1538/3257 [00:08<00:09, 177.94it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1557/3257 [00:08<00:09, 176.55it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1576/3257 [00:08<00:09, 172.81it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1595/3257 [00:08<00:09, 176.60it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:08<00:09, 181.61it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:09<00:09, 172.23it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1652/3257 [00:09<00:09, 169.13it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1670/3257 [00:09<00:09, 170.54it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1688/3257 [00:09<00:09, 168.12it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1706/3257 [00:09<00:09, 170.39it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1724/3257 [00:09<00:08, 173.06it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1742/3257 [00:09<00:09, 152.76it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1764/3257 [00:09<00:08, 168.82it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1784/3257 [00:09<00:08, 174.11it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1802/3257 [00:10<00:08, 172.28it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1820/3257 [00:10<00:08, 170.40it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1838/3257 [00:10<00:08, 164.24it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1855/3257 [00:10<00:08, 165.81it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1875/3257 [00:10<00:07, 174.72it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1893/3257 [00:10<00:08, 165.23it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1915/3257 [00:10<00:07, 179.71it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1934/3257 [00:10<00:07, 176.91it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1957/3257 [00:10<00:06, 190.49it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:11<00:06, 183.15it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1997/3257 [00:11<00:06, 183.19it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2016/3257 [00:11<00:06, 180.87it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2036/3257 [00:11<00:06, 184.98it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2055/3257 [00:11<00:06, 171.96it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2073/3257 [00:11<00:06, 173.54it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2091/3257 [00:11<00:06, 172.83it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2109/3257 [00:11<00:06, 168.58it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2126/3257 [00:11<00:07, 160.54it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2143/3257 [00:12<00:07, 156.46it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2163/3257 [00:12<00:06, 168.23it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:12<00:06, 171.16it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2201/3257 [00:12<00:05, 179.30it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2220/3257 [00:12<00:06, 172.08it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2238/3257 [00:12<00:06, 168.66it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2256/3257 [00:12<00:05, 170.51it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2274/3257 [00:13<00:10, 92.35it/s]  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2297/3257 [00:13<00:08, 116.06it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2315/3257 [00:13<00:07, 128.79it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2340/3257 [00:13<00:05, 154.89it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2363/3257 [00:13<00:05, 171.02it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2384/3257 [00:13<00:04, 180.45it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:13<00:04, 181.70it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2427/3257 [00:13<00:04, 190.88it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2448/3257 [00:13<00:04, 176.60it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2474/3257 [00:14<00:03, 196.62it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2496/3257 [00:14<00:03, 202.05it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2519/3257 [00:14<00:03, 204.06it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2540/3257 [00:14<00:03, 202.19it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2561/3257 [00:14<00:03, 192.48it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2581/3257 [00:14<00:03, 181.24it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2601/3257 [00:14<00:03, 185.63it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:14<00:03, 206.43it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2649/3257 [00:14<00:03, 193.83it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2669/3257 [00:15<00:03, 191.38it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2690/3257 [00:15<00:02, 196.41it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2710/3257 [00:15<00:03, 170.89it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:15<00:02, 179.43it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:15<00:02, 190.02it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2776/3257 [00:15<00:02, 183.00it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2799/3257 [00:15<00:02, 195.29it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2819/3257 [00:15<00:02, 180.01it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2838/3257 [00:16<00:02, 176.49it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2862/3257 [00:16<00:02, 192.76it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:16<00:01, 200.90it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2907/3257 [00:16<00:01, 187.48it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2929/3257 [00:16<00:01, 193.00it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2949/3257 [00:16<00:01, 183.47it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2970/3257 [00:16<00:01, 189.79it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2990/3257 [00:16<00:01, 181.98it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:16<00:01, 191.13it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3032/3257 [00:17<00:01, 193.00it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3054/3257 [00:17<00:01, 198.73it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3078/3257 [00:17<00:00, 209.25it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3100/3257 [00:17<00:00, 201.89it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3123/3257 [00:17<00:00, 209.36it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3145/3257 [00:17<00:00, 194.18it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3165/3257 [00:17<00:00, 191.67it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3185/3257 [00:17<00:00, 186.59it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:17<00:00, 192.12it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3227/3257 [00:18<00:00, 186.82it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3252/3257 [00:18<00:00, 203.27it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 179.42it/s]
2023-02-07 20:48:26.417 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 20:48:26,418][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d238,n5,mc3,s0.238032,t4>', 'datetime': '2023-02-07T20:48:26.418870', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 20:48:26,419][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 20:48:26,419][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 20:48:26,947][gensim.models.doc2vec][INFO] - collected 42701 word types and 3257 unique tags from a corpus of 3257 examples and 5822992 words
[2023-02-07 20:48:26,948][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 20:48:27,029][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 30516 unique words (71.46% of original 42701, drops 12185)', 'datetime': '2023-02-07T20:48:27.029853', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:48:27,030][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 5804704 word corpus (99.69% of original 5822992, drops 18288)', 'datetime': '2023-02-07T20:48:27.030256', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:48:27,130][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 42701 items
[2023-02-07 20:48:27,131][gensim.models.word2vec][INFO] - sample=0.238032 downsamples 0 most-common words
[2023-02-07 20:48:27,131][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5804704 word corpus (100.0%% of prior 5804704)', 'datetime': '2023-02-07T20:48:27.131661', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 20:48:27,299][gensim.models.word2vec][INFO] - estimated required memory for 30516 words and 238 dimensions: 77112528 bytes
[2023-02-07 20:48:27,299][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 20:48:27,330][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 30516 vocabulary and 238 features, using sg=1 hs=0 sample=0.23803240940019343 negative=5 window=6 shrink_windows=True', 'datetime': '2023-02-07T20:48:27.330412', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 20:48:28,336][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 36.35% examples, 2139141 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:29,338][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 76.27% examples, 2227428 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:29,879][gensim.models.word2vec][INFO] - EPOCH 0: training on 5822992 raw words (5778362 effective words) took 2.5s, 2268962 effective words/s
[2023-02-07 20:48:30,885][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 33.71% examples, 1969482 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:31,893][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 68.04% examples, 1999757 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:32,758][gensim.models.word2vec][INFO] - EPOCH 1: training on 5822992 raw words (5778362 effective words) took 2.9s, 2010043 effective words/s
[2023-02-07 20:48:33,761][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 36.11% examples, 2119193 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:34,763][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 71.69% examples, 2106610 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:35,503][gensim.models.word2vec][INFO] - EPOCH 2: training on 5822992 raw words (5778362 effective words) took 2.7s, 2106878 effective words/s
[2023-02-07 20:48:36,519][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 36.44% examples, 2116354 words/s, in_qsize 8, out_qsize 1
[2023-02-07 20:48:37,520][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 73.63% examples, 2140153 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:38,203][gensim.models.word2vec][INFO] - EPOCH 3: training on 5822992 raw words (5778362 effective words) took 2.7s, 2142099 effective words/s
[2023-02-07 20:48:39,210][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 36.32% examples, 2128022 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:40,212][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 73.32% examples, 2141936 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:40,894][gensim.models.word2vec][INFO] - EPOCH 4: training on 5822992 raw words (5778362 effective words) took 2.7s, 2149525 effective words/s
[2023-02-07 20:48:41,902][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 36.32% examples, 2123314 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:42,903][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 72.77% examples, 2127605 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:43,610][gensim.models.word2vec][INFO] - EPOCH 5: training on 5822992 raw words (5778362 effective words) took 2.7s, 2129102 effective words/s
[2023-02-07 20:48:44,619][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 36.44% examples, 2134164 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:45,623][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 73.81% examples, 2152584 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:46,299][gensim.models.word2vec][INFO] - EPOCH 6: training on 5822992 raw words (5778362 effective words) took 2.7s, 2151770 effective words/s
[2023-02-07 20:48:47,304][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 47.71% examples, 2813134 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:48,307][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 97.88% examples, 2826260 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:48,342][gensim.models.word2vec][INFO] - EPOCH 7: training on 5822992 raw words (5778362 effective words) took 2.0s, 2833173 effective words/s
[2023-02-07 20:48:49,349][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 38.66% examples, 2269108 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:48:50,353][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 75.28% examples, 2198789 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:48:51,012][gensim.models.word2vec][INFO] - EPOCH 8: training on 5822992 raw words (5778362 effective words) took 2.7s, 2165382 effective words/s
[2023-02-07 20:48:52,018][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 36.11% examples, 2116734 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:53,020][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 72.77% examples, 2130695 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:53,710][gensim.models.word2vec][INFO] - EPOCH 9: training on 5822992 raw words (5778362 effective words) took 2.7s, 2144720 effective words/s
[2023-02-07 20:48:54,716][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 35.71% examples, 2086787 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:55,722][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 71.72% examples, 2101665 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:56,450][gensim.models.word2vec][INFO] - EPOCH 10: training on 5822992 raw words (5778362 effective words) took 2.7s, 2112148 effective words/s
[2023-02-07 20:48:57,454][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 36.05% examples, 2111379 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:58,455][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 72.21% examples, 2118267 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:48:59,145][gensim.models.word2vec][INFO] - EPOCH 11: training on 5822992 raw words (5778362 effective words) took 2.7s, 2147455 effective words/s
[2023-02-07 20:49:00,154][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 42.09% examples, 2490875 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:49:01,155][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 80.75% examples, 2346521 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:49:01,635][gensim.models.word2vec][INFO] - EPOCH 12: training on 5822992 raw words (5778362 effective words) took 2.5s, 2324245 effective words/s
[2023-02-07 20:49:02,642][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 41.42% examples, 2450994 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:49:03,647][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 79.92% examples, 2326214 words/s, in_qsize 8, out_qsize 0
[2023-02-07 20:49:04,145][gensim.models.word2vec][INFO] - EPOCH 13: training on 5822992 raw words (5778362 effective words) took 2.5s, 2306796 effective words/s
[2023-02-07 20:49:05,153][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 40.90% examples, 2414915 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:49:06,155][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 79.37% examples, 2313132 words/s, in_qsize 7, out_qsize 0
[2023-02-07 20:49:06,673][gensim.models.word2vec][INFO] - EPOCH 14: training on 5822992 raw words (5778362 effective words) took 2.5s, 2288974 effective words/s
[2023-02-07 20:49:06,674][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 87344880 raw words (86675430 effective words) took 39.3s, 2203059 effective words/s', 'datetime': '2023-02-07T20:49:06.674040', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 20:49:06.674 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 20:49:09,639][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204759-jphfbaht/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T20:49:09.639513', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 20:49:09,641][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 20:49:09,779][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_204759-jphfbaht/files/../tmp/embedding_model.pt
2023-02-07 20:49:09.780 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 20:49:11.446 | INFO     | mofgraph2vec.data.datamodule:__init__:66 - Train: 2637 Valid: 294 Test: 326
2023-02-07 20:49:12.032 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 20:49:14.315 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0820994678770695, 'test_mae': 1.0586725661511094, 'test_r2': -1.7206859506825198}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.85
wandb: percentage 0.28536
wandb:   test_mae 1.05867
wandb:   test_mse 2.0821
wandb:    test_r2 -1.72069
wandb: 
wandb: üöÄ View run feasible-sweep-100 at: https://wandb.ai/xiaoqiz/mof2vec/runs/jphfbaht
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_204759-jphfbaht/logs
