2023-02-07 12:16:52.780 | DEBUG    | mofgraph2vec.trainer.sweep:sweep:19 - No sweep id provided, creating new sweep
wandb: WARNING Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.
wandb: WARNING To avoid this, please fix the sweep config schema violations below:
wandb: WARNING   Violation 1. model.sklearn.reg_alpha uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.
wandb: WARNING   Violation 2. model.gensim.alpha uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.
wandb: WARNING   Violation 3. model.sklearn.learning_rate uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.
wandb: WARNING   Violation 4. model.sklearn.reg_lambda uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.
Create sweep with ID: m4fg9fjd
Sweep URL: https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
[2023-02-07 12:16:58,451][wandb.agents.pyagent][INFO] - Starting sweep agent: entity=None, project=None, count=100
wandb: Agent Starting Run: a6m4gh6o with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 657
wandb: 	model.gensim.alpha: 0.0923879436024443
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.epochs: 92
wandb: 	model.gensim.min_count: 7
wandb: 	model.gensim.sample: 0.5412924749054749
wandb: 	model.gensim.vector_size: 128
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.00130906103419374
wandb: 	model.sklearn.max_depth: 84
wandb: 	model.sklearn.min_child_weight: 0.05166270703061661
wandb: 	model.sklearn.n_estimators: 2376
wandb: 	model.sklearn.num_leaves: 97
wandb: 	model.sklearn.reg_alpha: 0.009558575165932942
wandb: 	model.sklearn.reg_lambda: 0.9693537970479686
wandb: 	model.sklearn.subsample: 0.9972019835419286
wandb: Currently logged in as: xiaoqiz. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_121700-a6m4gh6o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/a6m4gh6o
2023-02-07 12:17:31.183 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 12:17:31.184 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 657 for sweep.
2023-02-07 12:17:31.184 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0923879436024443 for sweep.
2023-02-07 12:17:31.184 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 12:17:31.184 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 92 for sweep.
2023-02-07 12:17:31.185 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 7 for sweep.
2023-02-07 12:17:31.185 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5412924749054749 for sweep.
2023-02-07 12:17:31.185 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 128 for sweep.
2023-02-07 12:17:31.185 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 12:17:31.186 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.00130906103419374 for sweep.
2023-02-07 12:17:31.187 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 84 for sweep.
2023-02-07 12:17:31.187 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05166270703061661 for sweep.
2023-02-07 12:17:31.187 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2376 for sweep.
2023-02-07 12:17:31.187 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 97 for sweep.
2023-02-07 12:17:31.188 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.009558575165932942 for sweep.
2023-02-07 12:17:31.188 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.9693537970479686 for sweep.
2023-02-07 12:17:31.188 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.9972019835419286 for sweep.
2023-02-07 12:17:31.188 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:17:31.315 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_121700-a6m4gh6o/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 657, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 128, 'window': 3, 'min_count': 7, 'dm': 1, 'sample': 0.5412924749054749, 'workers': 4, 'alpha': 0.0923879436024443, 'epochs': 92}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2376, 'max_depth': 84, 'num_leaves': 97, 'reg_alpha': 0.009558575165932942, 'reg_lambda': 0.9693537970479686, 'subsample': 0.9972019835419286, 'min_child_weight': 0.05166270703061661, 'n_jobs': 4, 'learning_rate': 0.00130906103419374}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 11/3257 [00:00<00:30, 104.99it/s]  1%|          | 30/3257 [00:00<00:21, 150.97it/s]  1%|‚ñè         | 48/3257 [00:00<00:19, 162.78it/s]  2%|‚ñè         | 66/3257 [00:00<00:18, 168.43it/s]  3%|‚ñé         | 84/3257 [00:00<00:18, 167.53it/s]  3%|‚ñé         | 101/3257 [00:00<00:18, 167.68it/s]  4%|‚ñé         | 118/3257 [00:00<00:19, 160.91it/s]  4%|‚ñç         | 136/3257 [00:00<00:18, 165.99it/s]  5%|‚ñç         | 157/3257 [00:00<00:17, 177.81it/s]  5%|‚ñå         | 175/3257 [00:01<00:18, 165.83it/s]  6%|‚ñå         | 196/3257 [00:01<00:17, 177.31it/s]  7%|‚ñã         | 216/3257 [00:01<00:16, 180.55it/s]  7%|‚ñã         | 237/3257 [00:01<00:16, 185.59it/s]  8%|‚ñä         | 256/3257 [00:01<00:21, 136.89it/s]  8%|‚ñä         | 275/3257 [00:01<00:20, 148.40it/s]  9%|‚ñâ         | 297/3257 [00:01<00:17, 164.89it/s] 10%|‚ñâ         | 315/3257 [00:01<00:17, 165.62it/s] 10%|‚ñà         | 336/3257 [00:02<00:16, 173.43it/s] 11%|‚ñà         | 357/3257 [00:02<00:15, 182.24it/s] 12%|‚ñà‚ñè        | 376/3257 [00:02<00:16, 171.86it/s] 12%|‚ñà‚ñè        | 394/3257 [00:02<00:17, 166.62it/s] 13%|‚ñà‚ñé        | 414/3257 [00:02<00:16, 174.31it/s] 13%|‚ñà‚ñé        | 432/3257 [00:02<00:18, 154.85it/s] 14%|‚ñà‚ñç        | 449/3257 [00:02<00:18, 155.65it/s] 14%|‚ñà‚ñç        | 469/3257 [00:02<00:16, 167.11it/s] 15%|‚ñà‚ñç        | 487/3257 [00:02<00:16, 165.40it/s] 16%|‚ñà‚ñå        | 510/3257 [00:03<00:15, 180.26it/s] 16%|‚ñà‚ñå        | 529/3257 [00:03<00:15, 175.52it/s] 17%|‚ñà‚ñã        | 547/3257 [00:03<00:15, 174.61it/s] 17%|‚ñà‚ñã        | 565/3257 [00:03<00:16, 164.61it/s] 18%|‚ñà‚ñä        | 582/3257 [00:03<00:16, 159.28it/s] 19%|‚ñà‚ñä        | 603/3257 [00:03<00:15, 171.81it/s] 19%|‚ñà‚ñâ        | 621/3257 [00:03<00:15, 168.60it/s] 20%|‚ñà‚ñâ        | 640/3257 [00:03<00:15, 173.92it/s] 20%|‚ñà‚ñà        | 658/3257 [00:03<00:16, 158.51it/s] 21%|‚ñà‚ñà        | 678/3257 [00:04<00:15, 168.07it/s] 21%|‚ñà‚ñà‚ñè       | 696/3257 [00:04<00:15, 162.49it/s] 22%|‚ñà‚ñà‚ñè       | 717/3257 [00:04<00:14, 174.71it/s] 23%|‚ñà‚ñà‚ñé       | 735/3257 [00:04<00:15, 167.61it/s] 23%|‚ñà‚ñà‚ñé       | 752/3257 [00:04<00:15, 161.78it/s] 24%|‚ñà‚ñà‚ñé       | 771/3257 [00:04<00:14, 168.75it/s] 24%|‚ñà‚ñà‚ñç       | 789/3257 [00:04<00:15, 163.80it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:04<00:14, 165.15it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:04<00:14, 163.09it/s] 26%|‚ñà‚ñà‚ñå       | 840/3257 [00:05<00:14, 163.20it/s] 26%|‚ñà‚ñà‚ñã       | 857/3257 [00:05<00:14, 160.14it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:05<00:14, 160.71it/s] 27%|‚ñà‚ñà‚ñã       | 892/3257 [00:05<00:14, 162.57it/s] 28%|‚ñà‚ñà‚ñä       | 913/3257 [00:05<00:13, 173.81it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:05<00:13, 170.53it/s] 29%|‚ñà‚ñà‚ñâ       | 949/3257 [00:05<00:13, 171.52it/s] 30%|‚ñà‚ñà‚ñâ       | 968/3257 [00:05<00:13, 174.44it/s] 30%|‚ñà‚ñà‚ñà       | 986/3257 [00:05<00:13, 167.17it/s] 31%|‚ñà‚ñà‚ñà       | 1005/3257 [00:06<00:13, 171.47it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1023/3257 [00:06<00:13, 171.13it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1041/3257 [00:06<00:14, 153.38it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1060/3257 [00:06<00:13, 161.19it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:06<00:13, 161.99it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1097/3257 [00:06<00:13, 163.93it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:06<00:12, 167.47it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1134/3257 [00:06<00:12, 165.73it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1151/3257 [00:06<00:13, 161.02it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:07<00:12, 167.94it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1188/3257 [00:07<00:13, 151.16it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1204/3257 [00:07<00:13, 152.23it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1221/3257 [00:07<00:13, 156.48it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:07<00:11, 170.98it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1260/3257 [00:07<00:11, 168.94it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1278/3257 [00:07<00:12, 159.46it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1295/3257 [00:08<00:19, 100.73it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1311/3257 [00:08<00:17, 111.75it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:08<00:15, 123.61it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1346/3257 [00:08<00:14, 135.06it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1362/3257 [00:08<00:13, 138.65it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1378/3257 [00:08<00:13, 136.46it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1393/3257 [00:08<00:13, 136.94it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1413/3257 [00:08<00:12, 152.32it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:08<00:11, 157.44it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1452/3257 [00:09<00:10, 165.36it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1472/3257 [00:09<00:10, 171.40it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1490/3257 [00:09<00:10, 170.97it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:09<00:09, 178.49it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1529/3257 [00:09<00:10, 158.89it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1546/3257 [00:09<00:11, 149.09it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1563/3257 [00:09<00:11, 153.61it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1579/3257 [00:09<00:11, 149.13it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1598/3257 [00:09<00:10, 157.80it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1617/3257 [00:10<00:10, 163.10it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:10<00:10, 155.57it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1650/3257 [00:10<00:10, 151.20it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1666/3257 [00:10<00:10, 149.16it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1681/3257 [00:10<00:10, 147.50it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1698/3257 [00:10<00:10, 152.92it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:10<00:10, 152.72it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:10<00:10, 142.67it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1746/3257 [00:10<00:10, 143.53it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:11<00:09, 150.64it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1782/3257 [00:11<00:09, 160.99it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1799/3257 [00:11<00:09, 159.54it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1816/3257 [00:11<00:09, 151.93it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1832/3257 [00:11<00:09, 147.47it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1849/3257 [00:11<00:09, 152.84it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:11<00:08, 159.93it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1886/3257 [00:11<00:08, 161.74it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1904/3257 [00:11<00:08, 166.67it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1921/3257 [00:12<00:08, 162.22it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1945/3257 [00:12<00:07, 181.36it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1966/3257 [00:12<00:06, 187.75it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1985/3257 [00:12<00:07, 172.37it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:12<00:07, 173.48it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2023/3257 [00:12<00:07, 175.51it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2041/3257 [00:12<00:07, 163.85it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2058/3257 [00:12<00:07, 157.61it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2074/3257 [00:12<00:07, 158.01it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2090/3257 [00:13<00:07, 156.49it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2106/3257 [00:13<00:07, 155.59it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2122/3257 [00:13<00:07, 144.00it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2140/3257 [00:13<00:07, 151.76it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2156/3257 [00:13<00:07, 151.03it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2176/3257 [00:13<00:06, 164.29it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2193/3257 [00:13<00:06, 156.19it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2209/3257 [00:13<00:06, 151.28it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2226/3257 [00:13<00:06, 155.49it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2242/3257 [00:14<00:06, 150.52it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2259/3257 [00:14<00:06, 154.26it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2275/3257 [00:14<00:06, 141.27it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2296/3257 [00:14<00:06, 158.17it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2313/3257 [00:14<00:06, 157.33it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2335/3257 [00:14<00:05, 174.30it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2359/3257 [00:14<00:04, 191.36it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2379/3257 [00:14<00:04, 179.54it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2398/3257 [00:14<00:04, 182.32it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2417/3257 [00:15<00:05, 166.81it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2435/3257 [00:15<00:05, 157.80it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2452/3257 [00:15<00:05, 155.16it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2474/3257 [00:15<00:04, 171.61it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2492/3257 [00:15<00:04, 171.44it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2512/3257 [00:15<00:04, 178.09it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2531/3257 [00:15<00:04, 180.71it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2550/3257 [00:15<00:04, 174.13it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2568/3257 [00:15<00:04, 155.16it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2584/3257 [00:16<00:04, 154.97it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2600/3257 [00:16<00:07, 87.73it/s]  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2624/3257 [00:16<00:05, 114.39it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2641/3257 [00:16<00:04, 124.51it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2657/3257 [00:16<00:04, 129.16it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2674/3257 [00:16<00:04, 138.33it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2694/3257 [00:17<00:03, 146.41it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2711/3257 [00:17<00:03, 140.16it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2731/3257 [00:17<00:03, 154.13it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2750/3257 [00:17<00:03, 163.53it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:17<00:03, 156.11it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2787/3257 [00:17<00:02, 163.30it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:17<00:02, 167.80it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2824/3257 [00:17<00:02, 154.82it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2841/3257 [00:17<00:02, 158.01it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2865/3257 [00:18<00:02, 179.94it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:18<00:02, 182.03it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2905/3257 [00:18<00:02, 164.57it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2926/3257 [00:18<00:01, 175.60it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2945/3257 [00:18<00:01, 157.96it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2962/3257 [00:18<00:01, 159.74it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2979/3257 [00:18<00:01, 155.26it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2995/3257 [00:18<00:01, 155.95it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3014/3257 [00:18<00:01, 163.22it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3031/3257 [00:19<00:01, 162.70it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:19<00:01, 164.42it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3070/3257 [00:19<00:01, 176.09it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3088/3257 [00:19<00:01, 167.54it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3110/3257 [00:19<00:00, 179.30it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3129/3257 [00:19<00:00, 178.87it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3147/3257 [00:19<00:00, 162.46it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3164/3257 [00:19<00:00, 162.92it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3181/3257 [00:19<00:00, 155.79it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3201/3257 [00:20<00:00, 164.25it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3218/3257 [00:20<00:00, 154.00it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3238/3257 [00:20<00:00, 164.98it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3255/3257 [00:20<00:00, 162.11it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:20<00:00, 159.40it/s]
2023-02-07 12:17:59.694 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:17:59,740][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d128,n5,w3,mc7,s0.541292,t4>', 'datetime': '2023-02-07T12:17:59.696077', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:17:59,741][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:17:59,741][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:18:00,335][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 12:18:00,335][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:18:00,403][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 retains 23400 unique words (43.29% of original 54054, drops 30654)', 'datetime': '2023-02-07T12:18:00.403254', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:18:00,403][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 leaves 6462689 word corpus (98.65% of original 6550866, drops 88177)', 'datetime': '2023-02-07T12:18:00.403636', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:18:00,483][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 12:18:00,485][gensim.models.word2vec][INFO] - sample=0.541292 downsamples 0 most-common words
[2023-02-07 12:18:00,485][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6462689 word corpus (100.0%% of prior 6462689)', 'datetime': '2023-02-07T12:18:00.485241', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:18:00,622][gensim.models.word2vec][INFO] - estimated required memory for 23400 words and 128 dimensions: 37980584 bytes
[2023-02-07 12:18:00,623][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:18:00,636][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 23400 vocabulary and 128 features, using sg=0 hs=0 sample=0.5412924749054749 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T12:18:00.636525', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:18:01,643][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 41.42% examples, 2711803 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:02,645][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 91.37% examples, 2941005 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:02,802][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6408417 effective words) took 2.2s, 2963696 effective words/s
[2023-02-07 12:18:03,805][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 51.64% examples, 3375492 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:04,711][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6408417 effective words) took 1.9s, 3359028 effective words/s
[2023-02-07 12:18:05,713][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 52.04% examples, 3408549 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:18:06,592][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6408417 effective words) took 1.9s, 3410615 effective words/s
[2023-02-07 12:18:07,596][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 53.91% examples, 3532000 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:08,323][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6408417 effective words) took 1.7s, 3707203 effective words/s
[2023-02-07 12:18:09,325][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 59.96% examples, 3900687 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:09,949][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6408417 effective words) took 1.6s, 3943785 effective words/s
[2023-02-07 12:18:10,951][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 62.39% examples, 4045856 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:11,527][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6408417 effective words) took 1.6s, 4065450 effective words/s
[2023-02-07 12:18:12,529][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 59.53% examples, 3884342 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:13,199][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6408417 effective words) took 1.7s, 3834182 effective words/s
[2023-02-07 12:18:14,202][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 57.02% examples, 3730237 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:14,935][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6408417 effective words) took 1.7s, 3694171 effective words/s
[2023-02-07 12:18:15,937][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 56.74% examples, 3718184 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:16,641][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6408417 effective words) took 1.7s, 3760345 effective words/s
[2023-02-07 12:18:17,643][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 57.84% examples, 3785229 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:18,342][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6408417 effective words) took 1.7s, 3770393 effective words/s
[2023-02-07 12:18:19,344][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 58.83% examples, 3846700 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:18:20,010][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6408417 effective words) took 1.7s, 3843168 effective words/s
[2023-02-07 12:18:21,016][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 58.64% examples, 3827301 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:21,695][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6408417 effective words) took 1.7s, 3806094 effective words/s
[2023-02-07 12:18:22,700][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 57.57% examples, 3756807 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:23,386][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6408417 effective words) took 1.7s, 3793567 effective words/s
[2023-02-07 12:18:24,389][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 59.53% examples, 3880790 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:25,060][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6408417 effective words) took 1.7s, 3830753 effective words/s
[2023-02-07 12:18:26,063][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 58.80% examples, 3838785 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:26,739][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6408417 effective words) took 1.7s, 3821520 effective words/s
[2023-02-07 12:18:27,744][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 59.96% examples, 3891761 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:28,406][gensim.models.word2vec][INFO] - EPOCH 15: training on 6550866 raw words (6408417 effective words) took 1.7s, 3846513 effective words/s
[2023-02-07 12:18:29,409][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 57.60% examples, 3767084 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:30,079][gensim.models.word2vec][INFO] - EPOCH 16: training on 6550866 raw words (6408417 effective words) took 1.7s, 3833657 effective words/s
[2023-02-07 12:18:31,082][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 60.67% examples, 3941772 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:18:31,714][gensim.models.word2vec][INFO] - EPOCH 17: training on 6550866 raw words (6408417 effective words) took 1.6s, 3923393 effective words/s
[2023-02-07 12:18:32,716][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 60.09% examples, 3912366 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:33,371][gensim.models.word2vec][INFO] - EPOCH 18: training on 6550866 raw words (6408417 effective words) took 1.7s, 3871319 effective words/s
[2023-02-07 12:18:34,374][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 58.92% examples, 3854550 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:35,034][gensim.models.word2vec][INFO] - EPOCH 19: training on 6550866 raw words (6408417 effective words) took 1.7s, 3857307 effective words/s
[2023-02-07 12:18:36,036][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 58.61% examples, 3828502 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:36,700][gensim.models.word2vec][INFO] - EPOCH 20: training on 6550866 raw words (6408417 effective words) took 1.7s, 3848033 effective words/s
[2023-02-07 12:18:37,702][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 60.73% examples, 3954608 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:38,343][gensim.models.word2vec][INFO] - EPOCH 21: training on 6550866 raw words (6408417 effective words) took 1.6s, 3903529 effective words/s
[2023-02-07 12:18:39,345][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 60.73% examples, 3954213 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:39,978][gensim.models.word2vec][INFO] - EPOCH 22: training on 6550866 raw words (6408417 effective words) took 1.6s, 3923186 effective words/s
[2023-02-07 12:18:40,981][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 59.75% examples, 3892940 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:41,629][gensim.models.word2vec][INFO] - EPOCH 23: training on 6550866 raw words (6408417 effective words) took 1.6s, 3884153 effective words/s
[2023-02-07 12:18:42,631][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 58.80% examples, 3841114 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:43,277][gensim.models.word2vec][INFO] - EPOCH 24: training on 6550866 raw words (6408417 effective words) took 1.6s, 3891272 effective words/s
[2023-02-07 12:18:44,280][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 61.50% examples, 3995886 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:44,942][gensim.models.word2vec][INFO] - EPOCH 25: training on 6550866 raw words (6408417 effective words) took 1.7s, 3852003 effective words/s
[2023-02-07 12:18:45,947][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 56.09% examples, 3669647 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:46,693][gensim.models.word2vec][INFO] - EPOCH 26: training on 6550866 raw words (6408417 effective words) took 1.7s, 3663473 effective words/s
[2023-02-07 12:18:47,696][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 55.91% examples, 3660046 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:48,433][gensim.models.word2vec][INFO] - EPOCH 27: training on 6550866 raw words (6408417 effective words) took 1.7s, 3685886 effective words/s
[2023-02-07 12:18:49,435][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 54.65% examples, 3579640 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:50,192][gensim.models.word2vec][INFO] - EPOCH 28: training on 6550866 raw words (6408417 effective words) took 1.8s, 3646668 effective words/s
[2023-02-07 12:18:51,196][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 57.60% examples, 3760137 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:51,912][gensim.models.word2vec][INFO] - EPOCH 29: training on 6550866 raw words (6408417 effective words) took 1.7s, 3729700 effective words/s
[2023-02-07 12:18:52,914][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 58.21% examples, 3808010 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:53,614][gensim.models.word2vec][INFO] - EPOCH 30: training on 6550866 raw words (6408417 effective words) took 1.7s, 3767361 effective words/s
[2023-02-07 12:18:54,616][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 56.74% examples, 3719272 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:55,332][gensim.models.word2vec][INFO] - EPOCH 31: training on 6550866 raw words (6408417 effective words) took 1.7s, 3733130 effective words/s
[2023-02-07 12:18:56,334][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 56.09% examples, 3681263 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:57,054][gensim.models.word2vec][INFO] - EPOCH 32: training on 6550866 raw words (6408417 effective words) took 1.7s, 3725197 effective words/s
[2023-02-07 12:18:58,057][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 58.80% examples, 3837941 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:18:58,740][gensim.models.word2vec][INFO] - EPOCH 33: training on 6550866 raw words (6408417 effective words) took 1.7s, 3804643 effective words/s
[2023-02-07 12:18:59,743][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 58.80% examples, 3837162 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:00,426][gensim.models.word2vec][INFO] - EPOCH 34: training on 6550866 raw words (6408417 effective words) took 1.7s, 3804171 effective words/s
[2023-02-07 12:19:01,429][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 57.45% examples, 3758159 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:02,137][gensim.models.word2vec][INFO] - EPOCH 35: training on 6550866 raw words (6408417 effective words) took 1.7s, 3748383 effective words/s
[2023-02-07 12:19:03,142][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 55.60% examples, 3643475 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:03,881][gensim.models.word2vec][INFO] - EPOCH 36: training on 6550866 raw words (6408417 effective words) took 1.7s, 3679881 effective words/s
[2023-02-07 12:19:04,885][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 56.83% examples, 3723164 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:05,604][gensim.models.word2vec][INFO] - EPOCH 37: training on 6550866 raw words (6408417 effective words) took 1.7s, 3724653 effective words/s
[2023-02-07 12:19:06,606][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 58.61% examples, 3831519 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:07,290][gensim.models.word2vec][INFO] - EPOCH 38: training on 6550866 raw words (6408417 effective words) took 1.7s, 3805689 effective words/s
[2023-02-07 12:19:08,293][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 57.02% examples, 3729978 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:09,005][gensim.models.word2vec][INFO] - EPOCH 39: training on 6550866 raw words (6408417 effective words) took 1.7s, 3739580 effective words/s
[2023-02-07 12:19:10,007][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 58.27% examples, 3810680 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:10,656][gensim.models.word2vec][INFO] - EPOCH 40: training on 6550866 raw words (6408417 effective words) took 1.7s, 3883816 effective words/s
[2023-02-07 12:19:11,659][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 61.93% examples, 4019972 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:12,273][gensim.models.word2vec][INFO] - EPOCH 41: training on 6550866 raw words (6408417 effective words) took 1.6s, 3967479 effective words/s
[2023-02-07 12:19:13,274][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 61.19% examples, 3980900 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:13,901][gensim.models.word2vec][INFO] - EPOCH 42: training on 6550866 raw words (6408417 effective words) took 1.6s, 3938945 effective words/s
[2023-02-07 12:19:14,909][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 59.20% examples, 3842614 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:15,590][gensim.models.word2vec][INFO] - EPOCH 43: training on 6550866 raw words (6408417 effective words) took 1.7s, 3794899 effective words/s
[2023-02-07 12:19:16,595][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 57.02% examples, 3727287 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:17,303][gensim.models.word2vec][INFO] - EPOCH 44: training on 6550866 raw words (6408417 effective words) took 1.7s, 3745191 effective words/s
[2023-02-07 12:19:18,305][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 60.09% examples, 3909835 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:18,957][gensim.models.word2vec][INFO] - EPOCH 45: training on 6550866 raw words (6408417 effective words) took 1.7s, 3878035 effective words/s
[2023-02-07 12:19:19,959][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 60.73% examples, 3955773 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:20,599][gensim.models.word2vec][INFO] - EPOCH 46: training on 6550866 raw words (6408417 effective words) took 1.6s, 3905608 effective words/s
[2023-02-07 12:19:21,602][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 60.67% examples, 3942143 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:22,224][gensim.models.word2vec][INFO] - EPOCH 47: training on 6550866 raw words (6408417 effective words) took 1.6s, 3947454 effective words/s
[2023-02-07 12:19:23,227][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 59.10% examples, 3863496 words/s, in_qsize 6, out_qsize 1
[2023-02-07 12:19:23,860][gensim.models.word2vec][INFO] - EPOCH 48: training on 6550866 raw words (6408417 effective words) took 1.6s, 3919672 effective words/s
[2023-02-07 12:19:24,866][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 62.39% examples, 4029010 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:25,468][gensim.models.word2vec][INFO] - EPOCH 49: training on 6550866 raw words (6408417 effective words) took 1.6s, 3989283 effective words/s
[2023-02-07 12:19:26,470][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 59.96% examples, 3902922 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:27,119][gensim.models.word2vec][INFO] - EPOCH 50: training on 6550866 raw words (6408417 effective words) took 1.7s, 3883467 effective words/s
[2023-02-07 12:19:28,121][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 59.10% examples, 3868337 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:28,781][gensim.models.word2vec][INFO] - EPOCH 51: training on 6550866 raw words (6408417 effective words) took 1.7s, 3858035 effective words/s
[2023-02-07 12:19:29,816][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 58.64% examples, 3827783 words/s, in_qsize 6, out_qsize 1
[2023-02-07 12:19:30,461][gensim.models.word2vec][INFO] - EPOCH 52: training on 6550866 raw words (6408417 effective words) took 1.6s, 3885691 effective words/s
[2023-02-07 12:19:31,466][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 61.13% examples, 3967704 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:32,095][gensim.models.word2vec][INFO] - EPOCH 53: training on 6550866 raw words (6408417 effective words) took 1.6s, 3925017 effective words/s
[2023-02-07 12:19:33,098][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 61.50% examples, 3993655 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:33,706][gensim.models.word2vec][INFO] - EPOCH 54: training on 6550866 raw words (6408417 effective words) took 1.6s, 3981722 effective words/s
[2023-02-07 12:19:34,708][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 61.34% examples, 3988017 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:35,308][gensim.models.word2vec][INFO] - EPOCH 55: training on 6550866 raw words (6408417 effective words) took 1.6s, 4002204 effective words/s
[2023-02-07 12:19:36,311][gensim.models.word2vec][INFO] - EPOCH 56 - PROGRESS: at 59.96% examples, 3900885 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:36,935][gensim.models.word2vec][INFO] - EPOCH 56: training on 6550866 raw words (6408417 effective words) took 1.6s, 3943263 effective words/s
[2023-02-07 12:19:37,937][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 62.08% examples, 4031544 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:38,543][gensim.models.word2vec][INFO] - EPOCH 57: training on 6550866 raw words (6408417 effective words) took 1.6s, 3988198 effective words/s
[2023-02-07 12:19:39,545][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 63.22% examples, 4120262 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:40,105][gensim.models.word2vec][INFO] - EPOCH 58: training on 6550866 raw words (6408417 effective words) took 1.6s, 4104244 effective words/s
[2023-02-07 12:19:41,111][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 62.48% examples, 4042860 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:41,693][gensim.models.word2vec][INFO] - EPOCH 59: training on 6550866 raw words (6408417 effective words) took 1.6s, 4040730 effective words/s
[2023-02-07 12:19:42,696][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 62.39% examples, 4040382 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:43,260][gensim.models.word2vec][INFO] - EPOCH 60: training on 6550866 raw words (6408417 effective words) took 1.6s, 4092617 effective words/s
[2023-02-07 12:19:44,264][gensim.models.word2vec][INFO] - EPOCH 61 - PROGRESS: at 64.45% examples, 4205597 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:44,831][gensim.models.word2vec][INFO] - EPOCH 61: training on 6550866 raw words (6408417 effective words) took 1.6s, 4087302 effective words/s
[2023-02-07 12:19:45,835][gensim.models.word2vec][INFO] - EPOCH 62 - PROGRESS: at 61.93% examples, 4017807 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:46,437][gensim.models.word2vec][INFO] - EPOCH 62: training on 6550866 raw words (6408417 effective words) took 1.6s, 3995043 effective words/s
[2023-02-07 12:19:47,440][gensim.models.word2vec][INFO] - EPOCH 63 - PROGRESS: at 60.09% examples, 3907641 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:48,076][gensim.models.word2vec][INFO] - EPOCH 63: training on 6550866 raw words (6408417 effective words) took 1.6s, 3914342 effective words/s
[2023-02-07 12:19:49,079][gensim.models.word2vec][INFO] - EPOCH 64 - PROGRESS: at 60.67% examples, 3941185 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:49,683][gensim.models.word2vec][INFO] - EPOCH 64: training on 6550866 raw words (6408417 effective words) took 1.6s, 3991935 effective words/s
[2023-02-07 12:19:50,687][gensim.models.word2vec][INFO] - EPOCH 65 - PROGRESS: at 62.48% examples, 4049031 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:51,284][gensim.models.word2vec][INFO] - EPOCH 65: training on 6550866 raw words (6408417 effective words) took 1.6s, 4006194 effective words/s
[2023-02-07 12:19:52,286][gensim.models.word2vec][INFO] - EPOCH 66 - PROGRESS: at 62.27% examples, 4040506 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:52,871][gensim.models.word2vec][INFO] - EPOCH 66: training on 6550866 raw words (6408417 effective words) took 1.6s, 4041839 effective words/s
[2023-02-07 12:19:53,874][gensim.models.word2vec][INFO] - EPOCH 67 - PROGRESS: at 61.50% examples, 3993118 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:54,471][gensim.models.word2vec][INFO] - EPOCH 67: training on 6550866 raw words (6408417 effective words) took 1.6s, 4008219 effective words/s
[2023-02-07 12:19:55,472][gensim.models.word2vec][INFO] - EPOCH 68 - PROGRESS: at 60.98% examples, 3971523 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:56,081][gensim.models.word2vec][INFO] - EPOCH 68: training on 6550866 raw words (6408417 effective words) took 1.6s, 3982321 effective words/s
[2023-02-07 12:19:57,084][gensim.models.word2vec][INFO] - EPOCH 69 - PROGRESS: at 61.13% examples, 3976141 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:57,705][gensim.models.word2vec][INFO] - EPOCH 69: training on 6550866 raw words (6408417 effective words) took 1.6s, 3950663 effective words/s
[2023-02-07 12:19:58,710][gensim.models.word2vec][INFO] - EPOCH 70 - PROGRESS: at 62.20% examples, 4028186 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:19:59,301][gensim.models.word2vec][INFO] - EPOCH 70: training on 6550866 raw words (6408417 effective words) took 1.6s, 4017994 effective words/s
[2023-02-07 12:20:00,304][gensim.models.word2vec][INFO] - EPOCH 71 - PROGRESS: at 60.73% examples, 3949695 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:00,928][gensim.models.word2vec][INFO] - EPOCH 71: training on 6550866 raw words (6408417 effective words) took 1.6s, 3943009 effective words/s
[2023-02-07 12:20:01,932][gensim.models.word2vec][INFO] - EPOCH 72 - PROGRESS: at 60.18% examples, 3905942 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:02,549][gensim.models.word2vec][INFO] - EPOCH 72: training on 6550866 raw words (6408417 effective words) took 1.6s, 3955943 effective words/s
[2023-02-07 12:20:03,553][gensim.models.word2vec][INFO] - EPOCH 73 - PROGRESS: at 62.39% examples, 4040316 words/s, in_qsize 6, out_qsize 1
[2023-02-07 12:20:04,140][gensim.models.word2vec][INFO] - EPOCH 73: training on 6550866 raw words (6408417 effective words) took 1.6s, 4031566 effective words/s
[2023-02-07 12:20:05,143][gensim.models.word2vec][INFO] - EPOCH 74 - PROGRESS: at 62.39% examples, 4043775 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:05,729][gensim.models.word2vec][INFO] - EPOCH 74: training on 6550866 raw words (6408417 effective words) took 1.6s, 4037870 effective words/s
[2023-02-07 12:20:06,731][gensim.models.word2vec][INFO] - EPOCH 75 - PROGRESS: at 61.90% examples, 4020328 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:07,328][gensim.models.word2vec][INFO] - EPOCH 75: training on 6550866 raw words (6408417 effective words) took 1.6s, 4011667 effective words/s
[2023-02-07 12:20:08,329][gensim.models.word2vec][INFO] - EPOCH 76 - PROGRESS: at 58.92% examples, 3857451 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:08,985][gensim.models.word2vec][INFO] - EPOCH 76: training on 6550866 raw words (6408417 effective words) took 1.7s, 3869462 effective words/s
[2023-02-07 12:20:09,987][gensim.models.word2vec][INFO] - EPOCH 77 - PROGRESS: at 59.75% examples, 3896234 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:10,654][gensim.models.word2vec][INFO] - EPOCH 77: training on 6550866 raw words (6408417 effective words) took 1.7s, 3842992 effective words/s
[2023-02-07 12:20:11,658][gensim.models.word2vec][INFO] - EPOCH 78 - PROGRESS: at 59.96% examples, 3897544 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:12,285][gensim.models.word2vec][INFO] - EPOCH 78: training on 6550866 raw words (6408417 effective words) took 1.6s, 3934439 effective words/s
[2023-02-07 12:20:13,290][gensim.models.word2vec][INFO] - EPOCH 79 - PROGRESS: at 61.19% examples, 3966990 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:13,877][gensim.models.word2vec][INFO] - EPOCH 79: training on 6550866 raw words (6408417 effective words) took 1.6s, 4026976 effective words/s
[2023-02-07 12:20:14,879][gensim.models.word2vec][INFO] - EPOCH 80 - PROGRESS: at 63.89% examples, 4162066 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:15,407][gensim.models.word2vec][INFO] - EPOCH 80: training on 6550866 raw words (6408417 effective words) took 1.5s, 4192974 effective words/s
[2023-02-07 12:20:16,409][gensim.models.word2vec][INFO] - EPOCH 81 - PROGRESS: at 65.58% examples, 4287114 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:16,935][gensim.models.word2vec][INFO] - EPOCH 81: training on 6550866 raw words (6408417 effective words) took 1.5s, 4196668 effective words/s
[2023-02-07 12:20:17,936][gensim.models.word2vec][INFO] - EPOCH 82 - PROGRESS: at 62.85% examples, 4093289 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:18,511][gensim.models.word2vec][INFO] - EPOCH 82: training on 6550866 raw words (6408417 effective words) took 1.6s, 4068977 effective words/s
[2023-02-07 12:20:19,512][gensim.models.word2vec][INFO] - EPOCH 83 - PROGRESS: at 60.73% examples, 3956109 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:20,123][gensim.models.word2vec][INFO] - EPOCH 83: training on 6550866 raw words (6408417 effective words) took 1.6s, 3978143 effective words/s
[2023-02-07 12:20:21,127][gensim.models.word2vec][INFO] - EPOCH 84 - PROGRESS: at 59.75% examples, 3890378 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:21,754][gensim.models.word2vec][INFO] - EPOCH 84: training on 6550866 raw words (6408417 effective words) took 1.6s, 3933146 effective words/s
[2023-02-07 12:20:22,756][gensim.models.word2vec][INFO] - EPOCH 85 - PROGRESS: at 62.48% examples, 4057839 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:23,328][gensim.models.word2vec][INFO] - EPOCH 85: training on 6550866 raw words (6408417 effective words) took 1.6s, 4075441 effective words/s
[2023-02-07 12:20:24,330][gensim.models.word2vec][INFO] - EPOCH 86 - PROGRESS: at 65.77% examples, 4297750 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:24,823][gensim.models.word2vec][INFO] - EPOCH 86: training on 6550866 raw words (6408417 effective words) took 1.5s, 4290370 effective words/s
[2023-02-07 12:20:25,825][gensim.models.word2vec][INFO] - EPOCH 87 - PROGRESS: at 63.89% examples, 4160407 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:26,370][gensim.models.word2vec][INFO] - EPOCH 87: training on 6550866 raw words (6408417 effective words) took 1.5s, 4144339 effective words/s
[2023-02-07 12:20:27,373][gensim.models.word2vec][INFO] - EPOCH 88 - PROGRESS: at 64.35% examples, 4193510 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:27,891][gensim.models.word2vec][INFO] - EPOCH 88: training on 6550866 raw words (6408417 effective words) took 1.5s, 4216447 effective words/s
[2023-02-07 12:20:28,894][gensim.models.word2vec][INFO] - EPOCH 89 - PROGRESS: at 63.77% examples, 4147052 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:29,452][gensim.models.word2vec][INFO] - EPOCH 89: training on 6550866 raw words (6408417 effective words) took 1.6s, 4110332 effective words/s
[2023-02-07 12:20:30,454][gensim.models.word2vec][INFO] - EPOCH 90 - PROGRESS: at 65.34% examples, 4262333 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:30,952][gensim.models.word2vec][INFO] - EPOCH 90: training on 6550866 raw words (6408417 effective words) took 1.5s, 4274831 effective words/s
[2023-02-07 12:20:31,955][gensim.models.word2vec][INFO] - EPOCH 91 - PROGRESS: at 64.66% examples, 4209681 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:20:32,473][gensim.models.word2vec][INFO] - EPOCH 91: training on 6550866 raw words (6408417 effective words) took 1.5s, 4215074 effective words/s
[2023-02-07 12:20:32,474][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 602679672 raw words (589574364 effective words) took 151.8s, 3882936 effective words/s', 'datetime': '2023-02-07T12:20:32.474191', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:20:32.474 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:20:44,616][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_121700-a6m4gh6o/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:20:44.616506', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:20:44,618][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:20:44,704][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_121700-a6m4gh6o/files/../tmp/embedding_model.pt
2023-02-07 12:20:44.705 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:20:46.063 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:20:46.566 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: üöÄ View run warm-sweep-1 at: https://wandb.ai/xiaoqiz/mof2vec/runs/a6m4gh6o
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_121700-a6m4gh6o/logs
Run a6m4gh6o errored: InstantiationException("Error locating target 'mofgraph2vec.model.gbt.Regressor', set env var HYDRA_FULL_ERROR=1 to see chained exception.\nfull_key: model.sklearn")
wandb: ERROR Run a6m4gh6o errored: InstantiationException("Error locating target 'mofgraph2vec.model.gbt.Regressor', set env var HYDRA_FULL_ERROR=1 to see chained exception.\nfull_key: model.sklearn")
wandb: Agent Starting Run: p5l3h8e8 with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 194
wandb: 	model.gensim.alpha: 0.5416510079738404
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.epochs: 49
wandb: 	model.gensim.min_count: 9
wandb: 	model.gensim.sample: 0.5911663905662674
wandb: 	model.gensim.vector_size: 203
wandb: 	model.gensim.window: 16
wandb: 	model.sklearn.learning_rate: 0.2611626375071532
wandb: 	model.sklearn.max_depth: 41
wandb: 	model.sklearn.min_child_weight: 0.03818299690272906
wandb: 	model.sklearn.n_estimators: 4962
wandb: 	model.sklearn.num_leaves: 460
wandb: 	model.sklearn.reg_alpha: 0.9344510891999238
wandb: 	model.sklearn.reg_lambda: 0.01729955550392482
wandb: 	model.sklearn.subsample: 0.7946749141465637
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122057-p5l3h8e8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/p5l3h8e8
2023-02-07 12:21:05.244 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 12:21:05.245 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 194 for sweep.
2023-02-07 12:21:05.245 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.5416510079738404 for sweep.
2023-02-07 12:21:05.245 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 12:21:05.245 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 49 for sweep.
2023-02-07 12:21:05.246 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 9 for sweep.
2023-02-07 12:21:05.246 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5911663905662674 for sweep.
2023-02-07 12:21:05.246 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 203 for sweep.
2023-02-07 12:21:05.246 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 16 for sweep.
2023-02-07 12:21:05.246 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.2611626375071532 for sweep.
2023-02-07 12:21:05.247 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 41 for sweep.
2023-02-07 12:21:05.247 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03818299690272906 for sweep.
2023-02-07 12:21:05.247 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4962 for sweep.
2023-02-07 12:21:05.247 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 460 for sweep.
2023-02-07 12:21:05.247 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.9344510891999238 for sweep.
2023-02-07 12:21:05.248 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.01729955550392482 for sweep.
2023-02-07 12:21:05.248 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.7946749141465637 for sweep.
2023-02-07 12:21:05.248 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:21:05.254 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122057-p5l3h8e8/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 194, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 203, 'window': 16, 'min_count': 9, 'dm': 1, 'sample': 0.5911663905662674, 'workers': 4, 'alpha': 0.5416510079738404, 'epochs': 49}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4962, 'max_depth': 41, 'num_leaves': 460, 'reg_alpha': 0.9344510891999238, 'reg_lambda': 0.01729955550392482, 'subsample': 0.7946749141465637, 'min_child_weight': 0.03818299690272906, 'n_jobs': 4, 'learning_rate': 0.2611626375071532}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:20, 158.97it/s]  1%|          | 34/3257 [00:00<00:20, 160.97it/s]  2%|‚ñè         | 53/3257 [00:00<00:19, 164.21it/s]  2%|‚ñè         | 72/3257 [00:00<00:18, 171.71it/s]  3%|‚ñé         | 91/3257 [00:00<00:17, 175.99it/s]  3%|‚ñé         | 109/3257 [00:00<00:19, 164.75it/s]  4%|‚ñç         | 128/3257 [00:00<00:18, 171.90it/s]  5%|‚ñç         | 149/3257 [00:00<00:17, 180.06it/s]  5%|‚ñå         | 168/3257 [00:00<00:17, 174.73it/s]  6%|‚ñå         | 186/3257 [00:01<00:17, 172.44it/s]  6%|‚ñã         | 204/3257 [00:01<00:17, 170.83it/s]  7%|‚ñã         | 229/3257 [00:01<00:15, 191.64it/s]  8%|‚ñä         | 249/3257 [00:01<00:16, 187.71it/s]  8%|‚ñä         | 268/3257 [00:01<00:16, 180.27it/s]  9%|‚ñâ         | 294/3257 [00:01<00:14, 201.15it/s] 10%|‚ñâ         | 315/3257 [00:01<00:15, 191.04it/s] 10%|‚ñà         | 335/3257 [00:01<00:15, 192.25it/s] 11%|‚ñà         | 355/3257 [00:01<00:14, 194.10it/s] 12%|‚ñà‚ñè        | 375/3257 [00:02<00:15, 186.92it/s] 12%|‚ñà‚ñè        | 394/3257 [00:02<00:15, 180.55it/s] 13%|‚ñà‚ñé        | 415/3257 [00:02<00:15, 187.05it/s] 13%|‚ñà‚ñé        | 434/3257 [00:02<00:17, 158.88it/s] 14%|‚ñà‚ñç        | 453/3257 [00:02<00:16, 166.32it/s] 15%|‚ñà‚ñç        | 474/3257 [00:02<00:15, 176.72it/s] 15%|‚ñà‚ñå        | 493/3257 [00:02<00:15, 177.97it/s] 16%|‚ñà‚ñå        | 514/3257 [00:02<00:14, 186.58it/s] 16%|‚ñà‚ñã        | 534/3257 [00:02<00:14, 182.78it/s] 17%|‚ñà‚ñã        | 555/3257 [00:03<00:14, 188.36it/s] 18%|‚ñà‚ñä        | 575/3257 [00:03<00:16, 165.93it/s] 18%|‚ñà‚ñä        | 597/3257 [00:03<00:14, 179.82it/s] 19%|‚ñà‚ñâ        | 619/3257 [00:03<00:13, 189.99it/s] 20%|‚ñà‚ñâ        | 639/3257 [00:03<00:14, 184.00it/s] 20%|‚ñà‚ñà        | 658/3257 [00:03<00:15, 172.85it/s] 21%|‚ñà‚ñà        | 680/3257 [00:03<00:13, 184.64it/s] 21%|‚ñà‚ñà‚ñè       | 699/3257 [00:04<00:20, 127.37it/s] 22%|‚ñà‚ñà‚ñè       | 720/3257 [00:04<00:17, 144.42it/s] 23%|‚ñà‚ñà‚ñé       | 737/3257 [00:04<00:17, 147.75it/s] 23%|‚ñà‚ñà‚ñé       | 758/3257 [00:04<00:15, 162.14it/s] 24%|‚ñà‚ñà‚ñç       | 776/3257 [00:04<00:15, 165.12it/s] 24%|‚ñà‚ñà‚ñç       | 797/3257 [00:04<00:13, 176.85it/s] 25%|‚ñà‚ñà‚ñå       | 816/3257 [00:04<00:13, 179.67it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:04<00:13, 177.06it/s] 26%|‚ñà‚ñà‚ñå       | 854/3257 [00:04<00:14, 169.67it/s] 27%|‚ñà‚ñà‚ñã       | 873/3257 [00:05<00:13, 175.08it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:05<00:13, 174.76it/s] 28%|‚ñà‚ñà‚ñä       | 913/3257 [00:05<00:12, 187.07it/s] 29%|‚ñà‚ñà‚ñä       | 932/3257 [00:05<00:12, 184.65it/s] 29%|‚ñà‚ñà‚ñâ       | 954/3257 [00:05<00:11, 193.94it/s] 30%|‚ñà‚ñà‚ñâ       | 974/3257 [00:05<00:11, 190.38it/s] 31%|‚ñà‚ñà‚ñà       | 994/3257 [00:05<00:12, 181.69it/s] 31%|‚ñà‚ñà‚ñà       | 1013/3257 [00:05<00:12, 180.07it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1032/3257 [00:05<00:12, 178.35it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1050/3257 [00:05<00:12, 173.19it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1072/3257 [00:06<00:11, 186.13it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:06<00:12, 177.96it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1112/3257 [00:06<00:11, 185.98it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1131/3257 [00:06<00:12, 173.65it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1149/3257 [00:06<00:12, 171.41it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:06<00:11, 181.29it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1189/3257 [00:06<00:12, 160.51it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1206/3257 [00:06<00:12, 162.48it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1226/3257 [00:06<00:11, 172.29it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1245/3257 [00:07<00:11, 174.84it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1265/3257 [00:07<00:11, 180.74it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1284/3257 [00:07<00:12, 164.35it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1302/3257 [00:07<00:11, 167.22it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1322/3257 [00:07<00:11, 175.32it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1342/3257 [00:07<00:10, 181.46it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1361/3257 [00:07<00:10, 178.74it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1380/3257 [00:07<00:10, 176.55it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1398/3257 [00:07<00:10, 175.66it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1422/3257 [00:08<00:09, 191.82it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1442/3257 [00:08<00:09, 188.01it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1466/3257 [00:08<00:08, 199.41it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1487/3257 [00:08<00:08, 197.68it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:08<00:08, 207.87it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1532/3257 [00:08<00:09, 187.67it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1552/3257 [00:08<00:09, 178.01it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1572/3257 [00:08<00:09, 181.90it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1591/3257 [00:08<00:09, 181.46it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1614/3257 [00:09<00:08, 193.62it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:09<00:08, 180.93it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1653/3257 [00:09<00:08, 178.70it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1672/3257 [00:09<00:09, 173.49it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1690/3257 [00:09<00:09, 173.99it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1710/3257 [00:09<00:08, 180.88it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1729/3257 [00:09<00:09, 169.53it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1747/3257 [00:09<00:08, 171.37it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1767/3257 [00:09<00:08, 177.40it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1788/3257 [00:10<00:07, 186.56it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1807/3257 [00:10<00:08, 175.95it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1827/3257 [00:10<00:07, 181.48it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1847/3257 [00:10<00:07, 186.34it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:10<00:07, 189.99it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1889/3257 [00:10<00:07, 191.44it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1909/3257 [00:10<00:11, 119.24it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1926/3257 [00:11<00:10, 128.73it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1951/3257 [00:11<00:08, 155.31it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1973/3257 [00:11<00:07, 170.66it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1993/3257 [00:11<00:07, 170.59it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2012/3257 [00:11<00:07, 173.23it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2033/3257 [00:11<00:06, 181.05it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2053/3257 [00:11<00:07, 169.56it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2071/3257 [00:11<00:07, 167.24it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2091/3257 [00:11<00:06, 174.64it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2109/3257 [00:12<00:06, 174.78it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2127/3257 [00:12<00:06, 166.56it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2144/3257 [00:12<00:06, 160.63it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2166/3257 [00:12<00:06, 174.55it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2184/3257 [00:12<00:06, 173.96it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:12<00:05, 180.87it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2225/3257 [00:12<00:05, 177.72it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2243/3257 [00:12<00:05, 173.78it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2262/3257 [00:12<00:05, 177.67it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2280/3257 [00:12<00:05, 174.85it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2300/3257 [00:13<00:05, 181.21it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2324/3257 [00:13<00:04, 197.67it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2349/3257 [00:13<00:04, 212.05it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2371/3257 [00:13<00:04, 207.67it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2396/3257 [00:13<00:03, 217.88it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2418/3257 [00:13<00:04, 200.93it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2439/3257 [00:13<00:04, 193.66it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2459/3257 [00:13<00:04, 194.26it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2481/3257 [00:13<00:03, 198.59it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2505/3257 [00:14<00:03, 209.89it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2527/3257 [00:14<00:03, 206.86it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2549/3257 [00:14<00:03, 207.87it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2570/3257 [00:14<00:03, 186.94it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2590/3257 [00:14<00:03, 182.06it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2616/3257 [00:14<00:03, 202.84it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2639/3257 [00:14<00:02, 210.28it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2661/3257 [00:14<00:03, 191.40it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2684/3257 [00:14<00:02, 199.76it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2705/3257 [00:15<00:03, 182.33it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2724/3257 [00:15<00:02, 179.12it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2748/3257 [00:15<00:02, 193.82it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:15<00:02, 188.57it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2791/3257 [00:15<00:02, 196.99it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2812/3257 [00:15<00:02, 200.11it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2833/3257 [00:15<00:02, 184.58it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2853/3257 [00:15<00:02, 188.69it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:16<00:01, 211.22it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2903/3257 [00:16<00:01, 192.32it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2926/3257 [00:16<00:01, 202.11it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2947/3257 [00:16<00:01, 185.98it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2967/3257 [00:16<00:01, 182.86it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2986/3257 [00:16<00:01, 171.83it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3011/3257 [00:16<00:01, 189.22it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3031/3257 [00:16<00:01, 185.21it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3054/3257 [00:16<00:01, 195.57it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3078/3257 [00:17<00:00, 206.41it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3099/3257 [00:17<00:00, 199.68it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3123/3257 [00:17<00:00, 208.98it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3145/3257 [00:17<00:00, 192.45it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3165/3257 [00:17<00:00, 189.62it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3185/3257 [00:17<00:00, 185.58it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:17<00:00, 190.63it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3227/3257 [00:17<00:00, 189.51it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3252/3257 [00:17<00:00, 205.55it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:17<00:00, 181.14it/s]
2023-02-07 12:21:24.046 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:21:24,048][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d203,n5,w16,mc9,s0.591166,t4>', 'datetime': '2023-02-07T12:21:24.048307', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:21:24,049][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:21:24,049][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:21:24,588][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 12:21:24,588][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:21:24,638][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=9 retains 18315 unique words (33.88% of original 54054, drops 35739)', 'datetime': '2023-02-07T12:21:24.638613', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:21:24,638][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=9 leaves 6422560 word corpus (98.04% of original 6550866, drops 128306)', 'datetime': '2023-02-07T12:21:24.638934', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:21:24,694][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 12:21:24,695][gensim.models.word2vec][INFO] - sample=0.591166 downsamples 0 most-common words
[2023-02-07 12:21:24,695][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6422560 word corpus (100.0%% of prior 6422560)', 'datetime': '2023-02-07T12:21:24.695601', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:21:24,790][gensim.models.word2vec][INFO] - estimated required memory for 18315 words and 203 dimensions: 42197144 bytes
[2023-02-07 12:21:24,790][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:21:24,806][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 18315 vocabulary and 203 features, using sg=0 hs=0 sample=0.5911663905662674 negative=5 window=16 shrink_windows=True', 'datetime': '2023-02-07T12:21:24.806125', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:21:25,812][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 29.75% examples, 1889193 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:26,815][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 59.26% examples, 1920854 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:27,817][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 90.33% examples, 1922868 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:28,124][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6368654 effective words) took 3.3s, 1920371 effective words/s
[2023-02-07 12:21:29,127][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 30.06% examples, 1918834 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:30,133][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 59.10% examples, 1916226 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:31,136][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 89.16% examples, 1899983 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:31,470][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6368654 effective words) took 3.3s, 1904059 effective words/s
[2023-02-07 12:21:32,472][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 29.08% examples, 1855101 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:33,476][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 57.84% examples, 1877832 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:34,476][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 87.96% examples, 1876592 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:34,861][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6368654 effective words) took 3.4s, 1878651 effective words/s
[2023-02-07 12:21:35,863][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 29.75% examples, 1897347 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:36,868][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 58.12% examples, 1885742 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:37,868][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 88.58% examples, 1885776 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:38,263][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6368654 effective words) took 3.4s, 1873201 effective words/s
[2023-02-07 12:21:39,271][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 27.33% examples, 1729940 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:40,272][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 54.10% examples, 1755483 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:21:41,278][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 82.13% examples, 1749968 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:41,884][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6368654 effective words) took 3.6s, 1759020 effective words/s
[2023-02-07 12:21:42,888][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 27.85% examples, 1777985 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:21:43,893][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 55.63% examples, 1803544 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:44,896][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 84.59% examples, 1805207 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:45,416][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6368654 effective words) took 3.5s, 1804164 effective words/s
[2023-02-07 12:21:46,421][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 27.17% examples, 1738655 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:47,422][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 54.34% examples, 1766044 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:21:48,427][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 82.78% examples, 1764934 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:21:49,005][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6368654 effective words) took 3.6s, 1775278 effective words/s
[2023-02-07 12:21:50,012][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 28.55% examples, 1811681 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:21:51,015][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 56.74% examples, 1840754 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:52,018][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 85.48% examples, 1823914 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:52,489][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6368654 effective words) took 3.5s, 1828532 effective words/s
[2023-02-07 12:21:53,495][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 27.17% examples, 1738175 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:54,495][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 54.19% examples, 1763118 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:21:55,500][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 82.13% examples, 1753354 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:56,146][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6368654 effective words) took 3.7s, 1742736 effective words/s
[2023-02-07 12:21:57,150][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 27.54% examples, 1758779 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:58,159][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 54.19% examples, 1756679 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:21:59,161][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 83.11% examples, 1775115 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:21:59,727][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6368654 effective words) took 3.6s, 1779175 effective words/s
[2023-02-07 12:22:00,729][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 27.85% examples, 1779832 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:22:01,733][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 56.74% examples, 1844223 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:02,735][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 88.42% examples, 1881847 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:03,091][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6368654 effective words) took 3.4s, 1893783 effective words/s
[2023-02-07 12:22:04,095][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 29.87% examples, 1902934 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:05,095][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 59.10% examples, 1920531 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:06,109][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 89.99% examples, 1909921 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:22:06,428][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6368654 effective words) took 3.3s, 1909465 effective words/s
[2023-02-07 12:22:07,437][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 30.12% examples, 1915647 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:08,441][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 59.26% examples, 1916473 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:09,446][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 90.48% examples, 1920882 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:09,737][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6368654 effective words) took 3.3s, 1925106 effective words/s
[2023-02-07 12:22:10,739][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 30.06% examples, 1920570 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:11,740][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 58.80% examples, 1908019 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:12,747][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 89.16% examples, 1901859 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:22:13,074][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6368654 effective words) took 3.3s, 1909224 effective words/s
[2023-02-07 12:22:14,077][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 29.08% examples, 1853670 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:15,081][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 57.35% examples, 1860116 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:16,084][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 85.94% examples, 1832711 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:22:16,547][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6368654 effective words) took 3.5s, 1834617 effective words/s
[2023-02-07 12:22:17,554][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 28.55% examples, 1812596 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:18,559][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 56.19% examples, 1823655 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:19,560][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 86.52% examples, 1845462 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:20,000][gensim.models.word2vec][INFO] - EPOCH 15: training on 6550866 raw words (6368654 effective words) took 3.5s, 1845196 effective words/s
[2023-02-07 12:22:21,007][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 29.23% examples, 1854918 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:22,009][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 57.45% examples, 1862115 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:23,010][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 86.92% examples, 1857119 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:23,431][gensim.models.word2vec][INFO] - EPOCH 16: training on 6550866 raw words (6368654 effective words) took 3.4s, 1857043 effective words/s
[2023-02-07 12:22:24,433][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 28.55% examples, 1821309 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:25,436][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 55.48% examples, 1807144 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:26,438][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 83.33% examples, 1784923 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:27,004][gensim.models.word2vec][INFO] - EPOCH 17: training on 6550866 raw words (6368654 effective words) took 3.6s, 1783136 effective words/s
[2023-02-07 12:22:28,016][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 27.85% examples, 1762743 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:22:29,017][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 55.20% examples, 1786411 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:22:30,023][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 84.71% examples, 1803647 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:30,538][gensim.models.word2vec][INFO] - EPOCH 18: training on 6550866 raw words (6368654 effective words) took 3.5s, 1802924 effective words/s
[2023-02-07 12:22:31,543][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 29.54% examples, 1876973 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:32,543][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 57.35% examples, 1861462 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:33,548][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 85.94% examples, 1833372 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:34,007][gensim.models.word2vec][INFO] - EPOCH 19: training on 6550866 raw words (6368654 effective words) took 3.5s, 1836303 effective words/s
[2023-02-07 12:22:35,010][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 28.03% examples, 1787347 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:36,013][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 56.31% examples, 1829027 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:37,016][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 85.94% examples, 1833963 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:37,485][gensim.models.word2vec][INFO] - EPOCH 20: training on 6550866 raw words (6368654 effective words) took 3.5s, 1831871 effective words/s
[2023-02-07 12:22:38,489][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 28.34% examples, 1805076 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:39,492][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 55.88% examples, 1813737 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:40,493][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 84.99% examples, 1817725 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:40,981][gensim.models.word2vec][INFO] - EPOCH 21: training on 6550866 raw words (6368654 effective words) took 3.5s, 1822291 effective words/s
[2023-02-07 12:22:41,983][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 29.08% examples, 1856345 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:42,986][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 57.02% examples, 1853926 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:43,992][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 85.97% examples, 1833769 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:44,446][gensim.models.word2vec][INFO] - EPOCH 22: training on 6550866 raw words (6368654 effective words) took 3.5s, 1838933 effective words/s
[2023-02-07 12:22:45,457][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 27.85% examples, 1764971 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:46,461][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 54.47% examples, 1762838 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:47,462][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 84.16% examples, 1795303 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:47,968][gensim.models.word2vec][INFO] - EPOCH 23: training on 6550866 raw words (6368654 effective words) took 3.5s, 1808839 effective words/s
[2023-02-07 12:22:48,972][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 28.55% examples, 1818161 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:49,976][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 56.19% examples, 1827825 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:50,979][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 84.74% examples, 1807208 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:51,505][gensim.models.word2vec][INFO] - EPOCH 24: training on 6550866 raw words (6368654 effective words) took 3.5s, 1801375 effective words/s
[2023-02-07 12:22:52,507][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 27.11% examples, 1735142 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:53,513][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 54.53% examples, 1769587 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:22:54,517][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 83.36% examples, 1785311 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:55,049][gensim.models.word2vec][INFO] - EPOCH 25: training on 6550866 raw words (6368654 effective words) took 3.5s, 1797726 effective words/s
[2023-02-07 12:22:56,051][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 28.74% examples, 1838894 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:57,051][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 55.48% examples, 1809929 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:58,053][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 84.71% examples, 1812628 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:22:58,567][gensim.models.word2vec][INFO] - EPOCH 26: training on 6550866 raw words (6368654 effective words) took 3.5s, 1811258 effective words/s
[2023-02-07 12:22:59,570][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 28.34% examples, 1806131 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:00,570][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 56.00% examples, 1821669 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:01,571][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 85.05% examples, 1819631 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:02,059][gensim.models.word2vec][INFO] - EPOCH 27: training on 6550866 raw words (6368654 effective words) took 3.5s, 1824197 effective words/s
[2023-02-07 12:23:03,061][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 27.76% examples, 1770310 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:04,071][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 54.53% examples, 1765237 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:05,073][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 82.32% examples, 1754928 words/s, in_qsize 8, out_qsize 1
[2023-02-07 12:23:05,691][gensim.models.word2vec][INFO] - EPOCH 28: training on 6550866 raw words (6368654 effective words) took 3.6s, 1754474 effective words/s
[2023-02-07 12:23:06,695][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 28.34% examples, 1803631 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:07,698][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 55.48% examples, 1805423 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:08,702][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 84.59% examples, 1805487 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:09,218][gensim.models.word2vec][INFO] - EPOCH 29: training on 6550866 raw words (6368654 effective words) took 3.5s, 1806464 effective words/s
[2023-02-07 12:23:10,221][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 25.79% examples, 1639769 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:11,235][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 51.89% examples, 1676721 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:12,235][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 79.37% examples, 1695746 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:12,961][gensim.models.word2vec][INFO] - EPOCH 30: training on 6550866 raw words (6368654 effective words) took 3.7s, 1702070 effective words/s
[2023-02-07 12:23:13,970][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 29.08% examples, 1841465 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:14,971][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 56.83% examples, 1843995 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:15,975][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 86.12% examples, 1833650 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:16,441][gensim.models.word2vec][INFO] - EPOCH 31: training on 6550866 raw words (6368654 effective words) took 3.5s, 1830608 effective words/s
[2023-02-07 12:23:17,448][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 26.56% examples, 1689109 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:23:18,451][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 53.12% examples, 1725041 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:19,451][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 81.15% examples, 1732201 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:20,113][gensim.models.word2vec][INFO] - EPOCH 32: training on 6550866 raw words (6368654 effective words) took 3.7s, 1735139 effective words/s
[2023-02-07 12:23:21,121][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 29.08% examples, 1845772 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:22,124][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 57.14% examples, 1851261 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:23,128][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 86.83% examples, 1851847 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:23,556][gensim.models.word2vec][INFO] - EPOCH 33: training on 6550866 raw words (6368654 effective words) took 3.4s, 1850673 effective words/s
[2023-02-07 12:23:24,559][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 29.54% examples, 1880718 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:25,560][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 57.72% examples, 1875241 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:26,564][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 88.09% examples, 1877616 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:26,951][gensim.models.word2vec][INFO] - EPOCH 34: training on 6550866 raw words (6368654 effective words) took 3.4s, 1876644 effective words/s
[2023-02-07 12:23:27,954][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 28.74% examples, 1836839 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:28,955][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 56.74% examples, 1847359 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:29,956][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 86.28% examples, 1841539 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:30,422][gensim.models.word2vec][INFO] - EPOCH 35: training on 6550866 raw words (6368654 effective words) took 3.5s, 1835659 effective words/s
[2023-02-07 12:23:31,426][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 28.55% examples, 1818547 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:32,432][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 57.02% examples, 1849763 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:33,434][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 87.57% examples, 1868939 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:23:33,834][gensim.models.word2vec][INFO] - EPOCH 36: training on 6550866 raw words (6368654 effective words) took 3.4s, 1867421 effective words/s
[2023-02-07 12:23:34,843][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 29.08% examples, 1843513 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:35,845][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 57.60% examples, 1865263 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:36,849][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 86.83% examples, 1852255 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:37,267][gensim.models.word2vec][INFO] - EPOCH 37: training on 6550866 raw words (6368654 effective words) took 3.4s, 1855932 effective words/s
[2023-02-07 12:23:38,274][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 27.11% examples, 1727085 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:39,277][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 55.48% examples, 1802655 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:40,281][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 86.40% examples, 1842049 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:40,708][gensim.models.word2vec][INFO] - EPOCH 38: training on 6550866 raw words (6368654 effective words) took 3.4s, 1852110 effective words/s
[2023-02-07 12:23:41,710][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 30.43% examples, 1945198 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:42,711][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 60.09% examples, 1942716 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:43,713][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 90.97% examples, 1939962 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:43,985][gensim.models.word2vec][INFO] - EPOCH 39: training on 6550866 raw words (6368654 effective words) took 3.3s, 1943936 effective words/s
[2023-02-07 12:23:44,993][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 30.61% examples, 1946269 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:23:45,993][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 60.58% examples, 1950858 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:46,996][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 90.97% examples, 1935787 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:47,269][gensim.models.word2vec][INFO] - EPOCH 40: training on 6550866 raw words (6368654 effective words) took 3.3s, 1939953 effective words/s
[2023-02-07 12:23:48,275][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 28.74% examples, 1831724 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:49,277][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 57.02% examples, 1851590 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:50,282][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 86.80% examples, 1850232 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:50,702][gensim.models.word2vec][INFO] - EPOCH 41: training on 6550866 raw words (6368654 effective words) took 3.4s, 1856044 effective words/s
[2023-02-07 12:23:51,703][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 30.03% examples, 1913838 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:52,709][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 60.09% examples, 1939064 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:53,710][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 91.59% examples, 1955789 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:53,963][gensim.models.word2vec][INFO] - EPOCH 42: training on 6550866 raw words (6368654 effective words) took 3.3s, 1953706 effective words/s
[2023-02-07 12:23:54,967][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 30.30% examples, 1933404 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:55,968][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 59.53% examples, 1929363 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:56,976][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 90.45% examples, 1922878 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:57,266][gensim.models.word2vec][INFO] - EPOCH 43: training on 6550866 raw words (6368654 effective words) took 3.3s, 1928570 effective words/s
[2023-02-07 12:23:58,280][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 27.85% examples, 1759280 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:23:59,293][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 55.48% examples, 1788057 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:24:00,294][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 83.36% examples, 1775872 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:24:00,844][gensim.models.word2vec][INFO] - EPOCH 44: training on 6550866 raw words (6368654 effective words) took 3.6s, 1780945 effective words/s
[2023-02-07 12:24:01,846][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 28.55% examples, 1822354 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:24:02,846][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 57.02% examples, 1856609 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:24:03,846][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 88.36% examples, 1885382 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:24:04,205][gensim.models.word2vec][INFO] - EPOCH 45: training on 6550866 raw words (6368654 effective words) took 3.4s, 1895815 effective words/s
[2023-02-07 12:24:05,208][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 30.52% examples, 1952033 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:24:06,208][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 60.85% examples, 1967587 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:24:07,212][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 91.46% examples, 1951102 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:24:07,477][gensim.models.word2vec][INFO] - EPOCH 46: training on 6550866 raw words (6368654 effective words) took 3.3s, 1947178 effective words/s
[2023-02-07 12:24:08,481][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 29.90% examples, 1899763 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:24:09,487][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 58.89% examples, 1905693 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:24:10,496][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 88.85% examples, 1887241 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:24:10,849][gensim.models.word2vec][INFO] - EPOCH 47: training on 6550866 raw words (6368654 effective words) took 3.4s, 1889899 effective words/s
[2023-02-07 12:24:11,851][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 29.54% examples, 1881412 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:24:12,852][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 58.06% examples, 1888661 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:24:13,854][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 88.36% examples, 1883389 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:24:14,222][gensim.models.word2vec][INFO] - EPOCH 48: training on 6550866 raw words (6368654 effective words) took 3.4s, 1888252 effective words/s
[2023-02-07 12:24:14,223][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 320992434 raw words (312064046 effective words) took 169.4s, 1841988 effective words/s', 'datetime': '2023-02-07T12:24:14.223387', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:24:14.223 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:24:25,857][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122057-p5l3h8e8/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:24:25.857531', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:24:25,858][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:24:25,913][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122057-p5l3h8e8/files/../tmp/embedding_model.pt
2023-02-07 12:24:25.913 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:24:27.339 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:24:27.887 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.049 MB of 0.051 MB uploaded (0.000 MB deduped)wandb: | 0.049 MB of 0.051 MB uploaded (0.000 MB deduped)wandb: üöÄ View run cerulean-sweep-2 at: https://wandb.ai/xiaoqiz/mof2vec/runs/p5l3h8e8
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_122057-p5l3h8e8/logs
Run p5l3h8e8 errored: InstantiationException("Error locating target 'mofgraph2vec.model.gbt.Regressor', set env var HYDRA_FULL_ERROR=1 to see chained exception.\nfull_key: model.sklearn")
wandb: ERROR Run p5l3h8e8 errored: InstantiationException("Error locating target 'mofgraph2vec.model.gbt.Regressor', set env var HYDRA_FULL_ERROR=1 to see chained exception.\nfull_key: model.sklearn")
wandb: Agent Starting Run: ozwl7mgj with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 143
wandb: 	model.gensim.alpha: 0.01636432035507454
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 76
wandb: 	model.gensim.min_count: 9
wandb: 	model.gensim.sample: 0.8331460224328924
wandb: 	model.gensim.vector_size: 242
wandb: 	model.gensim.window: 14
wandb: 	model.sklearn.learning_rate: 0.0003574262315430006
wandb: 	model.sklearn.max_depth: 79
wandb: 	model.sklearn.min_child_weight: 0.09887016483013132
wandb: 	model.sklearn.n_estimators: 626
wandb: 	model.sklearn.num_leaves: 500
wandb: 	model.sklearn.reg_alpha: 0.14678180548805336
wandb: 	model.sklearn.reg_lambda: 0.0026092057322243743
wandb: 	model.sklearn.subsample: 0.9849967066220512
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122437-ozwl7mgj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/ozwl7mgj
2023-02-07 12:24:44.815 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 12:24:44.815 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 143 for sweep.
2023-02-07 12:24:44.816 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.01636432035507454 for sweep.
2023-02-07 12:24:44.816 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:24:44.816 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 76 for sweep.
2023-02-07 12:24:44.816 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 9 for sweep.
2023-02-07 12:24:44.816 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.8331460224328924 for sweep.
2023-02-07 12:24:44.817 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 242 for sweep.
2023-02-07 12:24:44.817 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 14 for sweep.
2023-02-07 12:24:44.817 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0003574262315430006 for sweep.
2023-02-07 12:24:44.817 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 79 for sweep.
2023-02-07 12:24:44.817 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.09887016483013132 for sweep.
2023-02-07 12:24:44.818 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 626 for sweep.
2023-02-07 12:24:44.818 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 500 for sweep.
2023-02-07 12:24:44.819 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.14678180548805336 for sweep.
2023-02-07 12:24:44.819 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.0026092057322243743 for sweep.
2023-02-07 12:24:44.819 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.9849967066220512 for sweep.
2023-02-07 12:24:44.819 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:24:44.824 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122437-ozwl7mgj/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 143, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 242, 'window': 14, 'min_count': 9, 'dm': 0, 'sample': 0.8331460224328924, 'workers': 4, 'alpha': 0.01636432035507454, 'epochs': 76}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 626, 'max_depth': 79, 'num_leaves': 500, 'reg_alpha': 0.14678180548805336, 'reg_lambda': 0.0026092057322243743, 'subsample': 0.9849967066220512, 'min_child_weight': 0.09887016483013132, 'n_jobs': 4, 'learning_rate': 0.0003574262315430006}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 4/3257 [00:00<01:49, 29.82it/s]  2%|‚ñè         | 49/3257 [00:00<00:13, 244.70it/s]  3%|‚ñé         | 93/3257 [00:00<00:09, 325.22it/s]  4%|‚ñç         | 136/3257 [00:00<00:08, 363.51it/s]  6%|‚ñå         | 180/3257 [00:00<00:07, 389.70it/s]  7%|‚ñã         | 231/3257 [00:00<00:07, 427.23it/s]  8%|‚ñä         | 275/3257 [00:00<00:07, 421.92it/s] 10%|‚ñâ         | 318/3257 [00:00<00:07, 399.16it/s] 11%|‚ñà         | 359/3257 [00:00<00:07, 388.62it/s] 12%|‚ñà‚ñè        | 399/3257 [00:01<00:07, 367.84it/s] 13%|‚ñà‚ñé        | 437/3257 [00:01<00:08, 352.13it/s] 15%|‚ñà‚ñç        | 478/3257 [00:01<00:07, 367.81it/s] 16%|‚ñà‚ñå        | 519/3257 [00:01<00:07, 377.33it/s] 17%|‚ñà‚ñã        | 558/3257 [00:01<00:07, 370.11it/s] 18%|‚ñà‚ñä        | 596/3257 [00:01<00:07, 357.74it/s] 20%|‚ñà‚ñâ        | 636/3257 [00:01<00:07, 367.22it/s] 21%|‚ñà‚ñà        | 673/3257 [00:01<00:07, 358.02it/s] 22%|‚ñà‚ñà‚ñè       | 709/3257 [00:01<00:07, 357.61it/s] 23%|‚ñà‚ñà‚ñé       | 745/3257 [00:02<00:07, 350.76it/s] 24%|‚ñà‚ñà‚ñç       | 783/3257 [00:02<00:06, 355.44it/s] 25%|‚ñà‚ñà‚ñå       | 821/3257 [00:02<00:06, 359.45it/s] 26%|‚ñà‚ñà‚ñã       | 857/3257 [00:02<00:06, 353.39it/s] 27%|‚ñà‚ñà‚ñã       | 895/3257 [00:02<00:06, 359.26it/s] 29%|‚ñà‚ñà‚ñä       | 934/3257 [00:02<00:06, 366.72it/s] 30%|‚ñà‚ñà‚ñâ       | 973/3257 [00:02<00:06, 373.32it/s] 31%|‚ñà‚ñà‚ñà       | 1011/3257 [00:02<00:06, 367.03it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1048/3257 [00:02<00:06, 359.08it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1086/3257 [00:03<00:05, 363.59it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1125/3257 [00:03<00:05, 369.51it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1164/3257 [00:03<00:05, 373.17it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1202/3257 [00:03<00:05, 355.41it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1238/3257 [00:03<00:07, 263.31it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1274/3257 [00:03<00:06, 284.91it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1306/3257 [00:03<00:06, 293.20it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1346/3257 [00:03<00:06, 318.49it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1381/3257 [00:03<00:05, 325.66it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:04<00:05, 349.32it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1465/3257 [00:04<00:04, 367.66it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1507/3257 [00:04<00:04, 380.20it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1546/3257 [00:04<00:04, 364.42it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1585/3257 [00:04<00:04, 369.10it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1624/3257 [00:04<00:04, 375.01it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1662/3257 [00:04<00:04, 368.07it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1701/3257 [00:04<00:04, 372.62it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1739/3257 [00:04<00:04, 361.40it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1780/3257 [00:05<00:03, 375.19it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1821/3257 [00:05<00:03, 382.02it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1860/3257 [00:05<00:03, 381.14it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1900/3257 [00:05<00:03, 386.48it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1945/3257 [00:05<00:03, 401.10it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1988/3257 [00:05<00:03, 408.82it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2033/3257 [00:05<00:02, 417.96it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2075/3257 [00:05<00:02, 398.61it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2116/3257 [00:05<00:02, 399.21it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2157/3257 [00:05<00:02, 384.00it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2199/3257 [00:06<00:02, 393.93it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2239/3257 [00:06<00:02, 384.98it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2278/3257 [00:06<00:02, 384.73it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2322/3257 [00:06<00:02, 399.71it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2369/3257 [00:06<00:02, 418.61it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2411/3257 [00:06<00:02, 414.01it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2453/3257 [00:06<00:02, 394.66it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2499/3257 [00:06<00:01, 411.67it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2541/3257 [00:06<00:01, 409.43it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2583/3257 [00:07<00:01, 397.45it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:07<00:01, 417.03it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2673/3257 [00:07<00:01, 415.38it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2715/3257 [00:07<00:01, 273.41it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:07<00:01, 311.11it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2805/3257 [00:07<00:01, 339.43it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:07<00:01, 346.44it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2892/3257 [00:07<00:00, 378.84it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2933/3257 [00:08<00:00, 384.90it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2974/3257 [00:08<00:00, 381.86it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3016/3257 [00:08<00:00, 387.73it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3063/3257 [00:08<00:00, 410.11it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3108/3257 [00:08<00:00, 420.80it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3151/3257 [00:08<00:00, 415.29it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3194/3257 [00:08<00:00, 418.74it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3237/3257 [00:08<00:00, 417.68it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 370.73it/s]
2023-02-07 12:24:53.785 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:24:53,786][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d242,n5,mc9,s0.833146,t4>', 'datetime': '2023-02-07T12:24:53.786386', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:24:53,786][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:24:53,786][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:24:53,909][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 12:24:53,909][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:24:53,910][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=9 retains 462 unique words (50.00% of original 924, drops 462)', 'datetime': '2023-02-07T12:24:53.910657', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:24:53,912][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=9 leaves 1453902 word corpus (99.87% of original 1455748, drops 1846)', 'datetime': '2023-02-07T12:24:53.912061', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:24:53,913][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 12:24:53,913][gensim.models.word2vec][INFO] - sample=0.833146 downsamples 0 most-common words
[2023-02-07 12:24:53,913][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1453902 word corpus (100.0%% of prior 1453902)', 'datetime': '2023-02-07T12:24:53.913912', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:24:53,916][gensim.models.word2vec][INFO] - estimated required memory for 462 words and 242 dimensions: 4929608 bytes
[2023-02-07 12:24:53,916][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:24:53,920][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 462 vocabulary and 242 features, using sg=1 hs=0 sample=0.8331460224328924 negative=5 window=14 shrink_windows=True', 'datetime': '2023-02-07T12:24:53.920164', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:24:54,425][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1457159 effective words) took 0.5s, 2889393 effective words/s
[2023-02-07 12:24:54,828][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1457159 effective words) took 0.4s, 3629276 effective words/s
[2023-02-07 12:24:55,233][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1457159 effective words) took 0.4s, 3605173 effective words/s
[2023-02-07 12:24:55,635][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1457159 effective words) took 0.4s, 3641733 effective words/s
[2023-02-07 12:24:56,039][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1457159 effective words) took 0.4s, 3617744 effective words/s
[2023-02-07 12:24:56,439][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1457159 effective words) took 0.4s, 3653343 effective words/s
[2023-02-07 12:24:56,837][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1457159 effective words) took 0.4s, 3670733 effective words/s
[2023-02-07 12:24:57,231][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1457159 effective words) took 0.4s, 3705484 effective words/s
[2023-02-07 12:24:57,637][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1457159 effective words) took 0.4s, 3602372 effective words/s
[2023-02-07 12:24:58,045][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1457159 effective words) took 0.4s, 3581367 effective words/s
[2023-02-07 12:24:58,448][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1457159 effective words) took 0.4s, 3622470 effective words/s
[2023-02-07 12:24:58,849][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1457159 effective words) took 0.4s, 3648386 effective words/s
[2023-02-07 12:24:59,250][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1457159 effective words) took 0.4s, 3643032 effective words/s
[2023-02-07 12:24:59,657][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1457159 effective words) took 0.4s, 3595447 effective words/s
[2023-02-07 12:25:00,074][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1457159 effective words) took 0.4s, 3505665 effective words/s
[2023-02-07 12:25:00,486][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1457159 effective words) took 0.4s, 3549992 effective words/s
[2023-02-07 12:25:00,893][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1457159 effective words) took 0.4s, 3590788 effective words/s
[2023-02-07 12:25:01,310][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1457159 effective words) took 0.4s, 3507285 effective words/s
[2023-02-07 12:25:01,738][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1457159 effective words) took 0.4s, 3413632 effective words/s
[2023-02-07 12:25:02,168][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1457159 effective words) took 0.4s, 3397955 effective words/s
[2023-02-07 12:25:02,603][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1457159 effective words) took 0.4s, 3363985 effective words/s
[2023-02-07 12:25:03,032][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1457159 effective words) took 0.4s, 3401986 effective words/s
[2023-02-07 12:25:03,467][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1457159 effective words) took 0.4s, 3366067 effective words/s
[2023-02-07 12:25:03,906][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1457159 effective words) took 0.4s, 3330158 effective words/s
[2023-02-07 12:25:04,351][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1457159 effective words) took 0.4s, 3279290 effective words/s
[2023-02-07 12:25:04,787][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1457159 effective words) took 0.4s, 3361322 effective words/s
[2023-02-07 12:25:05,225][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1457159 effective words) took 0.4s, 3334788 effective words/s
[2023-02-07 12:25:05,658][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1457159 effective words) took 0.4s, 3379397 effective words/s
[2023-02-07 12:25:06,095][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1457159 effective words) took 0.4s, 3351646 effective words/s
[2023-02-07 12:25:06,536][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1457159 effective words) took 0.4s, 3314396 effective words/s
[2023-02-07 12:25:06,978][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1457159 effective words) took 0.4s, 3310725 effective words/s
[2023-02-07 12:25:07,415][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1457159 effective words) took 0.4s, 3339646 effective words/s
[2023-02-07 12:25:07,856][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1457159 effective words) took 0.4s, 3316451 effective words/s
[2023-02-07 12:25:08,291][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1457159 effective words) took 0.4s, 3362759 effective words/s
[2023-02-07 12:25:08,728][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1457159 effective words) took 0.4s, 3346281 effective words/s
[2023-02-07 12:25:09,162][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1457159 effective words) took 0.4s, 3369124 effective words/s
[2023-02-07 12:25:09,598][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1457159 effective words) took 0.4s, 3351643 effective words/s
[2023-02-07 12:25:10,035][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1457159 effective words) took 0.4s, 3345812 effective words/s
[2023-02-07 12:25:10,465][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1457159 effective words) took 0.4s, 3397414 effective words/s
[2023-02-07 12:25:10,899][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1457159 effective words) took 0.4s, 3370679 effective words/s
[2023-02-07 12:25:11,325][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1457159 effective words) took 0.4s, 3434106 effective words/s
[2023-02-07 12:25:11,758][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1457159 effective words) took 0.4s, 3377854 effective words/s
[2023-02-07 12:25:12,175][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1457159 effective words) took 0.4s, 3516101 effective words/s
[2023-02-07 12:25:12,591][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1457159 effective words) took 0.4s, 3507580 effective words/s
[2023-02-07 12:25:13,000][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1457159 effective words) took 0.4s, 3578743 effective words/s
[2023-02-07 12:25:13,407][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1457159 effective words) took 0.4s, 3586006 effective words/s
[2023-02-07 12:25:13,816][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1457159 effective words) took 0.4s, 3578788 effective words/s
[2023-02-07 12:25:14,229][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1457159 effective words) took 0.4s, 3541312 effective words/s
[2023-02-07 12:25:14,643][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1457159 effective words) took 0.4s, 3533080 effective words/s
[2023-02-07 12:25:15,058][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1457159 effective words) took 0.4s, 3521590 effective words/s
[2023-02-07 12:25:15,473][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1457159 effective words) took 0.4s, 3517074 effective words/s
[2023-02-07 12:25:15,896][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1457159 effective words) took 0.4s, 3459091 effective words/s
[2023-02-07 12:25:16,304][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1457159 effective words) took 0.4s, 3578400 effective words/s
[2023-02-07 12:25:16,723][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1457159 effective words) took 0.4s, 3485821 effective words/s
[2023-02-07 12:25:17,167][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1457159 effective words) took 0.4s, 3299380 effective words/s
[2023-02-07 12:25:17,607][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1457159 effective words) took 0.4s, 3321361 effective words/s
[2023-02-07 12:25:18,054][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1457159 effective words) took 0.4s, 3273827 effective words/s
[2023-02-07 12:25:18,490][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1457159 effective words) took 0.4s, 3351630 effective words/s
[2023-02-07 12:25:18,941][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1457159 effective words) took 0.5s, 3234746 effective words/s
[2023-02-07 12:25:19,416][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1457159 effective words) took 0.5s, 3083571 effective words/s
[2023-02-07 12:25:19,896][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1457159 effective words) took 0.5s, 3040253 effective words/s
[2023-02-07 12:25:20,396][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1457159 effective words) took 0.5s, 2923545 effective words/s
[2023-02-07 12:25:20,884][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1457159 effective words) took 0.5s, 2992771 effective words/s
[2023-02-07 12:25:21,427][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1457159 effective words) took 0.5s, 2689112 effective words/s
[2023-02-07 12:25:21,882][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1457159 effective words) took 0.5s, 3216329 effective words/s
[2023-02-07 12:25:22,321][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1457159 effective words) took 0.4s, 3329181 effective words/s
[2023-02-07 12:25:22,776][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1457159 effective words) took 0.5s, 3215175 effective words/s
[2023-02-07 12:25:23,278][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1457159 effective words) took 0.5s, 2911035 effective words/s
[2023-02-07 12:25:23,719][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1457159 effective words) took 0.4s, 3315319 effective words/s
[2023-02-07 12:25:24,186][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1457159 effective words) took 0.5s, 3123956 effective words/s
[2023-02-07 12:25:24,675][gensim.models.word2vec][INFO] - EPOCH 70: training on 1455748 raw words (1457159 effective words) took 0.5s, 2989349 effective words/s
[2023-02-07 12:25:25,137][gensim.models.word2vec][INFO] - EPOCH 71: training on 1455748 raw words (1457159 effective words) took 0.5s, 3164759 effective words/s
[2023-02-07 12:25:25,602][gensim.models.word2vec][INFO] - EPOCH 72: training on 1455748 raw words (1457159 effective words) took 0.5s, 3140658 effective words/s
[2023-02-07 12:25:26,044][gensim.models.word2vec][INFO] - EPOCH 73: training on 1455748 raw words (1457159 effective words) took 0.4s, 3308694 effective words/s
[2023-02-07 12:25:26,484][gensim.models.word2vec][INFO] - EPOCH 74: training on 1455748 raw words (1457159 effective words) took 0.4s, 3316949 effective words/s
[2023-02-07 12:25:26,942][gensim.models.word2vec][INFO] - EPOCH 75: training on 1455748 raw words (1457159 effective words) took 0.5s, 3196327 effective words/s
[2023-02-07 12:25:26,942][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 110636848 raw words (110744084 effective words) took 33.0s, 3353641 effective words/s', 'datetime': '2023-02-07T12:25:26.942387', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:25:26.942 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:25:28,873][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122437-ozwl7mgj/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:25:28.873675', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:25:28,874][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:25:28,889][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122437-ozwl7mgj/files/../tmp/embedding_model.pt
2023-02-07 12:25:28.889 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:25:30.267 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:25:30.842 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: üöÄ View run soft-sweep-3 at: https://wandb.ai/xiaoqiz/mof2vec/runs/ozwl7mgj
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_122437-ozwl7mgj/logs
Run ozwl7mgj errored: InstantiationException("Error locating target 'mofgraph2vec.model.gbt.Regressor', set env var HYDRA_FULL_ERROR=1 to see chained exception.\nfull_key: model.sklearn")
wandb: ERROR Run ozwl7mgj errored: InstantiationException("Error locating target 'mofgraph2vec.model.gbt.Regressor', set env var HYDRA_FULL_ERROR=1 to see chained exception.\nfull_key: model.sklearn")
wandb: Agent Starting Run: vep2dxm9 with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 174
wandb: 	model.gensim.alpha: 0.05774625648937981
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 88
wandb: 	model.gensim.min_count: 7
wandb: 	model.gensim.sample: 0.2837998963567218
wandb: 	model.gensim.vector_size: 101
wandb: 	model.gensim.window: 17
wandb: 	model.sklearn.learning_rate: 0.005007868210784355
wandb: 	model.sklearn.max_depth: 20
wandb: 	model.sklearn.min_child_weight: 0.041923911492394474
wandb: 	model.sklearn.n_estimators: 2837
wandb: 	model.sklearn.num_leaves: 380
wandb: 	model.sklearn.reg_alpha: 0.3398316340465301
wandb: 	model.sklearn.reg_lambda: 0.00883702992186859
wandb: 	model.sklearn.subsample: 0.8069254027359718
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122543-vep2dxm9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-sweep-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/vep2dxm9
2023-02-07 12:25:51.268 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 12:25:51.268 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 174 for sweep.
2023-02-07 12:25:51.268 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.05774625648937981 for sweep.
2023-02-07 12:25:51.269 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:25:51.269 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 88 for sweep.
2023-02-07 12:25:51.269 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 7 for sweep.
2023-02-07 12:25:51.269 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2837998963567218 for sweep.
2023-02-07 12:25:51.270 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 101 for sweep.
2023-02-07 12:25:51.270 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 17 for sweep.
2023-02-07 12:25:51.270 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.005007868210784355 for sweep.
2023-02-07 12:25:51.271 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 20 for sweep.
2023-02-07 12:25:51.271 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.041923911492394474 for sweep.
2023-02-07 12:25:51.271 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2837 for sweep.
2023-02-07 12:25:51.271 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 380 for sweep.
2023-02-07 12:25:51.272 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.3398316340465301 for sweep.
2023-02-07 12:25:51.272 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.00883702992186859 for sweep.
2023-02-07 12:25:51.272 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.8069254027359718 for sweep.
2023-02-07 12:25:51.272 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:25:51.279 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122543-vep2dxm9/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 174, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 101, 'window': 17, 'min_count': 7, 'dm': 0, 'sample': 0.2837998963567218, 'workers': 4, 'alpha': 0.05774625648937981, 'epochs': 88}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2837, 'max_depth': 20, 'num_leaves': 380, 'reg_alpha': 0.3398316340465301, 'reg_lambda': 0.00883702992186859, 'subsample': 0.8069254027359718, 'min_child_weight': 0.041923911492394474, 'n_jobs': 4, 'learning_rate': 0.005007868210784355}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 32/3257 [00:00<00:10, 316.37it/s]  2%|‚ñè         | 65/3257 [00:00<00:09, 322.26it/s]  3%|‚ñé         | 98/3257 [00:00<00:10, 315.47it/s]  4%|‚ñç         | 131/3257 [00:00<00:09, 315.52it/s]  5%|‚ñå         | 163/3257 [00:00<00:09, 314.95it/s]  6%|‚ñå         | 197/3257 [00:00<00:09, 320.57it/s]  7%|‚ñã         | 235/3257 [00:00<00:08, 339.05it/s]  8%|‚ñä         | 269/3257 [00:00<00:09, 324.55it/s]  9%|‚ñâ         | 307/3257 [00:00<00:08, 339.98it/s] 11%|‚ñà         | 342/3257 [00:01<00:08, 338.03it/s] 12%|‚ñà‚ñè        | 376/3257 [00:01<00:08, 332.31it/s] 13%|‚ñà‚ñé        | 410/3257 [00:01<00:08, 328.77it/s] 14%|‚ñà‚ñé        | 443/3257 [00:01<00:09, 303.28it/s] 15%|‚ñà‚ñç        | 477/3257 [00:01<00:08, 312.40it/s] 16%|‚ñà‚ñå        | 512/3257 [00:01<00:08, 319.89it/s] 17%|‚ñà‚ñã        | 546/3257 [00:01<00:08, 318.62it/s] 18%|‚ñà‚ñä        | 579/3257 [00:01<00:08, 303.18it/s] 19%|‚ñà‚ñâ        | 614/3257 [00:01<00:08, 316.17it/s] 20%|‚ñà‚ñâ        | 646/3257 [00:02<00:08, 301.76it/s] 21%|‚ñà‚ñà        | 677/3257 [00:02<00:08, 298.18it/s] 22%|‚ñà‚ñà‚ñè       | 708/3257 [00:02<00:08, 295.87it/s] 23%|‚ñà‚ñà‚ñé       | 738/3257 [00:02<00:12, 209.79it/s] 24%|‚ñà‚ñà‚ñé       | 772/3257 [00:02<00:10, 237.86it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:02<00:09, 257.02it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:02<00:08, 269.52it/s] 27%|‚ñà‚ñà‚ñã       | 865/3257 [00:02<00:08, 273.28it/s] 28%|‚ñà‚ñà‚ñä       | 897/3257 [00:03<00:08, 284.38it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:03<00:07, 296.90it/s] 30%|‚ñà‚ñà‚ñâ       | 966/3257 [00:03<00:07, 310.25it/s] 31%|‚ñà‚ñà‚ñà       | 998/3257 [00:03<00:07, 304.83it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1029/3257 [00:03<00:07, 299.72it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1060/3257 [00:03<00:07, 299.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:03<00:07, 297.51it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1123/3257 [00:03<00:07, 302.89it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1154/3257 [00:03<00:07, 299.97it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1185/3257 [00:03<00:07, 295.67it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1215/3257 [00:04<00:07, 286.31it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1248/3257 [00:04<00:06, 297.70it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1278/3257 [00:04<00:06, 292.69it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1308/3257 [00:04<00:06, 292.55it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1341/3257 [00:04<00:06, 302.52it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1372/3257 [00:04<00:06, 293.07it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1403/3257 [00:04<00:06, 297.74it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1439/3257 [00:04<00:05, 313.98it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1475/3257 [00:04<00:05, 326.99it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:04<00:05, 336.02it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1545/3257 [00:05<00:05, 308.25it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1577/3257 [00:05<00:05, 307.51it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1613/3257 [00:05<00:05, 321.12it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1646/3257 [00:05<00:05, 313.59it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1678/3257 [00:05<00:05, 302.65it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1710/3257 [00:05<00:05, 306.74it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1741/3257 [00:05<00:05, 294.51it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1775/3257 [00:05<00:04, 306.87it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1807/3257 [00:05<00:04, 303.97it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1841/3257 [00:06<00:04, 313.55it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1875/3257 [00:06<00:04, 320.14it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1908/3257 [00:06<00:04, 321.89it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1944/3257 [00:06<00:03, 331.59it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1978/3257 [00:06<00:03, 332.64it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2012/3257 [00:06<00:03, 332.78it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2046/3257 [00:06<00:05, 222.66it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2075/3257 [00:06<00:05, 235.54it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2107/3257 [00:07<00:04, 253.82it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2136/3257 [00:07<00:04, 261.36it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2167/3257 [00:07<00:03, 273.50it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2200/3257 [00:07<00:03, 288.37it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2231/3257 [00:07<00:03, 290.74it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2262/3257 [00:07<00:03, 289.59it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2295/3257 [00:07<00:03, 301.03it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2330/3257 [00:07<00:02, 315.00it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2367/3257 [00:07<00:02, 326.56it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2404/3257 [00:07<00:02, 337.31it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2438/3257 [00:08<00:02, 320.26it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2473/3257 [00:08<00:02, 327.77it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2508/3257 [00:08<00:02, 333.50it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2542/3257 [00:08<00:02, 334.11it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2576/3257 [00:08<00:02, 316.44it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2611/3257 [00:08<00:01, 325.59it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2644/3257 [00:08<00:01, 326.73it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2677/3257 [00:08<00:01, 320.93it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2710/3257 [00:08<00:01, 306.88it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2748/3257 [00:09<00:01, 325.37it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2781/3257 [00:09<00:01, 323.14it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2815/3257 [00:09<00:01, 327.45it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2848/3257 [00:09<00:01, 321.96it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:09<00:01, 337.13it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2920/3257 [00:09<00:01, 328.88it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2953/3257 [00:09<00:00, 312.96it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:09<00:00, 312.86it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3018/3257 [00:09<00:00, 315.79it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3054/3257 [00:10<00:00, 328.11it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3089/3257 [00:10<00:00, 333.92it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3127/3257 [00:10<00:00, 344.26it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3162/3257 [00:10<00:00, 332.38it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3196/3257 [00:10<00:00, 328.32it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3229/3257 [00:10<00:00, 318.28it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:10<00:00, 306.45it/s]
2023-02-07 12:26:02.255 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:26:02,256][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d101,n5,mc7,s0.2838,t4>', 'datetime': '2023-02-07T12:26:02.256102', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:26:02,256][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:26:02,256][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:26:02,487][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 12:26:02,488][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:26:02,495][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 retains 3228 unique words (48.45% of original 6662, drops 3434)', 'datetime': '2023-02-07T12:26:02.495925', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:26:02,496][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 leaves 2901608 word corpus (99.66% of original 2911496, drops 9888)', 'datetime': '2023-02-07T12:26:02.496141', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:26:02,506][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 12:26:02,507][gensim.models.word2vec][INFO] - sample=0.2838 downsamples 0 most-common words
[2023-02-07 12:26:02,507][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2901608 word corpus (100.0%% of prior 2901608)', 'datetime': '2023-02-07T12:26:02.507215', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:26:02,524][gensim.models.word2vec][INFO] - estimated required memory for 3228 words and 101 dimensions: 6189452 bytes
[2023-02-07 12:26:02,524][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:26:02,527][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 3228 vocabulary and 101 features, using sg=1 hs=0 sample=0.2837998963567218 negative=5 window=17 shrink_windows=True', 'datetime': '2023-02-07T12:26:02.527374', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:26:03,186][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2904865 effective words) took 0.7s, 4419932 effective words/s
[2023-02-07 12:26:03,787][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2904865 effective words) took 0.6s, 4839896 effective words/s
[2023-02-07 12:26:04,388][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2904865 effective words) took 0.6s, 4840025 effective words/s
[2023-02-07 12:26:04,975][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2904865 effective words) took 0.6s, 4957042 effective words/s
[2023-02-07 12:26:05,572][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2904865 effective words) took 0.6s, 4879628 effective words/s
[2023-02-07 12:26:06,174][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2904865 effective words) took 0.6s, 4837389 effective words/s
[2023-02-07 12:26:06,784][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2904865 effective words) took 0.6s, 4776173 effective words/s
[2023-02-07 12:26:07,384][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2904865 effective words) took 0.6s, 4852154 effective words/s
[2023-02-07 12:26:07,992][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2904865 effective words) took 0.6s, 4787494 effective words/s
[2023-02-07 12:26:08,596][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2904865 effective words) took 0.6s, 4816522 effective words/s
[2023-02-07 12:26:09,189][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2904865 effective words) took 0.6s, 4919303 effective words/s
[2023-02-07 12:26:09,784][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2904865 effective words) took 0.6s, 4892168 effective words/s
[2023-02-07 12:26:10,381][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2904865 effective words) took 0.6s, 4877085 effective words/s
[2023-02-07 12:26:10,975][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2904865 effective words) took 0.6s, 4904267 effective words/s
[2023-02-07 12:26:11,574][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2904865 effective words) took 0.6s, 4856458 effective words/s
[2023-02-07 12:26:12,180][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2904865 effective words) took 0.6s, 4800478 effective words/s
[2023-02-07 12:26:12,788][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2904865 effective words) took 0.6s, 4800815 effective words/s
[2023-02-07 12:26:13,394][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2904865 effective words) took 0.6s, 4808112 effective words/s
[2023-02-07 12:26:14,000][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2904865 effective words) took 0.6s, 4800501 effective words/s
[2023-02-07 12:26:14,605][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2904865 effective words) took 0.6s, 4815767 effective words/s
[2023-02-07 12:26:15,206][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2904865 effective words) took 0.6s, 4841413 effective words/s
[2023-02-07 12:26:15,805][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2904865 effective words) took 0.6s, 4858933 effective words/s
[2023-02-07 12:26:16,412][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2904865 effective words) took 0.6s, 4795438 effective words/s
[2023-02-07 12:26:17,010][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2904865 effective words) took 0.6s, 4867293 effective words/s
[2023-02-07 12:26:17,606][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2904865 effective words) took 0.6s, 4890009 effective words/s
[2023-02-07 12:26:18,205][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2904865 effective words) took 0.6s, 4857241 effective words/s
[2023-02-07 12:26:18,822][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2904865 effective words) took 0.6s, 4721660 effective words/s
[2023-02-07 12:26:19,424][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2904865 effective words) took 0.6s, 4838312 effective words/s
[2023-02-07 12:26:20,030][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2904865 effective words) took 0.6s, 4798230 effective words/s
[2023-02-07 12:26:20,636][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2904865 effective words) took 0.6s, 4807417 effective words/s
[2023-02-07 12:26:21,238][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2904865 effective words) took 0.6s, 4833691 effective words/s
[2023-02-07 12:26:21,842][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2904865 effective words) took 0.6s, 4815983 effective words/s
[2023-02-07 12:26:22,444][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2904865 effective words) took 0.6s, 4839058 effective words/s
[2023-02-07 12:26:23,042][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2904865 effective words) took 0.6s, 4871407 effective words/s
[2023-02-07 12:26:23,641][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2904865 effective words) took 0.6s, 4859404 effective words/s
[2023-02-07 12:26:24,246][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2904865 effective words) took 0.6s, 4814385 effective words/s
[2023-02-07 12:26:24,847][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2904865 effective words) took 0.6s, 4848518 effective words/s
[2023-02-07 12:26:25,449][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2904865 effective words) took 0.6s, 4838661 effective words/s
[2023-02-07 12:26:26,042][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2904865 effective words) took 0.6s, 4907006 effective words/s
[2023-02-07 12:26:26,634][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2904865 effective words) took 0.6s, 4920409 effective words/s
[2023-02-07 12:26:27,225][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2904865 effective words) took 0.6s, 4927056 effective words/s
[2023-02-07 12:26:27,820][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2904865 effective words) took 0.6s, 4894858 effective words/s
[2023-02-07 12:26:28,383][gensim.models.word2vec][INFO] - EPOCH 42: training on 2911496 raw words (2904865 effective words) took 0.6s, 5177496 effective words/s
[2023-02-07 12:26:28,946][gensim.models.word2vec][INFO] - EPOCH 43: training on 2911496 raw words (2904865 effective words) took 0.6s, 5169071 effective words/s
[2023-02-07 12:26:29,511][gensim.models.word2vec][INFO] - EPOCH 44: training on 2911496 raw words (2904865 effective words) took 0.6s, 5152631 effective words/s
[2023-02-07 12:26:30,078][gensim.models.word2vec][INFO] - EPOCH 45: training on 2911496 raw words (2904865 effective words) took 0.6s, 5129451 effective words/s
[2023-02-07 12:26:30,655][gensim.models.word2vec][INFO] - EPOCH 46: training on 2911496 raw words (2904865 effective words) took 0.6s, 5044580 effective words/s
[2023-02-07 12:26:31,224][gensim.models.word2vec][INFO] - EPOCH 47: training on 2911496 raw words (2904865 effective words) took 0.6s, 5119342 effective words/s
[2023-02-07 12:26:31,795][gensim.models.word2vec][INFO] - EPOCH 48: training on 2911496 raw words (2904865 effective words) took 0.6s, 5098491 effective words/s
[2023-02-07 12:26:32,366][gensim.models.word2vec][INFO] - EPOCH 49: training on 2911496 raw words (2904865 effective words) took 0.6s, 5095527 effective words/s
[2023-02-07 12:26:32,962][gensim.models.word2vec][INFO] - EPOCH 50: training on 2911496 raw words (2904865 effective words) took 0.6s, 4883141 effective words/s
[2023-02-07 12:26:33,563][gensim.models.word2vec][INFO] - EPOCH 51: training on 2911496 raw words (2904865 effective words) took 0.6s, 4836327 effective words/s
[2023-02-07 12:26:34,154][gensim.models.word2vec][INFO] - EPOCH 52: training on 2911496 raw words (2904865 effective words) took 0.6s, 4929523 effective words/s
[2023-02-07 12:26:34,746][gensim.models.word2vec][INFO] - EPOCH 53: training on 2911496 raw words (2904865 effective words) took 0.6s, 4914559 effective words/s
[2023-02-07 12:26:35,351][gensim.models.word2vec][INFO] - EPOCH 54: training on 2911496 raw words (2904865 effective words) took 0.6s, 4811673 effective words/s
[2023-02-07 12:26:35,968][gensim.models.word2vec][INFO] - EPOCH 55: training on 2911496 raw words (2904865 effective words) took 0.6s, 4720183 effective words/s
[2023-02-07 12:26:36,598][gensim.models.word2vec][INFO] - EPOCH 56: training on 2911496 raw words (2904865 effective words) took 0.6s, 4621671 effective words/s
[2023-02-07 12:26:37,191][gensim.models.word2vec][INFO] - EPOCH 57: training on 2911496 raw words (2904865 effective words) took 0.6s, 4907208 effective words/s
[2023-02-07 12:26:37,818][gensim.models.word2vec][INFO] - EPOCH 58: training on 2911496 raw words (2904865 effective words) took 0.6s, 4641965 effective words/s
[2023-02-07 12:26:38,413][gensim.models.word2vec][INFO] - EPOCH 59: training on 2911496 raw words (2904865 effective words) took 0.6s, 4889981 effective words/s
[2023-02-07 12:26:39,026][gensim.models.word2vec][INFO] - EPOCH 60: training on 2911496 raw words (2904865 effective words) took 0.6s, 4749017 effective words/s
[2023-02-07 12:26:39,644][gensim.models.word2vec][INFO] - EPOCH 61: training on 2911496 raw words (2904865 effective words) took 0.6s, 4712518 effective words/s
[2023-02-07 12:26:40,257][gensim.models.word2vec][INFO] - EPOCH 62: training on 2911496 raw words (2904865 effective words) took 0.6s, 4748322 effective words/s
[2023-02-07 12:26:40,856][gensim.models.word2vec][INFO] - EPOCH 63: training on 2911496 raw words (2904865 effective words) took 0.6s, 4861153 effective words/s
[2023-02-07 12:26:41,470][gensim.models.word2vec][INFO] - EPOCH 64: training on 2911496 raw words (2904865 effective words) took 0.6s, 4738129 effective words/s
[2023-02-07 12:26:42,093][gensim.models.word2vec][INFO] - EPOCH 65: training on 2911496 raw words (2904865 effective words) took 0.6s, 4671676 effective words/s
[2023-02-07 12:26:42,703][gensim.models.word2vec][INFO] - EPOCH 66: training on 2911496 raw words (2904865 effective words) took 0.6s, 4771185 effective words/s
[2023-02-07 12:26:43,294][gensim.models.word2vec][INFO] - EPOCH 67: training on 2911496 raw words (2904865 effective words) took 0.6s, 4938823 effective words/s
[2023-02-07 12:26:43,900][gensim.models.word2vec][INFO] - EPOCH 68: training on 2911496 raw words (2904865 effective words) took 0.6s, 4799148 effective words/s
[2023-02-07 12:26:44,500][gensim.models.word2vec][INFO] - EPOCH 69: training on 2911496 raw words (2904865 effective words) took 0.6s, 4850552 effective words/s
[2023-02-07 12:26:45,123][gensim.models.word2vec][INFO] - EPOCH 70: training on 2911496 raw words (2904865 effective words) took 0.6s, 4669274 effective words/s
[2023-02-07 12:26:45,729][gensim.models.word2vec][INFO] - EPOCH 71: training on 2911496 raw words (2904865 effective words) took 0.6s, 4810678 effective words/s
[2023-02-07 12:26:46,341][gensim.models.word2vec][INFO] - EPOCH 72: training on 2911496 raw words (2904865 effective words) took 0.6s, 4755780 effective words/s
[2023-02-07 12:26:46,926][gensim.models.word2vec][INFO] - EPOCH 73: training on 2911496 raw words (2904865 effective words) took 0.6s, 4971665 effective words/s
[2023-02-07 12:26:47,511][gensim.models.word2vec][INFO] - EPOCH 74: training on 2911496 raw words (2904865 effective words) took 0.6s, 4978531 effective words/s
[2023-02-07 12:26:48,098][gensim.models.word2vec][INFO] - EPOCH 75: training on 2911496 raw words (2904865 effective words) took 0.6s, 4955338 effective words/s
[2023-02-07 12:26:48,700][gensim.models.word2vec][INFO] - EPOCH 76: training on 2911496 raw words (2904865 effective words) took 0.6s, 4840732 effective words/s
[2023-02-07 12:26:49,300][gensim.models.word2vec][INFO] - EPOCH 77: training on 2911496 raw words (2904865 effective words) took 0.6s, 4848741 effective words/s
[2023-02-07 12:26:49,909][gensim.models.word2vec][INFO] - EPOCH 78: training on 2911496 raw words (2904865 effective words) took 0.6s, 4776601 effective words/s
[2023-02-07 12:26:50,503][gensim.models.word2vec][INFO] - EPOCH 79: training on 2911496 raw words (2904865 effective words) took 0.6s, 4901160 effective words/s
[2023-02-07 12:26:51,107][gensim.models.word2vec][INFO] - EPOCH 80: training on 2911496 raw words (2904865 effective words) took 0.6s, 4818416 effective words/s
[2023-02-07 12:26:51,705][gensim.models.word2vec][INFO] - EPOCH 81: training on 2911496 raw words (2904865 effective words) took 0.6s, 4866282 effective words/s
[2023-02-07 12:26:52,303][gensim.models.word2vec][INFO] - EPOCH 82: training on 2911496 raw words (2904865 effective words) took 0.6s, 4867678 effective words/s
[2023-02-07 12:26:52,887][gensim.models.word2vec][INFO] - EPOCH 83: training on 2911496 raw words (2904865 effective words) took 0.6s, 4987269 effective words/s
[2023-02-07 12:26:53,491][gensim.models.word2vec][INFO] - EPOCH 84: training on 2911496 raw words (2904865 effective words) took 0.6s, 4816016 effective words/s
[2023-02-07 12:26:54,106][gensim.models.word2vec][INFO] - EPOCH 85: training on 2911496 raw words (2904865 effective words) took 0.6s, 4730327 effective words/s
[2023-02-07 12:26:54,743][gensim.models.word2vec][INFO] - EPOCH 86: training on 2911496 raw words (2904865 effective words) took 0.6s, 4573338 effective words/s
[2023-02-07 12:26:55,387][gensim.models.word2vec][INFO] - EPOCH 87: training on 2911496 raw words (2904865 effective words) took 0.6s, 4516511 effective words/s
[2023-02-07 12:26:55,387][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 256211648 raw words (255628120 effective words) took 52.9s, 4835927 effective words/s', 'datetime': '2023-02-07T12:26:55.387782', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:26:55.387 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:26:59,154][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122543-vep2dxm9/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:26:59.154288', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:26:59,156][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:26:59,170][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122543-vep2dxm9/files/../tmp/embedding_model.pt
2023-02-07 12:26:59.170 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:27:00.199 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:27:00.619 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:27:01.484 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0090033914244736, 'test_mae': 1.0746490362163152, 'test_r2': 0.049621277760190874}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.65
wandb: percentage 0.51546
wandb:   test_mae 1.07465
wandb:   test_mse 2.009
wandb:    test_r2 0.04962
wandb: 
wandb: üöÄ View run breezy-sweep-4 at: https://wandb.ai/xiaoqiz/mof2vec/runs/vep2dxm9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_122543-vep2dxm9/logs
wandb: Agent Starting Run: zyetbbuc with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 496
wandb: 	model.gensim.alpha: 0.7835353695260976
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.epochs: 92
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.7219309791562563
wandb: 	model.gensim.vector_size: 129
wandb: 	model.gensim.window: 13
wandb: 	model.sklearn.learning_rate: 0.07576698835433546
wandb: 	model.sklearn.max_depth: 93
wandb: 	model.sklearn.min_child_weight: 0.05135375365358661
wandb: 	model.sklearn.n_estimators: 2229
wandb: 	model.sklearn.num_leaves: 16
wandb: 	model.sklearn.reg_alpha: 0.010648880850680586
wandb: 	model.sklearn.reg_lambda: 0.0034477339197836704
wandb: 	model.sklearn.subsample: 0.6840170191159274
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122712-zyetbbuc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/zyetbbuc
2023-02-07 12:27:19.679 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 12:27:19.679 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 496 for sweep.
2023-02-07 12:27:19.680 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.7835353695260976 for sweep.
2023-02-07 12:27:19.680 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 12:27:19.680 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 92 for sweep.
2023-02-07 12:27:19.680 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 12:27:19.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7219309791562563 for sweep.
2023-02-07 12:27:19.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 129 for sweep.
2023-02-07 12:27:19.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 13 for sweep.
2023-02-07 12:27:19.682 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.07576698835433546 for sweep.
2023-02-07 12:27:19.682 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 93 for sweep.
2023-02-07 12:27:19.682 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05135375365358661 for sweep.
2023-02-07 12:27:19.682 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2229 for sweep.
2023-02-07 12:27:19.683 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 16 for sweep.
2023-02-07 12:27:19.683 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.010648880850680586 for sweep.
2023-02-07 12:27:19.683 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.0034477339197836704 for sweep.
2023-02-07 12:27:19.683 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6840170191159274 for sweep.
2023-02-07 12:27:19.683 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:27:19.687 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122712-zyetbbuc/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 496, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 129, 'window': 13, 'min_count': 3, 'dm': 1, 'sample': 0.7219309791562563, 'workers': 4, 'alpha': 0.7835353695260976, 'epochs': 92}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2229, 'max_depth': 93, 'num_leaves': 16, 'reg_alpha': 0.010648880850680586, 'reg_lambda': 0.0034477339197836704, 'subsample': 0.6840170191159274, 'min_child_weight': 0.05135375365358661, 'n_jobs': 4, 'learning_rate': 0.07576698835433546}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 16/3257 [00:00<00:20, 159.07it/s]  1%|          | 34/3257 [00:00<00:19, 162.70it/s]  2%|‚ñè         | 53/3257 [00:00<00:19, 166.92it/s]  2%|‚ñè         | 73/3257 [00:00<00:17, 177.92it/s]  3%|‚ñé         | 91/3257 [00:00<00:25, 125.89it/s]  3%|‚ñé         | 106/3257 [00:00<00:24, 130.85it/s]  4%|‚ñç         | 124/3257 [00:00<00:21, 143.60it/s]  5%|‚ñç         | 147/3257 [00:00<00:18, 165.45it/s]  5%|‚ñå         | 165/3257 [00:01<00:19, 162.20it/s]  6%|‚ñå         | 184/3257 [00:01<00:18, 168.57it/s]  6%|‚ñå         | 203/3257 [00:01<00:17, 173.86it/s]  7%|‚ñã         | 230/3257 [00:01<00:15, 195.59it/s]  8%|‚ñä         | 250/3257 [00:01<00:15, 195.14it/s]  8%|‚ñä         | 270/3257 [00:01<00:15, 188.89it/s]  9%|‚ñâ         | 296/3257 [00:01<00:14, 207.43it/s] 10%|‚ñâ         | 317/3257 [00:01<00:15, 194.30it/s] 10%|‚ñà         | 337/3257 [00:01<00:15, 193.69it/s] 11%|‚ñà         | 358/3257 [00:02<00:14, 197.09it/s] 12%|‚ñà‚ñè        | 378/3257 [00:02<00:16, 177.56it/s] 12%|‚ñà‚ñè        | 397/3257 [00:02<00:16, 177.08it/s] 13%|‚ñà‚ñé        | 417/3257 [00:02<00:15, 181.92it/s] 13%|‚ñà‚ñé        | 436/3257 [00:02<00:18, 152.46it/s] 14%|‚ñà‚ñç        | 456/3257 [00:02<00:17, 162.95it/s] 15%|‚ñà‚ñç        | 475/3257 [00:02<00:16, 166.95it/s] 15%|‚ñà‚ñå        | 493/3257 [00:02<00:16, 169.33it/s] 16%|‚ñà‚ñå        | 514/3257 [00:02<00:15, 179.04it/s] 16%|‚ñà‚ñã        | 533/3257 [00:03<00:15, 174.93it/s] 17%|‚ñà‚ñã        | 552/3257 [00:03<00:15, 177.73it/s] 18%|‚ñà‚ñä        | 570/3257 [00:03<00:16, 166.05it/s] 18%|‚ñà‚ñä        | 587/3257 [00:03<00:16, 159.37it/s] 19%|‚ñà‚ñä        | 605/3257 [00:03<00:16, 164.16it/s] 19%|‚ñà‚ñâ        | 622/3257 [00:03<00:16, 162.25it/s] 20%|‚ñà‚ñâ        | 641/3257 [00:03<00:15, 168.88it/s] 20%|‚ñà‚ñà        | 659/3257 [00:03<00:16, 154.01it/s] 21%|‚ñà‚ñà        | 678/3257 [00:03<00:15, 161.96it/s] 21%|‚ñà‚ñà‚ñè       | 695/3257 [00:04<00:16, 156.89it/s] 22%|‚ñà‚ñà‚ñè       | 714/3257 [00:04<00:15, 165.45it/s] 22%|‚ñà‚ñà‚ñè       | 731/3257 [00:04<00:16, 153.97it/s] 23%|‚ñà‚ñà‚ñé       | 747/3257 [00:04<00:16, 152.34it/s] 24%|‚ñà‚ñà‚ñé       | 768/3257 [00:04<00:15, 164.67it/s] 24%|‚ñà‚ñà‚ñç       | 785/3257 [00:04<00:15, 154.50it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:04<00:15, 161.37it/s] 25%|‚ñà‚ñà‚ñå       | 821/3257 [00:04<00:15, 159.74it/s] 26%|‚ñà‚ñà‚ñå       | 838/3257 [00:05<00:15, 154.52it/s] 26%|‚ñà‚ñà‚ñå       | 854/3257 [00:05<00:15, 150.74it/s] 27%|‚ñà‚ñà‚ñã       | 871/3257 [00:05<00:15, 155.17it/s] 27%|‚ñà‚ñà‚ñã       | 887/3257 [00:05<00:15, 154.33it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:05<00:14, 164.38it/s] 28%|‚ñà‚ñà‚ñä       | 925/3257 [00:05<00:13, 169.87it/s] 29%|‚ñà‚ñà‚ñâ       | 943/3257 [00:05<00:13, 166.45it/s] 30%|‚ñà‚ñà‚ñâ       | 963/3257 [00:05<00:13, 175.69it/s] 30%|‚ñà‚ñà‚ñà       | 981/3257 [00:05<00:13, 168.00it/s] 31%|‚ñà‚ñà‚ñà       | 998/3257 [00:05<00:13, 166.20it/s] 31%|‚ñà‚ñà‚ñà       | 1015/3257 [00:06<00:13, 165.08it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1032/3257 [00:06<00:13, 160.86it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1049/3257 [00:06<00:14, 156.70it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:06<00:13, 160.20it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1084/3257 [00:06<00:13, 160.81it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1101/3257 [00:06<00:13, 163.35it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1118/3257 [00:06<00:13, 164.27it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1135/3257 [00:06<00:13, 160.28it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1152/3257 [00:06<00:13, 158.05it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:07<00:12, 164.07it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1188/3257 [00:07<00:14, 146.27it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1203/3257 [00:07<00:13, 146.99it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1219/3257 [00:07<00:13, 148.35it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1241/3257 [00:07<00:12, 167.86it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1259/3257 [00:07<00:12, 162.94it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1276/3257 [00:07<00:12, 159.11it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1293/3257 [00:07<00:12, 152.91it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1309/3257 [00:07<00:12, 154.67it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:08<00:11, 161.84it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1345/3257 [00:08<00:18, 104.55it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1360/3257 [00:08<00:16, 113.12it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1374/3257 [00:08<00:15, 118.97it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1389/3257 [00:08<00:14, 126.25it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1411/3257 [00:08<00:12, 148.93it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:08<00:11, 158.75it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1452/3257 [00:08<00:10, 167.46it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1472/3257 [00:09<00:10, 176.17it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1492/3257 [00:09<00:09, 182.59it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1512/3257 [00:09<00:09, 186.50it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1531/3257 [00:09<00:10, 170.09it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1549/3257 [00:09<00:10, 161.91it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1566/3257 [00:09<00:10, 159.96it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1583/3257 [00:09<00:10, 160.85it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1602/3257 [00:09<00:09, 167.89it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1620/3257 [00:09<00:09, 169.94it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1638/3257 [00:10<00:09, 167.17it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1655/3257 [00:10<00:09, 165.44it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1672/3257 [00:10<00:09, 159.18it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1689/3257 [00:10<00:09, 161.77it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1707/3257 [00:10<00:09, 164.35it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1725/3257 [00:10<00:09, 168.06it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1742/3257 [00:10<00:10, 147.55it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:10<00:09, 162.11it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1784/3257 [00:10<00:08, 172.25it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1802/3257 [00:11<00:08, 173.75it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1820/3257 [00:11<00:08, 172.32it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1838/3257 [00:11<00:08, 168.94it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1856/3257 [00:11<00:08, 171.39it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1876/3257 [00:11<00:07, 177.84it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1894/3257 [00:11<00:07, 170.91it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:11<00:07, 175.37it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1937/3257 [00:11<00:07, 181.98it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1963/3257 [00:11<00:06, 201.15it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1984/3257 [00:12<00:06, 185.10it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:12<00:06, 188.91it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2024/3257 [00:12<00:06, 190.96it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2044/3257 [00:12<00:06, 184.67it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2063/3257 [00:12<00:07, 167.24it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2082/3257 [00:12<00:06, 173.11it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2100/3257 [00:12<00:07, 164.09it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:12<00:06, 165.07it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2137/3257 [00:12<00:06, 163.88it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2154/3257 [00:13<00:06, 159.49it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2173/3257 [00:13<00:06, 167.81it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2190/3257 [00:13<00:06, 164.38it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2208/3257 [00:13<00:06, 167.21it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2225/3257 [00:13<00:06, 164.41it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2242/3257 [00:13<00:06, 162.37it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2260/3257 [00:13<00:06, 163.07it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2277/3257 [00:13<00:06, 156.87it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:13<00:05, 167.41it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2321/3257 [00:14<00:05, 179.67it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2343/3257 [00:14<00:04, 189.42it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2364/3257 [00:14<00:04, 194.72it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2384/3257 [00:14<00:04, 195.97it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:14<00:04, 188.06it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2424/3257 [00:14<00:04, 188.46it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2443/3257 [00:14<00:04, 171.62it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2463/3257 [00:14<00:04, 178.60it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2483/3257 [00:14<00:04, 182.94it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2507/3257 [00:15<00:03, 193.83it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2527/3257 [00:15<00:03, 194.15it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2547/3257 [00:15<00:03, 191.26it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2567/3257 [00:15<00:03, 173.25it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2585/3257 [00:15<00:04, 165.74it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2607/3257 [00:15<00:03, 178.83it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:15<00:03, 194.23it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2651/3257 [00:15<00:03, 186.50it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2671/3257 [00:15<00:03, 188.96it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2691/3257 [00:16<00:02, 189.51it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2711/3257 [00:16<00:03, 166.43it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:16<00:03, 174.25it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2754/3257 [00:16<00:02, 184.77it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2773/3257 [00:16<00:04, 100.49it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2796/3257 [00:16<00:03, 122.08it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2815/3257 [00:17<00:03, 133.45it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2833/3257 [00:17<00:03, 139.46it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2853/3257 [00:17<00:02, 153.23it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:17<00:02, 181.34it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2902/3257 [00:17<00:02, 170.43it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2925/3257 [00:17<00:01, 184.32it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2945/3257 [00:17<00:01, 172.75it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2966/3257 [00:17<00:01, 179.41it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:17<00:01, 172.47it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3008/3257 [00:18<00:01, 186.62it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3028/3257 [00:18<00:01, 182.62it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3050/3257 [00:18<00:01, 192.33it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3075/3257 [00:18<00:00, 204.01it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3096/3257 [00:18<00:00, 197.30it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3121/3257 [00:18<00:00, 211.24it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3143/3257 [00:18<00:00, 194.12it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3163/3257 [00:18<00:00, 191.65it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3183/3257 [00:18<00:00, 182.05it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3206/3257 [00:19<00:00, 194.74it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3226/3257 [00:19<00:00, 186.79it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3250/3257 [00:19<00:00, 201.00it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 168.20it/s]
2023-02-07 12:27:39.793 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:27:39,794][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d129,n5,w13,mc3,s0.721931,t4>', 'datetime': '2023-02-07T12:27:39.794132', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:27:39,794][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:27:39,794][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:27:40,315][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 12:27:40,316][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:27:40,401][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 38627 unique words (71.46% of original 54054, drops 15427)', 'datetime': '2023-02-07T12:27:40.401030', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:27:40,402][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 6527597 word corpus (99.64% of original 6550866, drops 23269)', 'datetime': '2023-02-07T12:27:40.402201', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:27:40,515][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 12:27:40,516][gensim.models.word2vec][INFO] - sample=0.721931 downsamples 0 most-common words
[2023-02-07 12:27:40,516][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6527597 word corpus (100.0%% of prior 6527597)', 'datetime': '2023-02-07T12:27:40.516349', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:27:40,720][gensim.models.word2vec][INFO] - estimated required memory for 38627 words and 129 dimensions: 61508576 bytes
[2023-02-07 12:27:40,720][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:27:40,739][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 38627 vocabulary and 129 features, using sg=0 hs=0 sample=0.7219309791562563 negative=5 window=13 shrink_windows=True', 'datetime': '2023-02-07T12:27:40.739521', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:27:41,744][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 43.41% examples, 2872227 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:42,748][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 89.28% examples, 2897494 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:42,972][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6472763 effective words) took 2.2s, 2901653 effective words/s
[2023-02-07 12:27:43,974][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 44.12% examples, 2919618 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:44,977][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 87.35% examples, 2848316 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:45,249][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6472763 effective words) took 2.3s, 2843296 effective words/s
[2023-02-07 12:27:46,254][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 41.30% examples, 2730163 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:47,255][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 84.99% examples, 2770972 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:47,600][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6472763 effective words) took 2.3s, 2755521 effective words/s
[2023-02-07 12:27:48,602][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 41.57% examples, 2763448 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:49,604][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 83.57% examples, 2731546 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:49,973][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6472763 effective words) took 2.4s, 2729178 effective words/s
[2023-02-07 12:27:50,977][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 40.74% examples, 2694970 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:51,981][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 83.97% examples, 2736134 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:52,343][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6472763 effective words) took 2.4s, 2733395 effective words/s
[2023-02-07 12:27:53,347][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 41.94% examples, 2782979 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:54,348][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 85.48% examples, 2785832 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:54,667][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6472763 effective words) took 2.3s, 2786643 effective words/s
[2023-02-07 12:27:55,671][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 40.84% examples, 2705041 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:56,672][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 83.82% examples, 2735699 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:57,025][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6472763 effective words) took 2.4s, 2747546 effective words/s
[2023-02-07 12:27:58,027][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 41.63% examples, 2769135 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:59,027][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 84.16% examples, 2747652 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:27:59,383][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6472763 effective words) took 2.4s, 2745463 effective words/s
[2023-02-07 12:28:00,387][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 40.01% examples, 2653066 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:01,390][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 82.71% examples, 2693582 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:01,779][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6472763 effective words) took 2.4s, 2702755 effective words/s
[2023-02-07 12:28:02,783][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 41.57% examples, 2758620 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:03,786][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 83.33% examples, 2718749 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:04,159][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6472763 effective words) took 2.4s, 2721785 effective words/s
[2023-02-07 12:28:05,165][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 41.30% examples, 2726032 words/s, in_qsize 6, out_qsize 1
[2023-02-07 12:28:06,165][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 84.59% examples, 2754911 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:06,509][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6472763 effective words) took 2.3s, 2755990 effective words/s
[2023-02-07 12:28:07,511][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 41.42% examples, 2743769 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:08,512][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 83.57% examples, 2733768 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:08,882][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6472763 effective words) took 2.4s, 2729788 effective words/s
[2023-02-07 12:28:09,884][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 40.31% examples, 2676004 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:10,888][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 82.84% examples, 2702643 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:11,269][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6472763 effective words) took 2.4s, 2712416 effective words/s
[2023-02-07 12:28:12,271][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 40.93% examples, 2716729 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:28:13,272][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 83.33% examples, 2724442 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:13,640][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6472763 effective words) took 2.4s, 2731644 effective words/s
[2023-02-07 12:28:14,645][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 40.93% examples, 2710243 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:15,650][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 84.00% examples, 2733691 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:16,007][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6472763 effective words) took 2.4s, 2736511 effective words/s
[2023-02-07 12:28:17,013][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 41.42% examples, 2733437 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:18,019][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 83.33% examples, 2711330 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:18,387][gensim.models.word2vec][INFO] - EPOCH 15: training on 6550866 raw words (6472763 effective words) took 2.4s, 2720717 effective words/s
[2023-02-07 12:28:19,390][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 40.74% examples, 2699336 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:20,393][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 84.00% examples, 2739946 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:20,744][gensim.models.word2vec][INFO] - EPOCH 16: training on 6550866 raw words (6472763 effective words) took 2.4s, 2748262 effective words/s
[2023-02-07 12:28:21,747][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 41.82% examples, 2769262 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:22,747][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 82.13% examples, 2677734 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:23,172][gensim.models.word2vec][INFO] - EPOCH 17: training on 6550866 raw words (6472763 effective words) took 2.4s, 2666552 effective words/s
[2023-02-07 12:28:24,175][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 38.19% examples, 2521306 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:25,175][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 78.23% examples, 2553958 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:25,696][gensim.models.word2vec][INFO] - EPOCH 18: training on 6550866 raw words (6472763 effective words) took 2.5s, 2567330 effective words/s
[2023-02-07 12:28:26,701][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 39.48% examples, 2616631 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:27,701][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 79.15% examples, 2586638 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:28,202][gensim.models.word2vec][INFO] - EPOCH 19: training on 6550866 raw words (6472763 effective words) took 2.5s, 2585185 effective words/s
[2023-02-07 12:28:29,209][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 38.62% examples, 2539890 words/s, in_qsize 6, out_qsize 1
[2023-02-07 12:28:30,212][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 78.81% examples, 2566634 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:30,754][gensim.models.word2vec][INFO] - EPOCH 20: training on 6550866 raw words (6472763 effective words) took 2.6s, 2537577 effective words/s
[2023-02-07 12:28:31,757][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 39.48% examples, 2619621 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:32,760][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 79.28% examples, 2588723 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:33,263][gensim.models.word2vec][INFO] - EPOCH 21: training on 6550866 raw words (6472763 effective words) took 2.5s, 2580463 effective words/s
[2023-02-07 12:28:34,267][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 38.29% examples, 2523537 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:35,268][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 77.80% examples, 2536877 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:35,823][gensim.models.word2vec][INFO] - EPOCH 22: training on 6550866 raw words (6472763 effective words) took 2.6s, 2530404 effective words/s
[2023-02-07 12:28:36,827][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 39.05% examples, 2566164 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:37,827][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 78.02% examples, 2545673 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:38,367][gensim.models.word2vec][INFO] - EPOCH 23: training on 6550866 raw words (6472763 effective words) took 2.5s, 2545159 effective words/s
[2023-02-07 12:28:39,370][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 37.06% examples, 2446894 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:40,372][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 77.06% examples, 2516639 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:40,916][gensim.models.word2vec][INFO] - EPOCH 24: training on 6550866 raw words (6472763 effective words) took 2.5s, 2540828 effective words/s
[2023-02-07 12:28:41,922][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 38.19% examples, 2510709 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:42,923][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 78.02% examples, 2542626 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:43,468][gensim.models.word2vec][INFO] - EPOCH 25: training on 6550866 raw words (6472763 effective words) took 2.6s, 2537307 effective words/s
[2023-02-07 12:28:44,475][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 37.55% examples, 2474094 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:45,479][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 79.24% examples, 2582533 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:45,931][gensim.models.word2vec][INFO] - EPOCH 26: training on 6550866 raw words (6472763 effective words) took 2.5s, 2630297 effective words/s
[2023-02-07 12:28:46,932][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 42.62% examples, 2839148 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:47,933][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 86.40% examples, 2816821 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:48,227][gensim.models.word2vec][INFO] - EPOCH 27: training on 6550866 raw words (6472763 effective words) took 2.3s, 2820242 effective words/s
[2023-02-07 12:28:49,233][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 41.42% examples, 2734286 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:50,234][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 82.71% examples, 2693513 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:50,643][gensim.models.word2vec][INFO] - EPOCH 28: training on 6550866 raw words (6472763 effective words) took 2.4s, 2681160 effective words/s
[2023-02-07 12:28:51,647][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 39.39% examples, 2605176 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:52,649][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 78.88% examples, 2576659 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:53,158][gensim.models.word2vec][INFO] - EPOCH 29: training on 6550866 raw words (6472763 effective words) took 2.5s, 2574340 effective words/s
[2023-02-07 12:28:54,160][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 38.19% examples, 2521057 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:55,166][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 78.85% examples, 2569040 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:55,673][gensim.models.word2vec][INFO] - EPOCH 30: training on 6550866 raw words (6472763 effective words) took 2.5s, 2574807 effective words/s
[2023-02-07 12:28:56,674][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 39.21% examples, 2597263 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:57,681][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 78.81% examples, 2569528 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:28:58,182][gensim.models.word2vec][INFO] - EPOCH 31: training on 6550866 raw words (6472763 effective words) took 2.5s, 2580685 effective words/s
[2023-02-07 12:28:59,185][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 38.62% examples, 2551291 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:00,190][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 78.88% examples, 2575828 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:00,701][gensim.models.word2vec][INFO] - EPOCH 32: training on 6550866 raw words (6472763 effective words) took 2.5s, 2570987 effective words/s
[2023-02-07 12:29:01,706][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 40.01% examples, 2652659 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:02,711][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 80.66% examples, 2620570 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:03,177][gensim.models.word2vec][INFO] - EPOCH 33: training on 6550866 raw words (6472763 effective words) took 2.5s, 2616822 effective words/s
[2023-02-07 12:29:04,179][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 38.87% examples, 2563259 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:05,180][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 79.46% examples, 2601427 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:05,661][gensim.models.word2vec][INFO] - EPOCH 34: training on 6550866 raw words (6472763 effective words) took 2.5s, 2606817 effective words/s
[2023-02-07 12:29:06,667][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 39.85% examples, 2636711 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:07,672][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 79.77% examples, 2598790 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:08,134][gensim.models.word2vec][INFO] - EPOCH 35: training on 6550866 raw words (6472763 effective words) took 2.5s, 2618058 effective words/s
[2023-02-07 12:29:09,137][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 39.18% examples, 2584753 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:10,141][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 80.66% examples, 2624735 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:10,604][gensim.models.word2vec][INFO] - EPOCH 36: training on 6550866 raw words (6472763 effective words) took 2.5s, 2622840 effective words/s
[2023-02-07 12:29:11,606][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 40.01% examples, 2657409 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:12,610][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 81.15% examples, 2641662 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:13,052][gensim.models.word2vec][INFO] - EPOCH 37: training on 6550866 raw words (6472763 effective words) took 2.4s, 2645305 effective words/s
[2023-02-07 12:29:14,058][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 39.12% examples, 2572492 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:15,058][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 79.37% examples, 2592403 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:15,550][gensim.models.word2vec][INFO] - EPOCH 38: training on 6550866 raw words (6472763 effective words) took 2.5s, 2593131 effective words/s
[2023-02-07 12:29:16,558][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 40.01% examples, 2642750 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:17,559][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 80.66% examples, 2619805 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:18,014][gensim.models.word2vec][INFO] - EPOCH 39: training on 6550866 raw words (6472763 effective words) took 2.5s, 2627931 effective words/s
[2023-02-07 12:29:19,016][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 39.30% examples, 2606359 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:20,017][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 80.66% examples, 2628787 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:20,477][gensim.models.word2vec][INFO] - EPOCH 40: training on 6550866 raw words (6472763 effective words) took 2.5s, 2629332 effective words/s
[2023-02-07 12:29:21,481][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 39.85% examples, 2644272 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:22,482][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 81.09% examples, 2639380 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:22,927][gensim.models.word2vec][INFO] - EPOCH 41: training on 6550866 raw words (6472763 effective words) took 2.4s, 2643499 effective words/s
[2023-02-07 12:29:23,933][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 38.47% examples, 2533798 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:24,935][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 78.23% examples, 2547443 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:25,485][gensim.models.word2vec][INFO] - EPOCH 42: training on 6550866 raw words (6472763 effective words) took 2.6s, 2532340 effective words/s
[2023-02-07 12:29:26,490][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 38.72% examples, 2545602 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:27,494][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 77.25% examples, 2516254 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:28,054][gensim.models.word2vec][INFO] - EPOCH 43: training on 6550866 raw words (6472763 effective words) took 2.6s, 2520580 effective words/s
[2023-02-07 12:29:29,056][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 36.94% examples, 2441768 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:30,057][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 75.35% examples, 2475383 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:30,673][gensim.models.word2vec][INFO] - EPOCH 44: training on 6550866 raw words (6472763 effective words) took 2.6s, 2472871 effective words/s
[2023-02-07 12:29:31,680][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 38.38% examples, 2524044 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:32,682][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 77.06% examples, 2511770 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:33,245][gensim.models.word2vec][INFO] - EPOCH 45: training on 6550866 raw words (6472763 effective words) took 2.6s, 2517336 effective words/s
[2023-02-07 12:29:34,250][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 37.40% examples, 2471196 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:35,253][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 76.54% examples, 2501644 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:35,836][gensim.models.word2vec][INFO] - EPOCH 46: training on 6550866 raw words (6472763 effective words) took 2.6s, 2500083 effective words/s
[2023-02-07 12:29:36,839][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 38.38% examples, 2534350 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:37,839][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 77.37% examples, 2527360 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:38,397][gensim.models.word2vec][INFO] - EPOCH 47: training on 6550866 raw words (6472763 effective words) took 2.6s, 2528802 effective words/s
[2023-02-07 12:29:39,404][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 38.19% examples, 2510736 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:40,406][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 78.02% examples, 2541465 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:40,942][gensim.models.word2vec][INFO] - EPOCH 48: training on 6550866 raw words (6472763 effective words) took 2.5s, 2545421 effective words/s
[2023-02-07 12:29:41,944][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 39.21% examples, 2595182 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:42,946][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 78.72% examples, 2571226 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:43,453][gensim.models.word2vec][INFO] - EPOCH 49: training on 6550866 raw words (6472763 effective words) took 2.5s, 2579011 effective words/s
[2023-02-07 12:29:44,459][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 38.38% examples, 2528593 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:45,462][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 78.88% examples, 2574411 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:45,972][gensim.models.word2vec][INFO] - EPOCH 50: training on 6550866 raw words (6472763 effective words) took 2.5s, 2570986 effective words/s
[2023-02-07 12:29:46,975][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 38.72% examples, 2553224 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:47,979][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 78.63% examples, 2563468 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:48,465][gensim.models.word2vec][INFO] - EPOCH 51: training on 6550866 raw words (6472763 effective words) took 2.5s, 2598086 effective words/s
[2023-02-07 12:29:49,467][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 40.01% examples, 2656870 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:50,470][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 82.81% examples, 2699851 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:50,861][gensim.models.word2vec][INFO] - EPOCH 52: training on 6550866 raw words (6472763 effective words) took 2.4s, 2702677 effective words/s
[2023-02-07 12:29:51,867][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 40.74% examples, 2688023 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:52,871][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 82.38% examples, 2675120 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:53,282][gensim.models.word2vec][INFO] - EPOCH 53: training on 6550866 raw words (6472763 effective words) took 2.4s, 2674389 effective words/s
[2023-02-07 12:29:54,287][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 39.48% examples, 2616395 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:55,287][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 81.42% examples, 2655794 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:55,722][gensim.models.word2vec][INFO] - EPOCH 54: training on 6550866 raw words (6472763 effective words) took 2.4s, 2654317 effective words/s
[2023-02-07 12:29:56,725][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 39.51% examples, 2617516 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:57,726][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 79.58% examples, 2604798 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:29:58,202][gensim.models.word2vec][INFO] - EPOCH 55: training on 6550866 raw words (6472763 effective words) took 2.5s, 2611062 effective words/s
[2023-02-07 12:29:59,204][gensim.models.word2vec][INFO] - EPOCH 56 - PROGRESS: at 39.21% examples, 2597455 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:00,206][gensim.models.word2vec][INFO] - EPOCH 56 - PROGRESS: at 81.39% examples, 2654305 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:00,640][gensim.models.word2vec][INFO] - EPOCH 56: training on 6550866 raw words (6472763 effective words) took 2.4s, 2656531 effective words/s
[2023-02-07 12:30:01,642][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 40.01% examples, 2658577 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:02,643][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 80.01% examples, 2615473 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:03,132][gensim.models.word2vec][INFO] - EPOCH 57: training on 6550866 raw words (6472763 effective words) took 2.5s, 2598778 effective words/s
[2023-02-07 12:30:04,135][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 37.18% examples, 2458550 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:05,138][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 75.59% examples, 2476473 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:05,757][gensim.models.word2vec][INFO] - EPOCH 58: training on 6550866 raw words (6472763 effective words) took 2.6s, 2467448 effective words/s
[2023-02-07 12:30:06,760][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 37.40% examples, 2475954 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:07,765][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 75.35% examples, 2470084 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:08,365][gensim.models.word2vec][INFO] - EPOCH 59: training on 6550866 raw words (6472763 effective words) took 2.6s, 2483348 effective words/s
[2023-02-07 12:30:09,367][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 37.30% examples, 2469548 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:10,368][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 75.96% examples, 2489229 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:10,978][gensim.models.word2vec][INFO] - EPOCH 60: training on 6550866 raw words (6472763 effective words) took 2.6s, 2479298 effective words/s
[2023-02-07 12:30:11,980][gensim.models.word2vec][INFO] - EPOCH 61 - PROGRESS: at 38.10% examples, 2511536 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:12,984][gensim.models.word2vec][INFO] - EPOCH 61 - PROGRESS: at 75.35% examples, 2470976 words/s, in_qsize 7, out_qsize 1
[2023-02-07 12:30:13,576][gensim.models.word2vec][INFO] - EPOCH 61: training on 6550866 raw words (6472763 effective words) took 2.6s, 2492196 effective words/s
[2023-02-07 12:30:14,586][gensim.models.word2vec][INFO] - EPOCH 62 - PROGRESS: at 37.55% examples, 2465947 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:15,591][gensim.models.word2vec][INFO] - EPOCH 62 - PROGRESS: at 77.06% examples, 2504834 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:16,153][gensim.models.word2vec][INFO] - EPOCH 62: training on 6550866 raw words (6472763 effective words) took 2.6s, 2513730 effective words/s
[2023-02-07 12:30:17,154][gensim.models.word2vec][INFO] - EPOCH 63 - PROGRESS: at 37.83% examples, 2496441 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:18,160][gensim.models.word2vec][INFO] - EPOCH 63 - PROGRESS: at 76.05% examples, 2487069 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:18,744][gensim.models.word2vec][INFO] - EPOCH 63: training on 6550866 raw words (6472763 effective words) took 2.6s, 2499628 effective words/s
[2023-02-07 12:30:19,750][gensim.models.word2vec][INFO] - EPOCH 64 - PROGRESS: at 37.40% examples, 2468231 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:20,753][gensim.models.word2vec][INFO] - EPOCH 64 - PROGRESS: at 76.70% examples, 2502358 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:21,330][gensim.models.word2vec][INFO] - EPOCH 64: training on 6550866 raw words (6472763 effective words) took 2.6s, 2504048 effective words/s
[2023-02-07 12:30:22,332][gensim.models.word2vec][INFO] - EPOCH 65 - PROGRESS: at 38.19% examples, 2521517 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:23,335][gensim.models.word2vec][INFO] - EPOCH 65 - PROGRESS: at 76.79% examples, 2510159 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:23,899][gensim.models.word2vec][INFO] - EPOCH 65: training on 6550866 raw words (6472763 effective words) took 2.6s, 2521455 effective words/s
[2023-02-07 12:30:24,903][gensim.models.word2vec][INFO] - EPOCH 66 - PROGRESS: at 36.94% examples, 2438648 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:25,904][gensim.models.word2vec][INFO] - EPOCH 66 - PROGRESS: at 73.99% examples, 2424972 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:26,591][gensim.models.word2vec][INFO] - EPOCH 66: training on 6550866 raw words (6472763 effective words) took 2.7s, 2405794 effective words/s
[2023-02-07 12:30:27,593][gensim.models.word2vec][INFO] - EPOCH 67 - PROGRESS: at 36.35% examples, 2387705 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:28,595][gensim.models.word2vec][INFO] - EPOCH 67 - PROGRESS: at 72.64% examples, 2383218 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:29,286][gensim.models.word2vec][INFO] - EPOCH 67: training on 6550866 raw words (6472763 effective words) took 2.7s, 2402432 effective words/s
[2023-02-07 12:30:30,289][gensim.models.word2vec][INFO] - EPOCH 68 - PROGRESS: at 36.17% examples, 2372658 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:31,291][gensim.models.word2vec][INFO] - EPOCH 68 - PROGRESS: at 73.32% examples, 2401750 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:31,983][gensim.models.word2vec][INFO] - EPOCH 68: training on 6550866 raw words (6472763 effective words) took 2.7s, 2401596 effective words/s
[2023-02-07 12:30:32,986][gensim.models.word2vec][INFO] - EPOCH 69 - PROGRESS: at 36.66% examples, 2423198 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:33,986][gensim.models.word2vec][INFO] - EPOCH 69 - PROGRESS: at 73.50% examples, 2409277 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:34,669][gensim.models.word2vec][INFO] - EPOCH 69: training on 6550866 raw words (6472763 effective words) took 2.7s, 2411386 effective words/s
[2023-02-07 12:30:35,673][gensim.models.word2vec][INFO] - EPOCH 70 - PROGRESS: at 35.95% examples, 2357435 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:36,677][gensim.models.word2vec][INFO] - EPOCH 70 - PROGRESS: at 72.89% examples, 2390491 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:37,373][gensim.models.word2vec][INFO] - EPOCH 70: training on 6550866 raw words (6472763 effective words) took 2.7s, 2396675 effective words/s
[2023-02-07 12:30:38,376][gensim.models.word2vec][INFO] - EPOCH 71 - PROGRESS: at 37.18% examples, 2458406 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:39,379][gensim.models.word2vec][INFO] - EPOCH 71 - PROGRESS: at 74.33% examples, 2433170 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:40,035][gensim.models.word2vec][INFO] - EPOCH 71: training on 6550866 raw words (6472763 effective words) took 2.7s, 2432811 effective words/s
[2023-02-07 12:30:41,037][gensim.models.word2vec][INFO] - EPOCH 72 - PROGRESS: at 36.35% examples, 2390308 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:42,038][gensim.models.word2vec][INFO] - EPOCH 72 - PROGRESS: at 73.81% examples, 2418095 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:42,719][gensim.models.word2vec][INFO] - EPOCH 72: training on 6550866 raw words (6472763 effective words) took 2.7s, 2413617 effective words/s
[2023-02-07 12:30:43,723][gensim.models.word2vec][INFO] - EPOCH 73 - PROGRESS: at 36.94% examples, 2436227 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:44,723][gensim.models.word2vec][INFO] - EPOCH 73 - PROGRESS: at 73.35% examples, 2402676 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:45,401][gensim.models.word2vec][INFO] - EPOCH 73: training on 6550866 raw words (6472763 effective words) took 2.7s, 2414769 effective words/s
[2023-02-07 12:30:46,409][gensim.models.word2vec][INFO] - EPOCH 74 - PROGRESS: at 36.48% examples, 2395148 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:47,410][gensim.models.word2vec][INFO] - EPOCH 74 - PROGRESS: at 74.36% examples, 2431282 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:48,068][gensim.models.word2vec][INFO] - EPOCH 74: training on 6550866 raw words (6472763 effective words) took 2.7s, 2428113 effective words/s
[2023-02-07 12:30:49,072][gensim.models.word2vec][INFO] - EPOCH 75 - PROGRESS: at 36.81% examples, 2426634 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:50,074][gensim.models.word2vec][INFO] - EPOCH 75 - PROGRESS: at 75.84% examples, 2483088 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:50,641][gensim.models.word2vec][INFO] - EPOCH 75: training on 6550866 raw words (6472763 effective words) took 2.6s, 2518794 effective words/s
[2023-02-07 12:30:51,645][gensim.models.word2vec][INFO] - EPOCH 76 - PROGRESS: at 39.12% examples, 2578908 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:52,647][gensim.models.word2vec][INFO] - EPOCH 76 - PROGRESS: at 79.77% examples, 2607307 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:53,130][gensim.models.word2vec][INFO] - EPOCH 76: training on 6550866 raw words (6472763 effective words) took 2.5s, 2601926 effective words/s
[2023-02-07 12:30:54,134][gensim.models.word2vec][INFO] - EPOCH 77 - PROGRESS: at 39.05% examples, 2567478 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:55,134][gensim.models.word2vec][INFO] - EPOCH 77 - PROGRESS: at 76.54% examples, 2505383 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:55,736][gensim.models.word2vec][INFO] - EPOCH 77: training on 6550866 raw words (6472763 effective words) took 2.6s, 2485592 effective words/s
[2023-02-07 12:30:56,739][gensim.models.word2vec][INFO] - EPOCH 78 - PROGRESS: at 36.17% examples, 2370433 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:57,741][gensim.models.word2vec][INFO] - EPOCH 78 - PROGRESS: at 73.81% examples, 2415345 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:30:58,419][gensim.models.word2vec][INFO] - EPOCH 78: training on 6550866 raw words (6472763 effective words) took 2.7s, 2413572 effective words/s
[2023-02-07 12:30:59,422][gensim.models.word2vec][INFO] - EPOCH 79 - PROGRESS: at 37.40% examples, 2474962 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:00,430][gensim.models.word2vec][INFO] - EPOCH 79 - PROGRESS: at 74.92% examples, 2449246 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:01,046][gensim.models.word2vec][INFO] - EPOCH 79: training on 6550866 raw words (6472763 effective words) took 2.6s, 2465107 effective words/s
[2023-02-07 12:31:02,049][gensim.models.word2vec][INFO] - EPOCH 80 - PROGRESS: at 36.94% examples, 2440934 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:03,052][gensim.models.word2vec][INFO] - EPOCH 80 - PROGRESS: at 74.92% examples, 2456005 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:03,679][gensim.models.word2vec][INFO] - EPOCH 80: training on 6550866 raw words (6472763 effective words) took 2.6s, 2460531 effective words/s
[2023-02-07 12:31:04,684][gensim.models.word2vec][INFO] - EPOCH 81 - PROGRESS: at 38.19% examples, 2511174 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:05,687][gensim.models.word2vec][INFO] - EPOCH 81 - PROGRESS: at 75.35% examples, 2469499 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:31:06,295][gensim.models.word2vec][INFO] - EPOCH 81: training on 6550866 raw words (6472763 effective words) took 2.6s, 2475510 effective words/s
[2023-02-07 12:31:07,298][gensim.models.word2vec][INFO] - EPOCH 82 - PROGRESS: at 37.06% examples, 2447574 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:08,305][gensim.models.word2vec][INFO] - EPOCH 82 - PROGRESS: at 76.17% examples, 2488668 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:08,901][gensim.models.word2vec][INFO] - EPOCH 82: training on 6550866 raw words (6472763 effective words) took 2.6s, 2484842 effective words/s
[2023-02-07 12:31:09,903][gensim.models.word2vec][INFO] - EPOCH 83 - PROGRESS: at 38.53% examples, 2546336 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:10,906][gensim.models.word2vec][INFO] - EPOCH 83 - PROGRESS: at 77.53% examples, 2530482 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:11,452][gensim.models.word2vec][INFO] - EPOCH 83: training on 6550866 raw words (6472763 effective words) took 2.5s, 2539174 effective words/s
[2023-02-07 12:31:12,454][gensim.models.word2vec][INFO] - EPOCH 84 - PROGRESS: at 37.06% examples, 2450255 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:13,463][gensim.models.word2vec][INFO] - EPOCH 84 - PROGRESS: at 76.05% examples, 2482674 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:14,056][gensim.models.word2vec][INFO] - EPOCH 84: training on 6550866 raw words (6472763 effective words) took 2.6s, 2486628 effective words/s
[2023-02-07 12:31:15,061][gensim.models.word2vec][INFO] - EPOCH 85 - PROGRESS: at 38.19% examples, 2513700 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:16,067][gensim.models.word2vec][INFO] - EPOCH 85 - PROGRESS: at 75.35% examples, 2466500 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:16,668][gensim.models.word2vec][INFO] - EPOCH 85: training on 6550866 raw words (6472763 effective words) took 2.6s, 2479858 effective words/s
[2023-02-07 12:31:17,670][gensim.models.word2vec][INFO] - EPOCH 86 - PROGRESS: at 37.83% examples, 2496732 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:18,673][gensim.models.word2vec][INFO] - EPOCH 86 - PROGRESS: at 77.59% examples, 2531275 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:19,233][gensim.models.word2vec][INFO] - EPOCH 86: training on 6550866 raw words (6472763 effective words) took 2.6s, 2525250 effective words/s
[2023-02-07 12:31:20,235][gensim.models.word2vec][INFO] - EPOCH 87 - PROGRESS: at 38.72% examples, 2554240 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:21,238][gensim.models.word2vec][INFO] - EPOCH 87 - PROGRESS: at 77.59% examples, 2531572 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:21,781][gensim.models.word2vec][INFO] - EPOCH 87: training on 6550866 raw words (6472763 effective words) took 2.5s, 2541749 effective words/s
[2023-02-07 12:31:22,784][gensim.models.word2vec][INFO] - EPOCH 88 - PROGRESS: at 36.94% examples, 2440170 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:23,785][gensim.models.word2vec][INFO] - EPOCH 88 - PROGRESS: at 74.64% examples, 2449518 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:24,428][gensim.models.word2vec][INFO] - EPOCH 88: training on 6550866 raw words (6472763 effective words) took 2.6s, 2446464 effective words/s
[2023-02-07 12:31:25,430][gensim.models.word2vec][INFO] - EPOCH 89 - PROGRESS: at 37.30% examples, 2468720 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:26,431][gensim.models.word2vec][INFO] - EPOCH 89 - PROGRESS: at 73.84% examples, 2422103 words/s, in_qsize 7, out_qsize 1
[2023-02-07 12:31:27,090][gensim.models.word2vec][INFO] - EPOCH 89: training on 6550866 raw words (6472763 effective words) took 2.7s, 2432895 effective words/s
[2023-02-07 12:31:28,091][gensim.models.word2vec][INFO] - EPOCH 90 - PROGRESS: at 36.14% examples, 2372759 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:29,096][gensim.models.word2vec][INFO] - EPOCH 90 - PROGRESS: at 73.84% examples, 2418567 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:29,764][gensim.models.word2vec][INFO] - EPOCH 90: training on 6550866 raw words (6472763 effective words) took 2.7s, 2422289 effective words/s
[2023-02-07 12:31:30,767][gensim.models.word2vec][INFO] - EPOCH 91 - PROGRESS: at 36.94% examples, 2438708 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:31,767][gensim.models.word2vec][INFO] - EPOCH 91 - PROGRESS: at 74.15% examples, 2430584 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:31:32,410][gensim.models.word2vec][INFO] - EPOCH 91: training on 6550866 raw words (6472763 effective words) took 2.6s, 2447208 effective words/s
[2023-02-07 12:31:32,411][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 602679672 raw words (595494196 effective words) took 231.7s, 2570428 effective words/s', 'datetime': '2023-02-07T12:31:32.410991', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:31:32.411 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:31:50,664][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122712-zyetbbuc/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:31:50.664067', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:31:50,665][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:31:50,760][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_122712-zyetbbuc/files/../tmp/embedding_model.pt
2023-02-07 12:31:50.762 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:31:52.112 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:31:52.607 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:31:53.456 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.590359587930242, 'test_mae': 1.2436442701660233, 'test_r2': -0.22539496241130763}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.0
wandb: percentage 0.2854
wandb:   test_mae 1.24364
wandb:   test_mse 2.59036
wandb:    test_r2 -0.22539
wandb: 
wandb: üöÄ View run soft-sweep-5 at: https://wandb.ai/xiaoqiz/mof2vec/runs/zyetbbuc
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_122712-zyetbbuc/logs
wandb: Agent Starting Run: xkb9j3fu with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 951
wandb: 	model.gensim.alpha: 0.00919482739223637
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.epochs: 73
wandb: 	model.gensim.min_count: 8
wandb: 	model.gensim.sample: 0.3101622165985167
wandb: 	model.gensim.vector_size: 160
wandb: 	model.gensim.window: 8
wandb: 	model.sklearn.learning_rate: 0.012262953032497671
wandb: 	model.sklearn.max_depth: 31
wandb: 	model.sklearn.min_child_weight: 0.03233206746894821
wandb: 	model.sklearn.n_estimators: 1310
wandb: 	model.sklearn.num_leaves: 384
wandb: 	model.sklearn.reg_alpha: 0.014805423237540794
wandb: 	model.sklearn.reg_lambda: 0.028044872533436963
wandb: 	model.sklearn.subsample: 0.5243752935244369
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123207-xkb9j3fu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sweep-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/xkb9j3fu
2023-02-07 12:32:15.950 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 12:32:15.951 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 951 for sweep.
2023-02-07 12:32:15.951 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.00919482739223637 for sweep.
2023-02-07 12:32:15.951 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 12:32:15.951 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 73 for sweep.
2023-02-07 12:32:15.952 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 8 for sweep.
2023-02-07 12:32:15.952 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3101622165985167 for sweep.
2023-02-07 12:32:15.952 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 160 for sweep.
2023-02-07 12:32:15.952 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 8 for sweep.
2023-02-07 12:32:15.952 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.012262953032497671 for sweep.
2023-02-07 12:32:15.953 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 31 for sweep.
2023-02-07 12:32:15.953 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03233206746894821 for sweep.
2023-02-07 12:32:15.953 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1310 for sweep.
2023-02-07 12:32:15.953 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 384 for sweep.
2023-02-07 12:32:15.953 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.014805423237540794 for sweep.
2023-02-07 12:32:15.954 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.028044872533436963 for sweep.
2023-02-07 12:32:15.954 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5243752935244369 for sweep.
2023-02-07 12:32:15.954 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:32:15.958 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123207-xkb9j3fu/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 951, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 160, 'window': 8, 'min_count': 8, 'dm': 1, 'sample': 0.3101622165985167, 'workers': 4, 'alpha': 0.00919482739223637, 'epochs': 73}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1310, 'max_depth': 31, 'num_leaves': 384, 'reg_alpha': 0.014805423237540794, 'reg_lambda': 0.028044872533436963, 'subsample': 0.5243752935244369, 'min_child_weight': 0.03233206746894821, 'n_jobs': 4, 'learning_rate': 0.012262953032497671}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 19/3257 [00:00<00:17, 185.07it/s]  1%|‚ñè         | 41/3257 [00:00<00:16, 197.11it/s]  2%|‚ñè         | 62/3257 [00:00<00:15, 201.34it/s]  3%|‚ñé         | 86/3257 [00:00<00:14, 215.39it/s]  3%|‚ñé         | 108/3257 [00:00<00:16, 196.13it/s]  4%|‚ñç         | 130/3257 [00:00<00:15, 202.48it/s]  5%|‚ñç         | 153/3257 [00:00<00:14, 209.66it/s]  5%|‚ñå         | 175/3257 [00:00<00:15, 199.05it/s]  6%|‚ñå         | 199/3257 [00:00<00:14, 209.20it/s]  7%|‚ñã         | 223/3257 [00:01<00:13, 216.91it/s]  8%|‚ñä         | 245/3257 [00:01<00:13, 215.66it/s]  8%|‚ñä         | 267/3257 [00:01<00:14, 205.51it/s]  9%|‚ñâ         | 296/3257 [00:01<00:13, 227.21it/s] 10%|‚ñâ         | 319/3257 [00:01<00:13, 219.52it/s] 11%|‚ñà         | 342/3257 [00:01<00:13, 215.96it/s] 11%|‚ñà         | 366/3257 [00:01<00:13, 219.90it/s] 12%|‚ñà‚ñè        | 389/3257 [00:01<00:14, 203.61it/s] 13%|‚ñà‚ñé        | 413/3257 [00:01<00:13, 212.39it/s] 13%|‚ñà‚ñé        | 435/3257 [00:02<00:15, 182.57it/s] 14%|‚ñà‚ñç        | 459/3257 [00:02<00:14, 194.83it/s] 15%|‚ñà‚ñç        | 480/3257 [00:02<00:14, 196.02it/s] 16%|‚ñà‚ñå        | 505/3257 [00:02<00:13, 208.50it/s] 16%|‚ñà‚ñå        | 527/3257 [00:02<00:13, 206.38it/s] 17%|‚ñà‚ñã        | 550/3257 [00:02<00:12, 210.22it/s] 18%|‚ñà‚ñä        | 572/3257 [00:02<00:14, 187.48it/s] 18%|‚ñà‚ñä        | 597/3257 [00:02<00:13, 203.75it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:03<00:13, 200.20it/s] 20%|‚ñà‚ñâ        | 643/3257 [00:03<00:12, 206.15it/s] 20%|‚ñà‚ñà        | 664/3257 [00:03<00:13, 190.31it/s] 21%|‚ñà‚ñà        | 684/3257 [00:03<00:13, 190.25it/s] 22%|‚ñà‚ñà‚ñè       | 705/3257 [00:03<00:13, 195.57it/s] 22%|‚ñà‚ñà‚ñè       | 725/3257 [00:03<00:13, 194.15it/s] 23%|‚ñà‚ñà‚ñé       | 745/3257 [00:03<00:13, 191.14it/s] 24%|‚ñà‚ñà‚ñé       | 770/3257 [00:03<00:12, 202.02it/s] 24%|‚ñà‚ñà‚ñç       | 791/3257 [00:03<00:12, 196.80it/s] 25%|‚ñà‚ñà‚ñç       | 812/3257 [00:04<00:12, 197.68it/s] 26%|‚ñà‚ñà‚ñå       | 832/3257 [00:04<00:12, 192.72it/s] 26%|‚ñà‚ñà‚ñå       | 852/3257 [00:04<00:12, 185.36it/s] 27%|‚ñà‚ñà‚ñã       | 872/3257 [00:04<00:12, 188.48it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:04<00:19, 122.53it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:04<00:16, 141.05it/s] 29%|‚ñà‚ñà‚ñä       | 935/3257 [00:04<00:14, 156.36it/s] 29%|‚ñà‚ñà‚ñâ       | 958/3257 [00:04<00:13, 173.23it/s] 30%|‚ñà‚ñà‚ñà       | 979/3257 [00:05<00:12, 177.03it/s] 31%|‚ñà‚ñà‚ñà       | 999/3257 [00:05<00:12, 180.31it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1020/3257 [00:05<00:11, 187.18it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1040/3257 [00:05<00:12, 177.62it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1059/3257 [00:05<00:12, 178.86it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:05<00:11, 182.82it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1099/3257 [00:05<00:11, 183.98it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1119/3257 [00:05<00:11, 183.90it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1139/3257 [00:05<00:11, 184.18it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1159/3257 [00:06<00:11, 187.10it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1178/3257 [00:06<00:11, 177.23it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1196/3257 [00:06<00:12, 171.55it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1214/3257 [00:06<00:12, 165.88it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1238/3257 [00:06<00:10, 185.76it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1257/3257 [00:06<00:11, 180.15it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1276/3257 [00:06<00:11, 178.28it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1294/3257 [00:06<00:11, 170.79it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1314/3257 [00:06<00:10, 177.61it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1335/3257 [00:07<00:10, 186.13it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1354/3257 [00:07<00:10, 179.29it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1373/3257 [00:07<00:10, 180.01it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1392/3257 [00:07<00:10, 181.31it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1419/3257 [00:07<00:08, 205.66it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1440/3257 [00:07<00:09, 197.52it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1466/3257 [00:07<00:08, 212.38it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1488/3257 [00:07<00:08, 213.19it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:07<00:07, 220.22it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1536/3257 [00:08<00:08, 198.67it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1557/3257 [00:08<00:08, 192.66it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1577/3257 [00:08<00:08, 191.90it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1600/3257 [00:08<00:08, 201.55it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1621/3257 [00:08<00:08, 201.60it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1642/3257 [00:08<00:08, 197.22it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1662/3257 [00:08<00:08, 190.93it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1682/3257 [00:08<00:08, 190.57it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1703/3257 [00:08<00:08, 194.07it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1723/3257 [00:08<00:07, 194.72it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1743/3257 [00:09<00:08, 176.89it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1766/3257 [00:09<00:07, 189.87it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1790/3257 [00:09<00:07, 201.98it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1811/3257 [00:09<00:07, 190.33it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1831/3257 [00:09<00:07, 188.55it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1853/3257 [00:09<00:07, 197.00it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1876/3257 [00:09<00:06, 204.69it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1897/3257 [00:09<00:06, 198.69it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1918/3257 [00:09<00:06, 196.70it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1945/3257 [00:10<00:06, 215.05it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1971/3257 [00:10<00:05, 223.72it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1994/3257 [00:10<00:06, 207.03it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2015/3257 [00:10<00:06, 202.32it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2036/3257 [00:10<00:05, 203.89it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2057/3257 [00:10<00:06, 186.25it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2076/3257 [00:10<00:06, 186.89it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2096/3257 [00:10<00:06, 183.29it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2117/3257 [00:10<00:05, 190.50it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2137/3257 [00:11<00:06, 178.73it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2156/3257 [00:11<00:06, 175.69it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2174/3257 [00:11<00:10, 107.41it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2190/3257 [00:11<00:09, 117.34it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2209/3257 [00:11<00:08, 130.69it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2229/3257 [00:11<00:07, 146.00it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2247/3257 [00:11<00:06, 153.13it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2267/3257 [00:12<00:06, 163.44it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2286/3257 [00:12<00:05, 170.25it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2306/3257 [00:12<00:05, 176.47it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2332/3257 [00:12<00:04, 199.03it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:12<00:04, 212.16it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2383/3257 [00:12<00:04, 214.95it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:12<00:04, 210.69it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2427/3257 [00:12<00:03, 211.98it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2449/3257 [00:12<00:04, 191.93it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2475/3257 [00:13<00:03, 208.30it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2498/3257 [00:13<00:03, 213.38it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2522/3257 [00:13<00:03, 219.48it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2545/3257 [00:13<00:03, 216.32it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2567/3257 [00:13<00:03, 203.57it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2588/3257 [00:13<00:03, 192.55it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2615/3257 [00:13<00:03, 211.69it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:13<00:02, 217.44it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2663/3257 [00:13<00:02, 203.73it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2686/3257 [00:14<00:02, 210.47it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2708/3257 [00:14<00:02, 188.43it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2729/3257 [00:14<00:02, 193.09it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2751/3257 [00:14<00:02, 199.61it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2772/3257 [00:14<00:02, 191.32it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2800/3257 [00:14<00:02, 214.22it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2822/3257 [00:14<00:02, 211.41it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:14<00:02, 203.25it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2876/3257 [00:14<00:01, 235.15it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2900/3257 [00:15<00:01, 218.64it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2926/3257 [00:15<00:01, 229.60it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:15<00:01, 211.34it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2974/3257 [00:15<00:01, 217.28it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2997/3257 [00:15<00:01, 214.99it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3020/3257 [00:15<00:01, 217.94it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3048/3257 [00:15<00:00, 230.71it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3076/3257 [00:15<00:00, 244.20it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3101/3257 [00:15<00:00, 241.97it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3128/3257 [00:16<00:00, 246.06it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3153/3257 [00:16<00:00, 225.96it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3176/3257 [00:16<00:00, 223.15it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3200/3257 [00:16<00:00, 226.21it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3223/3257 [00:16<00:00, 218.54it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:16<00:00, 228.59it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 195.45it/s]
2023-02-07 12:32:33.238 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:32:33,239][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d160,n5,w8,mc8,s0.310162,t4>', 'datetime': '2023-02-07T12:32:33.239601', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:32:33,241][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:32:33,241][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:32:33,663][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 12:32:33,664][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:32:33,704][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 retains 13798 unique words (43.39% of original 31803, drops 18005)', 'datetime': '2023-02-07T12:32:33.704172', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:32:33,704][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 leaves 5042429 word corpus (98.97% of original 5095118, drops 52689)', 'datetime': '2023-02-07T12:32:33.704483', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:32:33,748][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 12:32:33,749][gensim.models.word2vec][INFO] - sample=0.310162 downsamples 0 most-common words
[2023-02-07 12:32:33,749][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5042429 word corpus (100.0%% of prior 5042429)', 'datetime': '2023-02-07T12:32:33.749418', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:32:33,825][gensim.models.word2vec][INFO] - estimated required memory for 13798 words and 160 dimensions: 27296320 bytes
[2023-02-07 12:32:33,825][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:32:33,836][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 13798 vocabulary and 160 features, using sg=0 hs=0 sample=0.3101622165985167 negative=5 window=8 shrink_windows=True', 'datetime': '2023-02-07T12:32:33.836761', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:32:34,839][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 49.68% examples, 2546212 words/s, in_qsize 6, out_qsize 1
[2023-02-07 12:32:35,768][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5034594 effective words) took 1.9s, 2607925 effective words/s
[2023-02-07 12:32:36,770][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 53.02% examples, 2726983 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:32:37,604][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5034594 effective words) took 1.8s, 2744424 effective words/s
[2023-02-07 12:32:38,610][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 52.78% examples, 2706285 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:32:39,443][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5034594 effective words) took 1.8s, 2738898 effective words/s
[2023-02-07 12:32:40,445][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 53.05% examples, 2737730 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:32:41,288][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5034594 effective words) took 1.8s, 2731784 effective words/s
[2023-02-07 12:32:42,292][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 53.85% examples, 2773911 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:32:43,116][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5034594 effective words) took 1.8s, 2755757 effective words/s
[2023-02-07 12:32:44,126][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 50.66% examples, 2584671 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:32:45,116][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5034594 effective words) took 2.0s, 2520330 effective words/s
[2023-02-07 12:32:46,118][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 47.56% examples, 2438601 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:32:47,120][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 96.47% examples, 2427602 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:32:47,187][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5034594 effective words) took 2.1s, 2432790 effective words/s
[2023-02-07 12:32:48,189][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 47.37% examples, 2433745 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:32:49,196][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 96.62% examples, 2425390 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:32:49,260][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5034594 effective words) took 2.1s, 2430959 effective words/s
[2023-02-07 12:32:50,264][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 50.14% examples, 2561244 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:32:51,264][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 99.97% examples, 2508224 words/s, in_qsize 1, out_qsize 1
[2023-02-07 12:32:51,267][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5034594 effective words) took 2.0s, 2510027 effective words/s
[2023-02-07 12:32:52,277][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 49.25% examples, 2506704 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:32:53,246][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5034594 effective words) took 2.0s, 2544814 effective words/s
[2023-02-07 12:32:54,250][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 49.00% examples, 2514999 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:32:55,252][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 98.86% examples, 2487869 words/s, in_qsize 6, out_qsize 0
[2023-02-07 12:32:55,266][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5034594 effective words) took 2.0s, 2493789 effective words/s
[2023-02-07 12:32:56,269][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 47.71% examples, 2452655 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:32:57,270][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 93.00% examples, 2353366 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:32:57,405][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5034594 effective words) took 2.1s, 2355456 effective words/s
[2023-02-07 12:32:58,410][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 50.14% examples, 2560590 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:32:59,362][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5034594 effective words) took 2.0s, 2574254 effective words/s
[2023-02-07 12:33:00,366][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 49.25% examples, 2523561 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:01,353][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5034594 effective words) took 2.0s, 2530743 effective words/s
[2023-02-07 12:33:02,361][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 49.86% examples, 2551877 words/s, in_qsize 6, out_qsize 1
[2023-02-07 12:33:03,283][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5034594 effective words) took 1.9s, 2609935 effective words/s
[2023-02-07 12:33:04,290][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 50.63% examples, 2591213 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:05,275][gensim.models.word2vec][INFO] - EPOCH 15: training on 5095118 raw words (5034594 effective words) took 2.0s, 2529039 effective words/s
[2023-02-07 12:33:06,281][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 48.82% examples, 2502052 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:07,254][gensim.models.word2vec][INFO] - EPOCH 16: training on 5095118 raw words (5034594 effective words) took 2.0s, 2547203 effective words/s
[2023-02-07 12:33:08,255][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 49.68% examples, 2549516 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:09,213][gensim.models.word2vec][INFO] - EPOCH 17: training on 5095118 raw words (5034594 effective words) took 2.0s, 2571908 effective words/s
[2023-02-07 12:33:10,216][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 49.25% examples, 2526853 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:11,158][gensim.models.word2vec][INFO] - EPOCH 18: training on 5095118 raw words (5034594 effective words) took 1.9s, 2590347 effective words/s
[2023-02-07 12:33:12,161][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 51.70% examples, 2660824 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:13,052][gensim.models.word2vec][INFO] - EPOCH 19: training on 5095118 raw words (5034594 effective words) took 1.9s, 2659632 effective words/s
[2023-02-07 12:33:14,057][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 51.21% examples, 2627435 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:14,999][gensim.models.word2vec][INFO] - EPOCH 20: training on 5095118 raw words (5034594 effective words) took 1.9s, 2588661 effective words/s
[2023-02-07 12:33:16,002][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 49.65% examples, 2544661 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:16,960][gensim.models.word2vec][INFO] - EPOCH 21: training on 5095118 raw words (5034594 effective words) took 2.0s, 2568558 effective words/s
[2023-02-07 12:33:17,965][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 48.57% examples, 2493828 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:18,911][gensim.models.word2vec][INFO] - EPOCH 22: training on 5095118 raw words (5034594 effective words) took 1.9s, 2582977 effective words/s
[2023-02-07 12:33:19,914][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 51.70% examples, 2660403 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:20,839][gensim.models.word2vec][INFO] - EPOCH 23: training on 5095118 raw words (5034594 effective words) took 1.9s, 2613596 effective words/s
[2023-02-07 12:33:21,849][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 50.14% examples, 2547358 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:22,800][gensim.models.word2vec][INFO] - EPOCH 24: training on 5095118 raw words (5034594 effective words) took 2.0s, 2568655 effective words/s
[2023-02-07 12:33:23,802][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 50.63% examples, 2601723 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:24,721][gensim.models.word2vec][INFO] - EPOCH 25: training on 5095118 raw words (5034594 effective words) took 1.9s, 2622345 effective words/s
[2023-02-07 12:33:25,723][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 49.46% examples, 2538875 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:26,687][gensim.models.word2vec][INFO] - EPOCH 26: training on 5095118 raw words (5034594 effective words) took 2.0s, 2563533 effective words/s
[2023-02-07 12:33:27,694][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 52.44% examples, 2684004 words/s, in_qsize 8, out_qsize 1
[2023-02-07 12:33:28,560][gensim.models.word2vec][INFO] - EPOCH 27: training on 5095118 raw words (5034594 effective words) took 1.9s, 2689053 effective words/s
[2023-02-07 12:33:29,563][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 51.06% examples, 2623734 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:30,488][gensim.models.word2vec][INFO] - EPOCH 28: training on 5095118 raw words (5034594 effective words) took 1.9s, 2613624 effective words/s
[2023-02-07 12:33:31,490][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 51.06% examples, 2626361 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:32,388][gensim.models.word2vec][INFO] - EPOCH 29: training on 5095118 raw words (5034594 effective words) took 1.9s, 2652676 effective words/s
[2023-02-07 12:33:33,398][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 50.75% examples, 2579094 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:34,288][gensim.models.word2vec][INFO] - EPOCH 30: training on 5095118 raw words (5034594 effective words) took 1.9s, 2651640 effective words/s
[2023-02-07 12:33:35,291][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 51.70% examples, 2661189 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:36,193][gensim.models.word2vec][INFO] - EPOCH 31: training on 5095118 raw words (5034594 effective words) took 1.9s, 2645152 effective words/s
[2023-02-07 12:33:37,196][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 50.84% examples, 2606581 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:38,119][gensim.models.word2vec][INFO] - EPOCH 32: training on 5095118 raw words (5034594 effective words) took 1.9s, 2614930 effective words/s
[2023-02-07 12:33:39,121][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 51.55% examples, 2653029 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:40,010][gensim.models.word2vec][INFO] - EPOCH 33: training on 5095118 raw words (5034594 effective words) took 1.9s, 2663936 effective words/s
[2023-02-07 12:33:41,013][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 51.37% examples, 2643319 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:33:41,896][gensim.models.word2vec][INFO] - EPOCH 34: training on 5095118 raw words (5034594 effective words) took 1.9s, 2671751 effective words/s
[2023-02-07 12:33:42,904][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 51.89% examples, 2658174 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:43,763][gensim.models.word2vec][INFO] - EPOCH 35: training on 5095118 raw words (5034594 effective words) took 1.9s, 2699161 effective words/s
[2023-02-07 12:33:44,765][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 52.29% examples, 2689449 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:45,640][gensim.models.word2vec][INFO] - EPOCH 36: training on 5095118 raw words (5034594 effective words) took 1.9s, 2684122 effective words/s
[2023-02-07 12:33:46,645][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 52.63% examples, 2697825 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:47,501][gensim.models.word2vec][INFO] - EPOCH 37: training on 5095118 raw words (5034594 effective words) took 1.9s, 2706411 effective words/s
[2023-02-07 12:33:48,510][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 51.89% examples, 2653423 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:49,379][gensim.models.word2vec][INFO] - EPOCH 38: training on 5095118 raw words (5034594 effective words) took 1.9s, 2682662 effective words/s
[2023-02-07 12:33:50,381][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 52.63% examples, 2707140 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:51,235][gensim.models.word2vec][INFO] - EPOCH 39: training on 5095118 raw words (5034594 effective words) took 1.9s, 2714763 effective words/s
[2023-02-07 12:33:52,238][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 53.30% examples, 2749933 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:53,078][gensim.models.word2vec][INFO] - EPOCH 40: training on 5095118 raw words (5034594 effective words) took 1.8s, 2733079 effective words/s
[2023-02-07 12:33:54,083][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 51.89% examples, 2665820 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:54,959][gensim.models.word2vec][INFO] - EPOCH 41: training on 5095118 raw words (5034594 effective words) took 1.9s, 2679466 effective words/s
[2023-02-07 12:33:55,962][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 50.66% examples, 2599942 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:33:56,869][gensim.models.word2vec][INFO] - EPOCH 42: training on 5095118 raw words (5034594 effective words) took 1.9s, 2638691 effective words/s
[2023-02-07 12:33:57,873][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 50.94% examples, 2611903 words/s, in_qsize 6, out_qsize 1
[2023-02-07 12:33:58,787][gensim.models.word2vec][INFO] - EPOCH 43: training on 5095118 raw words (5034594 effective words) took 1.9s, 2625828 effective words/s
[2023-02-07 12:33:59,791][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 51.55% examples, 2650906 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:00,713][gensim.models.word2vec][INFO] - EPOCH 44: training on 5095118 raw words (5034594 effective words) took 1.9s, 2616748 effective words/s
[2023-02-07 12:34:01,720][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 54.04% examples, 2772966 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:02,489][gensim.models.word2vec][INFO] - EPOCH 45: training on 5095118 raw words (5034594 effective words) took 1.8s, 2836558 effective words/s
[2023-02-07 12:34:03,493][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 55.57% examples, 2863308 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:04,231][gensim.models.word2vec][INFO] - EPOCH 46: training on 5095118 raw words (5034594 effective words) took 1.7s, 2893586 effective words/s
[2023-02-07 12:34:05,237][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 55.23% examples, 2829868 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:05,993][gensim.models.word2vec][INFO] - EPOCH 47: training on 5095118 raw words (5034594 effective words) took 1.8s, 2859534 effective words/s
[2023-02-07 12:34:06,995][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 55.76% examples, 2877177 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:07,734][gensim.models.word2vec][INFO] - EPOCH 48: training on 5095118 raw words (5034594 effective words) took 1.7s, 2894537 effective words/s
[2023-02-07 12:34:08,739][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 54.77% examples, 2815074 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:09,503][gensim.models.word2vec][INFO] - EPOCH 49: training on 5095118 raw words (5034594 effective words) took 1.8s, 2849974 effective words/s
[2023-02-07 12:34:10,504][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 54.59% examples, 2815078 words/s, in_qsize 6, out_qsize 1
[2023-02-07 12:34:11,299][gensim.models.word2vec][INFO] - EPOCH 50: training on 5095118 raw words (5034594 effective words) took 1.8s, 2804822 effective words/s
[2023-02-07 12:34:12,306][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 54.77% examples, 2807803 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:13,087][gensim.models.word2vec][INFO] - EPOCH 51: training on 5095118 raw words (5034594 effective words) took 1.8s, 2818518 effective words/s
[2023-02-07 12:34:14,089][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 54.59% examples, 2812384 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:14,882][gensim.models.word2vec][INFO] - EPOCH 52: training on 5095118 raw words (5034594 effective words) took 1.8s, 2806874 effective words/s
[2023-02-07 12:34:15,883][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 53.45% examples, 2764086 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:16,682][gensim.models.word2vec][INFO] - EPOCH 53: training on 5095118 raw words (5034594 effective words) took 1.8s, 2797863 effective words/s
[2023-02-07 12:34:17,684][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 58.86% examples, 3030867 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:18,305][gensim.models.word2vec][INFO] - EPOCH 54: training on 5095118 raw words (5034594 effective words) took 1.6s, 3104533 effective words/s
[2023-02-07 12:34:19,307][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 60.73% examples, 3104767 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:19,929][gensim.models.word2vec][INFO] - EPOCH 55: training on 5095118 raw words (5034594 effective words) took 1.6s, 3101954 effective words/s
[2023-02-07 12:34:20,933][gensim.models.word2vec][INFO] - EPOCH 56 - PROGRESS: at 60.39% examples, 3083002 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:21,556][gensim.models.word2vec][INFO] - EPOCH 56: training on 5095118 raw words (5034594 effective words) took 1.6s, 3099170 effective words/s
[2023-02-07 12:34:22,557][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 58.86% examples, 3031413 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:34:23,229][gensim.models.word2vec][INFO] - EPOCH 57: training on 5095118 raw words (5034594 effective words) took 1.7s, 3011950 effective words/s
[2023-02-07 12:34:24,232][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 58.27% examples, 2996189 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:24,884][gensim.models.word2vec][INFO] - EPOCH 58: training on 5095118 raw words (5034594 effective words) took 1.7s, 3043443 effective words/s
[2023-02-07 12:34:25,887][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 60.27% examples, 3078137 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:26,554][gensim.models.word2vec][INFO] - EPOCH 59: training on 5095118 raw words (5034594 effective words) took 1.7s, 3017357 effective words/s
[2023-02-07 12:34:27,565][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 58.80% examples, 2989694 words/s, in_qsize 6, out_qsize 1
[2023-02-07 12:34:28,225][gensim.models.word2vec][INFO] - EPOCH 60: training on 5095118 raw words (5034594 effective words) took 1.7s, 3015146 effective words/s
[2023-02-07 12:34:29,227][gensim.models.word2vec][INFO] - EPOCH 61 - PROGRESS: at 56.37% examples, 2909870 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:29,947][gensim.models.word2vec][INFO] - EPOCH 61: training on 5095118 raw words (5034594 effective words) took 1.7s, 2925689 effective words/s
[2023-02-07 12:34:30,949][gensim.models.word2vec][INFO] - EPOCH 62 - PROGRESS: at 57.66% examples, 2962629 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:31,642][gensim.models.word2vec][INFO] - EPOCH 62: training on 5095118 raw words (5034594 effective words) took 1.7s, 2971355 effective words/s
[2023-02-07 12:34:32,644][gensim.models.word2vec][INFO] - EPOCH 63 - PROGRESS: at 59.04% examples, 3038428 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:33,284][gensim.models.word2vec][INFO] - EPOCH 63: training on 5095118 raw words (5034594 effective words) took 1.6s, 3068868 effective words/s
[2023-02-07 12:34:34,288][gensim.models.word2vec][INFO] - EPOCH 64 - PROGRESS: at 55.23% examples, 2835818 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:35,065][gensim.models.word2vec][INFO] - EPOCH 64: training on 5095118 raw words (5034594 effective words) took 1.8s, 2828716 effective words/s
[2023-02-07 12:34:36,067][gensim.models.word2vec][INFO] - EPOCH 65 - PROGRESS: at 55.33% examples, 2852186 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:36,828][gensim.models.word2vec][INFO] - EPOCH 65: training on 5095118 raw words (5034594 effective words) took 1.8s, 2857914 effective words/s
[2023-02-07 12:34:37,835][gensim.models.word2vec][INFO] - EPOCH 66 - PROGRESS: at 53.30% examples, 2738612 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:38,646][gensim.models.word2vec][INFO] - EPOCH 66: training on 5095118 raw words (5034594 effective words) took 1.8s, 2770319 effective words/s
[2023-02-07 12:34:39,647][gensim.models.word2vec][INFO] - EPOCH 67 - PROGRESS: at 55.11% examples, 2838759 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:40,415][gensim.models.word2vec][INFO] - EPOCH 67: training on 5095118 raw words (5034594 effective words) took 1.8s, 2847254 effective words/s
[2023-02-07 12:34:41,421][gensim.models.word2vec][INFO] - EPOCH 68 - PROGRESS: at 54.10% examples, 2779498 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:42,220][gensim.models.word2vec][INFO] - EPOCH 68: training on 5095118 raw words (5034594 effective words) took 1.8s, 2792235 effective words/s
[2023-02-07 12:34:43,221][gensim.models.word2vec][INFO] - EPOCH 69 - PROGRESS: at 54.99% examples, 2830663 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:43,992][gensim.models.word2vec][INFO] - EPOCH 69: training on 5095118 raw words (5034594 effective words) took 1.8s, 2843141 effective words/s
[2023-02-07 12:34:44,996][gensim.models.word2vec][INFO] - EPOCH 70 - PROGRESS: at 54.81% examples, 2815981 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:45,763][gensim.models.word2vec][INFO] - EPOCH 70: training on 5095118 raw words (5034594 effective words) took 1.8s, 2844986 effective words/s
[2023-02-07 12:34:46,765][gensim.models.word2vec][INFO] - EPOCH 71 - PROGRESS: at 55.57% examples, 2865621 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:47,537][gensim.models.word2vec][INFO] - EPOCH 71: training on 5095118 raw words (5034594 effective words) took 1.8s, 2839483 effective words/s
[2023-02-07 12:34:48,542][gensim.models.word2vec][INFO] - EPOCH 72 - PROGRESS: at 54.77% examples, 2813785 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:34:49,329][gensim.models.word2vec][INFO] - EPOCH 72: training on 5095118 raw words (5034594 effective words) took 1.8s, 2812190 effective words/s
[2023-02-07 12:34:49,329][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 371943614 raw words (367525362 effective words) took 135.5s, 2712511 effective words/s', 'datetime': '2023-02-07T12:34:49.329822', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:34:49.330 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:34:58,922][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123207-xkb9j3fu/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:34:58.921980', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:34:58,922][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:34:58,962][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123207-xkb9j3fu/files/../tmp/embedding_model.pt
2023-02-07 12:34:58.962 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:35:00.276 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:35:00.773 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:35:01.841 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.172312226492048, 'test_mae': 1.1227899105202384, 'test_r2': -0.02763356544440221}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.87
wandb: percentage 0.56614
wandb:   test_mae 1.12279
wandb:   test_mse 2.17231
wandb:    test_r2 -0.02763
wandb: 
wandb: üöÄ View run northern-sweep-6 at: https://wandb.ai/xiaoqiz/mof2vec/runs/xkb9j3fu
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_123207-xkb9j3fu/logs
wandb: Agent Starting Run: kzkqwp5o with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 764
wandb: 	model.gensim.alpha: 0.057644996398782115
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.epochs: 50
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.986224811911123
wandb: 	model.gensim.vector_size: 243
wandb: 	model.gensim.window: 16
wandb: 	model.sklearn.learning_rate: 0.05914680170814578
wandb: 	model.sklearn.max_depth: 77
wandb: 	model.sklearn.min_child_weight: 0.0927237954497832
wandb: 	model.sklearn.n_estimators: 2839
wandb: 	model.sklearn.num_leaves: 245
wandb: 	model.sklearn.reg_alpha: 0.0046664602093550766
wandb: 	model.sklearn.reg_lambda: 0.006425192250749694
wandb: 	model.sklearn.subsample: 0.6327576883379704
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123511-kzkqwp5o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/kzkqwp5o
2023-02-07 12:35:19.967 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 12:35:19.968 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 764 for sweep.
2023-02-07 12:35:19.968 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.057644996398782115 for sweep.
2023-02-07 12:35:19.968 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 12:35:19.968 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 50 for sweep.
2023-02-07 12:35:19.969 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 12:35:19.969 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.986224811911123 for sweep.
2023-02-07 12:35:19.969 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 243 for sweep.
2023-02-07 12:35:19.969 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 16 for sweep.
2023-02-07 12:35:19.969 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.05914680170814578 for sweep.
2023-02-07 12:35:19.970 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 77 for sweep.
2023-02-07 12:35:19.970 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0927237954497832 for sweep.
2023-02-07 12:35:19.970 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2839 for sweep.
2023-02-07 12:35:19.970 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 245 for sweep.
2023-02-07 12:35:19.971 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.0046664602093550766 for sweep.
2023-02-07 12:35:19.971 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.006425192250749694 for sweep.
2023-02-07 12:35:19.971 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6327576883379704 for sweep.
2023-02-07 12:35:19.971 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:35:19.977 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123511-kzkqwp5o/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 764, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 243, 'window': 16, 'min_count': 6, 'dm': 1, 'sample': 0.986224811911123, 'workers': 4, 'alpha': 0.057644996398782115, 'epochs': 50}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2839, 'max_depth': 77, 'num_leaves': 245, 'reg_alpha': 0.0046664602093550766, 'reg_lambda': 0.006425192250749694, 'subsample': 0.6327576883379704, 'min_child_weight': 0.0927237954497832, 'n_jobs': 4, 'learning_rate': 0.05914680170814578}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 27/3257 [00:00<00:12, 260.88it/s]  2%|‚ñè         | 54/3257 [00:00<00:12, 257.83it/s]  3%|‚ñé         | 84/3257 [00:00<00:11, 274.01it/s]  3%|‚ñé         | 112/3257 [00:00<00:12, 260.01it/s]  4%|‚ñç         | 143/3257 [00:00<00:11, 275.89it/s]  5%|‚ñå         | 171/3257 [00:00<00:11, 270.93it/s]  6%|‚ñå         | 200/3257 [00:00<00:11, 275.26it/s]  7%|‚ñã         | 232/3257 [00:00<00:10, 286.80it/s]  8%|‚ñä         | 261/3257 [00:00<00:10, 283.41it/s]  9%|‚ñâ         | 296/3257 [00:01<00:09, 299.80it/s] 10%|‚ñà         | 327/3257 [00:01<00:09, 298.42it/s] 11%|‚ñà         | 357/3257 [00:01<00:09, 291.98it/s] 12%|‚ñà‚ñè        | 387/3257 [00:01<00:10, 272.43it/s] 13%|‚ñà‚ñé        | 415/3257 [00:01<00:14, 198.49it/s] 13%|‚ñà‚ñé        | 438/3257 [00:01<00:14, 196.68it/s] 14%|‚ñà‚ñç        | 468/3257 [00:01<00:12, 220.22it/s] 15%|‚ñà‚ñå        | 497/3257 [00:01<00:11, 237.30it/s] 16%|‚ñà‚ñå        | 526/3257 [00:02<00:11, 246.66it/s] 17%|‚ñà‚ñã        | 556/3257 [00:02<00:10, 259.13it/s] 18%|‚ñà‚ñä        | 583/3257 [00:02<00:11, 239.62it/s] 19%|‚ñà‚ñâ        | 614/3257 [00:02<00:10, 257.78it/s] 20%|‚ñà‚ñâ        | 641/3257 [00:02<00:10, 254.38it/s] 21%|‚ñà‚ñà        | 668/3257 [00:02<00:10, 247.43it/s] 21%|‚ñà‚ñà‚ñè       | 694/3257 [00:02<00:10, 250.77it/s] 22%|‚ñà‚ñà‚ñè       | 721/3257 [00:02<00:09, 254.12it/s] 23%|‚ñà‚ñà‚ñé       | 747/3257 [00:02<00:10, 247.76it/s] 24%|‚ñà‚ñà‚ñé       | 773/3257 [00:03<00:10, 246.55it/s] 25%|‚ñà‚ñà‚ñç       | 798/3257 [00:03<00:10, 240.59it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:03<00:10, 232.39it/s] 26%|‚ñà‚ñà‚ñå       | 847/3257 [00:03<00:10, 227.19it/s] 27%|‚ñà‚ñà‚ñã       | 872/3257 [00:03<00:10, 233.32it/s] 28%|‚ñà‚ñà‚ñä       | 896/3257 [00:03<00:10, 232.09it/s] 28%|‚ñà‚ñà‚ñä       | 920/3257 [00:03<00:10, 233.30it/s] 29%|‚ñà‚ñà‚ñâ       | 946/3257 [00:03<00:09, 238.62it/s] 30%|‚ñà‚ñà‚ñâ       | 971/3257 [00:03<00:09, 241.39it/s] 31%|‚ñà‚ñà‚ñà       | 996/3257 [00:03<00:09, 237.60it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1025/3257 [00:04<00:09, 246.15it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1050/3257 [00:04<00:09, 236.07it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:04<00:08, 243.77it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1107/3257 [00:04<00:08, 250.24it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1133/3257 [00:04<00:08, 244.07it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1158/3257 [00:04<00:08, 242.94it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1183/3257 [00:04<00:08, 232.81it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1207/3257 [00:04<00:09, 219.99it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1237/3257 [00:05<00:08, 240.70it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1263/3257 [00:05<00:08, 244.75it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1288/3257 [00:05<00:08, 224.44it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1313/3257 [00:05<00:08, 228.00it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:05<00:08, 236.15it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1363/3257 [00:05<00:08, 235.55it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1387/3257 [00:05<00:08, 231.78it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1420/3257 [00:05<00:07, 259.02it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1448/3257 [00:05<00:06, 262.78it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1479/3257 [00:05<00:06, 276.04it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:06<00:06, 287.59it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1540/3257 [00:06<00:06, 267.21it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1568/3257 [00:06<00:06, 264.72it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1596/3257 [00:06<00:06, 265.71it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1625/3257 [00:06<00:06, 271.25it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1653/3257 [00:06<00:06, 258.80it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1680/3257 [00:06<00:09, 160.22it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1707/3257 [00:07<00:08, 180.59it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:07<00:07, 191.16it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1759/3257 [00:07<00:07, 210.72it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1788/3257 [00:07<00:06, 229.47it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1814/3257 [00:07<00:06, 232.89it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1843/3257 [00:07<00:05, 245.77it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1872/3257 [00:07<00:05, 256.36it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1899/3257 [00:07<00:05, 256.42it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1926/3257 [00:07<00:05, 258.16it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1962/3257 [00:07<00:04, 287.30it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1992/3257 [00:08<00:04, 281.71it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2022/3257 [00:08<00:04, 283.58it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2051/3257 [00:08<00:04, 261.79it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2078/3257 [00:08<00:04, 251.31it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2104/3257 [00:08<00:04, 249.38it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2130/3257 [00:08<00:04, 241.91it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2156/3257 [00:08<00:04, 245.47it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2182/3257 [00:08<00:04, 248.22it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2209/3257 [00:08<00:04, 249.59it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:09<00:03, 255.39it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2262/3257 [00:09<00:04, 248.06it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2288/3257 [00:09<00:03, 248.34it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2315/3257 [00:09<00:03, 250.19it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2352/3257 [00:09<00:03, 282.89it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2381/3257 [00:09<00:03, 281.17it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2410/3257 [00:09<00:03, 275.48it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2438/3257 [00:09<00:03, 270.31it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2467/3257 [00:09<00:02, 275.86it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2497/3257 [00:10<00:02, 281.61it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2527/3257 [00:10<00:02, 285.74it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2556/3257 [00:10<00:02, 278.64it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2584/3257 [00:10<00:02, 265.96it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2617/3257 [00:10<00:02, 282.16it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2646/3257 [00:10<00:02, 279.71it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2675/3257 [00:10<00:02, 273.06it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2703/3257 [00:10<00:02, 251.17it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2730/3257 [00:10<00:02, 254.18it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2758/3257 [00:11<00:01, 260.52it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2785/3257 [00:11<00:01, 260.81it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2813/3257 [00:11<00:01, 261.86it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2840/3257 [00:11<00:01, 247.89it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2874/3257 [00:11<00:01, 272.94it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2902/3257 [00:11<00:01, 260.36it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2930/3257 [00:11<00:01, 263.02it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2957/3257 [00:11<00:01, 253.38it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2983/3257 [00:11<00:01, 252.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3013/3257 [00:12<00:00, 262.70it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3042/3257 [00:12<00:00, 268.58it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3075/3257 [00:12<00:00, 281.82it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3104/3257 [00:12<00:00, 281.47it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3133/3257 [00:12<00:00, 280.85it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3162/3257 [00:12<00:00, 274.66it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3190/3257 [00:12<00:00, 268.82it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3217/3257 [00:12<00:00, 261.25it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3250/3257 [00:12<00:00, 280.25it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 248.69it/s]
2023-02-07 12:35:33.475 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:35:33,477][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d243,n5,w16,mc6,s0.986225,t4>', 'datetime': '2023-02-07T12:35:33.477046', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:35:33,477][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:35:33,477][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:35:33,766][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 12:35:33,766][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:35:33,783][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 6701 unique words (51.31% of original 13061, drops 6360)', 'datetime': '2023-02-07T12:35:33.783088', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:35:33,783][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 3623361 word corpus (99.56% of original 3639370, drops 16009)', 'datetime': '2023-02-07T12:35:33.783259', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:35:33,802][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 12:35:33,802][gensim.models.word2vec][INFO] - sample=0.986225 downsamples 0 most-common words
[2023-02-07 12:35:33,803][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3623361 word corpus (100.0%% of prior 3623361)', 'datetime': '2023-02-07T12:35:33.803060', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:35:33,839][gensim.models.word2vec][INFO] - estimated required memory for 6701 words and 243 dimensions: 20194448 bytes
[2023-02-07 12:35:33,839][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:35:33,847][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 6701 vocabulary and 243 features, using sg=0 hs=0 sample=0.986224811911123 negative=5 window=16 shrink_windows=True', 'datetime': '2023-02-07T12:35:33.847212', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:35:34,850][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 36.26% examples, 1338160 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:35:35,855][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 73.93% examples, 1357077 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:35:36,514][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3626618 effective words) took 2.7s, 1360895 effective words/s
[2023-02-07 12:35:37,521][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 36.14% examples, 1325322 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:35:38,529][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 71.45% examples, 1312699 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:35:39,286][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3626618 effective words) took 2.8s, 1308859 effective words/s
[2023-02-07 12:35:40,293][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 35.34% examples, 1293485 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:35:41,294][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 72.86% examples, 1338877 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:35:41,989][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3626618 effective words) took 2.7s, 1342595 effective words/s
[2023-02-07 12:35:42,992][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 36.44% examples, 1346485 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:35:43,996][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 73.47% examples, 1347873 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:35:44,655][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3626618 effective words) took 2.7s, 1361299 effective words/s
[2023-02-07 12:35:45,663][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 37.95% examples, 1401957 words/s, in_qsize 6, out_qsize 1
[2023-02-07 12:35:46,664][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 74.61% examples, 1368887 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:35:47,319][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3626618 effective words) took 2.7s, 1362420 effective words/s
[2023-02-07 12:35:48,321][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 37.95% examples, 1410863 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:35:49,335][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 77.22% examples, 1405932 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:35:49,902][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3626618 effective words) took 2.6s, 1404575 effective words/s
[2023-02-07 12:35:50,912][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 36.66% examples, 1346395 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:35:51,919][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 74.79% examples, 1368379 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:35:52,528][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3626618 effective words) took 2.6s, 1382188 effective words/s
[2023-02-07 12:35:53,538][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 36.60% examples, 1346340 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:35:54,542][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 75.25% examples, 1379002 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:35:55,163][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3626618 effective words) took 2.6s, 1376827 effective words/s
[2023-02-07 12:35:56,166][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 38.07% examples, 1409250 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:35:57,170][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 75.99% examples, 1394199 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:35:57,754][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3626618 effective words) took 2.6s, 1400704 effective words/s
[2023-02-07 12:35:58,761][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 37.58% examples, 1384878 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:35:59,762][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 76.24% examples, 1397765 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:00,357][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3626618 effective words) took 2.6s, 1394041 effective words/s
[2023-02-07 12:36:01,359][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 36.66% examples, 1357025 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:36:02,361][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 74.79% examples, 1377154 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:02,965][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3626618 effective words) took 2.6s, 1391642 effective words/s
[2023-02-07 12:36:03,975][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 37.80% examples, 1390086 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:04,977][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 76.94% examples, 1404131 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:05,550][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3626618 effective words) took 2.6s, 1403867 effective words/s
[2023-02-07 12:36:06,554][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 37.18% examples, 1380578 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:07,561][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 74.79% examples, 1371981 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:08,176][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3626618 effective words) took 2.6s, 1381832 effective words/s
[2023-02-07 12:36:09,180][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 36.60% examples, 1354450 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:10,188][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 73.93% examples, 1353582 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:10,849][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3626618 effective words) took 2.7s, 1357611 effective words/s
[2023-02-07 12:36:11,860][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 36.26% examples, 1326258 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:12,864][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 72.86% examples, 1334219 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:13,575][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3626618 effective words) took 2.7s, 1330806 effective words/s
[2023-02-07 12:36:14,582][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 35.52% examples, 1304688 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:15,592][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 72.64% examples, 1328868 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:36:16,283][gensim.models.word2vec][INFO] - EPOCH 15: training on 3639370 raw words (3626618 effective words) took 2.7s, 1340532 effective words/s
[2023-02-07 12:36:17,290][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 36.26% examples, 1331316 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:18,296][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 73.93% examples, 1352997 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:18,982][gensim.models.word2vec][INFO] - EPOCH 16: training on 3639370 raw words (3626618 effective words) took 2.7s, 1344195 effective words/s
[2023-02-07 12:36:19,986][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 36.26% examples, 1336001 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:20,987][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 73.26% examples, 1346086 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:21,667][gensim.models.word2vec][INFO] - EPOCH 17: training on 3639370 raw words (3626618 effective words) took 2.7s, 1351651 effective words/s
[2023-02-07 12:36:22,674][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 36.60% examples, 1352487 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:23,680][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 73.93% examples, 1353162 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:36:24,317][gensim.models.word2vec][INFO] - EPOCH 18: training on 3639370 raw words (3626618 effective words) took 2.6s, 1369782 effective words/s
[2023-02-07 12:36:25,319][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 36.66% examples, 1357545 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:36:26,324][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 74.30% examples, 1365637 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:26,975][gensim.models.word2vec][INFO] - EPOCH 19: training on 3639370 raw words (3626618 effective words) took 2.7s, 1365735 effective words/s
[2023-02-07 12:36:27,979][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 39.82% examples, 1485072 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:28,984][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 80.38% examples, 1465524 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:29,442][gensim.models.word2vec][INFO] - EPOCH 20: training on 3639370 raw words (3626618 effective words) took 2.5s, 1471082 effective words/s
[2023-02-07 12:36:30,455][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 39.30% examples, 1444288 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:31,456][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 76.57% examples, 1398123 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:36:32,071][gensim.models.word2vec][INFO] - EPOCH 21: training on 3639370 raw words (3626618 effective words) took 2.6s, 1380490 effective words/s
[2023-02-07 12:36:33,083][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 34.42% examples, 1252601 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:34,083][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 71.72% examples, 1318446 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:34,809][gensim.models.word2vec][INFO] - EPOCH 22: training on 3639370 raw words (3626618 effective words) took 2.7s, 1324887 effective words/s
[2023-02-07 12:36:35,821][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 36.26% examples, 1325356 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:36,823][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 73.81% examples, 1348268 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:37,481][gensim.models.word2vec][INFO] - EPOCH 23: training on 3639370 raw words (3626618 effective words) took 2.7s, 1358174 effective words/s
[2023-02-07 12:36:38,483][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 36.60% examples, 1357995 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:39,483][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 74.49% examples, 1369556 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:40,117][gensim.models.word2vec][INFO] - EPOCH 24: training on 3639370 raw words (3626618 effective words) took 2.6s, 1376753 effective words/s
[2023-02-07 12:36:41,125][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 37.58% examples, 1382985 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:36:42,129][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 75.68% examples, 1385509 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:42,757][gensim.models.word2vec][INFO] - EPOCH 25: training on 3639370 raw words (3626618 effective words) took 2.6s, 1374533 effective words/s
[2023-02-07 12:36:43,770][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 37.58% examples, 1376880 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:44,774][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 75.99% examples, 1387547 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:36:45,377][gensim.models.word2vec][INFO] - EPOCH 26: training on 3639370 raw words (3626618 effective words) took 2.6s, 1385475 effective words/s
[2023-02-07 12:36:46,380][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 36.66% examples, 1355538 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:47,390][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 74.79% examples, 1370622 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:36:48,022][gensim.models.word2vec][INFO] - EPOCH 27: training on 3639370 raw words (3626618 effective words) took 2.6s, 1371710 effective words/s
[2023-02-07 12:36:49,028][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 36.44% examples, 1343668 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:50,038][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 73.81% examples, 1347417 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:50,695][gensim.models.word2vec][INFO] - EPOCH 28: training on 3639370 raw words (3626618 effective words) took 2.7s, 1358248 effective words/s
[2023-02-07 12:36:51,704][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 38.07% examples, 1399999 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:36:52,708][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 76.24% examples, 1394092 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:36:53,292][gensim.models.word2vec][INFO] - EPOCH 29: training on 3639370 raw words (3626618 effective words) took 2.6s, 1397157 effective words/s
[2023-02-07 12:36:54,303][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 38.50% examples, 1416596 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:36:55,317][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 78.54% examples, 1421631 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:55,834][gensim.models.word2vec][INFO] - EPOCH 30: training on 3639370 raw words (3626618 effective words) took 2.5s, 1427279 effective words/s
[2023-02-07 12:36:56,841][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 36.26% examples, 1332647 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:36:57,844][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 74.06% examples, 1359116 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:36:58,487][gensim.models.word2vec][INFO] - EPOCH 31: training on 3639370 raw words (3626618 effective words) took 2.7s, 1368173 effective words/s
[2023-02-07 12:36:59,494][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 38.07% examples, 1403402 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:37:00,495][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 75.25% examples, 1383786 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:01,129][gensim.models.word2vec][INFO] - EPOCH 32: training on 3639370 raw words (3626618 effective words) took 2.6s, 1373439 effective words/s
[2023-02-07 12:37:02,134][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 35.52% examples, 1307695 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:03,136][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 72.43% examples, 1330759 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:03,849][gensim.models.word2vec][INFO] - EPOCH 33: training on 3639370 raw words (3626618 effective words) took 2.7s, 1335079 effective words/s
[2023-02-07 12:37:04,861][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 36.66% examples, 1343278 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:37:05,876][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 74.79% examples, 1361491 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:06,521][gensim.models.word2vec][INFO] - EPOCH 34: training on 3639370 raw words (3626618 effective words) took 2.7s, 1358271 effective words/s
[2023-02-07 12:37:07,537][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 36.66% examples, 1339766 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:08,542][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 74.30% examples, 1356687 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:09,193][gensim.models.word2vec][INFO] - EPOCH 35: training on 3639370 raw words (3626618 effective words) took 2.7s, 1358688 effective words/s
[2023-02-07 12:37:10,203][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 36.32% examples, 1331533 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:11,211][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 73.81% examples, 1345917 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:11,895][gensim.models.word2vec][INFO] - EPOCH 36: training on 3639370 raw words (3626618 effective words) took 2.7s, 1343017 effective words/s
[2023-02-07 12:37:12,903][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 36.66% examples, 1350460 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:13,906][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 73.93% examples, 1355297 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:14,579][gensim.models.word2vec][INFO] - EPOCH 37: training on 3639370 raw words (3626618 effective words) took 2.7s, 1352609 effective words/s
[2023-02-07 12:37:15,584][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 36.44% examples, 1344897 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:16,599][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 73.81% examples, 1344088 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:17,267][gensim.models.word2vec][INFO] - EPOCH 38: training on 3639370 raw words (3626618 effective words) took 2.7s, 1350155 effective words/s
[2023-02-07 12:37:18,280][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 36.14% examples, 1319158 words/s, in_qsize 8, out_qsize 1
[2023-02-07 12:37:19,281][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 73.26% examples, 1340367 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:19,975][gensim.models.word2vec][INFO] - EPOCH 39: training on 3639370 raw words (3626618 effective words) took 2.7s, 1340513 effective words/s
[2023-02-07 12:37:20,977][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 38.78% examples, 1438550 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:37:21,981][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 80.29% examples, 1466516 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:22,438][gensim.models.word2vec][INFO] - EPOCH 40: training on 3639370 raw words (3626618 effective words) took 2.5s, 1472934 effective words/s
[2023-02-07 12:37:23,445][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 39.21% examples, 1453334 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:37:24,467][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 81.21% examples, 1464435 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:37:24,931][gensim.models.word2vec][INFO] - EPOCH 41: training on 3639370 raw words (3626618 effective words) took 2.5s, 1455364 effective words/s
[2023-02-07 12:37:25,936][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 35.34% examples, 1296128 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:37:26,942][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 70.56% examples, 1301898 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:27,699][gensim.models.word2vec][INFO] - EPOCH 42: training on 3639370 raw words (3626618 effective words) took 2.8s, 1311120 effective words/s
[2023-02-07 12:37:28,702][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 36.14% examples, 1330727 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:29,707][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 73.26% examples, 1343498 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:37:30,404][gensim.models.word2vec][INFO] - EPOCH 43: training on 3639370 raw words (3626618 effective words) took 2.7s, 1341230 effective words/s
[2023-02-07 12:37:31,409][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 36.97% examples, 1371827 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:32,422][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 75.93% examples, 1385733 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:33,013][gensim.models.word2vec][INFO] - EPOCH 44: training on 3639370 raw words (3626618 effective words) took 2.6s, 1391083 effective words/s
[2023-02-07 12:37:34,026][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 36.60% examples, 1342831 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:35,028][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 75.99% examples, 1388219 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:35,619][gensim.models.word2vec][INFO] - EPOCH 45: training on 3639370 raw words (3626618 effective words) took 2.6s, 1392818 effective words/s
[2023-02-07 12:37:36,624][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 36.44% examples, 1343796 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:37,631][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 74.79% examples, 1371197 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:37:38,270][gensim.models.word2vec][INFO] - EPOCH 46: training on 3639370 raw words (3626618 effective words) took 2.6s, 1368721 effective words/s
[2023-02-07 12:37:39,277][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 37.58% examples, 1384163 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:37:40,277][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 76.94% examples, 1406735 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:40,859][gensim.models.word2vec][INFO] - EPOCH 47: training on 3639370 raw words (3626618 effective words) took 2.6s, 1401222 effective words/s
[2023-02-07 12:37:41,865][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 37.40% examples, 1388913 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:42,869][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 75.93% examples, 1391240 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:37:43,463][gensim.models.word2vec][INFO] - EPOCH 48: training on 3639370 raw words (3626618 effective words) took 2.6s, 1393944 effective words/s
[2023-02-07 12:37:44,465][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 37.58% examples, 1391224 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:45,472][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 75.99% examples, 1392496 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:37:46,070][gensim.models.word2vec][INFO] - EPOCH 49: training on 3639370 raw words (3626618 effective words) took 2.6s, 1392063 effective words/s
[2023-02-07 12:37:46,070][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 181968500 raw words (181330900 effective words) took 132.2s, 1371409 effective words/s', 'datetime': '2023-02-07T12:37:46.070611', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:37:46.070 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:37:53,392][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123511-kzkqwp5o/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:37:53.392915', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:37:53,393][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:37:53,430][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123511-kzkqwp5o/files/../tmp/embedding_model.pt
2023-02-07 12:37:53.431 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:37:55.024 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:37:55.601 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:37:57.296 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.4122421014282143, 'test_mae': 1.1935677495927899, 'test_r2': -0.14113474167054663}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.0
wandb: percentage 0.48695
wandb:   test_mae 1.19357
wandb:   test_mse 2.41224
wandb:    test_r2 -0.14113
wandb: 
wandb: üöÄ View run sparkling-sweep-7 at: https://wandb.ai/xiaoqiz/mof2vec/runs/kzkqwp5o
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_123511-kzkqwp5o/logs
wandb: Agent Starting Run: 8e9q0pgz with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 448
wandb: 	model.gensim.alpha: 0.10885762563158762
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 36
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.4052242593998589
wandb: 	model.gensim.vector_size: 222
wandb: 	model.gensim.window: 12
wandb: 	model.sklearn.learning_rate: 0.017902990804870907
wandb: 	model.sklearn.max_depth: 79
wandb: 	model.sklearn.min_child_weight: 0.052174669919544246
wandb: 	model.sklearn.n_estimators: 2608
wandb: 	model.sklearn.num_leaves: 468
wandb: 	model.sklearn.reg_alpha: 0.4486784735561421
wandb: 	model.sklearn.reg_lambda: 0.664449646971625
wandb: 	model.sklearn.subsample: 0.870230179212802
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123810-8e9q0pgz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/8e9q0pgz
2023-02-07 12:38:19.467 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 12:38:19.467 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 448 for sweep.
2023-02-07 12:38:19.468 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.10885762563158762 for sweep.
2023-02-07 12:38:19.468 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:38:19.468 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 36 for sweep.
2023-02-07 12:38:19.468 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 12:38:19.468 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4052242593998589 for sweep.
2023-02-07 12:38:19.469 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 222 for sweep.
2023-02-07 12:38:19.469 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 12 for sweep.
2023-02-07 12:38:19.469 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.017902990804870907 for sweep.
2023-02-07 12:38:19.469 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 79 for sweep.
2023-02-07 12:38:19.470 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.052174669919544246 for sweep.
2023-02-07 12:38:19.470 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2608 for sweep.
2023-02-07 12:38:19.470 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 468 for sweep.
2023-02-07 12:38:19.470 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.4486784735561421 for sweep.
2023-02-07 12:38:19.470 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.664449646971625 for sweep.
2023-02-07 12:38:19.471 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.870230179212802 for sweep.
2023-02-07 12:38:19.471 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:38:19.475 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123810-8e9q0pgz/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 448, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 222, 'window': 12, 'min_count': 5, 'dm': 0, 'sample': 0.4052242593998589, 'workers': 4, 'alpha': 0.10885762563158762, 'epochs': 36}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2608, 'max_depth': 79, 'num_leaves': 468, 'reg_alpha': 0.4486784735561421, 'reg_lambda': 0.664449646971625, 'subsample': 0.870230179212802, 'min_child_weight': 0.052174669919544246, 'n_jobs': 4, 'learning_rate': 0.017902990804870907}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 34/3257 [00:00<00:10, 321.98it/s]  2%|‚ñè         | 71/3257 [00:00<00:09, 347.83it/s]  3%|‚ñé         | 107/3257 [00:00<00:08, 351.38it/s]  5%|‚ñç         | 148/3257 [00:00<00:08, 373.06it/s]  6%|‚ñå         | 186/3257 [00:00<00:08, 366.01it/s]  7%|‚ñã         | 230/3257 [00:00<00:07, 384.76it/s]  8%|‚ñä         | 269/3257 [00:00<00:07, 377.09it/s] 10%|‚ñâ         | 312/3257 [00:00<00:07, 386.16it/s] 11%|‚ñà         | 352/3257 [00:00<00:07, 388.84it/s] 12%|‚ñà‚ñè        | 391/3257 [00:01<00:07, 378.01it/s] 13%|‚ñà‚ñé        | 429/3257 [00:01<00:07, 358.77it/s] 14%|‚ñà‚ñç        | 467/3257 [00:01<00:07, 362.80it/s] 16%|‚ñà‚ñå        | 505/3257 [00:01<00:07, 367.01it/s] 17%|‚ñà‚ñã        | 544/3257 [00:01<00:07, 373.37it/s] 18%|‚ñà‚ñä        | 582/3257 [00:01<00:07, 344.50it/s] 19%|‚ñà‚ñâ        | 621/3257 [00:01<00:07, 355.26it/s] 20%|‚ñà‚ñà        | 657/3257 [00:01<00:07, 350.35it/s] 21%|‚ñà‚ñà‚ñè       | 695/3257 [00:01<00:07, 354.12it/s] 22%|‚ñà‚ñà‚ñè       | 731/3257 [00:02<00:07, 353.89it/s] 24%|‚ñà‚ñà‚ñé       | 770/3257 [00:02<00:06, 358.75it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:02<00:06, 353.83it/s] 26%|‚ñà‚ñà‚ñå       | 842/3257 [00:02<00:07, 344.82it/s] 27%|‚ñà‚ñà‚ñã       | 877/3257 [00:02<00:06, 340.20it/s] 28%|‚ñà‚ñà‚ñä       | 915/3257 [00:02<00:06, 351.58it/s] 29%|‚ñà‚ñà‚ñâ       | 956/3257 [00:02<00:06, 364.46it/s] 30%|‚ñà‚ñà‚ñà       | 993/3257 [00:02<00:06, 356.55it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1029/3257 [00:02<00:06, 350.44it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1065/3257 [00:02<00:06, 348.16it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1100/3257 [00:03<00:06, 346.47it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1136/3257 [00:03<00:06, 348.58it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:03<00:05, 348.93it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1206/3257 [00:03<00:09, 227.18it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1244/3257 [00:03<00:07, 257.61it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1277/3257 [00:03<00:07, 273.47it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1311/3257 [00:03<00:06, 289.05it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:03<00:06, 308.48it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1385/3257 [00:04<00:05, 317.47it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1426/3257 [00:04<00:05, 342.04it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1466/3257 [00:04<00:05, 357.73it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1508/3257 [00:04<00:04, 374.72it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1547/3257 [00:04<00:04, 351.95it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1584/3257 [00:04<00:04, 356.77it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1622/3257 [00:04<00:04, 362.29it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:04<00:04, 356.18it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1695/3257 [00:04<00:04, 349.21it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:05<00:04, 345.18it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1768/3257 [00:05<00:04, 349.96it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:05<00:04, 354.17it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1841/3257 [00:05<00:03, 355.60it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1879/3257 [00:05<00:03, 361.91it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:05<00:03, 357.48it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1958/3257 [00:05<00:03, 373.44it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1996/3257 [00:05<00:03, 374.77it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2034/3257 [00:05<00:03, 374.38it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2072/3257 [00:05<00:03, 357.20it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2108/3257 [00:06<00:03, 356.51it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2144/3257 [00:06<00:03, 341.73it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2182/3257 [00:06<00:03, 349.11it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2219/3257 [00:06<00:02, 355.06it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2255/3257 [00:06<00:02, 354.35it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2291/3257 [00:06<00:02, 353.80it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2334/3257 [00:06<00:02, 374.53it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2374/3257 [00:06<00:02, 381.82it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2413/3257 [00:06<00:02, 379.70it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2452/3257 [00:07<00:02, 366.46it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2495/3257 [00:07<00:01, 384.18it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2537/3257 [00:07<00:01, 393.40it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2577/3257 [00:07<00:01, 370.64it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2619/3257 [00:07<00:01, 383.43it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2658/3257 [00:07<00:02, 244.53it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2696/3257 [00:07<00:02, 270.98it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2731/3257 [00:07<00:01, 287.78it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2769/3257 [00:08<00:01, 308.29it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2810/3257 [00:08<00:01, 333.36it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2847/3257 [00:08<00:01, 335.37it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2890/3257 [00:08<00:01, 361.07it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:08<00:00, 358.55it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2966/3257 [00:08<00:00, 357.15it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3003/3257 [00:08<00:00, 360.10it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3042/3257 [00:08<00:00, 367.82it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3085/3257 [00:08<00:00, 384.36it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3127/3257 [00:08<00:00, 393.08it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3167/3257 [00:09<00:00, 381.96it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3206/3257 [00:09<00:00, 382.41it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:09<00:00, 386.84it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 349.81it/s]
2023-02-07 12:38:29.039 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:38:29,040][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d222,n5,mc5,s0.405224,t4>', 'datetime': '2023-02-07T12:38:29.040435', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:38:29,041][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:38:29,042][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:38:29,229][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 12:38:29,229][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:38:29,234][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1660 unique words (58.89% of original 2819, drops 1159)', 'datetime': '2023-02-07T12:38:29.234150', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:38:29,234][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 2180669 word corpus (99.86% of original 2183622, drops 2953)', 'datetime': '2023-02-07T12:38:29.234384', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:38:29,239][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 12:38:29,240][gensim.models.word2vec][INFO] - sample=0.405224 downsamples 0 most-common words
[2023-02-07 12:38:29,240][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2180669 word corpus (100.0%% of prior 2180669)', 'datetime': '2023-02-07T12:38:29.240126', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:38:29,249][gensim.models.word2vec][INFO] - estimated required memory for 1660 words and 222 dimensions: 7321776 bytes
[2023-02-07 12:38:29,250][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:38:29,254][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 1660 vocabulary and 222 features, using sg=1 hs=0 sample=0.4052242593998589 negative=5 window=12 shrink_windows=True', 'datetime': '2023-02-07T12:38:29.254096', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:38:29,936][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2183926 effective words) took 0.7s, 3208238 effective words/s
[2023-02-07 12:38:30,591][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2183926 effective words) took 0.7s, 3338882 effective words/s
[2023-02-07 12:38:31,244][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2183926 effective words) took 0.7s, 3354771 effective words/s
[2023-02-07 12:38:31,896][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2183926 effective words) took 0.7s, 3357215 effective words/s
[2023-02-07 12:38:32,553][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2183926 effective words) took 0.7s, 3331864 effective words/s
[2023-02-07 12:38:33,205][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2183926 effective words) took 0.7s, 3356832 effective words/s
[2023-02-07 12:38:33,869][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2183926 effective words) took 0.7s, 3293800 effective words/s
[2023-02-07 12:38:34,520][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2183926 effective words) took 0.7s, 3358978 effective words/s
[2023-02-07 12:38:35,162][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2183926 effective words) took 0.6s, 3411010 effective words/s
[2023-02-07 12:38:35,805][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2183926 effective words) took 0.6s, 3404191 effective words/s
[2023-02-07 12:38:36,458][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2183926 effective words) took 0.7s, 3354248 effective words/s
[2023-02-07 12:38:37,116][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2183926 effective words) took 0.7s, 3324312 effective words/s
[2023-02-07 12:38:37,759][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2183926 effective words) took 0.6s, 3401078 effective words/s
[2023-02-07 12:38:38,412][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2183926 effective words) took 0.7s, 3354083 effective words/s
[2023-02-07 12:38:39,065][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2183926 effective words) took 0.7s, 3348817 effective words/s
[2023-02-07 12:38:39,718][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2183926 effective words) took 0.7s, 3349518 effective words/s
[2023-02-07 12:38:40,375][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2183926 effective words) took 0.7s, 3336031 effective words/s
[2023-02-07 12:38:41,043][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2183926 effective words) took 0.7s, 3273895 effective words/s
[2023-02-07 12:38:41,700][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2183926 effective words) took 0.7s, 3331985 effective words/s
[2023-02-07 12:38:42,362][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2183926 effective words) took 0.7s, 3306669 effective words/s
[2023-02-07 12:38:43,027][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2183926 effective words) took 0.7s, 3293214 effective words/s
[2023-02-07 12:38:43,692][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2183926 effective words) took 0.7s, 3295933 effective words/s
[2023-02-07 12:38:44,353][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2183926 effective words) took 0.7s, 3307444 effective words/s
[2023-02-07 12:38:45,010][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2183926 effective words) took 0.7s, 3336247 effective words/s
[2023-02-07 12:38:45,676][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2183926 effective words) took 0.7s, 3287706 effective words/s
[2023-02-07 12:38:46,344][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2183926 effective words) took 0.7s, 3273781 effective words/s
[2023-02-07 12:38:47,012][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2183926 effective words) took 0.7s, 3276367 effective words/s
[2023-02-07 12:38:47,683][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2183926 effective words) took 0.7s, 3261041 effective words/s
[2023-02-07 12:38:48,342][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2183926 effective words) took 0.7s, 3323032 effective words/s
[2023-02-07 12:38:49,015][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2183926 effective words) took 0.7s, 3250959 effective words/s
[2023-02-07 12:38:49,687][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2183926 effective words) took 0.7s, 3261294 effective words/s
[2023-02-07 12:38:50,357][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2183926 effective words) took 0.7s, 3267554 effective words/s
[2023-02-07 12:38:51,030][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2183926 effective words) took 0.7s, 3249734 effective words/s
[2023-02-07 12:38:51,705][gensim.models.word2vec][INFO] - EPOCH 33: training on 2183622 raw words (2183926 effective words) took 0.7s, 3242626 effective words/s
[2023-02-07 12:38:52,430][gensim.models.word2vec][INFO] - EPOCH 34: training on 2183622 raw words (2183926 effective words) took 0.7s, 3019623 effective words/s
[2023-02-07 12:38:53,174][gensim.models.word2vec][INFO] - EPOCH 35: training on 2183622 raw words (2183926 effective words) took 0.7s, 2941046 effective words/s
[2023-02-07 12:38:53,175][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 78610392 raw words (78621336 effective words) took 23.9s, 3286662 effective words/s', 'datetime': '2023-02-07T12:38:53.175676', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:38:53.175 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:38:55,077][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123810-8e9q0pgz/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:38:55.077794', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:38:55,078][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:38:55,097][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123810-8e9q0pgz/files/../tmp/embedding_model.pt
2023-02-07 12:38:55.097 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:38:56.590 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:38:57.152 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:38:58.582 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1190117263201347, 'test_mae': 1.1300171425600358, 'test_r2': -0.0024192420319320007}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.81
wandb: percentage 0.41114
wandb:   test_mae 1.13002
wandb:   test_mse 2.11901
wandb:    test_r2 -0.00242
wandb: 
wandb: üöÄ View run misunderstood-sweep-8 at: https://wandb.ai/xiaoqiz/mof2vec/runs/8e9q0pgz
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_123810-8e9q0pgz/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: p3a8gph5 with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 652
wandb: 	model.gensim.alpha: 0.0989979925834801
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.epochs: 42
wandb: 	model.gensim.min_count: 8
wandb: 	model.gensim.sample: 0.4637068520220236
wandb: 	model.gensim.vector_size: 196
wandb: 	model.gensim.window: 13
wandb: 	model.sklearn.learning_rate: 0.0323531553116477
wandb: 	model.sklearn.max_depth: 77
wandb: 	model.sklearn.min_child_weight: 0.03038272554158781
wandb: 	model.sklearn.n_estimators: 3460
wandb: 	model.sklearn.num_leaves: 249
wandb: 	model.sklearn.reg_alpha: 0.053338401580877
wandb: 	model.sklearn.reg_lambda: 0.8845657314723416
wandb: 	model.sklearn.subsample: 0.7532510154818184
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123918-p3a8gph5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-9
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/p3a8gph5
2023-02-07 12:39:25.996 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 12:39:25.996 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 652 for sweep.
2023-02-07 12:39:25.997 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0989979925834801 for sweep.
2023-02-07 12:39:25.997 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 12:39:25.997 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 42 for sweep.
2023-02-07 12:39:25.997 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 8 for sweep.
2023-02-07 12:39:25.998 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4637068520220236 for sweep.
2023-02-07 12:39:25.998 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 196 for sweep.
2023-02-07 12:39:25.998 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 13 for sweep.
2023-02-07 12:39:25.998 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0323531553116477 for sweep.
2023-02-07 12:39:25.998 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 77 for sweep.
2023-02-07 12:39:25.999 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03038272554158781 for sweep.
2023-02-07 12:39:25.999 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3460 for sweep.
2023-02-07 12:39:25.999 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 249 for sweep.
2023-02-07 12:39:25.999 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.053338401580877 for sweep.
2023-02-07 12:39:26.000 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.8845657314723416 for sweep.
2023-02-07 12:39:26.000 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.7532510154818184 for sweep.
2023-02-07 12:39:26.000 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:39:26.004 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123918-p3a8gph5/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 652, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 196, 'window': 13, 'min_count': 8, 'dm': 1, 'sample': 0.4637068520220236, 'workers': 4, 'alpha': 0.0989979925834801, 'epochs': 42}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3460, 'max_depth': 77, 'num_leaves': 249, 'reg_alpha': 0.053338401580877, 'reg_lambda': 0.8845657314723416, 'subsample': 0.7532510154818184, 'min_child_weight': 0.03038272554158781, 'n_jobs': 4, 'learning_rate': 0.0323531553116477}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 32/3257 [00:00<00:10, 319.73it/s]  2%|‚ñè         | 64/3257 [00:00<00:10, 318.98it/s]  3%|‚ñé         | 96/3257 [00:00<00:10, 312.69it/s]  4%|‚ñç         | 128/3257 [00:00<00:10, 312.86it/s]  5%|‚ñç         | 162/3257 [00:00<00:09, 321.32it/s]  6%|‚ñå         | 196/3257 [00:00<00:09, 326.93it/s]  7%|‚ñã         | 234/3257 [00:00<00:08, 343.58it/s]  8%|‚ñä         | 269/3257 [00:00<00:08, 332.82it/s]  9%|‚ñâ         | 308/3257 [00:00<00:08, 348.52it/s] 11%|‚ñà         | 343/3257 [00:01<00:08, 344.75it/s] 12%|‚ñà‚ñè        | 378/3257 [00:01<00:08, 333.11it/s] 13%|‚ñà‚ñé        | 412/3257 [00:01<00:08, 326.79it/s] 14%|‚ñà‚ñé        | 445/3257 [00:01<00:09, 294.48it/s] 15%|‚ñà‚ñç        | 477/3257 [00:01<00:09, 299.76it/s] 16%|‚ñà‚ñå        | 510/3257 [00:01<00:08, 307.35it/s] 17%|‚ñà‚ñã        | 542/3257 [00:01<00:08, 305.20it/s] 18%|‚ñà‚ñä        | 573/3257 [00:01<00:09, 272.46it/s] 19%|‚ñà‚ñä        | 606/3257 [00:01<00:09, 286.10it/s] 20%|‚ñà‚ñâ        | 638/3257 [00:02<00:08, 293.82it/s] 21%|‚ñà‚ñà        | 668/3257 [00:02<00:09, 279.64it/s] 21%|‚ñà‚ñà‚ñè       | 697/3257 [00:02<00:09, 279.00it/s] 22%|‚ñà‚ñà‚ñè       | 726/3257 [00:02<00:09, 278.23it/s] 23%|‚ñà‚ñà‚ñé       | 755/3257 [00:02<00:08, 280.79it/s] 24%|‚ñà‚ñà‚ñç       | 784/3257 [00:02<00:08, 281.26it/s] 25%|‚ñà‚ñà‚ñç       | 813/3257 [00:02<00:12, 192.64it/s] 26%|‚ñà‚ñà‚ñå       | 837/3257 [00:02<00:11, 201.84it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:03<00:11, 215.63it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:03<00:10, 228.62it/s] 28%|‚ñà‚ñà‚ñä       | 922/3257 [00:03<00:09, 249.86it/s] 29%|‚ñà‚ñà‚ñâ       | 951/3257 [00:03<00:08, 260.61it/s] 30%|‚ñà‚ñà‚ñà       | 980/3257 [00:03<00:08, 266.06it/s] 31%|‚ñà‚ñà‚ñà       | 1008/3257 [00:03<00:08, 265.75it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1036/3257 [00:03<00:08, 267.33it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1064/3257 [00:03<00:08, 265.99it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:03<00:08, 263.86it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1119/3257 [00:03<00:07, 268.45it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:04<00:08, 263.21it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1176/3257 [00:04<00:07, 270.09it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1204/3257 [00:04<00:08, 253.19it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1235/3257 [00:04<00:07, 267.93it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1263/3257 [00:04<00:07, 269.01it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1291/3257 [00:04<00:07, 255.80it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1321/3257 [00:04<00:07, 264.92it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:04<00:07, 271.39it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1378/3257 [00:04<00:06, 271.90it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1408/3257 [00:05<00:06, 278.45it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1439/3257 [00:05<00:06, 286.73it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1474/3257 [00:05<00:05, 303.07it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1509/3257 [00:05<00:05, 314.48it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1541/3257 [00:05<00:05, 287.84it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1571/3257 [00:05<00:05, 285.36it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1602/3257 [00:05<00:05, 290.37it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:05<00:05, 287.89it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1663/3257 [00:05<00:05, 278.37it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1691/3257 [00:06<00:05, 273.65it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1721/3257 [00:06<00:05, 280.72it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1750/3257 [00:06<00:05, 271.61it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1781/3257 [00:06<00:05, 281.87it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1810/3257 [00:06<00:05, 277.85it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1841/3257 [00:06<00:04, 285.06it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1872/3257 [00:06<00:04, 290.72it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1902/3257 [00:06<00:04, 293.24it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1932/3257 [00:06<00:04, 288.55it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1969/3257 [00:06<00:04, 311.20it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2001/3257 [00:07<00:04, 300.81it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2033/3257 [00:07<00:04, 301.56it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2064/3257 [00:07<00:06, 182.93it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2095/3257 [00:07<00:05, 207.22it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2121/3257 [00:07<00:05, 214.27it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2147/3257 [00:07<00:04, 224.52it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:07<00:04, 245.36it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2208/3257 [00:08<00:04, 258.02it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2237/3257 [00:08<00:03, 263.68it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2265/3257 [00:08<00:03, 265.35it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2294/3257 [00:08<00:03, 271.72it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2328/3257 [00:08<00:03, 288.27it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2363/3257 [00:08<00:02, 304.26it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2397/3257 [00:08<00:02, 312.51it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2429/3257 [00:08<00:02, 297.24it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2460/3257 [00:08<00:02, 290.90it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2491/3257 [00:08<00:02, 294.62it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2526/3257 [00:09<00:02, 307.22it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2557/3257 [00:09<00:02, 299.73it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2588/3257 [00:09<00:02, 284.93it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2626/3257 [00:09<00:02, 310.60it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2658/3257 [00:09<00:02, 299.26it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2691/3257 [00:09<00:01, 303.54it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2722/3257 [00:09<00:01, 275.81it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:09<00:01, 291.99it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2788/3257 [00:09<00:01, 298.67it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2819/3257 [00:10<00:01, 294.67it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2850/3257 [00:10<00:01, 297.21it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2887/3257 [00:10<00:01, 314.37it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2919/3257 [00:10<00:01, 305.96it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:10<00:01, 288.80it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2980/3257 [00:10<00:00, 289.93it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:10<00:00, 293.40it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3045/3257 [00:10<00:00, 302.07it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:10<00:00, 311.49it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3112/3257 [00:11<00:00, 316.23it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3144/3257 [00:11<00:00, 308.31it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3175/3257 [00:11<00:00, 304.22it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3206/3257 [00:11<00:00, 299.08it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3237/3257 [00:11<00:00, 298.34it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 282.19it/s]
2023-02-07 12:39:37.935 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:39:37,937][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d196,n5,w13,mc8,s0.463707,t4>', 'datetime': '2023-02-07T12:39:37.936975', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:39:37,937][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:39:37,937][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:39:38,189][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 12:39:38,189][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:39:38,198][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 retains 3146 unique words (47.22% of original 6662, drops 3516)', 'datetime': '2023-02-07T12:39:38.198643', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:39:38,199][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 leaves 2901034 word corpus (99.64% of original 2911496, drops 10462)', 'datetime': '2023-02-07T12:39:38.199025', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:39:38,209][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 12:39:38,209][gensim.models.word2vec][INFO] - sample=0.463707 downsamples 0 most-common words
[2023-02-07 12:39:38,209][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2901034 word corpus (100.0%% of prior 2901034)', 'datetime': '2023-02-07T12:39:38.209934', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:39:38,227][gensim.models.word2vec][INFO] - estimated required memory for 3146 words and 196 dimensions: 9710816 bytes
[2023-02-07 12:39:38,228][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:39:38,232][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 3146 vocabulary and 196 features, using sg=0 hs=0 sample=0.4637068520220236 negative=5 window=13 shrink_windows=True', 'datetime': '2023-02-07T12:39:38.232966', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:39:39,237][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 56.16% examples, 1673535 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:39:39,917][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2904291 effective words) took 1.7s, 1727994 effective words/s
[2023-02-07 12:39:40,921][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 56.46% examples, 1682155 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:39:41,619][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2904291 effective words) took 1.7s, 1709195 effective words/s
[2023-02-07 12:39:42,622][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 58.46% examples, 1740486 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:39:43,300][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2904291 effective words) took 1.7s, 1730143 effective words/s
[2023-02-07 12:39:44,305][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 56.92% examples, 1688383 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:39:45,011][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2904291 effective words) took 1.7s, 1699403 effective words/s
[2023-02-07 12:39:46,021][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 57.69% examples, 1700537 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:39:46,704][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2904291 effective words) took 1.7s, 1718407 effective words/s
[2023-02-07 12:39:47,711][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 62.27% examples, 1823693 words/s, in_qsize 6, out_qsize 1
[2023-02-07 12:39:48,239][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2904291 effective words) took 1.5s, 1893684 effective words/s
[2023-02-07 12:39:49,245][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 67.70% examples, 1996921 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:39:49,690][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2904291 effective words) took 1.4s, 2003399 effective words/s
[2023-02-07 12:39:50,694][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 65.77% examples, 1947664 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:39:51,169][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2904291 effective words) took 1.5s, 1965734 effective words/s
[2023-02-07 12:39:52,174][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 63.00% examples, 1855886 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:39:52,717][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2904291 effective words) took 1.5s, 1878270 effective words/s
[2023-02-07 12:39:53,722][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 61.22% examples, 1799631 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:39:54,316][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2904291 effective words) took 1.6s, 1817066 effective words/s
[2023-02-07 12:39:55,323][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 60.79% examples, 1788361 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:39:55,929][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2904291 effective words) took 1.6s, 1801989 effective words/s
[2023-02-07 12:39:56,936][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 58.12% examples, 1721677 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:39:57,621][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2904291 effective words) took 1.7s, 1718606 effective words/s
[2023-02-07 12:39:58,627][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 54.07% examples, 1603281 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:39:59,378][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2904291 effective words) took 1.8s, 1653941 effective words/s
[2023-02-07 12:40:00,384][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 55.57% examples, 1649566 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:01,142][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2904291 effective words) took 1.8s, 1648839 effective words/s
[2023-02-07 12:40:02,148][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 55.08% examples, 1630832 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:02,880][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2904291 effective words) took 1.7s, 1672734 effective words/s
[2023-02-07 12:40:03,887][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 56.16% examples, 1666501 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:04,612][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2904291 effective words) took 1.7s, 1678698 effective words/s
[2023-02-07 12:40:05,616][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 60.79% examples, 1793273 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:06,182][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2904291 effective words) took 1.6s, 1851286 effective words/s
[2023-02-07 12:40:07,188][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 63.00% examples, 1852996 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:07,749][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2904291 effective words) took 1.6s, 1854538 effective words/s
[2023-02-07 12:40:08,752][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 65.06% examples, 1922478 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:09,247][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2904291 effective words) took 1.5s, 1941796 effective words/s
[2023-02-07 12:40:10,249][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 63.00% examples, 1859761 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:10,793][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2904291 effective words) took 1.5s, 1879281 effective words/s
[2023-02-07 12:40:11,803][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 61.81% examples, 1809911 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:40:12,364][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2904291 effective words) took 1.6s, 1850567 effective words/s
[2023-02-07 12:40:13,370][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 62.82% examples, 1844154 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:13,918][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2904291 effective words) took 1.6s, 1871095 effective words/s
[2023-02-07 12:40:14,923][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 61.50% examples, 1808431 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:15,535][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2904291 effective words) took 1.6s, 1797859 effective words/s
[2023-02-07 12:40:16,540][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 61.07% examples, 1798730 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:17,146][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2904291 effective words) took 1.6s, 1804102 effective words/s
[2023-02-07 12:40:18,162][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 59.53% examples, 1743812 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:18,775][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2904291 effective words) took 1.6s, 1784852 effective words/s
[2023-02-07 12:40:19,785][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 61.22% examples, 1793214 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:20,403][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2904291 effective words) took 1.6s, 1785556 effective words/s
[2023-02-07 12:40:21,412][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 59.20% examples, 1748303 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:22,041][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2904291 effective words) took 1.6s, 1775816 effective words/s
[2023-02-07 12:40:23,044][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 62.27% examples, 1833871 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:23,606][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2904291 effective words) took 1.6s, 1859017 effective words/s
[2023-02-07 12:40:24,610][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 61.22% examples, 1803956 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:25,204][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2904291 effective words) took 1.6s, 1818901 effective words/s
[2023-02-07 12:40:26,208][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 62.27% examples, 1829950 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:26,798][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2904291 effective words) took 1.6s, 1823460 effective words/s
[2023-02-07 12:40:27,808][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 61.07% examples, 1791520 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:40:28,408][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2904291 effective words) took 1.6s, 1806501 effective words/s
[2023-02-07 12:40:29,410][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 61.22% examples, 1806488 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:30,009][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2904291 effective words) took 1.6s, 1815865 effective words/s
[2023-02-07 12:40:31,015][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 58.86% examples, 1742514 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:31,676][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2904291 effective words) took 1.7s, 1744101 effective words/s
[2023-02-07 12:40:32,684][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 63.34% examples, 1858365 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:40:33,204][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2904291 effective words) took 1.5s, 1901894 effective words/s
[2023-02-07 12:40:34,207][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 63.22% examples, 1867955 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:34,752][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2904291 effective words) took 1.5s, 1877144 effective words/s
[2023-02-07 12:40:35,758][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 63.56% examples, 1872992 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:36,317][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2904291 effective words) took 1.6s, 1857604 effective words/s
[2023-02-07 12:40:37,319][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 62.54% examples, 1842321 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:37,873][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2904291 effective words) took 1.6s, 1867910 effective words/s
[2023-02-07 12:40:38,877][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 59.20% examples, 1754683 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:39,498][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2904291 effective words) took 1.6s, 1788626 effective words/s
[2023-02-07 12:40:40,503][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 63.56% examples, 1873794 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:41,064][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2904291 effective words) took 1.6s, 1855785 effective words/s
[2023-02-07 12:40:42,074][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 60.79% examples, 1783525 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:42,684][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2904291 effective words) took 1.6s, 1794847 effective words/s
[2023-02-07 12:40:43,690][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 62.27% examples, 1825503 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:40:44,279][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2904291 effective words) took 1.6s, 1822499 effective words/s
[2023-02-07 12:40:45,289][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 60.06% examples, 1762960 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:40:45,901][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2904291 effective words) took 1.6s, 1792663 effective words/s
[2023-02-07 12:40:45,901][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 122282832 raw words (121980222 effective words) took 67.7s, 1802661 effective words/s', 'datetime': '2023-02-07T12:40:45.901653', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:40:45.901 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:40:49,333][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123918-p3a8gph5/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:40:49.333021', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:40:49,334][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:40:49,355][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_123918-p3a8gph5/files/../tmp/embedding_model.pt
2023-02-07 12:40:49.356 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:40:50.675 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:40:51.204 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:40:52.490 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2572839757267564, 'test_mae': 1.152709261599951, 'test_r2': -0.06783028328413887}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.0
wandb: percentage 0.52777
wandb:   test_mae 1.15271
wandb:   test_mse 2.25728
wandb:    test_r2 -0.06783
wandb: 
wandb: üöÄ View run prime-sweep-9 at: https://wandb.ai/xiaoqiz/mof2vec/runs/p3a8gph5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_123918-p3a8gph5/logs
wandb: Agent Starting Run: 89661g6d with config:
wandb: 	data.data.wl_step: 5
wandb: 	data.nn.batch_size: 363
wandb: 	model.gensim.alpha: 0.10564253145678226
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.epochs: 55
wandb: 	model.gensim.min_count: 9
wandb: 	model.gensim.sample: 0.300911466661167
wandb: 	model.gensim.vector_size: 202
wandb: 	model.gensim.window: 13
wandb: 	model.sklearn.learning_rate: 0.012927781661214678
wandb: 	model.sklearn.max_depth: 23
wandb: 	model.sklearn.min_child_weight: 0.02829598475271436
wandb: 	model.sklearn.n_estimators: 3950
wandb: 	model.sklearn.num_leaves: 283
wandb: 	model.sklearn.reg_alpha: 0.1020966282417312
wandb: 	model.sklearn.reg_lambda: 0.10465747458844124
wandb: 	model.sklearn.subsample: 0.6857293766626451
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124104-89661g6d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/89661g6d
2023-02-07 12:41:12.839 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 5 for sweep.
2023-02-07 12:41:12.840 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 363 for sweep.
2023-02-07 12:41:12.840 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.10564253145678226 for sweep.
2023-02-07 12:41:12.840 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 12:41:12.840 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 55 for sweep.
2023-02-07 12:41:12.841 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 9 for sweep.
2023-02-07 12:41:12.841 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.300911466661167 for sweep.
2023-02-07 12:41:12.841 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 202 for sweep.
2023-02-07 12:41:12.842 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 13 for sweep.
2023-02-07 12:41:12.842 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.012927781661214678 for sweep.
2023-02-07 12:41:12.842 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 23 for sweep.
2023-02-07 12:41:12.842 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.02829598475271436 for sweep.
2023-02-07 12:41:12.843 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3950 for sweep.
2023-02-07 12:41:12.843 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 283 for sweep.
2023-02-07 12:41:12.843 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.1020966282417312 for sweep.
2023-02-07 12:41:12.843 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.10465747458844124 for sweep.
2023-02-07 12:41:12.843 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6857293766626451 for sweep.
2023-02-07 12:41:12.844 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:41:12.850 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 5}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124104-89661g6d/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 363, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 202, 'window': 13, 'min_count': 9, 'dm': 1, 'sample': 0.300911466661167, 'workers': 4, 'alpha': 0.10564253145678226, 'epochs': 55}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3950, 'max_depth': 23, 'num_leaves': 283, 'reg_alpha': 0.1020966282417312, 'reg_lambda': 0.10465747458844124, 'subsample': 0.6857293766626451, 'min_child_weight': 0.02829598475271436, 'n_jobs': 4, 'learning_rate': 0.012927781661214678}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 24/3257 [00:00<00:13, 235.64it/s]  2%|‚ñè         | 49/3257 [00:00<00:13, 241.25it/s]  2%|‚ñè         | 74/3257 [00:00<00:13, 238.98it/s]  3%|‚ñé         | 99/3257 [00:00<00:13, 241.98it/s]  4%|‚ñç         | 124/3257 [00:00<00:19, 163.52it/s]  5%|‚ñç         | 152/3257 [00:00<00:16, 192.37it/s]  5%|‚ñå         | 175/3257 [00:00<00:15, 199.57it/s]  6%|‚ñå         | 201/3257 [00:00<00:14, 214.74it/s]  7%|‚ñã         | 232/3257 [00:01<00:12, 241.07it/s]  8%|‚ñä         | 259/3257 [00:01<00:12, 245.08it/s]  9%|‚ñâ         | 291/3257 [00:01<00:11, 265.04it/s] 10%|‚ñâ         | 319/3257 [00:01<00:11, 263.94it/s] 11%|‚ñà         | 346/3257 [00:01<00:11, 262.16it/s] 12%|‚ñà‚ñè        | 375/3257 [00:01<00:10, 262.45it/s] 12%|‚ñà‚ñè        | 402/3257 [00:01<00:11, 257.79it/s] 13%|‚ñà‚ñé        | 428/3257 [00:01<00:11, 236.17it/s] 14%|‚ñà‚ñç        | 454/3257 [00:01<00:11, 241.72it/s] 15%|‚ñà‚ñç        | 480/3257 [00:02<00:11, 243.48it/s] 16%|‚ñà‚ñå        | 511/3257 [00:02<00:10, 261.62it/s] 17%|‚ñà‚ñã        | 538/3257 [00:02<00:10, 259.59it/s] 17%|‚ñà‚ñã        | 565/3257 [00:02<00:10, 245.19it/s] 18%|‚ñà‚ñä        | 590/3257 [00:02<00:11, 242.07it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:02<00:10, 247.16it/s] 20%|‚ñà‚ñâ        | 646/3257 [00:02<00:10, 249.50it/s] 21%|‚ñà‚ñà        | 672/3257 [00:02<00:10, 250.08it/s] 21%|‚ñà‚ñà‚ñè       | 698/3257 [00:02<00:10, 241.80it/s] 22%|‚ñà‚ñà‚ñè       | 723/3257 [00:03<00:10, 240.43it/s] 23%|‚ñà‚ñà‚ñé       | 748/3257 [00:03<00:10, 240.99it/s] 24%|‚ñà‚ñà‚ñç       | 774/3257 [00:03<00:10, 246.22it/s] 25%|‚ñà‚ñà‚ñç       | 801/3257 [00:03<00:09, 252.16it/s] 25%|‚ñà‚ñà‚ñå       | 827/3257 [00:03<00:10, 242.32it/s] 26%|‚ñà‚ñà‚ñå       | 852/3257 [00:03<00:10, 235.22it/s] 27%|‚ñà‚ñà‚ñã       | 877/3257 [00:03<00:10, 233.52it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:03<00:09, 248.80it/s] 29%|‚ñà‚ñà‚ñä       | 932/3257 [00:03<00:09, 247.32it/s] 29%|‚ñà‚ñà‚ñâ       | 959/3257 [00:03<00:09, 253.52it/s] 30%|‚ñà‚ñà‚ñà       | 985/3257 [00:04<00:09, 251.81it/s] 31%|‚ñà‚ñà‚ñà       | 1011/3257 [00:04<00:09, 240.46it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1036/3257 [00:04<00:09, 238.89it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1060/3257 [00:04<00:09, 236.81it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1086/3257 [00:04<00:09, 240.55it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1113/3257 [00:04<00:08, 248.39it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1138/3257 [00:04<00:08, 245.24it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1164/3257 [00:04<00:08, 248.43it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1189/3257 [00:04<00:09, 224.06it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1213/3257 [00:05<00:09, 225.30it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1243/3257 [00:05<00:08, 243.75it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1270/3257 [00:05<00:07, 249.83it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1296/3257 [00:05<00:08, 230.37it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1320/3257 [00:05<00:12, 161.21it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1346/3257 [00:05<00:10, 181.13it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:05<00:09, 191.10it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1395/3257 [00:05<00:09, 204.83it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1424/3257 [00:06<00:08, 226.25it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1453/3257 [00:06<00:07, 242.22it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1482/3257 [00:06<00:06, 254.80it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1512/3257 [00:06<00:06, 266.34it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1540/3257 [00:06<00:06, 248.66it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1566/3257 [00:06<00:06, 244.19it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1592/3257 [00:06<00:06, 247.07it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:06<00:06, 248.79it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1645/3257 [00:06<00:06, 244.26it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1670/3257 [00:07<00:06, 236.89it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1694/3257 [00:07<00:06, 231.06it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1720/3257 [00:07<00:06, 238.21it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1744/3257 [00:07<00:06, 222.83it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1771/3257 [00:07<00:06, 234.54it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1798/3257 [00:07<00:06, 242.57it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1823/3257 [00:07<00:05, 240.99it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1848/3257 [00:07<00:05, 238.84it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1876/3257 [00:07<00:05, 248.92it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1901/3257 [00:08<00:05, 248.63it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1926/3257 [00:08<00:05, 247.25it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1961/3257 [00:08<00:04, 275.77it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1989/3257 [00:08<00:04, 265.12it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2016/3257 [00:08<00:04, 265.25it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2043/3257 [00:08<00:04, 261.26it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2070/3257 [00:08<00:04, 243.82it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2096/3257 [00:08<00:04, 243.19it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2121/3257 [00:08<00:04, 234.52it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2145/3257 [00:09<00:04, 231.54it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2173/3257 [00:09<00:04, 244.00it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2198/3257 [00:09<00:04, 245.52it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2223/3257 [00:09<00:04, 243.28it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2248/3257 [00:09<00:04, 228.73it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2272/3257 [00:09<00:04, 225.66it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:09<00:04, 235.85it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2330/3257 [00:09<00:03, 256.74it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:09<00:03, 267.47it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2390/3257 [00:09<00:03, 275.23it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2418/3257 [00:10<00:03, 262.33it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2445/3257 [00:10<00:03, 250.94it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2473/3257 [00:10<00:03, 258.29it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2502/3257 [00:10<00:02, 266.01it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2531/3257 [00:10<00:02, 271.52it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2559/3257 [00:10<00:02, 257.87it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2586/3257 [00:10<00:02, 246.18it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2617/3257 [00:10<00:02, 262.90it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2644/3257 [00:10<00:02, 261.20it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2671/3257 [00:11<00:02, 257.97it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2697/3257 [00:11<00:02, 250.81it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2723/3257 [00:11<00:02, 237.01it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2753/3257 [00:11<00:01, 253.91it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2779/3257 [00:11<00:01, 246.82it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2807/3257 [00:11<00:01, 253.22it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2833/3257 [00:11<00:02, 148.34it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2864/3257 [00:12<00:02, 178.28it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2893/3257 [00:12<00:01, 196.22it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2920/3257 [00:12<00:01, 212.44it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2945/3257 [00:12<00:01, 214.57it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2971/3257 [00:12<00:01, 225.78it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2996/3257 [00:12<00:01, 224.96it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3022/3257 [00:12<00:01, 232.68it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3053/3257 [00:12<00:00, 252.74it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3082/3257 [00:12<00:00, 263.07it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3111/3257 [00:13<00:00, 269.52it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3139/3257 [00:13<00:00, 267.47it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3167/3257 [00:13<00:00, 257.60it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3194/3257 [00:13<00:00, 260.60it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3221/3257 [00:13<00:00, 249.17it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:13<00:00, 259.01it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 239.84it/s]
2023-02-07 12:41:26.926 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:41:26,927][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d202,n5,w13,mc9,s0.300911,t4>', 'datetime': '2023-02-07T12:41:26.927193', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:41:26,927][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:41:26,928][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:41:27,294][gensim.models.doc2vec][INFO] - collected 21699 word types and 3257 unique tags from a corpus of 3257 examples and 4367244 words
[2023-02-07 12:41:27,294][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:41:27,316][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=9 retains 8074 unique words (37.21% of original 21699, drops 13625)', 'datetime': '2023-02-07T12:41:27.316492', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:41:27,316][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=9 leaves 4319839 word corpus (98.91% of original 4367244, drops 47405)', 'datetime': '2023-02-07T12:41:27.316818', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:41:27,342][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 21699 items
[2023-02-07 12:41:27,342][gensim.models.word2vec][INFO] - sample=0.300911 downsamples 0 most-common words
[2023-02-07 12:41:27,342][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4319839 word corpus (100.0%% of prior 4319839)', 'datetime': '2023-02-07T12:41:27.342931', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:41:27,387][gensim.models.word2vec][INFO] - estimated required memory for 8074 words and 202 dimensions: 20367640 bytes
[2023-02-07 12:41:27,387][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:41:27,396][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 8074 vocabulary and 202 features, using sg=0 hs=0 sample=0.300911466661167 negative=5 window=13 shrink_windows=True', 'datetime': '2023-02-07T12:41:27.396466', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:41:28,404][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 42.49% examples, 1883427 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:29,405][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 89.87% examples, 1944974 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:29,615][gensim.models.word2vec][INFO] - EPOCH 0: training on 4367244 raw words (4321439 effective words) took 2.2s, 1949517 effective words/s
[2023-02-07 12:41:30,621][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 42.89% examples, 1902149 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:31,626][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 88.89% examples, 1924865 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:31,847][gensim.models.word2vec][INFO] - EPOCH 1: training on 4367244 raw words (4321439 effective words) took 2.2s, 1937346 effective words/s
[2023-02-07 12:41:32,858][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 41.73% examples, 1839005 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:33,861][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 85.02% examples, 1844026 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:34,201][gensim.models.word2vec][INFO] - EPOCH 2: training on 4367244 raw words (4321439 effective words) took 2.4s, 1837015 effective words/s
[2023-02-07 12:41:35,205][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 41.36% examples, 1834285 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:36,212][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 85.45% examples, 1857088 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:36,519][gensim.models.word2vec][INFO] - EPOCH 3: training on 4367244 raw words (4321439 effective words) took 2.3s, 1866247 effective words/s
[2023-02-07 12:41:37,522][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 41.88% examples, 1858907 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:38,529][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 85.23% examples, 1851411 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:38,870][gensim.models.word2vec][INFO] - EPOCH 4: training on 4367244 raw words (4321439 effective words) took 2.4s, 1838649 effective words/s
[2023-02-07 12:41:39,873][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 40.22% examples, 1789931 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:40,874][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 80.04% examples, 1745532 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:41,301][gensim.models.word2vec][INFO] - EPOCH 5: training on 4367244 raw words (4321439 effective words) took 2.4s, 1779095 effective words/s
[2023-02-07 12:41:42,310][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 42.77% examples, 1893136 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:41:43,317][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 86.95% examples, 1882511 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:43,605][gensim.models.word2vec][INFO] - EPOCH 6: training on 4367244 raw words (4321439 effective words) took 2.3s, 1876823 effective words/s
[2023-02-07 12:41:44,606][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 41.17% examples, 1826599 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:45,610][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 84.59% examples, 1840261 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:45,947][gensim.models.word2vec][INFO] - EPOCH 7: training on 4367244 raw words (4321439 effective words) took 2.3s, 1845699 effective words/s
[2023-02-07 12:41:46,956][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 41.73% examples, 1841526 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:47,959][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 86.52% examples, 1877338 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:48,247][gensim.models.word2vec][INFO] - EPOCH 8: training on 4367244 raw words (4321439 effective words) took 2.3s, 1879878 effective words/s
[2023-02-07 12:41:49,253][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 41.63% examples, 1846842 words/s, in_qsize 6, out_qsize 1
[2023-02-07 12:41:50,254][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 85.63% examples, 1860409 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:50,561][gensim.models.word2vec][INFO] - EPOCH 9: training on 4367244 raw words (4321439 effective words) took 2.3s, 1869250 effective words/s
[2023-02-07 12:41:51,562][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 42.22% examples, 1875070 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:52,566][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 88.15% examples, 1912048 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:52,827][gensim.models.word2vec][INFO] - EPOCH 10: training on 4367244 raw words (4321439 effective words) took 2.3s, 1907968 effective words/s
[2023-02-07 12:41:53,832][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 40.99% examples, 1811223 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:41:54,834][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 84.37% examples, 1834184 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:55,181][gensim.models.word2vec][INFO] - EPOCH 11: training on 4367244 raw words (4321439 effective words) took 2.4s, 1836574 effective words/s
[2023-02-07 12:41:56,183][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 41.88% examples, 1862489 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:57,184][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 86.25% examples, 1876852 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:57,483][gensim.models.word2vec][INFO] - EPOCH 12: training on 4367244 raw words (4321439 effective words) took 2.3s, 1878567 effective words/s
[2023-02-07 12:41:58,495][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 41.63% examples, 1833773 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:59,497][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 85.63% examples, 1853410 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:41:59,822][gensim.models.word2vec][INFO] - EPOCH 13: training on 4367244 raw words (4321439 effective words) took 2.3s, 1848446 effective words/s
[2023-02-07 12:42:00,828][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 38.72% examples, 1706617 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:01,834][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 82.84% examples, 1799926 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:02,203][gensim.models.word2vec][INFO] - EPOCH 14: training on 4367244 raw words (4321439 effective words) took 2.4s, 1816611 effective words/s
[2023-02-07 12:42:03,208][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 42.19% examples, 1872584 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:04,217][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 85.23% examples, 1847859 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:04,533][gensim.models.word2vec][INFO] - EPOCH 15: training on 4367244 raw words (4321439 effective words) took 2.3s, 1855781 effective words/s
[2023-02-07 12:42:05,539][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 41.63% examples, 1844908 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:06,548][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 85.51% examples, 1851744 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:06,851][gensim.models.word2vec][INFO] - EPOCH 16: training on 4367244 raw words (4321439 effective words) took 2.3s, 1865325 effective words/s
[2023-02-07 12:42:07,853][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 41.79% examples, 1858834 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:08,858][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 85.02% examples, 1850595 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:09,182][gensim.models.word2vec][INFO] - EPOCH 17: training on 4367244 raw words (4321439 effective words) took 2.3s, 1855219 effective words/s
[2023-02-07 12:42:10,185][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 42.62% examples, 1897928 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:11,191][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 87.84% examples, 1906674 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:11,439][gensim.models.word2vec][INFO] - EPOCH 18: training on 4367244 raw words (4321439 effective words) took 2.3s, 1915667 effective words/s
[2023-02-07 12:42:12,443][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 42.49% examples, 1887886 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:13,444][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 86.43% examples, 1879727 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:13,732][gensim.models.word2vec][INFO] - EPOCH 19: training on 4367244 raw words (4321439 effective words) took 2.3s, 1886132 effective words/s
[2023-02-07 12:42:14,736][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 41.17% examples, 1822112 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:15,741][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 85.02% examples, 1848410 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:16,053][gensim.models.word2vec][INFO] - EPOCH 20: training on 4367244 raw words (4321439 effective words) took 2.3s, 1863046 effective words/s
[2023-02-07 12:42:17,056][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 42.37% examples, 1880618 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:18,062][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 86.95% examples, 1888531 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:18,334][gensim.models.word2vec][INFO] - EPOCH 21: training on 4367244 raw words (4321439 effective words) took 2.3s, 1895178 effective words/s
[2023-02-07 12:42:19,337][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 42.89% examples, 1906465 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:20,338][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 87.69% examples, 1908305 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:20,600][gensim.models.word2vec][INFO] - EPOCH 22: training on 4367244 raw words (4321439 effective words) took 2.3s, 1908600 effective words/s
[2023-02-07 12:42:21,602][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 40.99% examples, 1815906 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:22,609][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 84.59% examples, 1835599 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:22,947][gensim.models.word2vec][INFO] - EPOCH 23: training on 4367244 raw words (4321439 effective words) took 2.3s, 1841899 effective words/s
[2023-02-07 12:42:23,950][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 42.19% examples, 1876301 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:24,954][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 87.38% examples, 1899820 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:25,220][gensim.models.word2vec][INFO] - EPOCH 24: training on 4367244 raw words (4321439 effective words) took 2.3s, 1902574 effective words/s
[2023-02-07 12:42:26,221][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 42.19% examples, 1878362 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:27,224][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 87.38% examples, 1903152 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:42:27,471][gensim.models.word2vec][INFO] - EPOCH 25: training on 4367244 raw words (4321439 effective words) took 2.2s, 1920685 effective words/s
[2023-02-07 12:42:28,473][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 42.89% examples, 1908824 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:29,477][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 90.02% examples, 1948992 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:29,682][gensim.models.word2vec][INFO] - EPOCH 26: training on 4367244 raw words (4321439 effective words) took 2.2s, 1955281 effective words/s
[2023-02-07 12:42:30,689][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 43.05% examples, 1905975 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:31,693][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 89.28% examples, 1933602 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:31,912][gensim.models.word2vec][INFO] - EPOCH 27: training on 4367244 raw words (4321439 effective words) took 2.2s, 1938981 effective words/s
[2023-02-07 12:42:32,919][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 42.49% examples, 1883849 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:33,920][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 87.38% examples, 1899755 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:34,191][gensim.models.word2vec][INFO] - EPOCH 28: training on 4367244 raw words (4321439 effective words) took 2.3s, 1897191 effective words/s
[2023-02-07 12:42:35,196][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 42.89% examples, 1903442 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:36,199][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 89.41% examples, 1937984 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:36,417][gensim.models.word2vec][INFO] - EPOCH 29: training on 4367244 raw words (4321439 effective words) took 2.2s, 1942607 effective words/s
[2023-02-07 12:42:37,422][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 43.66% examples, 1928732 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:38,426][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 90.08% examples, 1950820 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:38,629][gensim.models.word2vec][INFO] - EPOCH 30: training on 4367244 raw words (4321439 effective words) took 2.2s, 1955374 effective words/s
[2023-02-07 12:42:39,633][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 42.89% examples, 1907528 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:40,634][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 87.96% examples, 1912193 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:40,875][gensim.models.word2vec][INFO] - EPOCH 31: training on 4367244 raw words (4321439 effective words) took 2.2s, 1925771 effective words/s
[2023-02-07 12:42:41,879][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 43.87% examples, 1940252 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:42,880][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 90.33% examples, 1959316 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:43,080][gensim.models.word2vec][INFO] - EPOCH 32: training on 4367244 raw words (4321439 effective words) took 2.2s, 1960869 effective words/s
[2023-02-07 12:42:44,084][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 42.62% examples, 1897168 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:45,086][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 87.38% examples, 1901505 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:42:45,341][gensim.models.word2vec][INFO] - EPOCH 33: training on 4367244 raw words (4321439 effective words) took 2.3s, 1912740 effective words/s
[2023-02-07 12:42:46,345][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 43.81% examples, 1938263 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:47,347][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 88.89% examples, 1928954 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:47,577][gensim.models.word2vec][INFO] - EPOCH 34: training on 4367244 raw words (4321439 effective words) took 2.2s, 1933925 effective words/s
[2023-02-07 12:42:48,583][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 42.89% examples, 1901917 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:49,584][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 88.42% examples, 1914782 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:49,835][gensim.models.word2vec][INFO] - EPOCH 35: training on 4367244 raw words (4321439 effective words) took 2.3s, 1915468 effective words/s
[2023-02-07 12:42:50,839][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 44.24% examples, 1958327 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:51,842][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 90.08% examples, 1952875 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:52,056][gensim.models.word2vec][INFO] - EPOCH 36: training on 4367244 raw words (4321439 effective words) took 2.2s, 1947546 effective words/s
[2023-02-07 12:42:53,059][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 42.19% examples, 1875260 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:54,060][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 87.96% examples, 1912247 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:54,288][gensim.models.word2vec][INFO] - EPOCH 37: training on 4367244 raw words (4321439 effective words) took 2.2s, 1936885 effective words/s
[2023-02-07 12:42:55,295][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 46.98% examples, 2059842 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:56,306][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 96.81% examples, 2077661 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:42:56,364][gensim.models.word2vec][INFO] - EPOCH 38: training on 4367244 raw words (4321439 effective words) took 2.1s, 2082717 effective words/s
[2023-02-07 12:42:57,367][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 46.98% examples, 2068213 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:58,370][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 95.46% examples, 2060944 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:42:58,454][gensim.models.word2vec][INFO] - EPOCH 39: training on 4367244 raw words (4321439 effective words) took 2.1s, 2068776 effective words/s
[2023-02-07 12:42:59,462][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 46.95% examples, 2059660 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:00,467][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 96.81% examples, 2083923 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:00,532][gensim.models.word2vec][INFO] - EPOCH 40: training on 4367244 raw words (4321439 effective words) took 2.1s, 2082575 effective words/s
[2023-02-07 12:43:01,538][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 47.22% examples, 2078519 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:43:02,539][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 96.87% examples, 2087895 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:43:02,592][gensim.models.word2vec][INFO] - EPOCH 41: training on 4367244 raw words (4321439 effective words) took 2.1s, 2098115 effective words/s
[2023-02-07 12:43:03,596][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 46.98% examples, 2066507 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:04,596][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 95.98% examples, 2071290 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:04,681][gensim.models.word2vec][INFO] - EPOCH 42: training on 4367244 raw words (4321439 effective words) took 2.1s, 2069732 effective words/s
[2023-02-07 12:43:05,696][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 46.70% examples, 2024189 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:06,702][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 96.81% examples, 2075218 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:06,759][gensim.models.word2vec][INFO] - EPOCH 43: training on 4367244 raw words (4321439 effective words) took 2.1s, 2081123 effective words/s
[2023-02-07 12:43:07,760][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 46.79% examples, 2061014 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:08,764][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 96.10% examples, 2073722 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:08,842][gensim.models.word2vec][INFO] - EPOCH 44: training on 4367244 raw words (4321439 effective words) took 2.1s, 2075359 effective words/s
[2023-02-07 12:43:09,844][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 43.66% examples, 1933578 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:10,849][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 92.14% examples, 1998405 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:10,989][gensim.models.word2vec][INFO] - EPOCH 45: training on 4367244 raw words (4321439 effective words) took 2.1s, 2014092 effective words/s
[2023-02-07 12:43:11,996][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 44.34% examples, 1951502 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:13,003][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 92.14% examples, 1991381 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:13,153][gensim.models.word2vec][INFO] - EPOCH 46: training on 4367244 raw words (4321439 effective words) took 2.2s, 1998173 effective words/s
[2023-02-07 12:43:14,161][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 46.98% examples, 2057893 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:15,162][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 96.59% examples, 2082665 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:15,230][gensim.models.word2vec][INFO] - EPOCH 47: training on 4367244 raw words (4321439 effective words) took 2.1s, 2081946 effective words/s
[2023-02-07 12:43:16,233][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 44.34% examples, 1958610 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:17,236][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 91.25% examples, 1980938 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:17,400][gensim.models.word2vec][INFO] - EPOCH 48: training on 4367244 raw words (4321439 effective words) took 2.2s, 1992732 effective words/s
[2023-02-07 12:43:18,403][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 43.05% examples, 1913905 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:19,406][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 90.02% examples, 1949798 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:19,613][gensim.models.word2vec][INFO] - EPOCH 49: training on 4367244 raw words (4321439 effective words) took 2.2s, 1954456 effective words/s
[2023-02-07 12:43:20,618][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 40.71% examples, 1804465 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:21,620][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 82.99% examples, 1803391 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:21,982][gensim.models.word2vec][INFO] - EPOCH 50: training on 4367244 raw words (4321439 effective words) took 2.4s, 1824821 effective words/s
[2023-02-07 12:43:22,987][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 42.49% examples, 1885573 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:23,994][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 88.67% examples, 1916479 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:24,243][gensim.models.word2vec][INFO] - EPOCH 51: training on 4367244 raw words (4321439 effective words) took 2.3s, 1912898 effective words/s
[2023-02-07 12:43:25,247][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 41.73% examples, 1850556 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:26,248][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 86.12% examples, 1871075 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:26,549][gensim.models.word2vec][INFO] - EPOCH 52: training on 4367244 raw words (4321439 effective words) took 2.3s, 1875448 effective words/s
[2023-02-07 12:43:27,553][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 42.22% examples, 1869865 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:28,559][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 87.66% examples, 1901981 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:28,821][gensim.models.word2vec][INFO] - EPOCH 53: training on 4367244 raw words (4321439 effective words) took 2.3s, 1903034 effective words/s
[2023-02-07 12:43:29,824][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 42.49% examples, 1888632 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:30,828][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 86.95% examples, 1891238 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:43:31,098][gensim.models.word2vec][INFO] - EPOCH 54: training on 4367244 raw words (4321439 effective words) took 2.3s, 1898828 effective words/s
[2023-02-07 12:43:31,099][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 240198420 raw words (237679145 effective words) took 123.7s, 1921400 effective words/s', 'datetime': '2023-02-07T12:43:31.099135', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:43:31.099 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:43:39,112][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124104-89661g6d/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:43:39.112335', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:43:39,113][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:43:39,151][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124104-89661g6d/files/../tmp/embedding_model.pt
2023-02-07 12:43:39.152 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:43:40.599 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:43:41.129 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:43:42.501 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2821647942021763, 'test_mae': 1.1716486500016203, 'test_r2': -0.07960039804446484}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.0
wandb: percentage 0.62791
wandb:   test_mae 1.17165
wandb:   test_mse 2.28216
wandb:    test_r2 -0.0796
wandb: 
wandb: üöÄ View run exalted-sweep-10 at: https://wandb.ai/xiaoqiz/mof2vec/runs/89661g6d
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_124104-89661g6d/logs
wandb: Agent Starting Run: hc5lc9s0 with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 520
wandb: 	model.gensim.alpha: 0.06688387532210667
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 63
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.29519440487731174
wandb: 	model.gensim.vector_size: 54
wandb: 	model.gensim.window: 18
wandb: 	model.sklearn.learning_rate: 0.0031972394649062126
wandb: 	model.sklearn.max_depth: 19
wandb: 	model.sklearn.min_child_weight: 0.016501598377785975
wandb: 	model.sklearn.n_estimators: 3449
wandb: 	model.sklearn.num_leaves: 374
wandb: 	model.sklearn.reg_alpha: 0.011900555264672772
wandb: 	model.sklearn.reg_lambda: 0.0227677152282684
wandb: 	model.sklearn.subsample: 0.6333232264059243
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124352-hc5lc9s0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-sweep-11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/hc5lc9s0
2023-02-07 12:44:00.590 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 12:44:00.591 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 520 for sweep.
2023-02-07 12:44:00.591 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.06688387532210667 for sweep.
2023-02-07 12:44:00.591 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:44:00.591 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 63 for sweep.
2023-02-07 12:44:00.592 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 12:44:00.592 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.29519440487731174 for sweep.
2023-02-07 12:44:00.592 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 54 for sweep.
2023-02-07 12:44:00.592 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 18 for sweep.
2023-02-07 12:44:00.592 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0031972394649062126 for sweep.
2023-02-07 12:44:00.593 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 19 for sweep.
2023-02-07 12:44:00.593 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.016501598377785975 for sweep.
2023-02-07 12:44:00.593 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3449 for sweep.
2023-02-07 12:44:00.593 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 374 for sweep.
2023-02-07 12:44:00.594 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.011900555264672772 for sweep.
2023-02-07 12:44:00.594 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.0227677152282684 for sweep.
2023-02-07 12:44:00.594 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6333232264059243 for sweep.
2023-02-07 12:44:00.595 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:44:00.599 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124352-hc5lc9s0/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 520, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 54, 'window': 18, 'min_count': 6, 'dm': 0, 'sample': 0.29519440487731174, 'workers': 4, 'alpha': 0.06688387532210667, 'epochs': 63}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3449, 'max_depth': 19, 'num_leaves': 374, 'reg_alpha': 0.011900555264672772, 'reg_lambda': 0.0227677152282684, 'subsample': 0.6333232264059243, 'min_child_weight': 0.016501598377785975, 'n_jobs': 4, 'learning_rate': 0.0031972394649062126}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 25/3257 [00:00<00:13, 242.79it/s]  2%|‚ñè         | 50/3257 [00:00<00:13, 246.68it/s]  2%|‚ñè         | 76/3257 [00:00<00:12, 251.84it/s]  3%|‚ñé         | 103/3257 [00:00<00:12, 258.55it/s]  4%|‚ñç         | 129/3257 [00:00<00:12, 247.71it/s]  5%|‚ñç         | 158/3257 [00:00<00:12, 254.08it/s]  6%|‚ñå         | 184/3257 [00:00<00:12, 249.23it/s]  7%|‚ñã         | 214/3257 [00:00<00:11, 263.71it/s]  7%|‚ñã         | 244/3257 [00:00<00:11, 272.33it/s]  8%|‚ñä         | 272/3257 [00:01<00:11, 266.23it/s]  9%|‚ñâ         | 302/3257 [00:01<00:10, 275.02it/s] 10%|‚ñà         | 331/3257 [00:01<00:10, 275.84it/s] 11%|‚ñà         | 359/3257 [00:01<00:10, 271.57it/s] 12%|‚ñà‚ñè        | 387/3257 [00:01<00:11, 255.38it/s] 13%|‚ñà‚ñé        | 416/3257 [00:01<00:10, 264.17it/s] 14%|‚ñà‚ñé        | 443/3257 [00:01<00:11, 236.18it/s] 14%|‚ñà‚ñç        | 472/3257 [00:01<00:11, 249.00it/s] 15%|‚ñà‚ñå        | 498/3257 [00:01<00:10, 251.83it/s] 16%|‚ñà‚ñå        | 526/3257 [00:02<00:10, 254.32it/s] 17%|‚ñà‚ñã        | 554/3257 [00:02<00:10, 259.64it/s] 18%|‚ñà‚ñä        | 581/3257 [00:02<00:11, 235.16it/s] 19%|‚ñà‚ñä        | 610/3257 [00:02<00:10, 248.53it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:02<00:10, 251.64it/s] 20%|‚ñà‚ñà        | 663/3257 [00:02<00:11, 233.72it/s] 21%|‚ñà‚ñà        | 689/3257 [00:02<00:10, 240.64it/s] 22%|‚ñà‚ñà‚ñè       | 718/3257 [00:02<00:10, 252.08it/s] 23%|‚ñà‚ñà‚ñé       | 744/3257 [00:02<00:10, 238.01it/s] 24%|‚ñà‚ñà‚ñé       | 772/3257 [00:03<00:10, 247.95it/s] 25%|‚ñà‚ñà‚ñç       | 798/3257 [00:03<00:09, 249.93it/s] 25%|‚ñà‚ñà‚ñå       | 824/3257 [00:03<00:10, 241.47it/s] 26%|‚ñà‚ñà‚ñå       | 849/3257 [00:03<00:10, 232.92it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:03<00:10, 236.82it/s] 28%|‚ñà‚ñà‚ñä       | 898/3257 [00:03<00:09, 237.29it/s] 28%|‚ñà‚ñà‚ñä       | 927/3257 [00:03<00:09, 251.38it/s] 29%|‚ñà‚ñà‚ñâ       | 953/3257 [00:03<00:09, 250.92it/s] 30%|‚ñà‚ñà‚ñà       | 979/3257 [00:03<00:09, 249.11it/s] 31%|‚ñà‚ñà‚ñà       | 1004/3257 [00:04<00:13, 162.29it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:04<00:12, 173.52it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1050/3257 [00:04<00:12, 183.80it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1079/3257 [00:04<00:10, 209.70it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1103/3257 [00:04<00:10, 209.81it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1127/3257 [00:04<00:09, 213.82it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1150/3257 [00:04<00:09, 217.00it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1176/3257 [00:04<00:09, 228.58it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1200/3257 [00:05<00:09, 213.93it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1223/3257 [00:05<00:09, 218.16it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:05<00:08, 228.10it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:05<00:08, 232.64it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1299/3257 [00:05<00:08, 226.58it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1327/3257 [00:05<00:08, 239.53it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1352/3257 [00:05<00:08, 233.41it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1376/3257 [00:05<00:08, 233.33it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1402/3257 [00:05<00:07, 237.20it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:06<00:07, 251.68it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1463/3257 [00:06<00:06, 264.41it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1490/3257 [00:06<00:06, 265.42it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1518/3257 [00:06<00:06, 268.35it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1545/3257 [00:06<00:07, 240.37it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1570/3257 [00:06<00:06, 241.60it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1595/3257 [00:06<00:06, 243.15it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1621/3257 [00:06<00:06, 244.55it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1646/3257 [00:06<00:06, 242.65it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1671/3257 [00:06<00:06, 227.93it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1695/3257 [00:07<00:06, 231.14it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1721/3257 [00:07<00:06, 237.47it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1745/3257 [00:07<00:06, 224.04it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1773/3257 [00:07<00:06, 238.88it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1799/3257 [00:07<00:05, 243.55it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1824/3257 [00:07<00:05, 243.60it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1849/3257 [00:07<00:05, 238.99it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1878/3257 [00:07<00:05, 252.43it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1904/3257 [00:07<00:05, 250.30it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1930/3257 [00:08<00:05, 248.63it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:08<00:04, 273.90it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1993/3257 [00:08<00:04, 270.11it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2021/3257 [00:08<00:04, 269.27it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2048/3257 [00:08<00:04, 252.62it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2074/3257 [00:08<00:04, 249.87it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2100/3257 [00:08<00:04, 237.89it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2124/3257 [00:08<00:04, 234.40it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2148/3257 [00:08<00:04, 232.24it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:09<00:04, 245.86it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2203/3257 [00:09<00:06, 157.08it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2226/3257 [00:09<00:06, 171.35it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2251/3257 [00:09<00:05, 188.02it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2273/3257 [00:09<00:05, 193.24it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2301/3257 [00:09<00:04, 213.43it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2335/3257 [00:09<00:03, 244.82it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2367/3257 [00:09<00:03, 260.19it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2398/3257 [00:10<00:03, 271.25it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2427/3257 [00:10<00:03, 262.08it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2454/3257 [00:10<00:03, 241.81it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2484/3257 [00:10<00:03, 255.19it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2515/3257 [00:10<00:02, 269.70it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2543/3257 [00:10<00:02, 268.96it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2571/3257 [00:10<00:02, 253.12it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2597/3257 [00:10<00:02, 251.10it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2632/3257 [00:10<00:02, 275.77it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2660/3257 [00:11<00:02, 261.05it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2690/3257 [00:11<00:02, 269.11it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2718/3257 [00:11<00:02, 240.38it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2749/3257 [00:11<00:01, 257.43it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2776/3257 [00:11<00:01, 249.04it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:11<00:01, 261.69it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2833/3257 [00:11<00:01, 247.91it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2865/3257 [00:11<00:01, 266.59it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2893/3257 [00:11<00:01, 261.54it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2920/3257 [00:12<00:01, 263.20it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2947/3257 [00:12<00:01, 252.15it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2973/3257 [00:12<00:01, 253.31it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2999/3257 [00:12<00:01, 252.92it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3025/3257 [00:12<00:00, 253.61it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3057/3257 [00:12<00:00, 270.60it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3086/3257 [00:12<00:00, 274.92it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3118/3257 [00:12<00:00, 283.12it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3147/3257 [00:12<00:00, 266.43it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3174/3257 [00:13<00:00, 267.12it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3201/3257 [00:13<00:00, 261.99it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3228/3257 [00:13<00:00, 260.32it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:13<00:00, 243.65it/s]
2023-02-07 12:44:14.446 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:44:14,447][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d54,n5,mc6,s0.295194,t4>', 'datetime': '2023-02-07T12:44:14.447750', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:44:14,449][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:44:14,449][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:44:14,772][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 12:44:14,773][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:44:14,792][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 6701 unique words (51.31% of original 13061, drops 6360)', 'datetime': '2023-02-07T12:44:14.792645', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:44:14,794][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 3623361 word corpus (99.56% of original 3639370, drops 16009)', 'datetime': '2023-02-07T12:44:14.794204', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:44:14,816][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 12:44:14,817][gensim.models.word2vec][INFO] - sample=0.295194 downsamples 0 most-common words
[2023-02-07 12:44:14,817][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3623361 word corpus (100.0%% of prior 3623361)', 'datetime': '2023-02-07T12:44:14.817473', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:44:14,854][gensim.models.word2vec][INFO] - estimated required memory for 6701 words and 54 dimensions: 7600244 bytes
[2023-02-07 12:44:14,855][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:44:14,858][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 6701 vocabulary and 54 features, using sg=1 hs=0 sample=0.29519440487731174 negative=5 window=18 shrink_windows=True', 'datetime': '2023-02-07T12:44:14.858649', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:44:15,852][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3626618 effective words) took 1.0s, 3660954 effective words/s
[2023-02-07 12:44:16,731][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3626618 effective words) took 0.9s, 4139076 effective words/s
[2023-02-07 12:44:17,584][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3626618 effective words) took 0.9s, 4258813 effective words/s
[2023-02-07 12:44:18,431][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3626618 effective words) took 0.8s, 4284311 effective words/s
[2023-02-07 12:44:19,278][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3626618 effective words) took 0.8s, 4290099 effective words/s
[2023-02-07 12:44:20,164][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3626618 effective words) took 0.9s, 4097051 effective words/s
[2023-02-07 12:44:21,063][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3626618 effective words) took 0.9s, 4042229 effective words/s
[2023-02-07 12:44:21,977][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3626618 effective words) took 0.9s, 3970790 effective words/s
[2023-02-07 12:44:22,865][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3626618 effective words) took 0.9s, 4093767 effective words/s
[2023-02-07 12:44:23,765][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3626618 effective words) took 0.9s, 4032952 effective words/s
[2023-02-07 12:44:24,651][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3626618 effective words) took 0.9s, 4100527 effective words/s
[2023-02-07 12:44:25,550][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3626618 effective words) took 0.9s, 4041443 effective words/s
[2023-02-07 12:44:26,411][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3626618 effective words) took 0.9s, 4216881 effective words/s
[2023-02-07 12:44:27,288][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3626618 effective words) took 0.9s, 4139400 effective words/s
[2023-02-07 12:44:28,174][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3626618 effective words) took 0.9s, 4102379 effective words/s
[2023-02-07 12:44:29,051][gensim.models.word2vec][INFO] - EPOCH 15: training on 3639370 raw words (3626618 effective words) took 0.9s, 4143114 effective words/s
[2023-02-07 12:44:29,927][gensim.models.word2vec][INFO] - EPOCH 16: training on 3639370 raw words (3626618 effective words) took 0.9s, 4148250 effective words/s
[2023-02-07 12:44:30,804][gensim.models.word2vec][INFO] - EPOCH 17: training on 3639370 raw words (3626618 effective words) took 0.9s, 4140956 effective words/s
[2023-02-07 12:44:31,680][gensim.models.word2vec][INFO] - EPOCH 18: training on 3639370 raw words (3626618 effective words) took 0.9s, 4144961 effective words/s
[2023-02-07 12:44:32,557][gensim.models.word2vec][INFO] - EPOCH 19: training on 3639370 raw words (3626618 effective words) took 0.9s, 4138466 effective words/s
[2023-02-07 12:44:33,408][gensim.models.word2vec][INFO] - EPOCH 20: training on 3639370 raw words (3626618 effective words) took 0.8s, 4268152 effective words/s
[2023-02-07 12:44:34,266][gensim.models.word2vec][INFO] - EPOCH 21: training on 3639370 raw words (3626618 effective words) took 0.9s, 4232399 effective words/s
[2023-02-07 12:44:35,133][gensim.models.word2vec][INFO] - EPOCH 22: training on 3639370 raw words (3626618 effective words) took 0.9s, 4191560 effective words/s
[2023-02-07 12:44:35,975][gensim.models.word2vec][INFO] - EPOCH 23: training on 3639370 raw words (3626618 effective words) took 0.8s, 4314159 effective words/s
[2023-02-07 12:44:36,825][gensim.models.word2vec][INFO] - EPOCH 24: training on 3639370 raw words (3626618 effective words) took 0.8s, 4270654 effective words/s
[2023-02-07 12:44:37,676][gensim.models.word2vec][INFO] - EPOCH 25: training on 3639370 raw words (3626618 effective words) took 0.8s, 4271981 effective words/s
[2023-02-07 12:44:38,535][gensim.models.word2vec][INFO] - EPOCH 26: training on 3639370 raw words (3626618 effective words) took 0.9s, 4225137 effective words/s
[2023-02-07 12:44:39,393][gensim.models.word2vec][INFO] - EPOCH 27: training on 3639370 raw words (3626618 effective words) took 0.9s, 4238312 effective words/s
[2023-02-07 12:44:40,252][gensim.models.word2vec][INFO] - EPOCH 28: training on 3639370 raw words (3626618 effective words) took 0.9s, 4226300 effective words/s
[2023-02-07 12:44:41,103][gensim.models.word2vec][INFO] - EPOCH 29: training on 3639370 raw words (3626618 effective words) took 0.8s, 4270235 effective words/s
[2023-02-07 12:44:41,955][gensim.models.word2vec][INFO] - EPOCH 30: training on 3639370 raw words (3626618 effective words) took 0.9s, 4259785 effective words/s
[2023-02-07 12:44:42,811][gensim.models.word2vec][INFO] - EPOCH 31: training on 3639370 raw words (3626618 effective words) took 0.9s, 4242243 effective words/s
[2023-02-07 12:44:43,662][gensim.models.word2vec][INFO] - EPOCH 32: training on 3639370 raw words (3626618 effective words) took 0.8s, 4267817 effective words/s
[2023-02-07 12:44:44,516][gensim.models.word2vec][INFO] - EPOCH 33: training on 3639370 raw words (3626618 effective words) took 0.9s, 4254142 effective words/s
[2023-02-07 12:44:45,368][gensim.models.word2vec][INFO] - EPOCH 34: training on 3639370 raw words (3626618 effective words) took 0.9s, 4264056 effective words/s
[2023-02-07 12:44:46,222][gensim.models.word2vec][INFO] - EPOCH 35: training on 3639370 raw words (3626618 effective words) took 0.9s, 4252968 effective words/s
[2023-02-07 12:44:47,079][gensim.models.word2vec][INFO] - EPOCH 36: training on 3639370 raw words (3626618 effective words) took 0.9s, 4238548 effective words/s
[2023-02-07 12:44:47,936][gensim.models.word2vec][INFO] - EPOCH 37: training on 3639370 raw words (3626618 effective words) took 0.9s, 4236083 effective words/s
[2023-02-07 12:44:48,780][gensim.models.word2vec][INFO] - EPOCH 38: training on 3639370 raw words (3626618 effective words) took 0.8s, 4301124 effective words/s
[2023-02-07 12:44:49,625][gensim.models.word2vec][INFO] - EPOCH 39: training on 3639370 raw words (3626618 effective words) took 0.8s, 4300068 effective words/s
[2023-02-07 12:44:50,469][gensim.models.word2vec][INFO] - EPOCH 40: training on 3639370 raw words (3626618 effective words) took 0.8s, 4303370 effective words/s
[2023-02-07 12:44:51,348][gensim.models.word2vec][INFO] - EPOCH 41: training on 3639370 raw words (3626618 effective words) took 0.9s, 4135390 effective words/s
[2023-02-07 12:44:52,230][gensim.models.word2vec][INFO] - EPOCH 42: training on 3639370 raw words (3626618 effective words) took 0.9s, 4118248 effective words/s
[2023-02-07 12:44:53,117][gensim.models.word2vec][INFO] - EPOCH 43: training on 3639370 raw words (3626618 effective words) took 0.9s, 4099783 effective words/s
[2023-02-07 12:44:54,002][gensim.models.word2vec][INFO] - EPOCH 44: training on 3639370 raw words (3626618 effective words) took 0.9s, 4102943 effective words/s
[2023-02-07 12:44:54,886][gensim.models.word2vec][INFO] - EPOCH 45: training on 3639370 raw words (3626618 effective words) took 0.9s, 4108809 effective words/s
[2023-02-07 12:44:55,767][gensim.models.word2vec][INFO] - EPOCH 46: training on 3639370 raw words (3626618 effective words) took 0.9s, 4125261 effective words/s
[2023-02-07 12:44:56,652][gensim.models.word2vec][INFO] - EPOCH 47: training on 3639370 raw words (3626618 effective words) took 0.9s, 4100838 effective words/s
[2023-02-07 12:44:57,534][gensim.models.word2vec][INFO] - EPOCH 48: training on 3639370 raw words (3626618 effective words) took 0.9s, 4121110 effective words/s
[2023-02-07 12:44:58,419][gensim.models.word2vec][INFO] - EPOCH 49: training on 3639370 raw words (3626618 effective words) took 0.9s, 4100076 effective words/s
[2023-02-07 12:44:59,316][gensim.models.word2vec][INFO] - EPOCH 50: training on 3639370 raw words (3626618 effective words) took 0.9s, 4050588 effective words/s
[2023-02-07 12:45:00,205][gensim.models.word2vec][INFO] - EPOCH 51: training on 3639370 raw words (3626618 effective words) took 0.9s, 4088382 effective words/s
[2023-02-07 12:45:01,097][gensim.models.word2vec][INFO] - EPOCH 52: training on 3639370 raw words (3626618 effective words) took 0.9s, 4074224 effective words/s
[2023-02-07 12:45:01,996][gensim.models.word2vec][INFO] - EPOCH 53: training on 3639370 raw words (3626618 effective words) took 0.9s, 4040020 effective words/s
[2023-02-07 12:45:02,893][gensim.models.word2vec][INFO] - EPOCH 54: training on 3639370 raw words (3626618 effective words) took 0.9s, 4053130 effective words/s
[2023-02-07 12:45:03,786][gensim.models.word2vec][INFO] - EPOCH 55: training on 3639370 raw words (3626618 effective words) took 0.9s, 4066077 effective words/s
[2023-02-07 12:45:04,678][gensim.models.word2vec][INFO] - EPOCH 56: training on 3639370 raw words (3626618 effective words) took 0.9s, 4070995 effective words/s
[2023-02-07 12:45:05,584][gensim.models.word2vec][INFO] - EPOCH 57: training on 3639370 raw words (3626618 effective words) took 0.9s, 4010910 effective words/s
[2023-02-07 12:45:06,485][gensim.models.word2vec][INFO] - EPOCH 58: training on 3639370 raw words (3626618 effective words) took 0.9s, 4030148 effective words/s
[2023-02-07 12:45:07,396][gensim.models.word2vec][INFO] - EPOCH 59: training on 3639370 raw words (3626618 effective words) took 0.9s, 3991674 effective words/s
[2023-02-07 12:45:08,316][gensim.models.word2vec][INFO] - EPOCH 60: training on 3639370 raw words (3626618 effective words) took 0.9s, 3945270 effective words/s
[2023-02-07 12:45:09,222][gensim.models.word2vec][INFO] - EPOCH 61: training on 3639370 raw words (3626618 effective words) took 0.9s, 4008927 effective words/s
[2023-02-07 12:45:10,070][gensim.models.word2vec][INFO] - EPOCH 62: training on 3639370 raw words (3626618 effective words) took 0.8s, 4282987 effective words/s
[2023-02-07 12:45:10,070][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 229280310 raw words (228476934 effective words) took 55.2s, 4138208 effective words/s', 'datetime': '2023-02-07T12:45:10.070549', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:45:10.071 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:45:14,245][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124352-hc5lc9s0/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:45:14.245766', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:45:14,246][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:45:14,258][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124352-hc5lc9s0/files/../tmp/embedding_model.pt
2023-02-07 12:45:14.258 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:45:15.249 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:45:15.632 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:45:16.043 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9766910082060933, 'test_mae': 1.0632258243287451, 'test_r2': 0.06490696697639131}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.76
wandb: percentage 0.48695
wandb:   test_mae 1.06323
wandb:   test_mse 1.97669
wandb:    test_r2 0.06491
wandb: 
wandb: üöÄ View run morning-sweep-11 at: https://wandb.ai/xiaoqiz/mof2vec/runs/hc5lc9s0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_124352-hc5lc9s0/logs
wandb: Agent Starting Run: odtp50uo with config:
wandb: 	data.data.wl_step: 5
wandb: 	data.nn.batch_size: 372
wandb: 	model.gensim.alpha: 0.0021115851545807614
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 33
wandb: 	model.gensim.min_count: 7
wandb: 	model.gensim.sample: 0.273730768072423
wandb: 	model.gensim.vector_size: 109
wandb: 	model.gensim.window: 13
wandb: 	model.sklearn.learning_rate: 0.014942209707473175
wandb: 	model.sklearn.max_depth: 32
wandb: 	model.sklearn.min_child_weight: 0.014052492492906674
wandb: 	model.sklearn.n_estimators: 2703
wandb: 	model.sklearn.num_leaves: 312
wandb: 	model.sklearn.reg_alpha: 0.12169885388007128
wandb: 	model.sklearn.reg_lambda: 0.5424799559506913
wandb: 	model.sklearn.subsample: 0.562448383888798
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124526-odtp50uo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-12
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/odtp50uo
2023-02-07 12:45:33.938 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 5 for sweep.
2023-02-07 12:45:33.938 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 372 for sweep.
2023-02-07 12:45:33.938 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0021115851545807614 for sweep.
2023-02-07 12:45:33.939 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:45:33.939 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 33 for sweep.
2023-02-07 12:45:33.939 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 7 for sweep.
2023-02-07 12:45:33.940 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.273730768072423 for sweep.
2023-02-07 12:45:33.940 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 109 for sweep.
2023-02-07 12:45:33.940 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 13 for sweep.
2023-02-07 12:45:33.940 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.014942209707473175 for sweep.
2023-02-07 12:45:33.941 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 32 for sweep.
2023-02-07 12:45:33.941 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.014052492492906674 for sweep.
2023-02-07 12:45:33.941 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2703 for sweep.
2023-02-07 12:45:33.941 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 312 for sweep.
2023-02-07 12:45:33.941 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.12169885388007128 for sweep.
2023-02-07 12:45:33.942 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.5424799559506913 for sweep.
2023-02-07 12:45:33.942 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.562448383888798 for sweep.
2023-02-07 12:45:33.942 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:45:33.945 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 5}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124526-odtp50uo/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 372, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 109, 'window': 13, 'min_count': 7, 'dm': 0, 'sample': 0.273730768072423, 'workers': 4, 'alpha': 0.0021115851545807614, 'epochs': 33}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2703, 'max_depth': 32, 'num_leaves': 312, 'reg_alpha': 0.12169885388007128, 'reg_lambda': 0.5424799559506913, 'subsample': 0.562448383888798, 'min_child_weight': 0.014052492492906674, 'n_jobs': 4, 'learning_rate': 0.014942209707473175}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 25/3257 [00:00<00:13, 244.10it/s]  2%|‚ñè         | 50/3257 [00:00<00:13, 237.48it/s]  2%|‚ñè         | 76/3257 [00:00<00:13, 244.11it/s]  3%|‚ñé         | 102/3257 [00:00<00:12, 249.32it/s]  4%|‚ñç         | 127/3257 [00:00<00:13, 238.56it/s]  5%|‚ñç         | 154/3257 [00:00<00:12, 246.45it/s]  5%|‚ñå         | 179/3257 [00:00<00:12, 237.93it/s]  6%|‚ñã         | 204/3257 [00:00<00:12, 241.51it/s]  7%|‚ñã         | 236/3257 [00:00<00:11, 261.25it/s]  8%|‚ñä         | 263/3257 [00:01<00:11, 255.34it/s]  9%|‚ñâ         | 295/3257 [00:01<00:10, 271.55it/s] 10%|‚ñâ         | 323/3257 [00:01<00:10, 268.70it/s] 11%|‚ñà         | 350/3257 [00:01<00:11, 256.84it/s] 12%|‚ñà‚ñè        | 376/3257 [00:01<00:15, 180.87it/s] 12%|‚ñà‚ñè        | 399/3257 [00:01<00:14, 191.44it/s] 13%|‚ñà‚ñé        | 425/3257 [00:01<00:14, 199.00it/s] 14%|‚ñà‚ñé        | 447/3257 [00:01<00:13, 201.80it/s] 15%|‚ñà‚ñç        | 475/3257 [00:02<00:12, 219.61it/s] 15%|‚ñà‚ñå        | 502/3257 [00:02<00:11, 232.47it/s] 16%|‚ñà‚ñå        | 527/3257 [00:02<00:11, 233.34it/s] 17%|‚ñà‚ñã        | 555/3257 [00:02<00:10, 245.79it/s] 18%|‚ñà‚ñä        | 581/3257 [00:02<00:11, 224.28it/s] 19%|‚ñà‚ñä        | 609/3257 [00:02<00:11, 239.00it/s] 20%|‚ñà‚ñâ        | 636/3257 [00:02<00:10, 245.03it/s] 20%|‚ñà‚ñà        | 662/3257 [00:02<00:11, 225.01it/s] 21%|‚ñà‚ñà        | 686/3257 [00:02<00:11, 228.28it/s] 22%|‚ñà‚ñà‚ñè       | 714/3257 [00:03<00:10, 242.05it/s] 23%|‚ñà‚ñà‚ñé       | 739/3257 [00:03<00:11, 223.08it/s] 24%|‚ñà‚ñà‚ñé       | 768/3257 [00:03<00:10, 240.65it/s] 24%|‚ñà‚ñà‚ñç       | 793/3257 [00:03<00:10, 234.72it/s] 25%|‚ñà‚ñà‚ñå       | 818/3257 [00:03<00:10, 235.80it/s] 26%|‚ñà‚ñà‚ñå       | 842/3257 [00:03<00:10, 220.76it/s] 27%|‚ñà‚ñà‚ñã       | 866/3257 [00:03<00:10, 225.97it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:03<00:10, 228.71it/s] 28%|‚ñà‚ñà‚ñä       | 916/3257 [00:03<00:10, 233.31it/s] 29%|‚ñà‚ñà‚ñâ       | 942/3257 [00:04<00:09, 240.39it/s] 30%|‚ñà‚ñà‚ñâ       | 968/3257 [00:04<00:09, 245.44it/s] 30%|‚ñà‚ñà‚ñà       | 993/3257 [00:04<00:09, 237.30it/s] 31%|‚ñà‚ñà‚ñà       | 1017/3257 [00:04<00:09, 236.90it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1041/3257 [00:04<00:10, 221.08it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:04<00:09, 228.45it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:04<00:09, 226.13it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:04<00:09, 233.31it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1141/3257 [00:04<00:09, 226.31it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1168/3257 [00:05<00:08, 237.64it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1192/3257 [00:05<00:09, 215.26it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1214/3257 [00:05<00:09, 211.27it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:05<00:08, 228.72it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1268/3257 [00:05<00:08, 234.89it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1292/3257 [00:05<00:09, 216.49it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1318/3257 [00:05<00:08, 226.58it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1344/3257 [00:05<00:08, 235.33it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1368/3257 [00:05<00:08, 228.59it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1392/3257 [00:06<00:08, 225.70it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:06<00:07, 241.75it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1451/3257 [00:06<00:07, 251.23it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1477/3257 [00:06<00:07, 253.03it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1507/3257 [00:06<00:06, 265.94it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1534/3257 [00:06<00:07, 241.32it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1559/3257 [00:06<00:07, 237.00it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1584/3257 [00:06<00:06, 240.28it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1612/3257 [00:06<00:06, 251.11it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1638/3257 [00:07<00:06, 242.59it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1663/3257 [00:07<00:06, 236.38it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1687/3257 [00:07<00:10, 150.02it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1714/3257 [00:07<00:08, 173.09it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1736/3257 [00:07<00:08, 175.54it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:07<00:07, 195.93it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1792/3257 [00:07<00:06, 216.78it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1816/3257 [00:07<00:06, 218.26it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1842/3257 [00:08<00:06, 227.57it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:08<00:05, 232.87it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1894/3257 [00:08<00:05, 230.37it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1918/3257 [00:08<00:05, 229.25it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1948/3257 [00:08<00:05, 248.27it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1975/3257 [00:08<00:05, 252.16it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2001/3257 [00:08<00:05, 243.01it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2026/3257 [00:08<00:05, 244.74it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2051/3257 [00:08<00:05, 227.93it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2075/3257 [00:09<00:05, 225.00it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2098/3257 [00:09<00:05, 217.55it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:09<00:05, 217.01it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2142/3257 [00:09<00:05, 212.57it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2164/3257 [00:09<00:05, 214.50it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2187/3257 [00:09<00:04, 216.37it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2209/3257 [00:09<00:04, 216.10it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2234/3257 [00:09<00:04, 225.68it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2257/3257 [00:09<00:04, 218.45it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2279/3257 [00:10<00:04, 207.15it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2302/3257 [00:10<00:04, 212.24it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2331/3257 [00:10<00:03, 233.97it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:10<00:03, 244.29it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2386/3257 [00:10<00:03, 247.60it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2411/3257 [00:10<00:03, 240.44it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2436/3257 [00:10<00:03, 227.41it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2461/3257 [00:10<00:03, 232.54it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2485/3257 [00:10<00:03, 234.59it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2514/3257 [00:10<00:02, 250.10it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2540/3257 [00:11<00:02, 248.00it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2565/3257 [00:11<00:02, 235.78it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2589/3257 [00:11<00:03, 220.84it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2619/3257 [00:11<00:02, 238.24it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2644/3257 [00:11<00:02, 237.99it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2668/3257 [00:11<00:02, 233.48it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2693/3257 [00:11<00:02, 236.47it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2717/3257 [00:11<00:02, 208.04it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2746/3257 [00:11<00:02, 228.77it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2770/3257 [00:12<00:02, 223.26it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2799/3257 [00:12<00:01, 239.61it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2824/3257 [00:12<00:01, 225.55it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2848/3257 [00:12<00:01, 227.43it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:12<00:01, 251.14it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2907/3257 [00:12<00:01, 233.64it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2931/3257 [00:12<00:01, 235.01it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2955/3257 [00:12<00:01, 226.20it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2978/3257 [00:13<00:01, 223.45it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3004/3257 [00:13<00:01, 231.42it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3028/3257 [00:13<00:01, 226.56it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3057/3257 [00:13<00:00, 242.05it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3082/3257 [00:13<00:01, 140.20it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3109/3257 [00:13<00:00, 163.32it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3134/3257 [00:13<00:00, 180.74it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3157/3257 [00:13<00:00, 190.38it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3180/3257 [00:14<00:00, 193.53it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:14<00:00, 209.15it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3230/3257 [00:14<00:00, 213.68it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3255/3257 [00:14<00:00, 221.64it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:14<00:00, 225.74it/s]
2023-02-07 12:45:48.893 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:45:48,894][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d109,n5,mc7,s0.273731,t4>', 'datetime': '2023-02-07T12:45:48.894894', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:45:48,895][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:45:48,895][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:45:49,288][gensim.models.doc2vec][INFO] - collected 21699 word types and 3257 unique tags from a corpus of 3257 examples and 4367244 words
[2023-02-07 12:45:49,288][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:45:49,318][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 retains 9824 unique words (45.27% of original 21699, drops 11875)', 'datetime': '2023-02-07T12:45:49.318320', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:45:49,320][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 leaves 4333599 word corpus (99.23% of original 4367244, drops 33645)', 'datetime': '2023-02-07T12:45:49.320543', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:45:49,353][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 21699 items
[2023-02-07 12:45:49,354][gensim.models.word2vec][INFO] - sample=0.273731 downsamples 0 most-common words
[2023-02-07 12:45:49,354][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4333599 word corpus (100.0%% of prior 4333599)', 'datetime': '2023-02-07T12:45:49.354378', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:45:49,410][gensim.models.word2vec][INFO] - estimated required memory for 9824 words and 109 dimensions: 15549980 bytes
[2023-02-07 12:45:49,411][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:45:49,418][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 9824 vocabulary and 109 features, using sg=1 hs=0 sample=0.273730768072423 negative=5 window=13 shrink_windows=True', 'datetime': '2023-02-07T12:45:49.418238', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:45:50,420][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 67.92% examples, 3011064 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:45:50,854][gensim.models.word2vec][INFO] - EPOCH 0: training on 4367244 raw words (4335166 effective words) took 1.4s, 3023769 effective words/s
[2023-02-07 12:45:51,859][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 71.38% examples, 3145431 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:45:52,235][gensim.models.word2vec][INFO] - EPOCH 1: training on 4367244 raw words (4335166 effective words) took 1.4s, 3142571 effective words/s
[2023-02-07 12:45:53,240][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 71.38% examples, 3144336 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:45:53,609][gensim.models.word2vec][INFO] - EPOCH 2: training on 4367244 raw words (4335166 effective words) took 1.4s, 3156981 effective words/s
[2023-02-07 12:45:54,611][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 71.05% examples, 3148138 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:45:54,984][gensim.models.word2vec][INFO] - EPOCH 3: training on 4367244 raw words (4335166 effective words) took 1.4s, 3155288 effective words/s
[2023-02-07 12:45:55,990][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 72.34% examples, 3173297 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:45:56,344][gensim.models.word2vec][INFO] - EPOCH 4: training on 4367244 raw words (4335166 effective words) took 1.4s, 3191449 effective words/s
[2023-02-07 12:45:57,349][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 71.05% examples, 3139872 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:45:57,725][gensim.models.word2vec][INFO] - EPOCH 5: training on 4367244 raw words (4335166 effective words) took 1.4s, 3143746 effective words/s
[2023-02-07 12:45:58,727][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 71.66% examples, 3163984 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:45:59,089][gensim.models.word2vec][INFO] - EPOCH 6: training on 4367244 raw words (4335166 effective words) took 1.4s, 3183428 effective words/s
[2023-02-07 12:46:00,095][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 72.64% examples, 3188276 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:00,447][gensim.models.word2vec][INFO] - EPOCH 7: training on 4367244 raw words (4335166 effective words) took 1.4s, 3195169 effective words/s
[2023-02-07 12:46:01,451][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 72.98% examples, 3207451 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:01,791][gensim.models.word2vec][INFO] - EPOCH 8: training on 4367244 raw words (4335166 effective words) took 1.3s, 3229807 effective words/s
[2023-02-07 12:46:02,792][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 78.69% examples, 3445956 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:03,049][gensim.models.word2vec][INFO] - EPOCH 9: training on 4367244 raw words (4335166 effective words) took 1.3s, 3449542 effective words/s
[2023-02-07 12:46:04,054][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 79.71% examples, 3482462 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:04,287][gensim.models.word2vec][INFO] - EPOCH 10: training on 4367244 raw words (4335166 effective words) took 1.2s, 3503211 effective words/s
[2023-02-07 12:46:05,289][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 80.38% examples, 3513218 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:05,525][gensim.models.word2vec][INFO] - EPOCH 11: training on 4367244 raw words (4335166 effective words) took 1.2s, 3506348 effective words/s
[2023-02-07 12:46:06,527][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 75.19% examples, 3311498 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:46:06,839][gensim.models.word2vec][INFO] - EPOCH 12: training on 4367244 raw words (4335166 effective words) took 1.3s, 3302106 effective words/s
[2023-02-07 12:46:07,841][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 73.90% examples, 3251600 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:08,177][gensim.models.word2vec][INFO] - EPOCH 13: training on 4367244 raw words (4335166 effective words) took 1.3s, 3243487 effective words/s
[2023-02-07 12:46:09,179][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 74.39% examples, 3268141 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:09,500][gensim.models.word2vec][INFO] - EPOCH 14: training on 4367244 raw words (4335166 effective words) took 1.3s, 3281224 effective words/s
[2023-02-07 12:46:10,503][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 70.59% examples, 3125377 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:10,867][gensim.models.word2vec][INFO] - EPOCH 15: training on 4367244 raw words (4335166 effective words) took 1.4s, 3173157 effective words/s
[2023-02-07 12:46:11,870][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 75.53% examples, 3319504 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:12,168][gensim.models.word2vec][INFO] - EPOCH 16: training on 4367244 raw words (4335166 effective words) took 1.3s, 3335972 effective words/s
[2023-02-07 12:46:13,174][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 76.51% examples, 3345384 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:13,464][gensim.models.word2vec][INFO] - EPOCH 17: training on 4367244 raw words (4335166 effective words) took 1.3s, 3348638 effective words/s
[2023-02-07 12:46:14,468][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 74.15% examples, 3253807 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:14,797][gensim.models.word2vec][INFO] - EPOCH 18: training on 4367244 raw words (4335166 effective words) took 1.3s, 3255685 effective words/s
[2023-02-07 12:46:15,800][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 76.51% examples, 3355826 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:16,086][gensim.models.word2vec][INFO] - EPOCH 19: training on 4367244 raw words (4335166 effective words) took 1.3s, 3366809 effective words/s
[2023-02-07 12:46:17,087][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 76.85% examples, 3369473 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:17,373][gensim.models.word2vec][INFO] - EPOCH 20: training on 4367244 raw words (4335166 effective words) took 1.3s, 3372340 effective words/s
[2023-02-07 12:46:18,375][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 74.79% examples, 3285472 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:18,692][gensim.models.word2vec][INFO] - EPOCH 21: training on 4367244 raw words (4335166 effective words) took 1.3s, 3288870 effective words/s
[2023-02-07 12:46:19,696][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 77.43% examples, 3384793 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:19,972][gensim.models.word2vec][INFO] - EPOCH 22: training on 4367244 raw words (4335166 effective words) took 1.3s, 3391643 effective words/s
[2023-02-07 12:46:20,979][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 76.51% examples, 3342605 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:21,253][gensim.models.word2vec][INFO] - EPOCH 23: training on 4367244 raw words (4335166 effective words) took 1.3s, 3388190 effective words/s
[2023-02-07 12:46:22,258][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 78.35% examples, 3414722 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:22,518][gensim.models.word2vec][INFO] - EPOCH 24: training on 4367244 raw words (4335166 effective words) took 1.3s, 3431028 effective words/s
[2023-02-07 12:46:23,520][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 78.60% examples, 3441279 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:23,782][gensim.models.word2vec][INFO] - EPOCH 25: training on 4367244 raw words (4335166 effective words) took 1.3s, 3431292 effective words/s
[2023-02-07 12:46:24,784][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 75.53% examples, 3323820 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:25,082][gensim.models.word2vec][INFO] - EPOCH 26: training on 4367244 raw words (4335166 effective words) took 1.3s, 3338940 effective words/s
[2023-02-07 12:46:26,085][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 77.53% examples, 3390176 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:26,355][gensim.models.word2vec][INFO] - EPOCH 27: training on 4367244 raw words (4335166 effective words) took 1.3s, 3411060 effective words/s
[2023-02-07 12:46:27,360][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 77.43% examples, 3379126 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:27,631][gensim.models.word2vec][INFO] - EPOCH 28: training on 4367244 raw words (4335166 effective words) took 1.3s, 3400996 effective words/s
[2023-02-07 12:46:28,635][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 78.35% examples, 3419587 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:28,898][gensim.models.word2vec][INFO] - EPOCH 29: training on 4367244 raw words (4335166 effective words) took 1.3s, 3424682 effective words/s
[2023-02-07 12:46:29,900][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 77.43% examples, 3388852 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:30,175][gensim.models.word2vec][INFO] - EPOCH 30: training on 4367244 raw words (4335166 effective words) took 1.3s, 3398433 effective words/s
[2023-02-07 12:46:31,177][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 77.65% examples, 3399075 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:31,451][gensim.models.word2vec][INFO] - EPOCH 31: training on 4367244 raw words (4335166 effective words) took 1.3s, 3401787 effective words/s
[2023-02-07 12:46:32,452][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 78.14% examples, 3419515 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:46:32,721][gensim.models.word2vec][INFO] - EPOCH 32: training on 4367244 raw words (4335166 effective words) took 1.3s, 3416109 effective words/s
[2023-02-07 12:46:32,722][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 144119052 raw words (143060478 effective words) took 43.3s, 3303675 effective words/s', 'datetime': '2023-02-07T12:46:32.722047', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:46:32.722 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:46:36,125][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124526-odtp50uo/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:46:36.125161', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:46:36,127][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:46:36,153][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124526-odtp50uo/files/../tmp/embedding_model.pt
2023-02-07 12:46:36.153 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:46:37.375 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:46:37.801 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:46:38.572 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0694089212043334, 'test_mae': 1.090604495200003, 'test_r2': 0.021045850534208266}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.8
wandb: percentage 0.54726
wandb:   test_mae 1.0906
wandb:   test_mse 2.06941
wandb:    test_r2 0.02105
wandb: 
wandb: üöÄ View run olive-sweep-12 at: https://wandb.ai/xiaoqiz/mof2vec/runs/odtp50uo
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_124526-odtp50uo/logs
wandb: Agent Starting Run: qbx281ts with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 936
wandb: 	model.gensim.alpha: 0.011981801518888734
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 41
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.46165112157754146
wandb: 	model.gensim.vector_size: 92
wandb: 	model.gensim.window: 12
wandb: 	model.sklearn.learning_rate: 0.003701200072117769
wandb: 	model.sklearn.max_depth: 26
wandb: 	model.sklearn.min_child_weight: 0.0033200780997570648
wandb: 	model.sklearn.n_estimators: 1621
wandb: 	model.sklearn.num_leaves: 408
wandb: 	model.sklearn.reg_alpha: 0.07312227241753345
wandb: 	model.sklearn.reg_lambda: 0.0582019101826951
wandb: 	model.sklearn.subsample: 0.3505693481765586
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124650-qbx281ts
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-13
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/qbx281ts
2023-02-07 12:46:58.086 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 12:46:58.086 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 936 for sweep.
2023-02-07 12:46:58.087 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.011981801518888734 for sweep.
2023-02-07 12:46:58.087 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:46:58.087 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 41 for sweep.
2023-02-07 12:46:58.087 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 12:46:58.088 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.46165112157754146 for sweep.
2023-02-07 12:46:58.088 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 92 for sweep.
2023-02-07 12:46:58.088 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 12 for sweep.
2023-02-07 12:46:58.088 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.003701200072117769 for sweep.
2023-02-07 12:46:58.088 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 26 for sweep.
2023-02-07 12:46:58.089 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0033200780997570648 for sweep.
2023-02-07 12:46:58.089 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1621 for sweep.
2023-02-07 12:46:58.089 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 408 for sweep.
2023-02-07 12:46:58.089 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.07312227241753345 for sweep.
2023-02-07 12:46:58.089 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.0582019101826951 for sweep.
2023-02-07 12:46:58.090 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3505693481765586 for sweep.
2023-02-07 12:46:58.090 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:46:58.094 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124650-qbx281ts/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 936, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 92, 'window': 12, 'min_count': 6, 'dm': 0, 'sample': 0.46165112157754146, 'workers': 4, 'alpha': 0.011981801518888734, 'epochs': 41}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1621, 'max_depth': 26, 'num_leaves': 408, 'reg_alpha': 0.07312227241753345, 'reg_lambda': 0.0582019101826951, 'subsample': 0.3505693481765586, 'min_child_weight': 0.0033200780997570648, 'n_jobs': 4, 'learning_rate': 0.003701200072117769}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|‚ñè         | 42/3257 [00:00<00:07, 417.49it/s]  3%|‚ñé         | 90/3257 [00:00<00:07, 441.88it/s]  4%|‚ñç         | 135/3257 [00:00<00:07, 437.04it/s]  6%|‚ñå         | 180/3257 [00:00<00:06, 440.11it/s]  7%|‚ñã         | 231/3257 [00:00<00:06, 464.88it/s]  9%|‚ñä         | 278/3257 [00:00<00:06, 461.22it/s] 10%|‚ñâ         | 325/3257 [00:00<00:06, 459.33it/s] 11%|‚ñà‚ñè        | 372/3257 [00:00<00:06, 461.03it/s] 13%|‚ñà‚ñé        | 419/3257 [00:00<00:06, 451.05it/s] 14%|‚ñà‚ñç        | 465/3257 [00:01<00:06, 432.10it/s] 16%|‚ñà‚ñå        | 511/3257 [00:01<00:06, 439.57it/s] 17%|‚ñà‚ñã        | 556/3257 [00:01<00:06, 439.71it/s] 18%|‚ñà‚ñä        | 601/3257 [00:01<00:06, 428.98it/s] 20%|‚ñà‚ñâ        | 645/3257 [00:01<00:06, 430.89it/s] 21%|‚ñà‚ñà        | 689/3257 [00:01<00:06, 426.31it/s] 23%|‚ñà‚ñà‚ñé       | 733/3257 [00:01<00:05, 429.89it/s] 24%|‚ñà‚ñà‚ñç       | 777/3257 [00:01<00:05, 426.79it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:01<00:05, 429.38it/s] 27%|‚ñà‚ñà‚ñã       | 866/3257 [00:01<00:05, 419.29it/s] 28%|‚ñà‚ñà‚ñä       | 908/3257 [00:02<00:05, 415.68it/s] 29%|‚ñà‚ñà‚ñâ       | 950/3257 [00:02<00:05, 410.63it/s] 30%|‚ñà‚ñà‚ñà       | 992/3257 [00:02<00:05, 407.70it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1033/3257 [00:02<00:05, 404.29it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1075/3257 [00:02<00:05, 408.59it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:02<00:05, 406.44it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1159/3257 [00:02<00:05, 409.38it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1200/3257 [00:02<00:07, 266.50it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1238/3257 [00:03<00:06, 288.90it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:03<00:06, 303.67it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1312/3257 [00:03<00:06, 319.88it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:03<00:05, 334.45it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1387/3257 [00:03<00:05, 342.35it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1429/3257 [00:03<00:05, 363.80it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1472/3257 [00:03<00:04, 379.24it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1515/3257 [00:03<00:04, 390.98it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1555/3257 [00:03<00:04, 377.04it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1594/3257 [00:04<00:04, 378.52it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:04<00:04, 378.89it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1673/3257 [00:04<00:04, 370.51it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1714/3257 [00:04<00:04, 381.45it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1753/3257 [00:04<00:04, 368.73it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1799/3257 [00:04<00:03, 392.22it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1843/3257 [00:04<00:03, 404.75it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1887/3257 [00:04<00:03, 412.78it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1933/3257 [00:04<00:03, 424.89it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1981/3257 [00:04<00:02, 440.46it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2029/3257 [00:05<00:02, 450.74it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2075/3257 [00:05<00:02, 432.85it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2119/3257 [00:05<00:02, 433.11it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2163/3257 [00:05<00:02, 420.58it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2208/3257 [00:05<00:02, 426.78it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2251/3257 [00:05<00:02, 424.21it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2294/3257 [00:05<00:02, 425.12it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2343/3257 [00:05<00:02, 443.56it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2393/3257 [00:05<00:01, 460.19it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2440/3257 [00:06<00:01, 435.89it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2484/3257 [00:06<00:01, 425.05it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2529/3257 [00:06<00:01, 430.22it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2573/3257 [00:06<00:01, 426.21it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2621/3257 [00:06<00:01, 441.26it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2666/3257 [00:06<00:01, 297.11it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2704/3257 [00:06<00:01, 314.95it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2751/3257 [00:06<00:01, 350.19it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2797/3257 [00:07<00:01, 376.31it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2839/3257 [00:07<00:01, 376.89it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2890/3257 [00:07<00:00, 411.16it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2934/3257 [00:07<00:00, 411.88it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2977/3257 [00:07<00:00, 409.12it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3019/3257 [00:07<00:00, 402.02it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3066/3257 [00:07<00:00, 419.76it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3112/3257 [00:07<00:00, 430.06it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3156/3257 [00:07<00:00, 432.30it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3201/3257 [00:07<00:00, 435.17it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3247/3257 [00:08<00:00, 441.54it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 403.34it/s]
2023-02-07 12:47:06.349 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:47:06,350][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d92,n5,mc6,s0.461651,t4>', 'datetime': '2023-02-07T12:47:06.350674', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:47:06,350][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:47:06,351][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:47:06,483][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 12:47:06,484][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:47:06,485][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 585 unique words (63.31% of original 924, drops 339)', 'datetime': '2023-02-07T12:47:06.485875', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:47:06,486][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 1454759 word corpus (99.93% of original 1455748, drops 989)', 'datetime': '2023-02-07T12:47:06.486936', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:47:06,489][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 12:47:06,489][gensim.models.word2vec][INFO] - sample=0.461651 downsamples 0 most-common words
[2023-02-07 12:47:06,489][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1454759 word corpus (100.0%% of prior 1454759)', 'datetime': '2023-02-07T12:47:06.489276', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:47:06,492][gensim.models.word2vec][INFO] - estimated required memory for 585 words and 92 dimensions: 2573036 bytes
[2023-02-07 12:47:06,493][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:47:06,494][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 585 vocabulary and 92 features, using sg=1 hs=0 sample=0.46165112157754146 negative=5 window=12 shrink_windows=True', 'datetime': '2023-02-07T12:47:06.494682', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:47:06,951][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458016 effective words) took 0.5s, 3200580 effective words/s
[2023-02-07 12:47:07,334][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458016 effective words) took 0.4s, 3819286 effective words/s
[2023-02-07 12:47:07,717][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458016 effective words) took 0.4s, 3822654 effective words/s
[2023-02-07 12:47:08,094][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458016 effective words) took 0.4s, 3877542 effective words/s
[2023-02-07 12:47:08,459][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458016 effective words) took 0.4s, 4012112 effective words/s
[2023-02-07 12:47:08,818][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458016 effective words) took 0.4s, 4086166 effective words/s
[2023-02-07 12:47:09,176][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458016 effective words) took 0.4s, 4080889 effective words/s
[2023-02-07 12:47:09,540][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458016 effective words) took 0.4s, 4014427 effective words/s
[2023-02-07 12:47:09,903][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458016 effective words) took 0.4s, 4035189 effective words/s
[2023-02-07 12:47:10,264][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458016 effective words) took 0.4s, 4053759 effective words/s
[2023-02-07 12:47:10,625][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458016 effective words) took 0.4s, 4053998 effective words/s
[2023-02-07 12:47:10,980][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458016 effective words) took 0.4s, 4123461 effective words/s
[2023-02-07 12:47:11,335][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458016 effective words) took 0.4s, 4111132 effective words/s
[2023-02-07 12:47:11,693][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458016 effective words) took 0.4s, 4095897 effective words/s
[2023-02-07 12:47:12,048][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458016 effective words) took 0.4s, 4120656 effective words/s
[2023-02-07 12:47:12,402][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458016 effective words) took 0.4s, 4135419 effective words/s
[2023-02-07 12:47:12,759][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458016 effective words) took 0.4s, 4098287 effective words/s
[2023-02-07 12:47:13,115][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458016 effective words) took 0.4s, 4118307 effective words/s
[2023-02-07 12:47:13,471][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458016 effective words) took 0.4s, 4099145 effective words/s
[2023-02-07 12:47:13,835][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458016 effective words) took 0.4s, 4021218 effective words/s
[2023-02-07 12:47:14,196][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458016 effective words) took 0.4s, 4051811 effective words/s
[2023-02-07 12:47:14,561][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458016 effective words) took 0.4s, 4015215 effective words/s
[2023-02-07 12:47:14,927][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458016 effective words) took 0.4s, 3994888 effective words/s
[2023-02-07 12:47:15,285][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458016 effective words) took 0.4s, 4085491 effective words/s
[2023-02-07 12:47:15,644][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458016 effective words) took 0.4s, 4076999 effective words/s
[2023-02-07 12:47:16,006][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458016 effective words) took 0.4s, 4048327 effective words/s
[2023-02-07 12:47:16,375][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458016 effective words) took 0.4s, 3960123 effective words/s
[2023-02-07 12:47:16,733][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458016 effective words) took 0.4s, 4092956 effective words/s
[2023-02-07 12:47:17,089][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458016 effective words) took 0.4s, 4111474 effective words/s
[2023-02-07 12:47:17,450][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458016 effective words) took 0.4s, 4051404 effective words/s
[2023-02-07 12:47:17,811][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458016 effective words) took 0.4s, 4053155 effective words/s
[2023-02-07 12:47:18,170][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458016 effective words) took 0.4s, 4082036 effective words/s
[2023-02-07 12:47:18,530][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458016 effective words) took 0.4s, 4059095 effective words/s
[2023-02-07 12:47:18,896][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458016 effective words) took 0.4s, 3994761 effective words/s
[2023-02-07 12:47:19,282][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458016 effective words) took 0.4s, 3793881 effective words/s
[2023-02-07 12:47:19,669][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458016 effective words) took 0.4s, 3776454 effective words/s
[2023-02-07 12:47:20,062][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458016 effective words) took 0.4s, 3725784 effective words/s
[2023-02-07 12:47:20,452][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458016 effective words) took 0.4s, 3746874 effective words/s
[2023-02-07 12:47:20,844][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458016 effective words) took 0.4s, 3735169 effective words/s
[2023-02-07 12:47:21,233][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458016 effective words) took 0.4s, 3771495 effective words/s
[2023-02-07 12:47:21,623][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458016 effective words) took 0.4s, 3747455 effective words/s
[2023-02-07 12:47:21,623][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 59685668 raw words (59778656 effective words) took 15.1s, 3951248 effective words/s', 'datetime': '2023-02-07T12:47:21.623917', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:47:21.624 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:47:22,784][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124650-qbx281ts/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:47:22.784468', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:47:22,785][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:47:22,790][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124650-qbx281ts/files/../tmp/embedding_model.pt
2023-02-07 12:47:22.791 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:47:23.810 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:47:24.225 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:47:24.898 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9284182274973893, 'test_mae': 1.0482401608038077, 'test_r2': 0.08774287847595896}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.030 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.030 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.78
wandb: percentage 0.36688
wandb:   test_mae 1.04824
wandb:   test_mse 1.92842
wandb:    test_r2 0.08774
wandb: 
wandb: üöÄ View run dainty-sweep-13 at: https://wandb.ai/xiaoqiz/mof2vec/runs/qbx281ts
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_124650-qbx281ts/logs
wandb: Agent Starting Run: dzkmgxnm with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 814
wandb: 	model.gensim.alpha: 0.2473949042841755
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 56
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.250599424887094
wandb: 	model.gensim.vector_size: 19
wandb: 	model.gensim.window: 13
wandb: 	model.sklearn.learning_rate: 0.02322517703901367
wandb: 	model.sklearn.max_depth: 8
wandb: 	model.sklearn.min_child_weight: 0.05836691527890642
wandb: 	model.sklearn.n_estimators: 2728
wandb: 	model.sklearn.num_leaves: 187
wandb: 	model.sklearn.reg_alpha: 0.12365351909236263
wandb: 	model.sklearn.reg_lambda: 0.13180664562660413
wandb: 	model.sklearn.subsample: 0.7657350950027684
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124733-dzkmgxnm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-sweep-14
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/dzkmgxnm
2023-02-07 12:47:41.413 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 12:47:41.414 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 814 for sweep.
2023-02-07 12:47:41.414 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.2473949042841755 for sweep.
2023-02-07 12:47:41.414 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:47:41.414 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 56 for sweep.
2023-02-07 12:47:41.415 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 12:47:41.415 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.250599424887094 for sweep.
2023-02-07 12:47:41.415 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 19 for sweep.
2023-02-07 12:47:41.415 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 13 for sweep.
2023-02-07 12:47:41.415 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.02322517703901367 for sweep.
2023-02-07 12:47:41.416 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 8 for sweep.
2023-02-07 12:47:41.416 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05836691527890642 for sweep.
2023-02-07 12:47:41.416 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2728 for sweep.
2023-02-07 12:47:41.416 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 187 for sweep.
2023-02-07 12:47:41.416 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.12365351909236263 for sweep.
2023-02-07 12:47:41.417 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.13180664562660413 for sweep.
2023-02-07 12:47:41.417 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.7657350950027684 for sweep.
2023-02-07 12:47:41.417 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:47:41.418 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124733-dzkmgxnm/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 814, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 19, 'window': 13, 'min_count': 5, 'dm': 0, 'sample': 0.250599424887094, 'workers': 4, 'alpha': 0.2473949042841755, 'epochs': 56}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2728, 'max_depth': 8, 'num_leaves': 187, 'reg_alpha': 0.12365351909236263, 'reg_lambda': 0.13180664562660413, 'subsample': 0.7657350950027684, 'min_child_weight': 0.05836691527890642, 'n_jobs': 4, 'learning_rate': 0.02322517703901367}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 40/3257 [00:00<00:08, 397.28it/s]  3%|‚ñé         | 84/3257 [00:00<00:07, 416.11it/s]  4%|‚ñç         | 126/3257 [00:00<00:07, 415.30it/s]  5%|‚ñå         | 172/3257 [00:00<00:07, 431.91it/s]  7%|‚ñã         | 220/3257 [00:00<00:06, 447.43it/s]  8%|‚ñä         | 267/3257 [00:00<00:06, 451.22it/s] 10%|‚ñâ         | 318/3257 [00:00<00:06, 469.19it/s] 11%|‚ñà         | 365/3257 [00:00<00:06, 465.27it/s] 13%|‚ñà‚ñé        | 412/3257 [00:00<00:06, 435.51it/s] 14%|‚ñà‚ñç        | 456/3257 [00:01<00:07, 399.03it/s] 15%|‚ñà‚ñå        | 500/3257 [00:01<00:06, 408.45it/s] 17%|‚ñà‚ñã        | 545/3257 [00:01<00:06, 419.76it/s] 18%|‚ñà‚ñä        | 588/3257 [00:01<00:06, 413.11it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:01<00:06, 430.79it/s] 21%|‚ñà‚ñà        | 681/3257 [00:01<00:05, 432.27it/s] 22%|‚ñà‚ñà‚ñè       | 725/3257 [00:01<00:05, 431.14it/s] 24%|‚ñà‚ñà‚ñé       | 772/3257 [00:01<00:05, 438.77it/s] 25%|‚ñà‚ñà‚ñå       | 818/3257 [00:01<00:05, 443.62it/s] 26%|‚ñà‚ñà‚ñã       | 863/3257 [00:01<00:05, 438.05it/s] 28%|‚ñà‚ñà‚ñä       | 909/3257 [00:02<00:05, 442.87it/s] 29%|‚ñà‚ñà‚ñâ       | 957/3257 [00:02<00:05, 451.73it/s] 31%|‚ñà‚ñà‚ñà       | 1003/3257 [00:02<00:07, 316.62it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1044/3257 [00:02<00:06, 337.34it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1090/3257 [00:02<00:05, 365.28it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1135/3257 [00:02<00:05, 386.59it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1179/3257 [00:02<00:05, 399.81it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1222/3257 [00:02<00:05, 401.37it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:03<00:04, 422.18it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1315/3257 [00:03<00:04, 422.38it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1362/3257 [00:03<00:04, 435.39it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1407/3257 [00:03<00:04, 437.34it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1460/3257 [00:03<00:03, 462.34it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:03<00:03, 475.80it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1559/3257 [00:03<00:03, 436.45it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1604/3257 [00:03<00:03, 438.36it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1649/3257 [00:03<00:03, 435.34it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1699/3257 [00:04<00:03, 452.63it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1745/3257 [00:04<00:03, 444.60it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1793/3257 [00:04<00:03, 452.18it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1839/3257 [00:04<00:03, 441.48it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1885/3257 [00:04<00:03, 443.45it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1930/3257 [00:04<00:03, 440.85it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1978/3257 [00:04<00:02, 451.63it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2024/3257 [00:04<00:02, 453.87it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2070/3257 [00:04<00:02, 422.35it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2113/3257 [00:04<00:02, 421.92it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2156/3257 [00:05<00:02, 395.80it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2201/3257 [00:05<00:02, 407.90it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2243/3257 [00:05<00:02, 396.44it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2283/3257 [00:05<00:02, 395.57it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2328/3257 [00:05<00:02, 409.07it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2370/3257 [00:05<00:03, 286.03it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2412/3257 [00:05<00:02, 315.62it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2452/3257 [00:05<00:02, 334.91it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2500/3257 [00:06<00:02, 371.73it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2543/3257 [00:06<00:01, 385.54it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2584/3257 [00:06<00:01, 382.56it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2629/3257 [00:06<00:01, 400.12it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2671/3257 [00:06<00:01, 398.12it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2712/3257 [00:06<00:01, 388.24it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2760/3257 [00:06<00:01, 412.22it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2809/3257 [00:06<00:01, 432.61it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2858/3257 [00:06<00:00, 448.16it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2906/3257 [00:06<00:00, 455.95it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2952/3257 [00:07<00:00, 450.33it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2998/3257 [00:07<00:00, 442.83it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3043/3257 [00:07<00:00, 441.10it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3088/3257 [00:07<00:00, 442.70it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3133/3257 [00:07<00:00, 444.74it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3178/3257 [00:07<00:00, 432.32it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3230/3257 [00:07<00:00, 456.76it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:07<00:00, 418.71it/s]
2023-02-07 12:47:49.372 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:47:49,373][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d19,n5,mc5,s0.250599,t4>', 'datetime': '2023-02-07T12:47:49.373654', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:47:49,374][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:47:49,374][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:47:49,499][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 12:47:49,499][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:47:49,500][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 599 unique words (64.83% of original 924, drops 325)', 'datetime': '2023-02-07T12:47:49.500825', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:47:49,501][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1454829 word corpus (99.94% of original 1455748, drops 919)', 'datetime': '2023-02-07T12:47:49.501701', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:47:49,503][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 12:47:49,503][gensim.models.word2vec][INFO] - sample=0.250599 downsamples 0 most-common words
[2023-02-07 12:47:49,503][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1454829 word corpus (100.0%% of prior 1454829)', 'datetime': '2023-02-07T12:47:49.503974', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:47:49,507][gensim.models.word2vec][INFO] - estimated required memory for 599 words and 19 dimensions: 1289480 bytes
[2023-02-07 12:47:49,508][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:47:49,508][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 599 vocabulary and 19 features, using sg=1 hs=0 sample=0.250599424887094 negative=5 window=13 shrink_windows=True', 'datetime': '2023-02-07T12:47:49.508844', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:47:49,747][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458086 effective words) took 0.2s, 6148302 effective words/s
[2023-02-07 12:47:49,976][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458086 effective words) took 0.2s, 6393851 effective words/s
[2023-02-07 12:47:50,202][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458086 effective words) took 0.2s, 6485809 effective words/s
[2023-02-07 12:47:50,434][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458086 effective words) took 0.2s, 6332265 effective words/s
[2023-02-07 12:47:50,665][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458086 effective words) took 0.2s, 6361550 effective words/s
[2023-02-07 12:47:50,892][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458086 effective words) took 0.2s, 6462769 effective words/s
[2023-02-07 12:47:51,117][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458086 effective words) took 0.2s, 6508038 effective words/s
[2023-02-07 12:47:51,340][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458086 effective words) took 0.2s, 6589927 effective words/s
[2023-02-07 12:47:51,564][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458086 effective words) took 0.2s, 6540364 effective words/s
[2023-02-07 12:47:51,785][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458086 effective words) took 0.2s, 6620094 effective words/s
[2023-02-07 12:47:52,016][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458086 effective words) took 0.2s, 6356261 effective words/s
[2023-02-07 12:47:52,247][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458086 effective words) took 0.2s, 6338106 effective words/s
[2023-02-07 12:47:52,475][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458086 effective words) took 0.2s, 6445450 effective words/s
[2023-02-07 12:47:52,703][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458086 effective words) took 0.2s, 6422998 effective words/s
[2023-02-07 12:47:52,936][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458086 effective words) took 0.2s, 6291238 effective words/s
[2023-02-07 12:47:53,162][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458086 effective words) took 0.2s, 6498223 effective words/s
[2023-02-07 12:47:53,386][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458086 effective words) took 0.2s, 6565870 effective words/s
[2023-02-07 12:47:53,609][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458086 effective words) took 0.2s, 6554669 effective words/s
[2023-02-07 12:47:53,833][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458086 effective words) took 0.2s, 6533742 effective words/s
[2023-02-07 12:47:54,061][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458086 effective words) took 0.2s, 6438163 effective words/s
[2023-02-07 12:47:54,288][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458086 effective words) took 0.2s, 6445367 effective words/s
[2023-02-07 12:47:54,518][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458086 effective words) took 0.2s, 6387025 effective words/s
[2023-02-07 12:47:54,748][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458086 effective words) took 0.2s, 6363525 effective words/s
[2023-02-07 12:47:54,979][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458086 effective words) took 0.2s, 6353903 effective words/s
[2023-02-07 12:47:55,213][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458086 effective words) took 0.2s, 6244658 effective words/s
[2023-02-07 12:47:55,446][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458086 effective words) took 0.2s, 6296531 effective words/s
[2023-02-07 12:47:55,678][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458086 effective words) took 0.2s, 6303352 effective words/s
[2023-02-07 12:47:55,911][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458086 effective words) took 0.2s, 6291212 effective words/s
[2023-02-07 12:47:56,143][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458086 effective words) took 0.2s, 6314962 effective words/s
[2023-02-07 12:47:56,374][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458086 effective words) took 0.2s, 6365194 effective words/s
[2023-02-07 12:47:56,607][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458086 effective words) took 0.2s, 6277909 effective words/s
[2023-02-07 12:47:56,839][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458086 effective words) took 0.2s, 6308400 effective words/s
[2023-02-07 12:47:57,081][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458086 effective words) took 0.2s, 6062107 effective words/s
[2023-02-07 12:47:57,324][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458086 effective words) took 0.2s, 6036066 effective words/s
[2023-02-07 12:47:57,569][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458086 effective words) took 0.2s, 5964797 effective words/s
[2023-02-07 12:47:57,817][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458086 effective words) took 0.2s, 5916497 effective words/s
[2023-02-07 12:47:58,064][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458086 effective words) took 0.2s, 5925849 effective words/s
[2023-02-07 12:47:58,309][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458086 effective words) took 0.2s, 5987563 effective words/s
[2023-02-07 12:47:58,548][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458086 effective words) took 0.2s, 6116461 effective words/s
[2023-02-07 12:47:58,791][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458086 effective words) took 0.2s, 6022758 effective words/s
[2023-02-07 12:47:59,032][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458086 effective words) took 0.2s, 6081812 effective words/s
[2023-02-07 12:47:59,274][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458086 effective words) took 0.2s, 6061424 effective words/s
[2023-02-07 12:47:59,523][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458086 effective words) took 0.2s, 5897767 effective words/s
[2023-02-07 12:47:59,772][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458086 effective words) took 0.2s, 5869073 effective words/s
[2023-02-07 12:48:00,025][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458086 effective words) took 0.3s, 5795967 effective words/s
[2023-02-07 12:48:00,277][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458086 effective words) took 0.3s, 5821056 effective words/s
[2023-02-07 12:48:00,531][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458086 effective words) took 0.3s, 5773165 effective words/s
[2023-02-07 12:48:00,786][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458086 effective words) took 0.3s, 5733838 effective words/s
[2023-02-07 12:48:01,043][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458086 effective words) took 0.3s, 5700897 effective words/s
[2023-02-07 12:48:01,299][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458086 effective words) took 0.3s, 5732238 effective words/s
[2023-02-07 12:48:01,553][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458086 effective words) took 0.3s, 5764322 effective words/s
[2023-02-07 12:48:01,815][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458086 effective words) took 0.3s, 5589525 effective words/s
[2023-02-07 12:48:02,071][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458086 effective words) took 0.3s, 5719060 effective words/s
[2023-02-07 12:48:02,331][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458086 effective words) took 0.3s, 5627994 effective words/s
[2023-02-07 12:48:02,596][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458086 effective words) took 0.3s, 5525845 effective words/s
[2023-02-07 12:48:02,863][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458086 effective words) took 0.3s, 5498103 effective words/s
[2023-02-07 12:48:02,863][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 81521888 raw words (81652816 effective words) took 13.4s, 6114301 effective words/s', 'datetime': '2023-02-07T12:48:02.863421', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:48:02.863 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:48:04,006][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124733-dzkmgxnm/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:48:04.006194', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:48:04,006][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:48:04,010][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124733-dzkmgxnm/files/../tmp/embedding_model.pt
2023-02-07 12:48:04.010 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:48:04.753 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:48:05.068 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:48:05.281 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.335783783074736, 'test_mae': 1.1603884065089507, 'test_r2': -0.10496538565474545}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.01
wandb: percentage 0.35173
wandb:   test_mae 1.16039
wandb:   test_mse 2.33578
wandb:    test_r2 -0.10497
wandb: 
wandb: üöÄ View run genial-sweep-14 at: https://wandb.ai/xiaoqiz/mof2vec/runs/dzkmgxnm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_124733-dzkmgxnm/logs
wandb: Agent Starting Run: uwsy9kmv with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 694
wandb: 	model.gensim.alpha: 0.002677708824561615
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 32
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.45642256304069007
wandb: 	model.gensim.vector_size: 23
wandb: 	model.gensim.window: 20
wandb: 	model.sklearn.learning_rate: 0.000995069712157944
wandb: 	model.sklearn.max_depth: 83
wandb: 	model.sklearn.min_child_weight: 0.017110491043188084
wandb: 	model.sklearn.n_estimators: 2455
wandb: 	model.sklearn.num_leaves: 468
wandb: 	model.sklearn.reg_alpha: 0.02199965158601938
wandb: 	model.sklearn.reg_lambda: 0.1661288666684398
wandb: 	model.sklearn.subsample: 0.43814509034193433
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124816-uwsy9kmv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/uwsy9kmv
2023-02-07 12:48:23.958 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 12:48:23.958 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 694 for sweep.
2023-02-07 12:48:23.959 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.002677708824561615 for sweep.
2023-02-07 12:48:23.959 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:48:23.959 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 32 for sweep.
2023-02-07 12:48:23.959 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 12:48:23.959 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.45642256304069007 for sweep.
2023-02-07 12:48:23.960 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 23 for sweep.
2023-02-07 12:48:23.960 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 20 for sweep.
2023-02-07 12:48:23.960 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.000995069712157944 for sweep.
2023-02-07 12:48:23.960 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 83 for sweep.
2023-02-07 12:48:23.961 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.017110491043188084 for sweep.
2023-02-07 12:48:23.961 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2455 for sweep.
2023-02-07 12:48:23.961 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 468 for sweep.
2023-02-07 12:48:23.961 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.02199965158601938 for sweep.
2023-02-07 12:48:23.961 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.1661288666684398 for sweep.
2023-02-07 12:48:23.962 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.43814509034193433 for sweep.
2023-02-07 12:48:23.962 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:48:23.966 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124816-uwsy9kmv/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 694, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 23, 'window': 20, 'min_count': 2, 'dm': 0, 'sample': 0.45642256304069007, 'workers': 4, 'alpha': 0.002677708824561615, 'epochs': 32}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2455, 'max_depth': 83, 'num_leaves': 468, 'reg_alpha': 0.02199965158601938, 'reg_lambda': 0.1661288666684398, 'subsample': 0.43814509034193433, 'min_child_weight': 0.017110491043188084, 'n_jobs': 4, 'learning_rate': 0.000995069712157944}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|‚ñè         | 41/3257 [00:00<00:07, 402.63it/s]  3%|‚ñé         | 88/3257 [00:00<00:07, 438.37it/s]  4%|‚ñç         | 132/3257 [00:00<00:07, 422.75it/s]  5%|‚ñå         | 176/3257 [00:00<00:07, 428.00it/s]  7%|‚ñã         | 224/3257 [00:00<00:06, 445.72it/s]  8%|‚ñä         | 269/3257 [00:00<00:06, 446.00it/s] 10%|‚ñâ         | 318/3257 [00:00<00:06, 457.70it/s] 11%|‚ñà         | 364/3257 [00:00<00:06, 456.93it/s] 13%|‚ñà‚ñé        | 410/3257 [00:00<00:06, 450.28it/s] 14%|‚ñà‚ñç        | 456/3257 [00:01<00:06, 427.75it/s] 15%|‚ñà‚ñå        | 502/3257 [00:01<00:06, 434.96it/s] 17%|‚ñà‚ñã        | 546/3257 [00:01<00:06, 430.73it/s] 18%|‚ñà‚ñä        | 590/3257 [00:01<00:08, 301.66it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:01<00:07, 337.72it/s] 21%|‚ñà‚ñà        | 678/3257 [00:01<00:07, 354.51it/s] 22%|‚ñà‚ñà‚ñè       | 721/3257 [00:01<00:06, 371.97it/s] 23%|‚ñà‚ñà‚ñé       | 764/3257 [00:01<00:06, 386.83it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:02<00:06, 392.83it/s] 26%|‚ñà‚ñà‚ñå       | 847/3257 [00:02<00:06, 394.55it/s] 27%|‚ñà‚ñà‚ñã       | 890/3257 [00:02<00:05, 403.91it/s] 29%|‚ñà‚ñà‚ñä       | 934/3257 [00:02<00:05, 412.95it/s] 30%|‚ñà‚ñà‚ñà       | 979/3257 [00:02<00:05, 419.00it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1022/3257 [00:02<00:05, 418.60it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1065/3257 [00:02<00:05, 410.48it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1107/3257 [00:02<00:05, 412.78it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1149/3257 [00:02<00:05, 407.98it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1190/3257 [00:02<00:05, 402.48it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1234/3257 [00:03<00:04, 411.44it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1276/3257 [00:03<00:04, 408.94it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1318/3257 [00:03<00:04, 409.52it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1361/3257 [00:03<00:04, 415.14it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1403/3257 [00:03<00:04, 414.48it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1452/3257 [00:03<00:04, 434.46it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1501/3257 [00:03<00:03, 449.93it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1547/3257 [00:03<00:03, 429.73it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1591/3257 [00:03<00:03, 431.45it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1635/3257 [00:03<00:03, 430.31it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1679/3257 [00:04<00:03, 425.41it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1723/3257 [00:04<00:03, 426.42it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1766/3257 [00:04<00:03, 417.17it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1808/3257 [00:04<00:03, 417.96it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1852/3257 [00:04<00:03, 424.39it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1897/3257 [00:04<00:03, 431.63it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1945/3257 [00:04<00:02, 444.15it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1990/3257 [00:04<00:03, 327.07it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2038/3257 [00:05<00:03, 361.45it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2081/3257 [00:05<00:03, 377.21it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2122/3257 [00:05<00:02, 384.83it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:05<00:02, 405.72it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2215/3257 [00:05<00:02, 420.00it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2260/3257 [00:05<00:02, 428.00it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2307/3257 [00:05<00:02, 438.37it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2362/3257 [00:05<00:01, 470.01it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2410/3257 [00:05<00:01, 469.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2458/3257 [00:05<00:01, 464.77it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2509/3257 [00:06<00:01, 477.80it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2558/3257 [00:06<00:01, 475.53it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2606/3257 [00:06<00:01, 464.29it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2656/3257 [00:06<00:01, 472.21it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2704/3257 [00:06<00:01, 458.10it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2754/3257 [00:06<00:01, 469.18it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2803/3257 [00:06<00:00, 473.84it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2851/3257 [00:06<00:00, 463.61it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2900/3257 [00:06<00:00, 469.46it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2948/3257 [00:06<00:00, 465.57it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2995/3257 [00:07<00:00, 464.21it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3047/3257 [00:07<00:00, 478.28it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3099/3257 [00:07<00:00, 489.21it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3148/3257 [00:07<00:00, 484.79it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3197/3257 [00:07<00:00, 476.77it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3247/3257 [00:07<00:00, 483.15it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:07<00:00, 427.68it/s]
2023-02-07 12:48:31.759 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:48:31,760][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d23,n5,mc2,s0.456423,t4>', 'datetime': '2023-02-07T12:48:31.760600', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:48:31,760][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:48:31,761][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:48:31,887][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 12:48:31,888][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:48:31,890][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 881 unique words (95.35% of original 924, drops 43)', 'datetime': '2023-02-07T12:48:31.889994', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:48:31,890][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 1455705 word corpus (100.00% of original 1455748, drops 43)', 'datetime': '2023-02-07T12:48:31.890141', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:48:31,892][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 12:48:31,892][gensim.models.word2vec][INFO] - sample=0.456423 downsamples 0 most-common words
[2023-02-07 12:48:31,892][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455705 word corpus (100.0%% of prior 1455705)', 'datetime': '2023-02-07T12:48:31.892932', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:48:31,897][gensim.models.word2vec][INFO] - estimated required memory for 881 words and 23 dimensions: 1553648 bytes
[2023-02-07 12:48:31,897][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:48:31,898][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 881 vocabulary and 23 features, using sg=1 hs=0 sample=0.45642256304069007 negative=5 window=20 shrink_windows=True', 'datetime': '2023-02-07T12:48:31.898448', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:48:32,305][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458962 effective words) took 0.4s, 3598626 effective words/s
[2023-02-07 12:48:32,644][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458962 effective words) took 0.3s, 4320086 effective words/s
[2023-02-07 12:48:32,965][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458962 effective words) took 0.3s, 4549617 effective words/s
[2023-02-07 12:48:33,285][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458962 effective words) took 0.3s, 4585468 effective words/s
[2023-02-07 12:48:33,599][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458962 effective words) took 0.3s, 4657175 effective words/s
[2023-02-07 12:48:33,914][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458962 effective words) took 0.3s, 4650192 effective words/s
[2023-02-07 12:48:34,233][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458962 effective words) took 0.3s, 4579766 effective words/s
[2023-02-07 12:48:34,549][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458962 effective words) took 0.3s, 4637529 effective words/s
[2023-02-07 12:48:34,863][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458962 effective words) took 0.3s, 4659635 effective words/s
[2023-02-07 12:48:35,180][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458962 effective words) took 0.3s, 4624986 effective words/s
[2023-02-07 12:48:35,493][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458962 effective words) took 0.3s, 4677692 effective words/s
[2023-02-07 12:48:35,808][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458962 effective words) took 0.3s, 4657069 effective words/s
[2023-02-07 12:48:36,122][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458962 effective words) took 0.3s, 4670344 effective words/s
[2023-02-07 12:48:36,435][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458962 effective words) took 0.3s, 4669140 effective words/s
[2023-02-07 12:48:36,748][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458962 effective words) took 0.3s, 4677747 effective words/s
[2023-02-07 12:48:37,060][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458962 effective words) took 0.3s, 4694974 effective words/s
[2023-02-07 12:48:37,373][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458962 effective words) took 0.3s, 4671501 effective words/s
[2023-02-07 12:48:37,687][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458962 effective words) took 0.3s, 4675385 effective words/s
[2023-02-07 12:48:37,999][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458962 effective words) took 0.3s, 4684610 effective words/s
[2023-02-07 12:48:38,311][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458962 effective words) took 0.3s, 4689267 effective words/s
[2023-02-07 12:48:38,622][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458962 effective words) took 0.3s, 4707080 effective words/s
[2023-02-07 12:48:38,934][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458962 effective words) took 0.3s, 4694663 effective words/s
[2023-02-07 12:48:39,251][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458962 effective words) took 0.3s, 4615887 effective words/s
[2023-02-07 12:48:39,571][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458962 effective words) took 0.3s, 4573260 effective words/s
[2023-02-07 12:48:39,892][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458962 effective words) took 0.3s, 4568078 effective words/s
[2023-02-07 12:48:40,209][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458962 effective words) took 0.3s, 4615847 effective words/s
[2023-02-07 12:48:40,533][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458962 effective words) took 0.3s, 4527525 effective words/s
[2023-02-07 12:48:40,845][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458962 effective words) took 0.3s, 4698863 effective words/s
[2023-02-07 12:48:41,170][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458962 effective words) took 0.3s, 4502236 effective words/s
[2023-02-07 12:48:41,482][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458962 effective words) took 0.3s, 4694729 effective words/s
[2023-02-07 12:48:41,797][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458962 effective words) took 0.3s, 4647567 effective words/s
[2023-02-07 12:48:42,114][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458962 effective words) took 0.3s, 4622001 effective words/s
[2023-02-07 12:48:42,114][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 46583936 raw words (46686784 effective words) took 10.2s, 4569959 effective words/s', 'datetime': '2023-02-07T12:48:42.114611', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:48:42.114 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:48:42,978][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124816-uwsy9kmv/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:48:42.978889', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:48:42,980][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:48:42,982][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124816-uwsy9kmv/files/../tmp/embedding_model.pt
2023-02-07 12:48:42.982 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:48:43.761 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:48:44.097 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:48:44.306 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.041151509053987, 'test_mae': 1.0715360988839957, 'test_r2': 0.03441329598894605}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.46
wandb: percentage 0.04654
wandb:   test_mae 1.07154
wandb:   test_mse 2.04115
wandb:    test_r2 0.03441
wandb: 
wandb: üöÄ View run misty-sweep-15 at: https://wandb.ai/xiaoqiz/mof2vec/runs/uwsy9kmv
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_124816-uwsy9kmv/logs
wandb: Agent Starting Run: sxr0udux with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 323
wandb: 	model.gensim.alpha: 0.001082273285724852
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.epochs: 52
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.5351733507215986
wandb: 	model.gensim.vector_size: 70
wandb: 	model.gensim.window: 14
wandb: 	model.sklearn.learning_rate: 0.413003912568918
wandb: 	model.sklearn.max_depth: 7
wandb: 	model.sklearn.min_child_weight: 0.025494303763663316
wandb: 	model.sklearn.n_estimators: 1321
wandb: 	model.sklearn.num_leaves: 423
wandb: 	model.sklearn.reg_alpha: 0.007719300147996295
wandb: 	model.sklearn.reg_lambda: 0.013947286843146622
wandb: 	model.sklearn.subsample: 0.20796973187999976
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124856-sxr0udux
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-16
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/sxr0udux
2023-02-07 12:49:03.378 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 12:49:03.378 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 323 for sweep.
2023-02-07 12:49:03.379 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.001082273285724852 for sweep.
2023-02-07 12:49:03.379 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 12:49:03.379 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 52 for sweep.
2023-02-07 12:49:03.379 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 12:49:03.380 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5351733507215986 for sweep.
2023-02-07 12:49:03.380 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 70 for sweep.
2023-02-07 12:49:03.380 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 14 for sweep.
2023-02-07 12:49:03.381 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.413003912568918 for sweep.
2023-02-07 12:49:03.381 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 7 for sweep.
2023-02-07 12:49:03.381 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.025494303763663316 for sweep.
2023-02-07 12:49:03.381 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1321 for sweep.
2023-02-07 12:49:03.381 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 423 for sweep.
2023-02-07 12:49:03.382 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.007719300147996295 for sweep.
2023-02-07 12:49:03.382 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.013947286843146622 for sweep.
2023-02-07 12:49:03.382 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.20796973187999976 for sweep.
2023-02-07 12:49:03.382 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:49:03.387 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124856-sxr0udux/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 323, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 70, 'window': 14, 'min_count': 6, 'dm': 1, 'sample': 0.5351733507215986, 'workers': 4, 'alpha': 0.001082273285724852, 'epochs': 52}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1321, 'max_depth': 7, 'num_leaves': 423, 'reg_alpha': 0.007719300147996295, 'reg_lambda': 0.013947286843146622, 'subsample': 0.20796973187999976, 'min_child_weight': 0.025494303763663316, 'n_jobs': 4, 'learning_rate': 0.413003912568918}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 32/3257 [00:00<00:10, 314.85it/s]  2%|‚ñè         | 66/3257 [00:00<00:09, 326.75it/s]  3%|‚ñé         | 99/3257 [00:00<00:15, 197.59it/s]  4%|‚ñç         | 132/3257 [00:00<00:13, 233.68it/s]  5%|‚ñå         | 166/3257 [00:00<00:11, 263.33it/s]  6%|‚ñå         | 201/3257 [00:00<00:10, 283.35it/s]  7%|‚ñã         | 238/3257 [00:00<00:09, 308.33it/s]  8%|‚ñä         | 273/3257 [00:00<00:09, 320.17it/s] 10%|‚ñâ         | 312/3257 [00:01<00:08, 335.32it/s] 11%|‚ñà         | 347/3257 [00:01<00:08, 334.01it/s] 12%|‚ñà‚ñè        | 382/3257 [00:01<00:08, 328.65it/s] 13%|‚ñà‚ñé        | 417/3257 [00:01<00:08, 332.68it/s] 14%|‚ñà‚ñç        | 451/3257 [00:01<00:09, 305.01it/s] 15%|‚ñà‚ñç        | 485/3257 [00:01<00:08, 313.03it/s] 16%|‚ñà‚ñå        | 522/3257 [00:01<00:08, 326.70it/s] 17%|‚ñà‚ñã        | 557/3257 [00:01<00:08, 331.22it/s] 18%|‚ñà‚ñä        | 591/3257 [00:01<00:08, 312.81it/s] 19%|‚ñà‚ñâ        | 625/3257 [00:02<00:08, 319.62it/s] 20%|‚ñà‚ñà        | 658/3257 [00:02<00:08, 315.64it/s] 21%|‚ñà‚ñà        | 691/3257 [00:02<00:08, 318.88it/s] 22%|‚ñà‚ñà‚ñè       | 724/3257 [00:02<00:07, 317.83it/s] 23%|‚ñà‚ñà‚ñé       | 756/3257 [00:02<00:07, 317.69it/s] 24%|‚ñà‚ñà‚ñç       | 788/3257 [00:02<00:07, 316.39it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:02<00:07, 318.76it/s] 26%|‚ñà‚ñà‚ñã       | 855/3257 [00:02<00:07, 311.59it/s] 27%|‚ñà‚ñà‚ñã       | 888/3257 [00:02<00:07, 315.68it/s] 28%|‚ñà‚ñà‚ñä       | 923/3257 [00:02<00:07, 325.63it/s] 29%|‚ñà‚ñà‚ñâ       | 957/3257 [00:03<00:07, 328.09it/s] 30%|‚ñà‚ñà‚ñà       | 990/3257 [00:03<00:07, 323.58it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1023/3257 [00:03<00:06, 325.39it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1056/3257 [00:03<00:07, 309.75it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1090/3257 [00:03<00:06, 315.92it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1122/3257 [00:03<00:06, 315.69it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1154/3257 [00:03<00:06, 313.21it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1186/3257 [00:03<00:06, 310.50it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1218/3257 [00:03<00:06, 299.18it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1255/3257 [00:04<00:09, 216.75it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1284/3257 [00:04<00:08, 230.29it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1319/3257 [00:04<00:07, 257.16it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1351/3257 [00:04<00:07, 272.11it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1382/3257 [00:04<00:06, 281.13it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1422/3257 [00:04<00:05, 310.87it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1459/3257 [00:04<00:05, 326.26it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1494/3257 [00:04<00:05, 332.85it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1529/3257 [00:05<00:05, 326.63it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1563/3257 [00:05<00:05, 320.44it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1596/3257 [00:05<00:05, 321.60it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1632/3257 [00:05<00:04, 331.54it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1666/3257 [00:05<00:05, 310.32it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1699/3257 [00:05<00:04, 314.50it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:05<00:05, 304.16it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:05<00:04, 306.14it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1798/3257 [00:05<00:04, 318.30it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1831/3257 [00:05<00:04, 312.36it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:06<00:04, 327.51it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1904/3257 [00:06<00:04, 331.50it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1939/3257 [00:06<00:03, 336.23it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:06<00:03, 347.62it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2012/3257 [00:06<00:03, 345.74it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2047/3257 [00:06<00:03, 338.04it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2081/3257 [00:06<00:03, 333.93it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2115/3257 [00:06<00:03, 333.05it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2149/3257 [00:06<00:03, 306.82it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2183/3257 [00:07<00:03, 315.03it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2218/3257 [00:07<00:03, 323.30it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2251/3257 [00:07<00:03, 321.28it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2284/3257 [00:07<00:03, 319.40it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2319/3257 [00:07<00:02, 327.76it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:07<00:02, 348.54it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2398/3257 [00:07<00:02, 356.65it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2434/3257 [00:07<00:02, 334.65it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2471/3257 [00:07<00:02, 343.86it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2507/3257 [00:08<00:02, 344.16it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2542/3257 [00:08<00:02, 335.92it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2576/3257 [00:08<00:02, 317.41it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2612/3257 [00:08<00:01, 328.58it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2646/3257 [00:08<00:02, 218.66it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2678/3257 [00:08<00:02, 239.83it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2707/3257 [00:08<00:02, 248.56it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2742/3257 [00:08<00:01, 272.04it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2772/3257 [00:09<00:01, 279.20it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2807/3257 [00:09<00:01, 297.67it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2839/3257 [00:09<00:01, 292.03it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:09<00:01, 325.66it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2915/3257 [00:09<00:01, 321.48it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2948/3257 [00:09<00:00, 315.22it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2981/3257 [00:09<00:00, 310.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3016/3257 [00:09<00:00, 319.69it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3054/3257 [00:09<00:00, 335.79it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3090/3257 [00:09<00:00, 342.46it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3129/3257 [00:10<00:00, 353.22it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3165/3257 [00:10<00:00, 336.41it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3199/3257 [00:10<00:00, 336.59it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3233/3257 [00:10<00:00, 335.65it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:10<00:00, 311.06it/s]
2023-02-07 12:49:14.123 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:49:14,124][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d70,n5,w14,mc6,s0.535173,t4>', 'datetime': '2023-02-07T12:49:14.124909', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:49:14,125][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:49:14,125][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:49:14,322][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 12:49:14,322][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:49:14,326][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 1604 unique words (56.90% of original 2819, drops 1215)', 'datetime': '2023-02-07T12:49:14.326772', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:49:14,327][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 2180389 word corpus (99.85% of original 2183622, drops 3233)', 'datetime': '2023-02-07T12:49:14.327084', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:49:14,332][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 12:49:14,332][gensim.models.word2vec][INFO] - sample=0.535173 downsamples 0 most-common words
[2023-02-07 12:49:14,334][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2180389 word corpus (100.0%% of prior 2180389)', 'datetime': '2023-02-07T12:49:14.334195', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:49:14,343][gensim.models.word2vec][INFO] - estimated required memory for 1604 words and 70 dimensions: 3263600 bytes
[2023-02-07 12:49:14,344][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:49:14,346][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 1604 vocabulary and 70 features, using sg=0 hs=0 sample=0.5351733507215986 negative=5 window=14 shrink_windows=True', 'datetime': '2023-02-07T12:49:14.346226', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:49:15,351][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 86.83% examples, 1907358 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:15,492][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2183646 effective words) took 1.1s, 1908607 effective words/s
[2023-02-07 12:49:16,496][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 88.82% examples, 1948403 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:16,599][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2183646 effective words) took 1.1s, 1976066 effective words/s
[2023-02-07 12:49:17,603][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 97.14% examples, 2118589 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:17,624][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2183646 effective words) took 1.0s, 2132523 effective words/s
[2023-02-07 12:49:18,627][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 97.51% examples, 2128295 words/s, in_qsize 6, out_qsize 0
[2023-02-07 12:49:18,646][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2183646 effective words) took 1.0s, 2138164 effective words/s
[2023-02-07 12:49:19,654][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 97.14% examples, 2111044 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:19,673][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2183646 effective words) took 1.0s, 2128975 effective words/s
[2023-02-07 12:49:20,681][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 95.15% examples, 2070060 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:20,718][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2183646 effective words) took 1.0s, 2091063 effective words/s
[2023-02-07 12:49:21,720][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 87.23% examples, 1921968 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:21,851][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2183646 effective words) took 1.1s, 1929729 effective words/s
[2023-02-07 12:49:22,853][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 86.12% examples, 1894109 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:23,003][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2183646 effective words) took 1.2s, 1898252 effective words/s
[2023-02-07 12:49:24,006][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 87.87% examples, 1930382 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:24,137][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2183646 effective words) took 1.1s, 1929527 effective words/s
[2023-02-07 12:49:25,148][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 88.58% examples, 1923285 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:49:25,264][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2183646 effective words) took 1.1s, 1939338 effective words/s
[2023-02-07 12:49:26,266][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 90.24% examples, 1981549 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:26,362][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2183646 effective words) took 1.1s, 1991771 effective words/s
[2023-02-07 12:49:27,365][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 86.46% examples, 1901547 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:27,505][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2183646 effective words) took 1.1s, 1912054 effective words/s
[2023-02-07 12:49:28,508][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 91.65% examples, 2015914 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:28,587][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2183646 effective words) took 1.1s, 2020751 effective words/s
[2023-02-07 12:49:29,591][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 87.87% examples, 1928145 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:29,719][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2183646 effective words) took 1.1s, 1932340 effective words/s
[2023-02-07 12:49:30,727][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 87.23% examples, 1911489 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:30,853][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2183646 effective words) took 1.1s, 1928854 effective words/s
[2023-02-07 12:49:31,865][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 91.65% examples, 1994756 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:31,937][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2183646 effective words) took 1.1s, 2016761 effective words/s
[2023-02-07 12:49:32,941][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 90.51% examples, 1986029 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:33,026][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2183646 effective words) took 1.1s, 2007571 effective words/s
[2023-02-07 12:49:34,030][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 90.14% examples, 1977600 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:34,130][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2183646 effective words) took 1.1s, 1981293 effective words/s
[2023-02-07 12:49:35,138][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 90.82% examples, 1986000 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:49:35,225][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2183646 effective words) took 1.1s, 1997279 effective words/s
[2023-02-07 12:49:36,231][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 88.82% examples, 1943388 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:36,344][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2183646 effective words) took 1.1s, 1954407 effective words/s
[2023-02-07 12:49:37,357][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 90.82% examples, 1974596 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:37,444][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2183646 effective words) took 1.1s, 1987051 effective words/s
[2023-02-07 12:49:38,450][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 92.63% examples, 2028454 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:38,517][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2183646 effective words) took 1.1s, 2038322 effective words/s
[2023-02-07 12:49:39,520][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 92.20% examples, 2023338 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:39,592][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2183646 effective words) took 1.1s, 2034075 effective words/s
[2023-02-07 12:49:40,606][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 89.62% examples, 1949465 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:40,700][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2183646 effective words) took 1.1s, 1973919 effective words/s
[2023-02-07 12:49:41,704][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 90.82% examples, 1995485 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:49:41,793][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2183646 effective words) took 1.1s, 2003551 effective words/s
[2023-02-07 12:49:42,797][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 92.63% examples, 2029584 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:42,862][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2183646 effective words) took 1.1s, 2044253 effective words/s
[2023-02-07 12:49:43,866][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 90.24% examples, 1975941 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:43,963][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2183646 effective words) took 1.1s, 1987099 effective words/s
[2023-02-07 12:49:44,969][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 92.20% examples, 2017264 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:45,038][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2183646 effective words) took 1.1s, 2032996 effective words/s
[2023-02-07 12:49:46,041][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 89.87% examples, 1969730 words/s, in_qsize 8, out_qsize 0
[2023-02-07 12:49:46,142][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2183646 effective words) took 1.1s, 1982106 effective words/s
[2023-02-07 12:49:47,148][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 89.87% examples, 1962507 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:47,243][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2183646 effective words) took 1.1s, 1985143 effective words/s
[2023-02-07 12:49:48,248][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 91.65% examples, 2012451 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:48,324][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2183646 effective words) took 1.1s, 2023439 effective words/s
[2023-02-07 12:49:49,333][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 90.51% examples, 1977387 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:49,422][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2183646 effective words) took 1.1s, 1991125 effective words/s
[2023-02-07 12:49:50,429][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 90.14% examples, 1971643 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:50,524][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2183646 effective words) took 1.1s, 1985067 effective words/s
[2023-02-07 12:49:51,528][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 92.20% examples, 2023333 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:51,596][gensim.models.word2vec][INFO] - EPOCH 33: training on 2183622 raw words (2183646 effective words) took 1.1s, 2041068 effective words/s
[2023-02-07 12:49:52,603][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 91.03% examples, 1991450 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:52,688][gensim.models.word2vec][INFO] - EPOCH 34: training on 2183622 raw words (2183646 effective words) took 1.1s, 2003579 effective words/s
[2023-02-07 12:49:53,695][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 88.82% examples, 1943872 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:53,806][gensim.models.word2vec][INFO] - EPOCH 35: training on 2183622 raw words (2183646 effective words) took 1.1s, 1957670 effective words/s
[2023-02-07 12:49:54,814][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 90.51% examples, 1978023 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:54,903][gensim.models.word2vec][INFO] - EPOCH 36: training on 2183622 raw words (2183646 effective words) took 1.1s, 1993057 effective words/s
[2023-02-07 12:49:55,906][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 90.82% examples, 1997102 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:55,993][gensim.models.word2vec][INFO] - EPOCH 37: training on 2183622 raw words (2183646 effective words) took 1.1s, 2007062 effective words/s
[2023-02-07 12:49:56,995][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 89.25% examples, 1961136 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:57,106][gensim.models.word2vec][INFO] - EPOCH 38: training on 2183622 raw words (2183646 effective words) took 1.1s, 1965035 effective words/s
[2023-02-07 12:49:58,116][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 90.24% examples, 1965818 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:58,211][gensim.models.word2vec][INFO] - EPOCH 39: training on 2183622 raw words (2183646 effective words) took 1.1s, 1980005 effective words/s
[2023-02-07 12:49:59,216][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 93.21% examples, 2040600 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:49:59,278][gensim.models.word2vec][INFO] - EPOCH 40: training on 2183622 raw words (2183646 effective words) took 1.1s, 2051246 effective words/s
[2023-02-07 12:50:00,281][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 90.51% examples, 1989654 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:50:00,379][gensim.models.word2vec][INFO] - EPOCH 41: training on 2183622 raw words (2183646 effective words) took 1.1s, 1988160 effective words/s
[2023-02-07 12:50:01,388][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 90.14% examples, 1964796 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:50:01,482][gensim.models.word2vec][INFO] - EPOCH 42: training on 2183622 raw words (2183646 effective words) took 1.1s, 1981278 effective words/s
[2023-02-07 12:50:02,490][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 90.82% examples, 1985289 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:50:02,579][gensim.models.word2vec][INFO] - EPOCH 43: training on 2183622 raw words (2183646 effective words) took 1.1s, 1993525 effective words/s
[2023-02-07 12:50:03,583][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 93.74% examples, 2051715 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:50:03,635][gensim.models.word2vec][INFO] - EPOCH 44: training on 2183622 raw words (2183646 effective words) took 1.1s, 2070637 effective words/s
[2023-02-07 12:50:04,638][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 100.00% examples, 2180450 words/s, in_qsize 0, out_qsize 1
[2023-02-07 12:50:04,638][gensim.models.word2vec][INFO] - EPOCH 45: training on 2183622 raw words (2183646 effective words) took 1.0s, 2179786 effective words/s
[2023-02-07 12:50:05,642][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 99.51% examples, 2170334 words/s, in_qsize 1, out_qsize 1
[2023-02-07 12:50:05,642][gensim.models.word2vec][INFO] - EPOCH 46: training on 2183622 raw words (2183646 effective words) took 1.0s, 2177122 effective words/s
[2023-02-07 12:50:06,637][gensim.models.word2vec][INFO] - EPOCH 47: training on 2183622 raw words (2183646 effective words) took 1.0s, 2196284 effective words/s
[2023-02-07 12:50:07,645][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 98.89% examples, 2152384 words/s, in_qsize 2, out_qsize 1
[2023-02-07 12:50:07,647][gensim.models.word2vec][INFO] - EPOCH 48: training on 2183622 raw words (2183646 effective words) took 1.0s, 2165116 effective words/s
[2023-02-07 12:50:08,654][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 98.59% examples, 2143810 words/s, in_qsize 3, out_qsize 1
[2023-02-07 12:50:08,657][gensim.models.word2vec][INFO] - EPOCH 49: training on 2183622 raw words (2183646 effective words) took 1.0s, 2163604 effective words/s
[2023-02-07 12:50:09,659][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 98.04% examples, 2140239 words/s, in_qsize 5, out_qsize 0
[2023-02-07 12:50:09,674][gensim.models.word2vec][INFO] - EPOCH 50: training on 2183622 raw words (2183646 effective words) took 1.0s, 2149491 effective words/s
[2023-02-07 12:50:10,678][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 96.68% examples, 2108667 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:50:10,707][gensim.models.word2vec][INFO] - EPOCH 51: training on 2183622 raw words (2183646 effective words) took 1.0s, 2115444 effective words/s
[2023-02-07 12:50:10,708][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 113548344 raw words (113549592 effective words) took 56.4s, 2014667 effective words/s', 'datetime': '2023-02-07T12:50:10.708007', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:50:10.708 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:50:13,398][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124856-sxr0udux/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:50:13.398451', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:50:13,399][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:50:13,405][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_124856-sxr0udux/files/../tmp/embedding_model.pt
2023-02-07 12:50:13.405 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:50:14.424 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:50:14.820 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:50:15.350 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.110303232146359, 'test_mae': 1.1141908950646597, 'test_r2': 0.0017003963921909948}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.17
wandb: percentage 0.431
wandb:   test_mae 1.11419
wandb:   test_mse 2.1103
wandb:    test_r2 0.0017
wandb: 
wandb: üöÄ View run twilight-sweep-16 at: https://wandb.ai/xiaoqiz/mof2vec/runs/sxr0udux
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_124856-sxr0udux/logs
wandb: Agent Starting Run: lxcw23ym with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 337
wandb: 	model.gensim.alpha: 0.002746779621944732
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 45
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.26147538125045333
wandb: 	model.gensim.vector_size: 50
wandb: 	model.gensim.window: 14
wandb: 	model.sklearn.learning_rate: 0.0566780374216806
wandb: 	model.sklearn.max_depth: 48
wandb: 	model.sklearn.min_child_weight: 0.011485628463109049
wandb: 	model.sklearn.n_estimators: 2685
wandb: 	model.sklearn.num_leaves: 492
wandb: 	model.sklearn.reg_alpha: 0.019713459398811507
wandb: 	model.sklearn.reg_lambda: 0.15686923813229456
wandb: 	model.sklearn.subsample: 0.34797375993570834
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125025-lxcw23ym
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-17
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/lxcw23ym
2023-02-07 12:50:32.685 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 12:50:32.686 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 337 for sweep.
2023-02-07 12:50:32.686 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.002746779621944732 for sweep.
2023-02-07 12:50:32.686 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:50:32.686 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 45 for sweep.
2023-02-07 12:50:32.686 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 12:50:32.687 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.26147538125045333 for sweep.
2023-02-07 12:50:32.687 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 50 for sweep.
2023-02-07 12:50:32.687 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 14 for sweep.
2023-02-07 12:50:32.687 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0566780374216806 for sweep.
2023-02-07 12:50:32.687 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 48 for sweep.
2023-02-07 12:50:32.688 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.011485628463109049 for sweep.
2023-02-07 12:50:32.688 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2685 for sweep.
2023-02-07 12:50:32.688 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 492 for sweep.
2023-02-07 12:50:32.688 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.019713459398811507 for sweep.
2023-02-07 12:50:32.688 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.15686923813229456 for sweep.
2023-02-07 12:50:32.689 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.34797375993570834 for sweep.
2023-02-07 12:50:32.689 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:50:32.694 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125025-lxcw23ym/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 337, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 50, 'window': 14, 'min_count': 2, 'dm': 0, 'sample': 0.26147538125045333, 'workers': 4, 'alpha': 0.002746779621944732, 'epochs': 45}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2685, 'max_depth': 48, 'num_leaves': 492, 'reg_alpha': 0.019713459398811507, 'reg_lambda': 0.15686923813229456, 'subsample': 0.34797375993570834, 'min_child_weight': 0.011485628463109049, 'n_jobs': 4, 'learning_rate': 0.0566780374216806}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 34/3257 [00:00<00:09, 335.65it/s]  2%|‚ñè         | 71/3257 [00:00<00:09, 352.81it/s]  3%|‚ñé         | 107/3257 [00:00<00:08, 350.96it/s]  5%|‚ñç         | 148/3257 [00:00<00:08, 371.20it/s]  6%|‚ñå         | 186/3257 [00:00<00:08, 365.10it/s]  7%|‚ñã         | 230/3257 [00:00<00:07, 384.32it/s]  8%|‚ñä         | 269/3257 [00:00<00:07, 378.15it/s] 10%|‚ñâ         | 312/3257 [00:00<00:07, 392.72it/s] 11%|‚ñà         | 352/3257 [00:00<00:07, 394.32it/s] 12%|‚ñà‚ñè        | 392/3257 [00:01<00:07, 378.12it/s] 13%|‚ñà‚ñé        | 430/3257 [00:01<00:07, 356.54it/s] 14%|‚ñà‚ñç        | 468/3257 [00:01<00:07, 362.04it/s] 16%|‚ñà‚ñå        | 505/3257 [00:01<00:07, 362.95it/s] 17%|‚ñà‚ñã        | 543/3257 [00:01<00:07, 367.01it/s] 18%|‚ñà‚ñä        | 580/3257 [00:01<00:07, 345.63it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:01<00:07, 350.80it/s] 20%|‚ñà‚ñà        | 656/3257 [00:01<00:07, 346.79it/s] 21%|‚ñà‚ñà        | 692/3257 [00:01<00:07, 348.64it/s] 22%|‚ñà‚ñà‚ñè       | 727/3257 [00:02<00:10, 230.79it/s] 23%|‚ñà‚ñà‚ñé       | 763/3257 [00:02<00:09, 257.40it/s] 25%|‚ñà‚ñà‚ñç       | 798/3257 [00:02<00:08, 278.47it/s] 25%|‚ñà‚ñà‚ñå       | 830/3257 [00:02<00:08, 288.28it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:02<00:08, 296.75it/s] 28%|‚ñà‚ñà‚ñä       | 900/3257 [00:02<00:07, 313.65it/s] 29%|‚ñà‚ñà‚ñä       | 935/3257 [00:02<00:07, 323.51it/s] 30%|‚ñà‚ñà‚ñâ       | 972/3257 [00:02<00:06, 336.67it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:03<00:06, 334.10it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1042/3257 [00:03<00:06, 330.31it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:03<00:06, 336.38it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:03<00:06, 342.76it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1152/3257 [00:03<00:06, 340.03it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1187/3257 [00:03<00:06, 338.86it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1222/3257 [00:03<00:06, 326.47it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1260/3257 [00:03<00:05, 339.85it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1295/3257 [00:03<00:05, 327.98it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1332/3257 [00:03<00:05, 338.54it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1368/3257 [00:04<00:05, 343.86it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1403/3257 [00:04<00:05, 345.58it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1443/3257 [00:04<00:05, 360.21it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1484/3257 [00:04<00:04, 371.08it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1522/3257 [00:04<00:04, 367.54it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1559/3257 [00:04<00:04, 356.50it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1597/3257 [00:04<00:04, 362.21it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:04<00:04, 358.06it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1670/3257 [00:04<00:04, 351.17it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1706/3257 [00:05<00:04, 350.12it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1742/3257 [00:05<00:04, 333.32it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1780/3257 [00:05<00:04, 345.16it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1815/3257 [00:05<00:04, 345.41it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1852/3257 [00:05<00:04, 349.31it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1889/3257 [00:05<00:03, 354.06it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1925/3257 [00:05<00:05, 228.44it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1971/3257 [00:05<00:04, 275.83it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2005/3257 [00:06<00:04, 290.57it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2040/3257 [00:06<00:03, 304.47it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2074/3257 [00:06<00:03, 313.55it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2108/3257 [00:06<00:03, 320.40it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2142/3257 [00:06<00:03, 319.28it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:06<00:03, 323.88it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2212/3257 [00:06<00:03, 322.61it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2247/3257 [00:06<00:03, 328.93it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2281/3257 [00:06<00:02, 330.12it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2321/3257 [00:06<00:02, 349.00it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2363/3257 [00:07<00:02, 368.36it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2403/3257 [00:07<00:02, 376.06it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2441/3257 [00:07<00:02, 354.68it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2480/3257 [00:07<00:02, 364.19it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2522/3257 [00:07<00:01, 378.26it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2561/3257 [00:07<00:01, 369.03it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2599/3257 [00:07<00:01, 357.00it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2643/3257 [00:07<00:01, 375.66it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2681/3257 [00:07<00:01, 371.29it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2719/3257 [00:08<00:01, 346.53it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:08<00:01, 362.15it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2801/3257 [00:08<00:01, 372.22it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2839/3257 [00:08<00:01, 355.05it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2885/3257 [00:08<00:00, 383.54it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2924/3257 [00:08<00:00, 366.61it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2962/3257 [00:08<00:00, 352.87it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2998/3257 [00:08<00:00, 353.74it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3037/3257 [00:08<00:00, 362.61it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:09<00:00, 378.99it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3121/3257 [00:09<00:00, 387.98it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3160/3257 [00:09<00:00, 375.31it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3198/3257 [00:09<00:00, 372.51it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3237/3257 [00:09<00:00, 374.67it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 343.33it/s]
2023-02-07 12:50:42.447 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:50:42,448][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d50,n5,mc2,s0.261475,t4>', 'datetime': '2023-02-07T12:50:42.448661', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:50:42,449][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:50:42,450][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:50:42,643][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 12:50:42,644][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:50:42,650][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 2542 unique words (90.17% of original 2819, drops 277)', 'datetime': '2023-02-07T12:50:42.650155', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:50:42,650][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 2183345 word corpus (99.99% of original 2183622, drops 277)', 'datetime': '2023-02-07T12:50:42.650425', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:50:42,659][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 12:50:42,660][gensim.models.word2vec][INFO] - sample=0.261475 downsamples 0 most-common words
[2023-02-07 12:50:42,660][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2183345 word corpus (100.0%% of prior 2183345)', 'datetime': '2023-02-07T12:50:42.660241', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:50:42,674][gensim.models.word2vec][INFO] - estimated required memory for 2542 words and 50 dimensions: 3590600 bytes
[2023-02-07 12:50:42,675][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:50:42,677][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2542 vocabulary and 50 features, using sg=1 hs=0 sample=0.26147538125045333 negative=5 window=14 shrink_windows=True', 'datetime': '2023-02-07T12:50:42.676999', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:50:43,373][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2186602 effective words) took 0.7s, 3145575 effective words/s
[2023-02-07 12:50:43,984][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2186602 effective words) took 0.6s, 3587884 effective words/s
[2023-02-07 12:50:44,580][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2186602 effective words) took 0.6s, 3680950 effective words/s
[2023-02-07 12:50:45,183][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2186602 effective words) took 0.6s, 3630959 effective words/s
[2023-02-07 12:50:45,790][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2186602 effective words) took 0.6s, 3609092 effective words/s
[2023-02-07 12:50:46,402][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2186602 effective words) took 0.6s, 3581306 effective words/s
[2023-02-07 12:50:47,013][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2186602 effective words) took 0.6s, 3585335 effective words/s
[2023-02-07 12:50:47,623][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2186602 effective words) took 0.6s, 3595059 effective words/s
[2023-02-07 12:50:48,221][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2186602 effective words) took 0.6s, 3667166 effective words/s
[2023-02-07 12:50:48,820][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2186602 effective words) took 0.6s, 3658658 effective words/s
[2023-02-07 12:50:49,414][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2186602 effective words) took 0.6s, 3694175 effective words/s
[2023-02-07 12:50:50,010][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2186602 effective words) took 0.6s, 3675916 effective words/s
[2023-02-07 12:50:50,610][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2186602 effective words) took 0.6s, 3655214 effective words/s
[2023-02-07 12:50:51,204][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2186602 effective words) took 0.6s, 3684494 effective words/s
[2023-02-07 12:50:51,796][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2186602 effective words) took 0.6s, 3707900 effective words/s
[2023-02-07 12:50:52,391][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2186602 effective words) took 0.6s, 3682417 effective words/s
[2023-02-07 12:50:52,981][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2186602 effective words) took 0.6s, 3716879 effective words/s
[2023-02-07 12:50:53,581][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2186602 effective words) took 0.6s, 3648161 effective words/s
[2023-02-07 12:50:54,174][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2186602 effective words) took 0.6s, 3700303 effective words/s
[2023-02-07 12:50:54,768][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2186602 effective words) took 0.6s, 3689066 effective words/s
[2023-02-07 12:50:55,369][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2186602 effective words) took 0.6s, 3643264 effective words/s
[2023-02-07 12:50:55,961][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2186602 effective words) took 0.6s, 3706916 effective words/s
[2023-02-07 12:50:56,549][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2186602 effective words) took 0.6s, 3723155 effective words/s
[2023-02-07 12:50:57,128][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2186602 effective words) took 0.6s, 3784028 effective words/s
[2023-02-07 12:50:57,699][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2186602 effective words) took 0.6s, 3842722 effective words/s
[2023-02-07 12:50:58,280][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2186602 effective words) took 0.6s, 3773217 effective words/s
[2023-02-07 12:50:58,864][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2186602 effective words) took 0.6s, 3747687 effective words/s
[2023-02-07 12:50:59,448][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2186602 effective words) took 0.6s, 3752448 effective words/s
[2023-02-07 12:51:00,024][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2186602 effective words) took 0.6s, 3804751 effective words/s
[2023-02-07 12:51:00,613][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2186602 effective words) took 0.6s, 3725412 effective words/s
[2023-02-07 12:51:01,194][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2186602 effective words) took 0.6s, 3768946 effective words/s
[2023-02-07 12:51:01,780][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2186602 effective words) took 0.6s, 3741121 effective words/s
[2023-02-07 12:51:02,360][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2186602 effective words) took 0.6s, 3778605 effective words/s
[2023-02-07 12:51:02,933][gensim.models.word2vec][INFO] - EPOCH 33: training on 2183622 raw words (2186602 effective words) took 0.6s, 3823735 effective words/s
[2023-02-07 12:51:03,512][gensim.models.word2vec][INFO] - EPOCH 34: training on 2183622 raw words (2186602 effective words) took 0.6s, 3783588 effective words/s
[2023-02-07 12:51:04,081][gensim.models.word2vec][INFO] - EPOCH 35: training on 2183622 raw words (2186602 effective words) took 0.6s, 3849266 effective words/s
[2023-02-07 12:51:04,649][gensim.models.word2vec][INFO] - EPOCH 36: training on 2183622 raw words (2186602 effective words) took 0.6s, 3862459 effective words/s
[2023-02-07 12:51:05,210][gensim.models.word2vec][INFO] - EPOCH 37: training on 2183622 raw words (2186602 effective words) took 0.6s, 3910031 effective words/s
[2023-02-07 12:51:05,781][gensim.models.word2vec][INFO] - EPOCH 38: training on 2183622 raw words (2186602 effective words) took 0.6s, 3836664 effective words/s
[2023-02-07 12:51:06,356][gensim.models.word2vec][INFO] - EPOCH 39: training on 2183622 raw words (2186602 effective words) took 0.6s, 3811328 effective words/s
[2023-02-07 12:51:06,930][gensim.models.word2vec][INFO] - EPOCH 40: training on 2183622 raw words (2186602 effective words) took 0.6s, 3820344 effective words/s
[2023-02-07 12:51:07,507][gensim.models.word2vec][INFO] - EPOCH 41: training on 2183622 raw words (2186602 effective words) took 0.6s, 3804517 effective words/s
[2023-02-07 12:51:08,089][gensim.models.word2vec][INFO] - EPOCH 42: training on 2183622 raw words (2186602 effective words) took 0.6s, 3763784 effective words/s
[2023-02-07 12:51:08,666][gensim.models.word2vec][INFO] - EPOCH 43: training on 2183622 raw words (2186602 effective words) took 0.6s, 3803507 effective words/s
[2023-02-07 12:51:09,242][gensim.models.word2vec][INFO] - EPOCH 44: training on 2183622 raw words (2186602 effective words) took 0.6s, 3805136 effective words/s
[2023-02-07 12:51:09,242][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98262990 raw words (98397090 effective words) took 26.6s, 3703949 effective words/s', 'datetime': '2023-02-07T12:51:09.242686', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:51:09.242 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:51:11,068][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125025-lxcw23ym/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:51:11.067934', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:51:11,068][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:51:11,076][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125025-lxcw23ym/files/../tmp/embedding_model.pt
2023-02-07 12:51:11.076 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:51:12.008 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:51:12.393 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:51:12.808 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.822323766433769, 'test_mae': 1.047706462362884, 'test_r2': 0.13793190193543237}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.89
wandb: percentage 0.09826
wandb:   test_mae 1.04771
wandb:   test_mse 1.82232
wandb:    test_r2 0.13793
wandb: 
wandb: üöÄ View run rare-sweep-17 at: https://wandb.ai/xiaoqiz/mof2vec/runs/lxcw23ym
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_125025-lxcw23ym/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: n4put40g with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 627
wandb: 	model.gensim.alpha: 0.0007130485671193477
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 25
wandb: 	model.gensim.min_count: 8
wandb: 	model.gensim.sample: 0.2612650547493335
wandb: 	model.gensim.vector_size: 122
wandb: 	model.gensim.window: 7
wandb: 	model.sklearn.learning_rate: 0.07551129949047553
wandb: 	model.sklearn.max_depth: 26
wandb: 	model.sklearn.min_child_weight: 0.011753795853038356
wandb: 	model.sklearn.n_estimators: 1951
wandb: 	model.sklearn.num_leaves: 440
wandb: 	model.sklearn.reg_alpha: 0.04624148087257892
wandb: 	model.sklearn.reg_lambda: 0.012637799873619205
wandb: 	model.sklearn.subsample: 0.44134552614648065
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125132-n4put40g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-18
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/n4put40g
2023-02-07 12:51:40.317 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 12:51:40.317 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 627 for sweep.
2023-02-07 12:51:40.317 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0007130485671193477 for sweep.
2023-02-07 12:51:40.318 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:51:40.318 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 25 for sweep.
2023-02-07 12:51:40.318 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 8 for sweep.
2023-02-07 12:51:40.318 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2612650547493335 for sweep.
2023-02-07 12:51:40.318 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 122 for sweep.
2023-02-07 12:51:40.319 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 7 for sweep.
2023-02-07 12:51:40.319 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.07551129949047553 for sweep.
2023-02-07 12:51:40.319 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 26 for sweep.
2023-02-07 12:51:40.319 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.011753795853038356 for sweep.
2023-02-07 12:51:40.319 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1951 for sweep.
2023-02-07 12:51:40.320 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 440 for sweep.
2023-02-07 12:51:40.320 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.04624148087257892 for sweep.
2023-02-07 12:51:40.320 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.012637799873619205 for sweep.
2023-02-07 12:51:40.320 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.44134552614648065 for sweep.
2023-02-07 12:51:40.321 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:51:40.325 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125132-n4put40g/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 627, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 122, 'window': 7, 'min_count': 8, 'dm': 0, 'sample': 0.2612650547493335, 'workers': 4, 'alpha': 0.0007130485671193477, 'epochs': 25}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1951, 'max_depth': 26, 'num_leaves': 440, 'reg_alpha': 0.04624148087257892, 'reg_lambda': 0.012637799873619205, 'subsample': 0.44134552614648065, 'min_child_weight': 0.011753795853038356, 'n_jobs': 4, 'learning_rate': 0.07551129949047553}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 4/3257 [00:00<02:04, 26.08it/s]  1%|‚ñè         | 42/3257 [00:00<00:16, 194.25it/s]  3%|‚ñé         | 82/3257 [00:00<00:11, 277.30it/s]  4%|‚ñé         | 117/3257 [00:00<00:10, 303.55it/s]  5%|‚ñç         | 158/3257 [00:00<00:09, 338.01it/s]  6%|‚ñå         | 197/3257 [00:00<00:08, 349.59it/s]  7%|‚ñã         | 237/3257 [00:00<00:08, 362.20it/s]  9%|‚ñä         | 282/3257 [00:00<00:07, 385.73it/s] 10%|‚ñâ         | 322/3257 [00:00<00:07, 388.98it/s] 11%|‚ñà         | 362/3257 [00:01<00:07, 385.64it/s] 12%|‚ñà‚ñè        | 401/3257 [00:01<00:07, 375.97it/s] 13%|‚ñà‚ñé        | 439/3257 [00:01<00:07, 353.55it/s] 15%|‚ñà‚ñç        | 479/3257 [00:01<00:07, 362.71it/s] 16%|‚ñà‚ñå        | 522/3257 [00:01<00:07, 376.43it/s] 17%|‚ñà‚ñã        | 560/3257 [00:01<00:07, 367.35it/s] 18%|‚ñà‚ñä        | 597/3257 [00:01<00:07, 366.50it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:01<00:07, 372.21it/s] 21%|‚ñà‚ñà        | 675/3257 [00:01<00:07, 365.08it/s] 22%|‚ñà‚ñà‚ñè       | 713/3257 [00:02<00:06, 367.85it/s] 23%|‚ñà‚ñà‚ñé       | 750/3257 [00:02<00:07, 353.92it/s] 24%|‚ñà‚ñà‚ñç       | 788/3257 [00:02<00:06, 359.78it/s] 25%|‚ñà‚ñà‚ñå       | 825/3257 [00:02<00:06, 355.88it/s] 26%|‚ñà‚ñà‚ñã       | 861/3257 [00:02<00:06, 354.12it/s] 28%|‚ñà‚ñà‚ñä       | 897/3257 [00:02<00:06, 353.65it/s] 29%|‚ñà‚ñà‚ñä       | 935/3257 [00:02<00:06, 360.44it/s] 30%|‚ñà‚ñà‚ñâ       | 974/3257 [00:02<00:06, 368.39it/s] 31%|‚ñà‚ñà‚ñà       | 1011/3257 [00:02<00:06, 358.92it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:02<00:06, 350.47it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1084/3257 [00:03<00:06, 354.61it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1120/3257 [00:03<00:06, 355.92it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1156/3257 [00:03<00:05, 356.43it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1192/3257 [00:03<00:06, 343.70it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1229/3257 [00:03<00:05, 350.10it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1265/3257 [00:03<00:05, 352.84it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1301/3257 [00:03<00:05, 339.52it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1341/3257 [00:03<00:05, 356.73it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1377/3257 [00:04<00:07, 245.11it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1417/3257 [00:04<00:06, 278.87it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1458/3257 [00:04<00:05, 309.27it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1500/3257 [00:04<00:05, 336.32it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1537/3257 [00:04<00:05, 334.40it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1574/3257 [00:04<00:04, 342.43it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:04<00:04, 358.75it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1653/3257 [00:04<00:04, 349.61it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1689/3257 [00:04<00:04, 349.16it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:05<00:04, 346.27it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1764/3257 [00:05<00:04, 347.63it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1804/3257 [00:05<00:04, 360.06it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1841/3257 [00:05<00:03, 358.89it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1881/3257 [00:05<00:03, 368.22it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1918/3257 [00:05<00:03, 363.94it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:05<00:03, 387.69it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:05<00:03, 381.99it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2043/3257 [00:05<00:03, 379.10it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2081/3257 [00:05<00:03, 366.09it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2118/3257 [00:06<00:03, 364.53it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2155/3257 [00:06<00:03, 346.76it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2193/3257 [00:06<00:02, 354.83it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2229/3257 [00:06<00:02, 355.53it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2265/3257 [00:06<00:02, 355.28it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2301/3257 [00:06<00:02, 351.65it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2345/3257 [00:06<00:02, 376.48it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2386/3257 [00:06<00:02, 386.06it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2425/3257 [00:06<00:02, 381.84it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2464/3257 [00:07<00:02, 371.69it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2507/3257 [00:07<00:01, 386.09it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2547/3257 [00:07<00:01, 388.46it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2586/3257 [00:07<00:01, 366.55it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:07<00:01, 386.07it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:07<00:01, 377.51it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2708/3257 [00:07<00:01, 359.79it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2748/3257 [00:07<00:01, 370.02it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2786/3257 [00:08<00:01, 248.91it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2823/3257 [00:08<00:01, 274.63it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2862/3257 [00:08<00:01, 299.47it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2900/3257 [00:08<00:01, 315.86it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2940/3257 [00:08<00:00, 335.81it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:08<00:00, 334.20it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:08<00:00, 333.80it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3047/3257 [00:08<00:00, 338.15it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3088/3257 [00:08<00:00, 357.77it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3131/3257 [00:08<00:00, 376.00it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3170/3257 [00:09<00:00, 371.88it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3208/3257 [00:09<00:00, 369.01it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3251/3257 [00:09<00:00, 385.54it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 349.28it/s]
2023-02-07 12:51:49.896 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:51:49,897][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d122,n5,mc8,s0.261265,t4>', 'datetime': '2023-02-07T12:51:49.897434', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:51:49,897][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:51:49,897][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:51:50,079][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 12:51:50,080][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:51:50,084][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 retains 1410 unique words (50.02% of original 2819, drops 1409)', 'datetime': '2023-02-07T12:51:50.084886', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:51:50,085][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 leaves 2179184 word corpus (99.80% of original 2183622, drops 4438)', 'datetime': '2023-02-07T12:51:50.085132', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:51:50,090][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 12:51:50,090][gensim.models.word2vec][INFO] - sample=0.261265 downsamples 0 most-common words
[2023-02-07 12:51:50,090][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2179184 word corpus (100.0%% of prior 2179184)', 'datetime': '2023-02-07T12:51:50.090642', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:51:50,098][gensim.models.word2vec][INFO] - estimated required memory for 1410 words and 122 dimensions: 4321976 bytes
[2023-02-07 12:51:50,099][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:51:50,101][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 1410 vocabulary and 122 features, using sg=1 hs=0 sample=0.2612650547493335 negative=5 window=7 shrink_windows=True', 'datetime': '2023-02-07T12:51:50.101712', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:51:50,908][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2182441 effective words) took 0.8s, 2711098 effective words/s
[2023-02-07 12:51:51,724][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2182441 effective words) took 0.8s, 2677459 effective words/s
[2023-02-07 12:51:52,539][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2182441 effective words) took 0.8s, 2680782 effective words/s
[2023-02-07 12:51:53,358][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2182441 effective words) took 0.8s, 2672189 effective words/s
[2023-02-07 12:51:54,170][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2182441 effective words) took 0.8s, 2689415 effective words/s
[2023-02-07 12:51:55,015][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2182441 effective words) took 0.8s, 2588940 effective words/s
[2023-02-07 12:51:55,823][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2182441 effective words) took 0.8s, 2702882 effective words/s
[2023-02-07 12:51:56,624][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2182441 effective words) took 0.8s, 2729100 effective words/s
[2023-02-07 12:51:57,431][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2182441 effective words) took 0.8s, 2708362 effective words/s
[2023-02-07 12:51:58,234][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2182441 effective words) took 0.8s, 2721385 effective words/s
[2023-02-07 12:51:59,054][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2182441 effective words) took 0.8s, 2665716 effective words/s
[2023-02-07 12:51:59,864][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2182441 effective words) took 0.8s, 2699041 effective words/s
[2023-02-07 12:52:00,675][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2182441 effective words) took 0.8s, 2695440 effective words/s
[2023-02-07 12:52:01,480][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2182441 effective words) took 0.8s, 2715366 effective words/s
[2023-02-07 12:52:02,305][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2182441 effective words) took 0.8s, 2651142 effective words/s
[2023-02-07 12:52:03,095][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2182441 effective words) took 0.8s, 2765175 effective words/s
[2023-02-07 12:52:03,952][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2182441 effective words) took 0.9s, 2552722 effective words/s
[2023-02-07 12:52:04,834][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2182441 effective words) took 0.9s, 2478150 effective words/s
[2023-02-07 12:52:05,749][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2182441 effective words) took 0.9s, 2389885 effective words/s
[2023-02-07 12:52:06,690][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2182441 effective words) took 0.9s, 2322311 effective words/s
[2023-02-07 12:52:07,616][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2182441 effective words) took 0.9s, 2362588 effective words/s
[2023-02-07 12:52:08,534][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2182441 effective words) took 0.9s, 2381057 effective words/s
[2023-02-07 12:52:09,427][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2182441 effective words) took 0.9s, 2448268 effective words/s
[2023-02-07 12:52:10,391][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2182441 effective words) took 1.0s, 2267260 effective words/s
[2023-02-07 12:52:11,277][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2182441 effective words) took 0.9s, 2466221 effective words/s
[2023-02-07 12:52:11,278][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 54590550 raw words (54561025 effective words) took 21.2s, 2576500 effective words/s', 'datetime': '2023-02-07T12:52:11.278332', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:52:11.278 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:52:12,721][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125132-n4put40g/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:52:12.721183', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:52:12,721][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:52:12,730][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125132-n4put40g/files/../tmp/embedding_model.pt
2023-02-07 12:52:12.730 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:52:13.916 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:52:14.363 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:52:15.205 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.7988353151635241, 'test_mae': 0.9929876651706323, 'test_r2': 0.1490433437582257}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.55
wandb: percentage 0.49982
wandb:   test_mae 0.99299
wandb:   test_mse 1.79884
wandb:    test_r2 0.14904
wandb: 
wandb: üöÄ View run winter-sweep-18 at: https://wandb.ai/xiaoqiz/mof2vec/runs/n4put40g
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_125132-n4put40g/logs
wandb: Agent Starting Run: blak1nea with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 433
wandb: 	model.gensim.alpha: 0.00288460166156059
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 20
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.3930515577416808
wandb: 	model.gensim.vector_size: 204
wandb: 	model.gensim.window: 9
wandb: 	model.sklearn.learning_rate: 0.4034658187620732
wandb: 	model.sklearn.max_depth: 42
wandb: 	model.sklearn.min_child_weight: 0.007695927132424475
wandb: 	model.sklearn.n_estimators: 1796
wandb: 	model.sklearn.num_leaves: 487
wandb: 	model.sklearn.reg_alpha: 0.04150020481396405
wandb: 	model.sklearn.reg_lambda: 0.011657215579275506
wandb: 	model.sklearn.subsample: 0.22257120408943035
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125225-blak1nea
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-19
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/blak1nea
2023-02-07 12:52:33.160 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 12:52:33.161 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 433 for sweep.
2023-02-07 12:52:33.161 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.00288460166156059 for sweep.
2023-02-07 12:52:33.161 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:52:33.162 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 20 for sweep.
2023-02-07 12:52:33.162 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 12:52:33.162 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3930515577416808 for sweep.
2023-02-07 12:52:33.162 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 204 for sweep.
2023-02-07 12:52:33.163 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 9 for sweep.
2023-02-07 12:52:33.163 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.4034658187620732 for sweep.
2023-02-07 12:52:33.163 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 42 for sweep.
2023-02-07 12:52:33.163 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.007695927132424475 for sweep.
2023-02-07 12:52:33.163 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1796 for sweep.
2023-02-07 12:52:33.164 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 487 for sweep.
2023-02-07 12:52:33.164 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.04150020481396405 for sweep.
2023-02-07 12:52:33.164 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.011657215579275506 for sweep.
2023-02-07 12:52:33.164 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.22257120408943035 for sweep.
2023-02-07 12:52:33.165 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:52:33.168 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125225-blak1nea/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 433, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 204, 'window': 9, 'min_count': 3, 'dm': 0, 'sample': 0.3930515577416808, 'workers': 4, 'alpha': 0.00288460166156059, 'epochs': 20}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1796, 'max_depth': 42, 'num_leaves': 487, 'reg_alpha': 0.04150020481396405, 'reg_lambda': 0.011657215579275506, 'subsample': 0.22257120408943035, 'min_child_weight': 0.007695927132424475, 'n_jobs': 4, 'learning_rate': 0.4034658187620732}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 30/3257 [00:00<00:11, 288.65it/s]  2%|‚ñè         | 59/3257 [00:00<00:11, 284.25it/s]  3%|‚ñé         | 92/3257 [00:00<00:10, 300.92it/s]  4%|‚ñç         | 123/3257 [00:00<00:10, 295.14it/s]  5%|‚ñç         | 155/3257 [00:00<00:10, 302.46it/s]  6%|‚ñå         | 186/3257 [00:00<00:10, 298.51it/s]  7%|‚ñã         | 220/3257 [00:00<00:09, 310.65it/s]  8%|‚ñä         | 254/3257 [00:00<00:09, 318.81it/s]  9%|‚ñâ         | 290/3257 [00:00<00:08, 330.21it/s] 10%|‚ñâ         | 324/3257 [00:01<00:08, 332.02it/s] 11%|‚ñà         | 358/3257 [00:01<00:08, 326.41it/s] 12%|‚ñà‚ñè        | 391/3257 [00:01<00:09, 307.14it/s] 13%|‚ñà‚ñé        | 423/3257 [00:01<00:09, 309.08it/s] 14%|‚ñà‚ñç        | 455/3257 [00:01<00:09, 288.13it/s] 15%|‚ñà‚ñç        | 486/3257 [00:01<00:09, 292.50it/s] 16%|‚ñà‚ñå        | 520/3257 [00:01<00:08, 305.80it/s] 17%|‚ñà‚ñã        | 551/3257 [00:01<00:08, 301.90it/s] 18%|‚ñà‚ñä        | 582/3257 [00:01<00:09, 283.15it/s] 19%|‚ñà‚ñâ        | 619/3257 [00:02<00:08, 306.14it/s] 20%|‚ñà‚ñâ        | 651/3257 [00:02<00:08, 294.85it/s] 21%|‚ñà‚ñà        | 681/3257 [00:02<00:08, 291.93it/s] 22%|‚ñà‚ñà‚ñè       | 711/3257 [00:02<00:08, 286.82it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:02<00:09, 275.41it/s] 24%|‚ñà‚ñà‚ñé       | 773/3257 [00:02<00:08, 288.04it/s] 25%|‚ñà‚ñà‚ñç       | 805/3257 [00:02<00:08, 296.26it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:02<00:08, 289.63it/s] 27%|‚ñà‚ñà‚ñã       | 865/3257 [00:02<00:08, 282.64it/s] 27%|‚ñà‚ñà‚ñã       | 894/3257 [00:03<00:12, 192.22it/s] 28%|‚ñà‚ñà‚ñä       | 928/3257 [00:03<00:10, 222.91it/s] 29%|‚ñà‚ñà‚ñâ       | 959/3257 [00:03<00:09, 241.41it/s] 30%|‚ñà‚ñà‚ñà       | 988/3257 [00:03<00:08, 252.17it/s] 31%|‚ñà‚ñà‚ñà       | 1017/3257 [00:03<00:08, 260.95it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1045/3257 [00:03<00:08, 259.14it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:03<00:08, 270.90it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1112/3257 [00:03<00:07, 282.76it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1141/3257 [00:04<00:07, 277.69it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:04<00:07, 287.46it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1203/3257 [00:04<00:07, 266.60it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1237/3257 [00:04<00:07, 285.56it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1268/3257 [00:04<00:06, 291.04it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1298/3257 [00:04<00:07, 272.04it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1330/3257 [00:04<00:06, 283.69it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1360/3257 [00:04<00:06, 287.44it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1390/3257 [00:04<00:06, 278.75it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1427/3257 [00:05<00:06, 302.93it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1462/3257 [00:05<00:05, 314.52it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1494/3257 [00:05<00:05, 315.74it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1526/3257 [00:05<00:05, 305.51it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1557/3257 [00:05<00:05, 294.79it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1588/3257 [00:05<00:05, 297.84it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1621/3257 [00:05<00:05, 305.87it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1652/3257 [00:05<00:05, 295.25it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1682/3257 [00:05<00:05, 290.07it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1714/3257 [00:05<00:05, 296.36it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1744/3257 [00:06<00:05, 275.96it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1777/3257 [00:06<00:05, 289.24it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1807/3257 [00:06<00:05, 287.38it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1839/3257 [00:06<00:04, 293.96it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1872/3257 [00:06<00:04, 302.82it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1904/3257 [00:06<00:04, 306.33it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1935/3257 [00:06<00:04, 305.41it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1973/3257 [00:06<00:03, 325.97it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2006/3257 [00:06<00:03, 312.93it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2039/3257 [00:07<00:03, 315.98it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2071/3257 [00:07<00:04, 294.38it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2101/3257 [00:07<00:03, 290.30it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2131/3257 [00:07<00:03, 286.59it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2161/3257 [00:07<00:03, 287.16it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2193/3257 [00:07<00:03, 295.10it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2223/3257 [00:07<00:05, 186.47it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2252/3257 [00:07<00:04, 206.19it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2279/3257 [00:08<00:04, 220.33it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2312/3257 [00:08<00:03, 246.07it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2353/3257 [00:08<00:03, 287.30it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2387/3257 [00:08<00:02, 299.92it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2420/3257 [00:08<00:02, 298.85it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2452/3257 [00:08<00:02, 289.36it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2488/3257 [00:08<00:02, 308.43it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2524/3257 [00:08<00:02, 320.76it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2557/3257 [00:08<00:02, 315.66it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2590/3257 [00:09<00:02, 298.70it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:09<00:01, 324.81it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2664/3257 [00:09<00:01, 313.58it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2696/3257 [00:09<00:01, 306.47it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2727/3257 [00:09<00:01, 298.03it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:09<00:01, 305.60it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2794/3257 [00:09<00:01, 310.95it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2826/3257 [00:09<00:01, 301.08it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2861/3257 [00:09<00:01, 313.49it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2893/3257 [00:10<00:01, 312.55it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2926/3257 [00:10<00:01, 317.55it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2958/3257 [00:10<00:01, 294.51it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2989/3257 [00:10<00:00, 297.68it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3023/3257 [00:10<00:00, 307.71it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3063/3257 [00:10<00:00, 330.73it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3097/3257 [00:10<00:00, 331.02it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3131/3257 [00:10<00:00, 331.12it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3165/3257 [00:10<00:00, 309.02it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3197/3257 [00:11<00:00, 299.72it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3228/3257 [00:11<00:00, 290.19it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 290.55it/s]
2023-02-07 12:52:44.739 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:52:44,740][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d204,n5,mc3,s0.393052,t4>', 'datetime': '2023-02-07T12:52:44.740552', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:52:44,740][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:52:44,741][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:52:45,037][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 12:52:45,037][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:52:45,050][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 4875 unique words (73.18% of original 6662, drops 1787)', 'datetime': '2023-02-07T12:52:45.050716', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:52:45,051][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 2908813 word corpus (99.91% of original 2911496, drops 2683)', 'datetime': '2023-02-07T12:52:45.051094', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:52:45,069][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 12:52:45,069][gensim.models.word2vec][INFO] - sample=0.393052 downsamples 0 most-common words
[2023-02-07 12:52:45,069][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2908813 word corpus (100.0%% of prior 2908813)', 'datetime': '2023-02-07T12:52:45.069837', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:52:45,098][gensim.models.word2vec][INFO] - estimated required memory for 4875 words and 204 dimensions: 13702612 bytes
[2023-02-07 12:52:45,098][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:52:45,105][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 4875 vocabulary and 204 features, using sg=1 hs=0 sample=0.3930515577416808 negative=5 window=9 shrink_windows=True', 'datetime': '2023-02-07T12:52:45.105405', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:52:46,114][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 83.85% examples, 2456244 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:52:46,280][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2912070 effective words) took 1.2s, 2487788 effective words/s
[2023-02-07 12:52:47,282][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 95.03% examples, 2771206 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:52:47,325][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2912070 effective words) took 1.0s, 2789022 effective words/s
[2023-02-07 12:52:48,333][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 94.69% examples, 2749401 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:52:48,381][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2912070 effective words) took 1.1s, 2762251 effective words/s
[2023-02-07 12:52:49,386][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 93.49% examples, 2726836 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:52:49,448][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2912070 effective words) took 1.1s, 2734759 effective words/s
[2023-02-07 12:52:50,453][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 90.48% examples, 2646092 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:52:50,547][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2912070 effective words) took 1.1s, 2654293 effective words/s
[2023-02-07 12:52:51,553][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 91.53% examples, 2676380 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:52:51,634][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2912070 effective words) took 1.1s, 2685089 effective words/s
[2023-02-07 12:52:52,638][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 91.68% examples, 2684061 words/s, in_qsize 6, out_qsize 1
[2023-02-07 12:52:52,717][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2912070 effective words) took 1.1s, 2695062 effective words/s
[2023-02-07 12:52:53,720][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 91.34% examples, 2672648 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:52:53,805][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2912070 effective words) took 1.1s, 2680920 effective words/s
[2023-02-07 12:52:54,809][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 91.34% examples, 2673037 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:52:54,893][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2912070 effective words) took 1.1s, 2681279 effective words/s
[2023-02-07 12:52:55,899][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 91.86% examples, 2685892 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:52:55,973][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2912070 effective words) took 1.1s, 2699882 effective words/s
[2023-02-07 12:52:56,976][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 90.48% examples, 2649853 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:52:57,068][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2912070 effective words) took 1.1s, 2664360 effective words/s
[2023-02-07 12:52:58,071][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 91.53% examples, 2684938 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:52:58,146][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2912070 effective words) took 1.1s, 2705310 effective words/s
[2023-02-07 12:52:59,149][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 92.39% examples, 2706215 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:52:59,222][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2912070 effective words) took 1.1s, 2713044 effective words/s
[2023-02-07 12:53:00,226][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 90.73% examples, 2654031 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:53:00,313][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2912070 effective words) took 1.1s, 2673872 effective words/s
[2023-02-07 12:53:01,317][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 92.11% examples, 2698774 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:53:01,386][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2912070 effective words) took 1.1s, 2716763 effective words/s
[2023-02-07 12:53:02,390][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 94.69% examples, 2758525 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:53:02,436][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2912070 effective words) took 1.0s, 2776406 effective words/s
[2023-02-07 12:53:03,416][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2912070 effective words) took 1.0s, 2975828 effective words/s
[2023-02-07 12:53:04,369][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2912070 effective words) took 1.0s, 3060669 effective words/s
[2023-02-07 12:53:05,370][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 99.82% examples, 2901736 words/s, in_qsize 1, out_qsize 1
[2023-02-07 12:53:05,371][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2912070 effective words) took 1.0s, 2908923 effective words/s
[2023-02-07 12:53:06,378][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 93.00% examples, 2710340 words/s, in_qsize 7, out_qsize 0
[2023-02-07 12:53:06,442][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2912070 effective words) took 1.1s, 2722154 effective words/s
[2023-02-07 12:53:06,443][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 58229920 raw words (58241400 effective words) took 21.3s, 2729803 effective words/s', 'datetime': '2023-02-07T12:53:06.443109', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:53:06.443 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:53:08,044][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125225-blak1nea/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:53:08.044824', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:53:08,045][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:53:08,065][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125225-blak1nea/files/../tmp/embedding_model.pt
2023-02-07 12:53:08.066 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:53:09.465 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:53:10.010 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:53:11.382 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9729301274023991, 'test_mae': 1.0666492958124614, 'test_r2': 0.0666860884591961}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.030 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.030 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.66
wandb: percentage 0.26824
wandb:   test_mae 1.06665
wandb:   test_mse 1.97293
wandb:    test_r2 0.06669
wandb: 
wandb: üöÄ View run true-sweep-19 at: https://wandb.ai/xiaoqiz/mof2vec/runs/blak1nea
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_125225-blak1nea/logs
wandb: Agent Starting Run: askgn3mg with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 766
wandb: 	model.gensim.alpha: 0.0003637582128091388
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 94
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.3332084294204541
wandb: 	model.gensim.vector_size: 48
wandb: 	model.gensim.window: 10
wandb: 	model.sklearn.learning_rate: 0.12301000975878744
wandb: 	model.sklearn.max_depth: 63
wandb: 	model.sklearn.min_child_weight: 0.0035518330517583492
wandb: 	model.sklearn.n_estimators: 1694
wandb: 	model.sklearn.num_leaves: 440
wandb: 	model.sklearn.reg_alpha: 0.20373324337828727
wandb: 	model.sklearn.reg_lambda: 0.06582874787154977
wandb: 	model.sklearn.subsample: 0.4408715789893285
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125323-askgn3mg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/askgn3mg
2023-02-07 12:53:30.718 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 12:53:30.718 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 766 for sweep.
2023-02-07 12:53:30.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0003637582128091388 for sweep.
2023-02-07 12:53:30.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:53:30.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 94 for sweep.
2023-02-07 12:53:30.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 12:53:30.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3332084294204541 for sweep.
2023-02-07 12:53:30.720 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 48 for sweep.
2023-02-07 12:53:30.720 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 10 for sweep.
2023-02-07 12:53:30.720 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.12301000975878744 for sweep.
2023-02-07 12:53:30.720 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 63 for sweep.
2023-02-07 12:53:30.721 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0035518330517583492 for sweep.
2023-02-07 12:53:30.721 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1694 for sweep.
2023-02-07 12:53:30.721 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 440 for sweep.
2023-02-07 12:53:30.721 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.20373324337828727 for sweep.
2023-02-07 12:53:30.721 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.06582874787154977 for sweep.
2023-02-07 12:53:30.722 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.4408715789893285 for sweep.
2023-02-07 12:53:30.722 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:53:30.726 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125323-askgn3mg/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 766, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 48, 'window': 10, 'min_count': 2, 'dm': 0, 'sample': 0.3332084294204541, 'workers': 4, 'alpha': 0.0003637582128091388, 'epochs': 94}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1694, 'max_depth': 63, 'num_leaves': 440, 'reg_alpha': 0.20373324337828727, 'reg_lambda': 0.06582874787154977, 'subsample': 0.4408715789893285, 'min_child_weight': 0.0035518330517583492, 'n_jobs': 4, 'learning_rate': 0.12301000975878744}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|‚ñè         | 41/3257 [00:00<00:08, 398.75it/s]  3%|‚ñé         | 84/3257 [00:00<00:07, 416.04it/s]  4%|‚ñç         | 126/3257 [00:00<00:07, 413.91it/s]  5%|‚ñå         | 171/3257 [00:00<00:07, 424.06it/s]  7%|‚ñã         | 218/3257 [00:00<00:06, 439.88it/s]  8%|‚ñä         | 264/3257 [00:00<00:06, 446.64it/s] 10%|‚ñâ         | 312/3257 [00:00<00:09, 310.44it/s] 11%|‚ñà         | 359/3257 [00:00<00:08, 346.39it/s] 12%|‚ñà‚ñè        | 402/3257 [00:01<00:07, 366.29it/s] 14%|‚ñà‚ñé        | 443/3257 [00:01<00:07, 361.69it/s] 15%|‚ñà‚ñç        | 487/3257 [00:01<00:07, 382.24it/s] 16%|‚ñà‚ñã        | 532/3257 [00:01<00:06, 399.91it/s] 18%|‚ñà‚ñä        | 574/3257 [00:01<00:06, 393.31it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:01<00:06, 408.12it/s] 20%|‚ñà‚ñà        | 662/3257 [00:01<00:06, 404.83it/s] 22%|‚ñà‚ñà‚ñè       | 707/3257 [00:01<00:06, 416.97it/s] 23%|‚ñà‚ñà‚ñé       | 750/3257 [00:01<00:06, 409.89it/s] 24%|‚ñà‚ñà‚ñç       | 795/3257 [00:02<00:05, 419.37it/s] 26%|‚ñà‚ñà‚ñå       | 838/3257 [00:02<00:05, 413.67it/s] 27%|‚ñà‚ñà‚ñã       | 880/3257 [00:02<00:05, 399.87it/s] 28%|‚ñà‚ñà‚ñä       | 927/3257 [00:02<00:05, 417.33it/s] 30%|‚ñà‚ñà‚ñâ       | 970/3257 [00:02<00:05, 420.69it/s] 31%|‚ñà‚ñà‚ñà       | 1013/3257 [00:02<00:05, 411.18it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1055/3257 [00:02<00:05, 408.16it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1097/3257 [00:02<00:05, 409.35it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1140/3257 [00:02<00:05, 413.63it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1182/3257 [00:02<00:05, 413.42it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1224/3257 [00:03<00:05, 404.42it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1270/3257 [00:03<00:04, 419.52it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1313/3257 [00:03<00:04, 407.47it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1357/3257 [00:03<00:04, 416.75it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1399/3257 [00:03<00:04, 404.93it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1442/3257 [00:03<00:04, 411.73it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1484/3257 [00:03<00:06, 287.10it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1525/3257 [00:03<00:05, 312.57it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1563/3257 [00:04<00:05, 328.62it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1605/3257 [00:04<00:04, 350.08it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1643/3257 [00:04<00:04, 356.20it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1681/3257 [00:04<00:04, 361.60it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1721/3257 [00:04<00:04, 371.04it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1760/3257 [00:04<00:04, 366.49it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1802/3257 [00:04<00:03, 381.18it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1841/3257 [00:04<00:03, 383.27it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1883/3257 [00:04<00:03, 390.48it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1923/3257 [00:04<00:03, 393.09it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1973/3257 [00:05<00:03, 422.31it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2016/3257 [00:05<00:02, 413.79it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2058/3257 [00:05<00:03, 395.38it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2098/3257 [00:05<00:03, 383.46it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2137/3257 [00:05<00:02, 376.96it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2175/3257 [00:05<00:02, 377.22it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2213/3257 [00:05<00:02, 365.58it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2251/3257 [00:05<00:02, 367.68it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2288/3257 [00:05<00:02, 365.48it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2329/3257 [00:06<00:02, 377.42it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2370/3257 [00:06<00:02, 386.84it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2410/3257 [00:06<00:02, 387.92it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2449/3257 [00:06<00:02, 374.01it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2493/3257 [00:06<00:01, 392.87it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2538/3257 [00:06<00:01, 409.05it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2580/3257 [00:06<00:01, 384.11it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2627/3257 [00:06<00:01, 406.31it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2669/3257 [00:06<00:01, 394.07it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2709/3257 [00:07<00:01, 377.89it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2753/3257 [00:07<00:01, 395.06it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2794/3257 [00:07<00:01, 398.72it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2835/3257 [00:07<00:01, 390.00it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2884/3257 [00:07<00:00, 417.83it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2927/3257 [00:07<00:01, 267.09it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2962/3257 [00:07<00:01, 283.94it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3001/3257 [00:07<00:00, 307.48it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3044/3257 [00:08<00:00, 336.86it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3088/3257 [00:08<00:00, 362.85it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3132/3257 [00:08<00:00, 380.95it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3173/3257 [00:08<00:00, 376.06it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3213/3257 [00:08<00:00, 369.84it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3256/3257 [00:08<00:00, 385.71it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 381.11it/s]
2023-02-07 12:53:39.449 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:53:39,450][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d48,n5,mc2,s0.333208,t4>', 'datetime': '2023-02-07T12:53:39.450715', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:53:39,452][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:53:39,453][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:53:39,589][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 12:53:39,589][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:53:39,591][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 881 unique words (95.35% of original 924, drops 43)', 'datetime': '2023-02-07T12:53:39.591735', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:53:39,591][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 1455705 word corpus (100.00% of original 1455748, drops 43)', 'datetime': '2023-02-07T12:53:39.591956', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:53:39,594][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 12:53:39,595][gensim.models.word2vec][INFO] - sample=0.333208 downsamples 0 most-common words
[2023-02-07 12:53:39,595][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455705 word corpus (100.0%% of prior 1455705)', 'datetime': '2023-02-07T12:53:39.595209', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:53:39,600][gensim.models.word2vec][INFO] - estimated required memory for 881 words and 48 dimensions: 2055548 bytes
[2023-02-07 12:53:39,600][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:53:39,602][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 881 vocabulary and 48 features, using sg=1 hs=0 sample=0.3332084294204541 negative=5 window=10 shrink_windows=True', 'datetime': '2023-02-07T12:53:39.602125', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:53:40,115][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458962 effective words) took 0.5s, 2853583 effective words/s
[2023-02-07 12:53:40,627][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458962 effective words) took 0.5s, 2860564 effective words/s
[2023-02-07 12:53:41,148][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458962 effective words) took 0.5s, 2815301 effective words/s
[2023-02-07 12:53:41,655][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458962 effective words) took 0.5s, 2886006 effective words/s
[2023-02-07 12:53:42,174][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458962 effective words) took 0.5s, 2816537 effective words/s
[2023-02-07 12:53:42,684][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458962 effective words) took 0.5s, 2874580 effective words/s
[2023-02-07 12:53:43,193][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458962 effective words) took 0.5s, 2877829 effective words/s
[2023-02-07 12:53:43,696][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458962 effective words) took 0.5s, 2910182 effective words/s
[2023-02-07 12:53:44,211][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458962 effective words) took 0.5s, 2839816 effective words/s
[2023-02-07 12:53:44,723][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458962 effective words) took 0.5s, 2864327 effective words/s
[2023-02-07 12:53:45,235][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458962 effective words) took 0.5s, 2863925 effective words/s
[2023-02-07 12:53:45,747][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458962 effective words) took 0.5s, 2855846 effective words/s
[2023-02-07 12:53:46,258][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458962 effective words) took 0.5s, 2863788 effective words/s
[2023-02-07 12:53:46,771][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458962 effective words) took 0.5s, 2853422 effective words/s
[2023-02-07 12:53:47,286][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458962 effective words) took 0.5s, 2842707 effective words/s
[2023-02-07 12:53:47,808][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458962 effective words) took 0.5s, 2800074 effective words/s
[2023-02-07 12:53:48,326][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458962 effective words) took 0.5s, 2830055 effective words/s
[2023-02-07 12:53:48,842][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458962 effective words) took 0.5s, 2834756 effective words/s
[2023-02-07 12:53:49,353][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458962 effective words) took 0.5s, 2863569 effective words/s
[2023-02-07 12:53:49,864][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458962 effective words) took 0.5s, 2863221 effective words/s
[2023-02-07 12:53:50,375][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458962 effective words) took 0.5s, 2862448 effective words/s
[2023-02-07 12:53:50,885][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458962 effective words) took 0.5s, 2868290 effective words/s
[2023-02-07 12:53:51,400][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458962 effective words) took 0.5s, 2842063 effective words/s
[2023-02-07 12:53:51,906][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458962 effective words) took 0.5s, 2892018 effective words/s
[2023-02-07 12:53:52,380][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458962 effective words) took 0.5s, 3086755 effective words/s
[2023-02-07 12:53:52,853][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458962 effective words) took 0.5s, 3093431 effective words/s
[2023-02-07 12:53:53,323][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458962 effective words) took 0.5s, 3110591 effective words/s
[2023-02-07 12:53:53,792][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458962 effective words) took 0.5s, 3117929 effective words/s
[2023-02-07 12:53:54,260][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458962 effective words) took 0.5s, 3130279 effective words/s
[2023-02-07 12:53:54,713][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458962 effective words) took 0.5s, 3226593 effective words/s
[2023-02-07 12:53:55,208][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458962 effective words) took 0.5s, 2959264 effective words/s
[2023-02-07 12:53:55,740][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458962 effective words) took 0.5s, 2747787 effective words/s
[2023-02-07 12:53:56,270][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458962 effective words) took 0.5s, 2759960 effective words/s
[2023-02-07 12:53:56,834][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458962 effective words) took 0.6s, 2595125 effective words/s
[2023-02-07 12:53:57,396][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458962 effective words) took 0.6s, 2604594 effective words/s
[2023-02-07 12:53:57,911][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458962 effective words) took 0.5s, 2839959 effective words/s
[2023-02-07 12:53:58,438][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458962 effective words) took 0.5s, 2776791 effective words/s
[2023-02-07 12:53:58,980][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458962 effective words) took 0.5s, 2700233 effective words/s
[2023-02-07 12:53:59,520][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458962 effective words) took 0.5s, 2707120 effective words/s
[2023-02-07 12:54:00,024][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458962 effective words) took 0.5s, 2904001 effective words/s
[2023-02-07 12:54:00,546][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458962 effective words) took 0.5s, 2801774 effective words/s
[2023-02-07 12:54:01,062][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458962 effective words) took 0.5s, 2832894 effective words/s
[2023-02-07 12:54:01,565][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458962 effective words) took 0.5s, 2905250 effective words/s
[2023-02-07 12:54:02,075][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458962 effective words) took 0.5s, 2872480 effective words/s
[2023-02-07 12:54:02,623][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458962 effective words) took 0.5s, 2666263 effective words/s
[2023-02-07 12:54:03,166][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458962 effective words) took 0.5s, 2696587 effective words/s
[2023-02-07 12:54:03,700][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458962 effective words) took 0.5s, 2741505 effective words/s
[2023-02-07 12:54:04,218][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458962 effective words) took 0.5s, 2824526 effective words/s
[2023-02-07 12:54:04,733][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458962 effective words) took 0.5s, 2841304 effective words/s
[2023-02-07 12:54:05,258][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458962 effective words) took 0.5s, 2784088 effective words/s
[2023-02-07 12:54:05,770][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458962 effective words) took 0.5s, 2858300 effective words/s
[2023-02-07 12:54:06,312][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458962 effective words) took 0.5s, 2696282 effective words/s
[2023-02-07 12:54:06,851][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458962 effective words) took 0.5s, 2715409 effective words/s
[2023-02-07 12:54:07,352][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458962 effective words) took 0.5s, 2920651 effective words/s
[2023-02-07 12:54:07,848][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458962 effective words) took 0.5s, 2950728 effective words/s
[2023-02-07 12:54:08,352][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458962 effective words) took 0.5s, 2898914 effective words/s
[2023-02-07 12:54:08,855][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1458962 effective words) took 0.5s, 2909117 effective words/s
[2023-02-07 12:54:09,362][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1458962 effective words) took 0.5s, 2886734 effective words/s
[2023-02-07 12:54:09,856][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1458962 effective words) took 0.5s, 2962562 effective words/s
[2023-02-07 12:54:10,360][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1458962 effective words) took 0.5s, 2911728 effective words/s
[2023-02-07 12:54:10,881][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1458962 effective words) took 0.5s, 2809979 effective words/s
[2023-02-07 12:54:11,372][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1458962 effective words) took 0.5s, 2978422 effective words/s
[2023-02-07 12:54:11,898][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1458962 effective words) took 0.5s, 2779996 effective words/s
[2023-02-07 12:54:12,401][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1458962 effective words) took 0.5s, 2906531 effective words/s
[2023-02-07 12:54:12,943][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1458962 effective words) took 0.5s, 2698331 effective words/s
[2023-02-07 12:54:13,454][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1458962 effective words) took 0.5s, 2865066 effective words/s
[2023-02-07 12:54:13,982][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1458962 effective words) took 0.5s, 2773489 effective words/s
[2023-02-07 12:54:14,480][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1458962 effective words) took 0.5s, 2939850 effective words/s
[2023-02-07 12:54:15,009][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1458962 effective words) took 0.5s, 2765482 effective words/s
[2023-02-07 12:54:15,526][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1458962 effective words) took 0.5s, 2827991 effective words/s
[2023-02-07 12:54:16,027][gensim.models.word2vec][INFO] - EPOCH 70: training on 1455748 raw words (1458962 effective words) took 0.5s, 2919412 effective words/s
[2023-02-07 12:54:16,534][gensim.models.word2vec][INFO] - EPOCH 71: training on 1455748 raw words (1458962 effective words) took 0.5s, 2889338 effective words/s
[2023-02-07 12:54:17,034][gensim.models.word2vec][INFO] - EPOCH 72: training on 1455748 raw words (1458962 effective words) took 0.5s, 2925108 effective words/s
[2023-02-07 12:54:17,531][gensim.models.word2vec][INFO] - EPOCH 73: training on 1455748 raw words (1458962 effective words) took 0.5s, 2942173 effective words/s
[2023-02-07 12:54:18,037][gensim.models.word2vec][INFO] - EPOCH 74: training on 1455748 raw words (1458962 effective words) took 0.5s, 2891649 effective words/s
[2023-02-07 12:54:18,544][gensim.models.word2vec][INFO] - EPOCH 75: training on 1455748 raw words (1458962 effective words) took 0.5s, 2886745 effective words/s
[2023-02-07 12:54:19,063][gensim.models.word2vec][INFO] - EPOCH 76: training on 1455748 raw words (1458962 effective words) took 0.5s, 2818036 effective words/s
[2023-02-07 12:54:19,573][gensim.models.word2vec][INFO] - EPOCH 77: training on 1455748 raw words (1458962 effective words) took 0.5s, 2871509 effective words/s
[2023-02-07 12:54:20,088][gensim.models.word2vec][INFO] - EPOCH 78: training on 1455748 raw words (1458962 effective words) took 0.5s, 2836624 effective words/s
[2023-02-07 12:54:20,599][gensim.models.word2vec][INFO] - EPOCH 79: training on 1455748 raw words (1458962 effective words) took 0.5s, 2865443 effective words/s
[2023-02-07 12:54:21,104][gensim.models.word2vec][INFO] - EPOCH 80: training on 1455748 raw words (1458962 effective words) took 0.5s, 2894209 effective words/s
[2023-02-07 12:54:21,600][gensim.models.word2vec][INFO] - EPOCH 81: training on 1455748 raw words (1458962 effective words) took 0.5s, 2947152 effective words/s
[2023-02-07 12:54:22,106][gensim.models.word2vec][INFO] - EPOCH 82: training on 1455748 raw words (1458962 effective words) took 0.5s, 2895731 effective words/s
[2023-02-07 12:54:22,620][gensim.models.word2vec][INFO] - EPOCH 83: training on 1455748 raw words (1458962 effective words) took 0.5s, 2846669 effective words/s
[2023-02-07 12:54:23,140][gensim.models.word2vec][INFO] - EPOCH 84: training on 1455748 raw words (1458962 effective words) took 0.5s, 2814754 effective words/s
[2023-02-07 12:54:23,651][gensim.models.word2vec][INFO] - EPOCH 85: training on 1455748 raw words (1458962 effective words) took 0.5s, 2866832 effective words/s
[2023-02-07 12:54:24,168][gensim.models.word2vec][INFO] - EPOCH 86: training on 1455748 raw words (1458962 effective words) took 0.5s, 2835039 effective words/s
[2023-02-07 12:54:24,676][gensim.models.word2vec][INFO] - EPOCH 87: training on 1455748 raw words (1458962 effective words) took 0.5s, 2876995 effective words/s
[2023-02-07 12:54:25,193][gensim.models.word2vec][INFO] - EPOCH 88: training on 1455748 raw words (1458962 effective words) took 0.5s, 2829237 effective words/s
[2023-02-07 12:54:25,703][gensim.models.word2vec][INFO] - EPOCH 89: training on 1455748 raw words (1458962 effective words) took 0.5s, 2869640 effective words/s
[2023-02-07 12:54:26,217][gensim.models.word2vec][INFO] - EPOCH 90: training on 1455748 raw words (1458962 effective words) took 0.5s, 2846146 effective words/s
[2023-02-07 12:54:26,727][gensim.models.word2vec][INFO] - EPOCH 91: training on 1455748 raw words (1458962 effective words) took 0.5s, 2868851 effective words/s
[2023-02-07 12:54:27,242][gensim.models.word2vec][INFO] - EPOCH 92: training on 1455748 raw words (1458962 effective words) took 0.5s, 2838517 effective words/s
[2023-02-07 12:54:27,752][gensim.models.word2vec][INFO] - EPOCH 93: training on 1455748 raw words (1458962 effective words) took 0.5s, 2868135 effective words/s
[2023-02-07 12:54:27,753][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 136840312 raw words (137142428 effective words) took 48.2s, 2848183 effective words/s', 'datetime': '2023-02-07T12:54:27.753249', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:54:27.753 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:54:30,229][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125323-askgn3mg/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:54:30.229314', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:54:30,230][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:54:30,239][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125323-askgn3mg/files/../tmp/embedding_model.pt
2023-02-07 12:54:30.240 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:54:31.222 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:54:31.629 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:54:32.002 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9286377423960595, 'test_mae': 1.0513224146597406, 'test_r2': 0.08763903480411583}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.039 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.039 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.76
wandb: percentage 0.04654
wandb:   test_mae 1.05132
wandb:   test_mse 1.92864
wandb:    test_r2 0.08764
wandb: 
wandb: üöÄ View run grateful-sweep-20 at: https://wandb.ai/xiaoqiz/mof2vec/runs/askgn3mg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_125323-askgn3mg/logs
wandb: Agent Starting Run: zno88fbf with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 165
wandb: 	model.gensim.alpha: 0.000473985871647584
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 40
wandb: 	model.gensim.min_count: 8
wandb: 	model.gensim.sample: 0.261583647366611
wandb: 	model.gensim.vector_size: 159
wandb: 	model.gensim.window: 11
wandb: 	model.sklearn.learning_rate: 0.09639854848818036
wandb: 	model.sklearn.max_depth: 78
wandb: 	model.sklearn.min_child_weight: 0.01870153967581004
wandb: 	model.sklearn.n_estimators: 2028
wandb: 	model.sklearn.num_leaves: 470
wandb: 	model.sklearn.reg_alpha: 0.22411367950914313
wandb: 	model.sklearn.reg_lambda: 0.019863566734973657
wandb: 	model.sklearn.subsample: 0.34605882765257656
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125442-zno88fbf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-21
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/zno88fbf
2023-02-07 12:54:50.298 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 12:54:50.299 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 165 for sweep.
2023-02-07 12:54:50.299 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.000473985871647584 for sweep.
2023-02-07 12:54:50.299 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:54:50.299 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 40 for sweep.
2023-02-07 12:54:50.300 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 8 for sweep.
2023-02-07 12:54:50.300 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.261583647366611 for sweep.
2023-02-07 12:54:50.300 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 159 for sweep.
2023-02-07 12:54:50.300 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 11 for sweep.
2023-02-07 12:54:50.300 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.09639854848818036 for sweep.
2023-02-07 12:54:50.301 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 78 for sweep.
2023-02-07 12:54:50.301 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.01870153967581004 for sweep.
2023-02-07 12:54:50.301 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2028 for sweep.
2023-02-07 12:54:50.301 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 470 for sweep.
2023-02-07 12:54:50.301 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.22411367950914313 for sweep.
2023-02-07 12:54:50.302 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.019863566734973657 for sweep.
2023-02-07 12:54:50.302 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.34605882765257656 for sweep.
2023-02-07 12:54:50.302 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:54:50.309 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125442-zno88fbf/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 165, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 159, 'window': 11, 'min_count': 8, 'dm': 0, 'sample': 0.261583647366611, 'workers': 4, 'alpha': 0.000473985871647584, 'epochs': 40}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2028, 'max_depth': 78, 'num_leaves': 470, 'reg_alpha': 0.22411367950914313, 'reg_lambda': 0.019863566734973657, 'subsample': 0.34605882765257656, 'min_child_weight': 0.01870153967581004, 'n_jobs': 4, 'learning_rate': 0.09639854848818036}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 39/3257 [00:00<00:08, 387.86it/s]  2%|‚ñè         | 80/3257 [00:00<00:07, 399.18it/s]  4%|‚ñé         | 122/3257 [00:00<00:07, 405.48it/s]  5%|‚ñå         | 166/3257 [00:00<00:07, 415.58it/s]  6%|‚ñã         | 208/3257 [00:00<00:07, 396.97it/s]  8%|‚ñä         | 252/3257 [00:00<00:07, 409.39it/s]  9%|‚ñâ         | 294/3257 [00:00<00:07, 409.84it/s] 10%|‚ñà         | 336/3257 [00:00<00:07, 396.79it/s] 12%|‚ñà‚ñè        | 376/3257 [00:00<00:07, 390.78it/s] 13%|‚ñà‚ñé        | 418/3257 [00:01<00:07, 398.60it/s] 14%|‚ñà‚ñç        | 458/3257 [00:01<00:07, 383.66it/s] 15%|‚ñà‚ñå        | 497/3257 [00:01<00:07, 382.98it/s] 16%|‚ñà‚ñã        | 536/3257 [00:01<00:07, 384.72it/s] 18%|‚ñà‚ñä        | 575/3257 [00:01<00:07, 369.42it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:01<00:07, 376.68it/s] 20%|‚ñà‚ñà        | 653/3257 [00:01<00:07, 371.53it/s] 21%|‚ñà‚ñà        | 691/3257 [00:01<00:07, 365.03it/s] 22%|‚ñà‚ñà‚ñè       | 730/3257 [00:01<00:06, 370.44it/s] 24%|‚ñà‚ñà‚ñé       | 770/3257 [00:01<00:06, 378.27it/s] 25%|‚ñà‚ñà‚ñç       | 809/3257 [00:02<00:06, 381.28it/s] 26%|‚ñà‚ñà‚ñå       | 848/3257 [00:02<00:06, 373.34it/s] 27%|‚ñà‚ñà‚ñã       | 893/3257 [00:02<00:06, 393.83it/s] 29%|‚ñà‚ñà‚ñä       | 934/3257 [00:02<00:05, 394.09it/s] 30%|‚ñà‚ñà‚ñâ       | 974/3257 [00:02<00:05, 389.49it/s] 31%|‚ñà‚ñà‚ñà       | 1014/3257 [00:02<00:05, 379.63it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1053/3257 [00:02<00:05, 373.30it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:02<00:05, 371.65it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1129/3257 [00:03<00:08, 263.30it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:03<00:07, 295.53it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1207/3257 [00:03<00:06, 313.27it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1253/3257 [00:03<00:05, 350.36it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1294/3257 [00:03<00:05, 363.98it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1342/3257 [00:03<00:04, 393.82it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:03<00:04, 398.80it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1433/3257 [00:03<00:04, 424.20it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1483/3257 [00:03<00:03, 444.72it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1530/3257 [00:03<00:03, 450.37it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1576/3257 [00:04<00:03, 442.95it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1626/3257 [00:04<00:03, 458.66it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1673/3257 [00:04<00:03, 443.87it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1719/3257 [00:04<00:03, 446.67it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1764/3257 [00:04<00:03, 438.10it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1808/3257 [00:04<00:03, 436.39it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1855/3257 [00:04<00:03, 444.57it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1904/3257 [00:04<00:02, 455.36it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1955/3257 [00:04<00:02, 471.14it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2003/3257 [00:05<00:02, 472.67it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2051/3257 [00:05<00:02, 462.66it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2098/3257 [00:05<00:02, 455.40it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2144/3257 [00:05<00:02, 441.72it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2193/3257 [00:05<00:02, 453.53it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2239/3257 [00:05<00:02, 449.33it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2286/3257 [00:05<00:02, 453.92it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2337/3257 [00:05<00:01, 469.27it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2391/3257 [00:05<00:01, 489.91it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2441/3257 [00:05<00:01, 470.96it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2491/3257 [00:06<00:01, 479.04it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2542/3257 [00:06<00:01, 487.30it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2591/3257 [00:06<00:02, 328.94it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2644/3257 [00:06<00:01, 372.80it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2691/3257 [00:06<00:01, 394.85it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2736/3257 [00:06<00:01, 405.46it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2785/3257 [00:06<00:01, 427.19it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2831/3257 [00:06<00:00, 432.29it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:07<00:00, 462.29it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2934/3257 [00:07<00:00, 465.57it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2982/3257 [00:07<00:00, 458.07it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3035/3257 [00:07<00:00, 478.15it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3087/3257 [00:07<00:00, 489.14it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3138/3257 [00:07<00:00, 495.03it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3188/3257 [00:07<00:00, 480.47it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3240/3257 [00:07<00:00, 490.03it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:07<00:00, 416.52it/s]
2023-02-07 12:54:58.287 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:54:58,288][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d159,n5,mc8,s0.261584,t4>', 'datetime': '2023-02-07T12:54:58.288815', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:54:58,289][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:54:58,289][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:54:58,415][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 12:54:58,415][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:54:58,417][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 retains 515 unique words (55.74% of original 924, drops 409)', 'datetime': '2023-02-07T12:54:58.417420', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:54:58,417][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 leaves 1454326 word corpus (99.90% of original 1455748, drops 1422)', 'datetime': '2023-02-07T12:54:58.417601', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:54:58,419][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 12:54:58,419][gensim.models.word2vec][INFO] - sample=0.261584 downsamples 0 most-common words
[2023-02-07 12:54:58,420][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1454326 word corpus (100.0%% of prior 1454326)', 'datetime': '2023-02-07T12:54:58.420008', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:54:58,422][gensim.models.word2vec][INFO] - estimated required memory for 515 words and 159 dimensions: 3635432 bytes
[2023-02-07 12:54:58,423][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:54:58,425][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 515 vocabulary and 159 features, using sg=1 hs=0 sample=0.261583647366611 negative=5 window=11 shrink_windows=True', 'datetime': '2023-02-07T12:54:58.425683', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:54:59,075][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1457583 effective words) took 0.6s, 2249175 effective words/s
[2023-02-07 12:54:59,758][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1457583 effective words) took 0.7s, 2138521 effective words/s
[2023-02-07 12:55:00,456][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1457583 effective words) took 0.7s, 2093176 effective words/s
[2023-02-07 12:55:01,143][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1457583 effective words) took 0.7s, 2124362 effective words/s
[2023-02-07 12:55:01,830][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1457583 effective words) took 0.7s, 2124138 effective words/s
[2023-02-07 12:55:02,512][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1457583 effective words) took 0.7s, 2141301 effective words/s
[2023-02-07 12:55:03,198][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1457583 effective words) took 0.7s, 2129393 effective words/s
[2023-02-07 12:55:03,878][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1457583 effective words) took 0.7s, 2145635 effective words/s
[2023-02-07 12:55:04,565][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1457583 effective words) took 0.7s, 2126797 effective words/s
[2023-02-07 12:55:05,245][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1457583 effective words) took 0.7s, 2147448 effective words/s
[2023-02-07 12:55:05,923][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1457583 effective words) took 0.7s, 2154847 effective words/s
[2023-02-07 12:55:06,625][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1457583 effective words) took 0.7s, 2079738 effective words/s
[2023-02-07 12:55:07,305][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1457583 effective words) took 0.7s, 2146234 effective words/s
[2023-02-07 12:55:08,001][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1457583 effective words) took 0.7s, 2100526 effective words/s
[2023-02-07 12:55:08,678][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1457583 effective words) took 0.7s, 2157360 effective words/s
[2023-02-07 12:55:09,359][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1457583 effective words) took 0.7s, 2145172 effective words/s
[2023-02-07 12:55:10,065][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1457583 effective words) took 0.7s, 2070394 effective words/s
[2023-02-07 12:55:10,770][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1457583 effective words) took 0.7s, 2070280 effective words/s
[2023-02-07 12:55:11,475][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1457583 effective words) took 0.7s, 2070815 effective words/s
[2023-02-07 12:55:12,175][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1457583 effective words) took 0.7s, 2086684 effective words/s
[2023-02-07 12:55:12,872][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1457583 effective words) took 0.7s, 2095296 effective words/s
[2023-02-07 12:55:13,582][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1457583 effective words) took 0.7s, 2058617 effective words/s
[2023-02-07 12:55:14,286][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1457583 effective words) took 0.7s, 2073700 effective words/s
[2023-02-07 12:55:14,991][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1457583 effective words) took 0.7s, 2073559 effective words/s
[2023-02-07 12:55:15,696][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1457583 effective words) took 0.7s, 2073250 effective words/s
[2023-02-07 12:55:16,400][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1457583 effective words) took 0.7s, 2071846 effective words/s
[2023-02-07 12:55:17,108][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1457583 effective words) took 0.7s, 2061949 effective words/s
[2023-02-07 12:55:17,823][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1457583 effective words) took 0.7s, 2041682 effective words/s
[2023-02-07 12:55:18,529][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1457583 effective words) took 0.7s, 2069044 effective words/s
[2023-02-07 12:55:19,234][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1457583 effective words) took 0.7s, 2071705 effective words/s
[2023-02-07 12:55:19,940][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1457583 effective words) took 0.7s, 2068534 effective words/s
[2023-02-07 12:55:20,647][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1457583 effective words) took 0.7s, 2066798 effective words/s
[2023-02-07 12:55:21,347][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1457583 effective words) took 0.7s, 2088713 effective words/s
[2023-02-07 12:55:22,045][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1457583 effective words) took 0.7s, 2089567 effective words/s
[2023-02-07 12:55:22,745][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1457583 effective words) took 0.7s, 2087689 effective words/s
[2023-02-07 12:55:23,445][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1457583 effective words) took 0.7s, 2085048 effective words/s
[2023-02-07 12:55:24,144][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1457583 effective words) took 0.7s, 2089928 effective words/s
[2023-02-07 12:55:24,843][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1457583 effective words) took 0.7s, 2090389 effective words/s
[2023-02-07 12:55:25,544][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1457583 effective words) took 0.7s, 2086118 effective words/s
[2023-02-07 12:55:26,241][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1457583 effective words) took 0.7s, 2094590 effective words/s
[2023-02-07 12:55:26,242][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 58229920 raw words (58303320 effective words) took 27.8s, 2096006 effective words/s', 'datetime': '2023-02-07T12:55:26.242286', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:55:26.242 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:55:27,836][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125442-zno88fbf/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:55:27.836291', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:55:27,837][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:55:27,843][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125442-zno88fbf/files/../tmp/embedding_model.pt
2023-02-07 12:55:27.843 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:55:28.998 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:55:29.437 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:55:30.395 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8905511924754967, 'test_mae': 1.014267637605773, 'test_r2': 0.10565625010725288}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.48
wandb: percentage 0.44264
wandb:   test_mae 1.01427
wandb:   test_mse 1.89055
wandb:    test_r2 0.10566
wandb: 
wandb: üöÄ View run legendary-sweep-21 at: https://wandb.ai/xiaoqiz/mof2vec/runs/zno88fbf
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_125442-zno88fbf/logs
wandb: Agent Starting Run: o8cpgkd6 with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 856
wandb: 	model.gensim.alpha: 0.0015255979784154375
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 92
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.2617119893896576
wandb: 	model.gensim.vector_size: 139
wandb: 	model.gensim.window: 5
wandb: 	model.sklearn.learning_rate: 0.02637924541500788
wandb: 	model.sklearn.max_depth: 25
wandb: 	model.sklearn.min_child_weight: 0.018699591549868975
wandb: 	model.sklearn.n_estimators: 3926
wandb: 	model.sklearn.num_leaves: 484
wandb: 	model.sklearn.reg_alpha: 0.13580677962173127
wandb: 	model.sklearn.reg_lambda: 0.007690273008203789
wandb: 	model.sklearn.subsample: 0.2794723735301851
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125541-o8cpgkd6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-22
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/o8cpgkd6
2023-02-07 12:55:48.935 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 12:55:48.935 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 856 for sweep.
2023-02-07 12:55:48.936 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0015255979784154375 for sweep.
2023-02-07 12:55:48.936 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:55:48.936 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 92 for sweep.
2023-02-07 12:55:48.936 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 12:55:48.936 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2617119893896576 for sweep.
2023-02-07 12:55:48.937 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 139 for sweep.
2023-02-07 12:55:48.937 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 5 for sweep.
2023-02-07 12:55:48.937 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.02637924541500788 for sweep.
2023-02-07 12:55:48.937 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 25 for sweep.
2023-02-07 12:55:48.938 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.018699591549868975 for sweep.
2023-02-07 12:55:48.938 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3926 for sweep.
2023-02-07 12:55:48.938 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 484 for sweep.
2023-02-07 12:55:48.938 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.13580677962173127 for sweep.
2023-02-07 12:55:48.938 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.007690273008203789 for sweep.
2023-02-07 12:55:48.939 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2794723735301851 for sweep.
2023-02-07 12:55:48.939 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:55:48.944 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125541-o8cpgkd6/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 856, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 139, 'window': 5, 'min_count': 5, 'dm': 0, 'sample': 0.2617119893896576, 'workers': 4, 'alpha': 0.0015255979784154375, 'epochs': 92}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3926, 'max_depth': 25, 'num_leaves': 484, 'reg_alpha': 0.13580677962173127, 'reg_lambda': 0.007690273008203789, 'subsample': 0.2794723735301851, 'min_child_weight': 0.018699591549868975, 'n_jobs': 4, 'learning_rate': 0.02637924541500788}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 36/3257 [00:00<00:09, 351.64it/s]  2%|‚ñè         | 73/3257 [00:00<00:08, 362.09it/s]  3%|‚ñé         | 110/3257 [00:00<00:08, 358.87it/s]  5%|‚ñç         | 151/3257 [00:00<00:08, 375.25it/s]  6%|‚ñå         | 189/3257 [00:00<00:08, 373.94it/s]  7%|‚ñã         | 231/3257 [00:00<00:07, 386.47it/s]  8%|‚ñä         | 270/3257 [00:00<00:07, 383.68it/s] 10%|‚ñâ         | 312/3257 [00:00<00:07, 393.35it/s] 11%|‚ñà         | 353/3257 [00:00<00:07, 396.46it/s] 12%|‚ñà‚ñè        | 393/3257 [00:01<00:07, 382.11it/s] 13%|‚ñà‚ñé        | 432/3257 [00:01<00:07, 365.03it/s] 14%|‚ñà‚ñç        | 471/3257 [00:01<00:07, 370.65it/s] 16%|‚ñà‚ñå        | 511/3257 [00:01<00:07, 379.01it/s] 17%|‚ñà‚ñã        | 550/3257 [00:01<00:07, 376.17it/s] 18%|‚ñà‚ñä        | 588/3257 [00:01<00:07, 362.40it/s] 19%|‚ñà‚ñâ        | 625/3257 [00:01<00:10, 260.92it/s] 20%|‚ñà‚ñà        | 660/3257 [00:01<00:09, 279.46it/s] 21%|‚ñà‚ñà‚ñè       | 698/3257 [00:02<00:08, 303.49it/s] 23%|‚ñà‚ñà‚ñé       | 736/3257 [00:02<00:07, 316.96it/s] 24%|‚ñà‚ñà‚ñç       | 775/3257 [00:02<00:07, 334.43it/s] 25%|‚ñà‚ñà‚ñç       | 813/3257 [00:02<00:07, 345.80it/s] 26%|‚ñà‚ñà‚ñå       | 849/3257 [00:02<00:07, 337.60it/s] 27%|‚ñà‚ñà‚ñã       | 884/3257 [00:02<00:06, 340.38it/s] 28%|‚ñà‚ñà‚ñä       | 926/3257 [00:02<00:06, 362.90it/s] 30%|‚ñà‚ñà‚ñâ       | 964/3257 [00:02<00:06, 366.07it/s] 31%|‚ñà‚ñà‚ñà       | 1002/3257 [00:02<00:06, 360.93it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1039/3257 [00:02<00:06, 352.16it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1078/3257 [00:03<00:06, 362.62it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1115/3257 [00:03<00:06, 355.21it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1151/3257 [00:03<00:06, 347.95it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1186/3257 [00:03<00:05, 348.44it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1221/3257 [00:03<00:05, 341.38it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1261/3257 [00:03<00:05, 357.13it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1297/3257 [00:03<00:05, 341.99it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1337/3257 [00:03<00:05, 357.99it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1374/3257 [00:03<00:05, 351.64it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1413/3257 [00:04<00:05, 362.45it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1454/3257 [00:04<00:04, 375.87it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1495/3257 [00:04<00:04, 385.48it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1534/3257 [00:04<00:04, 371.98it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1572/3257 [00:04<00:04, 371.00it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1611/3257 [00:04<00:04, 374.73it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1649/3257 [00:04<00:04, 362.74it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1686/3257 [00:04<00:04, 359.83it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1727/3257 [00:04<00:04, 370.99it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1765/3257 [00:04<00:04, 356.15it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:05<00:04, 362.50it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1843/3257 [00:05<00:03, 365.86it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1883/3257 [00:05<00:03, 371.11it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1921/3257 [00:05<00:03, 369.42it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1967/3257 [00:05<00:03, 395.01it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2007/3257 [00:05<00:03, 385.79it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2046/3257 [00:05<00:04, 280.37it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2079/3257 [00:05<00:04, 290.39it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2117/3257 [00:06<00:03, 310.00it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2151/3257 [00:06<00:03, 311.90it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2190/3257 [00:06<00:03, 331.37it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2227/3257 [00:06<00:03, 340.27it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2264/3257 [00:06<00:02, 347.35it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2300/3257 [00:06<00:02, 350.87it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2346/3257 [00:06<00:02, 381.39it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2390/3257 [00:06<00:02, 395.34it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2430/3257 [00:06<00:02, 384.24it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2470/3257 [00:06<00:02, 387.69it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2512/3257 [00:07<00:01, 395.98it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2552/3257 [00:07<00:01, 393.42it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2592/3257 [00:07<00:01, 367.82it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2637/3257 [00:07<00:01, 390.45it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2677/3257 [00:07<00:01, 374.05it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2715/3257 [00:07<00:01, 352.29it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2757/3257 [00:07<00:01, 369.99it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2796/3257 [00:07<00:01, 373.51it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2834/3257 [00:07<00:01, 359.53it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2878/3257 [00:08<00:00, 381.87it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2917/3257 [00:08<00:00, 369.01it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2955/3257 [00:08<00:00, 358.66it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2992/3257 [00:08<00:00, 356.59it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3032/3257 [00:08<00:00, 368.81it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3074/3257 [00:08<00:00, 382.97it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3114/3257 [00:08<00:00, 386.25it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3153/3257 [00:08<00:00, 371.99it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3191/3257 [00:08<00:00, 368.06it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3228/3257 [00:09<00:00, 366.23it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 358.60it/s]
2023-02-07 12:55:58.299 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:55:58,300][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d139,n5,mc5,s0.261712,t4>', 'datetime': '2023-02-07T12:55:58.300355', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:55:58,300][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:55:58,300][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:55:58,494][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 12:55:58,494][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:55:58,499][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1660 unique words (58.89% of original 2819, drops 1159)', 'datetime': '2023-02-07T12:55:58.499689', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:55:58,499][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 2180669 word corpus (99.86% of original 2183622, drops 2953)', 'datetime': '2023-02-07T12:55:58.499941', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:55:58,505][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 12:55:58,506][gensim.models.word2vec][INFO] - sample=0.261712 downsamples 0 most-common words
[2023-02-07 12:55:58,506][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2180669 word corpus (100.0%% of prior 2180669)', 'datetime': '2023-02-07T12:55:58.506284', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:55:58,515][gensim.models.word2vec][INFO] - estimated required memory for 1660 words and 139 dimensions: 5138212 bytes
[2023-02-07 12:55:58,516][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:55:58,519][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 1660 vocabulary and 139 features, using sg=1 hs=0 sample=0.2617119893896576 negative=5 window=5 shrink_windows=True', 'datetime': '2023-02-07T12:55:58.519725', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:55:59,263][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2183926 effective words) took 0.7s, 2942237 effective words/s
[2023-02-07 12:55:59,958][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2183926 effective words) took 0.7s, 3146952 effective words/s
[2023-02-07 12:56:00,655][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2183926 effective words) took 0.7s, 3141179 effective words/s
[2023-02-07 12:56:01,355][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2183926 effective words) took 0.7s, 3125274 effective words/s
[2023-02-07 12:56:02,070][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2183926 effective words) took 0.7s, 3061437 effective words/s
[2023-02-07 12:56:02,790][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2183926 effective words) took 0.7s, 3040402 effective words/s
[2023-02-07 12:56:03,497][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2183926 effective words) took 0.7s, 3098323 effective words/s
[2023-02-07 12:56:04,223][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2183926 effective words) took 0.7s, 3013174 effective words/s
[2023-02-07 12:56:04,926][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2183926 effective words) took 0.7s, 3113614 effective words/s
[2023-02-07 12:56:05,633][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2183926 effective words) took 0.7s, 3093888 effective words/s
[2023-02-07 12:56:06,366][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2183926 effective words) took 0.7s, 2988132 effective words/s
[2023-02-07 12:56:07,112][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2183926 effective words) took 0.7s, 2930366 effective words/s
[2023-02-07 12:56:07,854][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2183926 effective words) took 0.7s, 2951879 effective words/s
[2023-02-07 12:56:08,584][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2183926 effective words) took 0.7s, 2998073 effective words/s
[2023-02-07 12:56:09,319][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2183926 effective words) took 0.7s, 2978442 effective words/s
[2023-02-07 12:56:10,055][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2183926 effective words) took 0.7s, 2970760 effective words/s
[2023-02-07 12:56:10,793][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2183926 effective words) took 0.7s, 2970400 effective words/s
[2023-02-07 12:56:11,531][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2183926 effective words) took 0.7s, 2965912 effective words/s
[2023-02-07 12:56:12,271][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2183926 effective words) took 0.7s, 2957370 effective words/s
[2023-02-07 12:56:13,012][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2183926 effective words) took 0.7s, 2953557 effective words/s
[2023-02-07 12:56:13,740][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2183926 effective words) took 0.7s, 3005717 effective words/s
[2023-02-07 12:56:14,474][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2183926 effective words) took 0.7s, 2978791 effective words/s
[2023-02-07 12:56:15,186][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2183926 effective words) took 0.7s, 3074259 effective words/s
[2023-02-07 12:56:15,866][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2183926 effective words) took 0.7s, 3218179 effective words/s
[2023-02-07 12:56:16,540][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2183926 effective words) took 0.7s, 3247113 effective words/s
[2023-02-07 12:56:17,214][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2183926 effective words) took 0.7s, 3245425 effective words/s
[2023-02-07 12:56:17,907][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2183926 effective words) took 0.7s, 3158140 effective words/s
[2023-02-07 12:56:18,607][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2183926 effective words) took 0.7s, 3123710 effective words/s
[2023-02-07 12:56:19,347][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2183926 effective words) took 0.7s, 2959118 effective words/s
[2023-02-07 12:56:20,108][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2183926 effective words) took 0.8s, 2876973 effective words/s
[2023-02-07 12:56:20,883][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2183926 effective words) took 0.8s, 2821414 effective words/s
[2023-02-07 12:56:21,647][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2183926 effective words) took 0.8s, 2864919 effective words/s
[2023-02-07 12:56:22,429][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2183926 effective words) took 0.8s, 2796355 effective words/s
[2023-02-07 12:56:23,175][gensim.models.word2vec][INFO] - EPOCH 33: training on 2183622 raw words (2183926 effective words) took 0.7s, 2934210 effective words/s
[2023-02-07 12:56:23,918][gensim.models.word2vec][INFO] - EPOCH 34: training on 2183622 raw words (2183926 effective words) took 0.7s, 2943134 effective words/s
[2023-02-07 12:56:24,656][gensim.models.word2vec][INFO] - EPOCH 35: training on 2183622 raw words (2183926 effective words) took 0.7s, 2967528 effective words/s
[2023-02-07 12:56:25,425][gensim.models.word2vec][INFO] - EPOCH 36: training on 2183622 raw words (2183926 effective words) took 0.8s, 2843712 effective words/s
[2023-02-07 12:56:26,135][gensim.models.word2vec][INFO] - EPOCH 37: training on 2183622 raw words (2183926 effective words) took 0.7s, 3083213 effective words/s
[2023-02-07 12:56:26,878][gensim.models.word2vec][INFO] - EPOCH 38: training on 2183622 raw words (2183926 effective words) took 0.7s, 2943125 effective words/s
[2023-02-07 12:56:27,607][gensim.models.word2vec][INFO] - EPOCH 39: training on 2183622 raw words (2183926 effective words) took 0.7s, 2998555 effective words/s
[2023-02-07 12:56:28,355][gensim.models.word2vec][INFO] - EPOCH 40: training on 2183622 raw words (2183926 effective words) took 0.7s, 2926465 effective words/s
[2023-02-07 12:56:29,067][gensim.models.word2vec][INFO] - EPOCH 41: training on 2183622 raw words (2183926 effective words) took 0.7s, 3072977 effective words/s
[2023-02-07 12:56:29,787][gensim.models.word2vec][INFO] - EPOCH 42: training on 2183622 raw words (2183926 effective words) took 0.7s, 3038807 effective words/s
[2023-02-07 12:56:30,509][gensim.models.word2vec][INFO] - EPOCH 43: training on 2183622 raw words (2183926 effective words) took 0.7s, 3030982 effective words/s
[2023-02-07 12:56:31,233][gensim.models.word2vec][INFO] - EPOCH 44: training on 2183622 raw words (2183926 effective words) took 0.7s, 3024467 effective words/s
[2023-02-07 12:56:31,947][gensim.models.word2vec][INFO] - EPOCH 45: training on 2183622 raw words (2183926 effective words) took 0.7s, 3061157 effective words/s
[2023-02-07 12:56:32,664][gensim.models.word2vec][INFO] - EPOCH 46: training on 2183622 raw words (2183926 effective words) took 0.7s, 3054554 effective words/s
[2023-02-07 12:56:33,385][gensim.models.word2vec][INFO] - EPOCH 47: training on 2183622 raw words (2183926 effective words) took 0.7s, 3032384 effective words/s
[2023-02-07 12:56:34,111][gensim.models.word2vec][INFO] - EPOCH 48: training on 2183622 raw words (2183926 effective words) took 0.7s, 3015001 effective words/s
[2023-02-07 12:56:34,816][gensim.models.word2vec][INFO] - EPOCH 49: training on 2183622 raw words (2183926 effective words) took 0.7s, 3100666 effective words/s
[2023-02-07 12:56:35,539][gensim.models.word2vec][INFO] - EPOCH 50: training on 2183622 raw words (2183926 effective words) took 0.7s, 3027804 effective words/s
[2023-02-07 12:56:36,254][gensim.models.word2vec][INFO] - EPOCH 51: training on 2183622 raw words (2183926 effective words) took 0.7s, 3059629 effective words/s
[2023-02-07 12:56:36,963][gensim.models.word2vec][INFO] - EPOCH 52: training on 2183622 raw words (2183926 effective words) took 0.7s, 3085792 effective words/s
[2023-02-07 12:56:37,667][gensim.models.word2vec][INFO] - EPOCH 53: training on 2183622 raw words (2183926 effective words) took 0.7s, 3109097 effective words/s
[2023-02-07 12:56:38,381][gensim.models.word2vec][INFO] - EPOCH 54: training on 2183622 raw words (2183926 effective words) took 0.7s, 3067205 effective words/s
[2023-02-07 12:56:39,083][gensim.models.word2vec][INFO] - EPOCH 55: training on 2183622 raw words (2183926 effective words) took 0.7s, 3117472 effective words/s
[2023-02-07 12:56:39,805][gensim.models.word2vec][INFO] - EPOCH 56: training on 2183622 raw words (2183926 effective words) took 0.7s, 3029528 effective words/s
[2023-02-07 12:56:40,509][gensim.models.word2vec][INFO] - EPOCH 57: training on 2183622 raw words (2183926 effective words) took 0.7s, 3106553 effective words/s
[2023-02-07 12:56:41,217][gensim.models.word2vec][INFO] - EPOCH 58: training on 2183622 raw words (2183926 effective words) took 0.7s, 3092774 effective words/s
[2023-02-07 12:56:41,928][gensim.models.word2vec][INFO] - EPOCH 59: training on 2183622 raw words (2183926 effective words) took 0.7s, 3081201 effective words/s
[2023-02-07 12:56:42,648][gensim.models.word2vec][INFO] - EPOCH 60: training on 2183622 raw words (2183926 effective words) took 0.7s, 3038058 effective words/s
[2023-02-07 12:56:43,358][gensim.models.word2vec][INFO] - EPOCH 61: training on 2183622 raw words (2183926 effective words) took 0.7s, 3081122 effective words/s
[2023-02-07 12:56:44,071][gensim.models.word2vec][INFO] - EPOCH 62: training on 2183622 raw words (2183926 effective words) took 0.7s, 3071263 effective words/s
[2023-02-07 12:56:44,789][gensim.models.word2vec][INFO] - EPOCH 63: training on 2183622 raw words (2183926 effective words) took 0.7s, 3044752 effective words/s
[2023-02-07 12:56:45,503][gensim.models.word2vec][INFO] - EPOCH 64: training on 2183622 raw words (2183926 effective words) took 0.7s, 3068202 effective words/s
[2023-02-07 12:56:46,213][gensim.models.word2vec][INFO] - EPOCH 65: training on 2183622 raw words (2183926 effective words) took 0.7s, 3081162 effective words/s
[2023-02-07 12:56:46,923][gensim.models.word2vec][INFO] - EPOCH 66: training on 2183622 raw words (2183926 effective words) took 0.7s, 3082805 effective words/s
[2023-02-07 12:56:47,635][gensim.models.word2vec][INFO] - EPOCH 67: training on 2183622 raw words (2183926 effective words) took 0.7s, 3081375 effective words/s
[2023-02-07 12:56:48,338][gensim.models.word2vec][INFO] - EPOCH 68: training on 2183622 raw words (2183926 effective words) took 0.7s, 3111014 effective words/s
[2023-02-07 12:56:49,047][gensim.models.word2vec][INFO] - EPOCH 69: training on 2183622 raw words (2183926 effective words) took 0.7s, 3090207 effective words/s
[2023-02-07 12:56:49,753][gensim.models.word2vec][INFO] - EPOCH 70: training on 2183622 raw words (2183926 effective words) took 0.7s, 3097098 effective words/s
[2023-02-07 12:56:50,476][gensim.models.word2vec][INFO] - EPOCH 71: training on 2183622 raw words (2183926 effective words) took 0.7s, 3029360 effective words/s
[2023-02-07 12:56:51,181][gensim.models.word2vec][INFO] - EPOCH 72: training on 2183622 raw words (2183926 effective words) took 0.7s, 3103374 effective words/s
[2023-02-07 12:56:51,924][gensim.models.word2vec][INFO] - EPOCH 73: training on 2183622 raw words (2183926 effective words) took 0.7s, 2947705 effective words/s
[2023-02-07 12:56:52,640][gensim.models.word2vec][INFO] - EPOCH 74: training on 2183622 raw words (2183926 effective words) took 0.7s, 3056898 effective words/s
[2023-02-07 12:56:53,344][gensim.models.word2vec][INFO] - EPOCH 75: training on 2183622 raw words (2183926 effective words) took 0.7s, 3110326 effective words/s
[2023-02-07 12:56:54,053][gensim.models.word2vec][INFO] - EPOCH 76: training on 2183622 raw words (2183926 effective words) took 0.7s, 3086535 effective words/s
[2023-02-07 12:56:54,767][gensim.models.word2vec][INFO] - EPOCH 77: training on 2183622 raw words (2183926 effective words) took 0.7s, 3064229 effective words/s
[2023-02-07 12:56:55,476][gensim.models.word2vec][INFO] - EPOCH 78: training on 2183622 raw words (2183926 effective words) took 0.7s, 3086022 effective words/s
[2023-02-07 12:56:56,190][gensim.models.word2vec][INFO] - EPOCH 79: training on 2183622 raw words (2183926 effective words) took 0.7s, 3074106 effective words/s
[2023-02-07 12:56:56,905][gensim.models.word2vec][INFO] - EPOCH 80: training on 2183622 raw words (2183926 effective words) took 0.7s, 3063001 effective words/s
[2023-02-07 12:56:57,615][gensim.models.word2vec][INFO] - EPOCH 81: training on 2183622 raw words (2183926 effective words) took 0.7s, 3080311 effective words/s
[2023-02-07 12:56:58,347][gensim.models.word2vec][INFO] - EPOCH 82: training on 2183622 raw words (2183926 effective words) took 0.7s, 2987736 effective words/s
[2023-02-07 12:56:59,066][gensim.models.word2vec][INFO] - EPOCH 83: training on 2183622 raw words (2183926 effective words) took 0.7s, 3043177 effective words/s
[2023-02-07 12:56:59,798][gensim.models.word2vec][INFO] - EPOCH 84: training on 2183622 raw words (2183926 effective words) took 0.7s, 2988038 effective words/s
[2023-02-07 12:57:00,460][gensim.models.word2vec][INFO] - EPOCH 85: training on 2183622 raw words (2183926 effective words) took 0.7s, 3304135 effective words/s
[2023-02-07 12:57:01,112][gensim.models.word2vec][INFO] - EPOCH 86: training on 2183622 raw words (2183926 effective words) took 0.7s, 3355673 effective words/s
[2023-02-07 12:57:01,771][gensim.models.word2vec][INFO] - EPOCH 87: training on 2183622 raw words (2183926 effective words) took 0.7s, 3321799 effective words/s
[2023-02-07 12:57:02,430][gensim.models.word2vec][INFO] - EPOCH 88: training on 2183622 raw words (2183926 effective words) took 0.7s, 3317511 effective words/s
[2023-02-07 12:57:03,092][gensim.models.word2vec][INFO] - EPOCH 89: training on 2183622 raw words (2183926 effective words) took 0.7s, 3308247 effective words/s
[2023-02-07 12:57:03,804][gensim.models.word2vec][INFO] - EPOCH 90: training on 2183622 raw words (2183926 effective words) took 0.7s, 3073188 effective words/s
[2023-02-07 12:57:04,547][gensim.models.word2vec][INFO] - EPOCH 91: training on 2183622 raw words (2183926 effective words) took 0.7s, 2941477 effective words/s
[2023-02-07 12:57:04,549][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 200893224 raw words (200921192 effective words) took 66.0s, 3042915 effective words/s', 'datetime': '2023-02-07T12:57:04.549144', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:57:04.549 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:57:08,091][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125541-o8cpgkd6/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:57:08.091538', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:57:08,092][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:57:08,100][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125541-o8cpgkd6/files/../tmp/embedding_model.pt
2023-02-07 12:57:08.101 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:57:09.317 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:57:09.784 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:57:10.737 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8193786466878934, 'test_mae': 1.0044411039436978, 'test_r2': 0.1393251196636235}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.038 MB of 0.038 MB uploaded (0.000 MB deduped)wandb: / 0.038 MB of 0.038 MB uploaded (0.000 MB deduped)wandb: - 0.038 MB of 0.038 MB uploaded (0.000 MB deduped)wandb: \ 0.038 MB of 0.038 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.94
wandb: percentage 0.41114
wandb:   test_mae 1.00444
wandb:   test_mse 1.81938
wandb:    test_r2 0.13933
wandb: 
wandb: üöÄ View run giddy-sweep-22 at: https://wandb.ai/xiaoqiz/mof2vec/runs/o8cpgkd6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_125541-o8cpgkd6/logs
wandb: Agent Starting Run: rpd8mobz with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 652
wandb: 	model.gensim.alpha: 0.003459830436117754
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 64
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.37879884699592625
wandb: 	model.gensim.vector_size: 98
wandb: 	model.gensim.window: 2
wandb: 	model.sklearn.learning_rate: 0.0005322256775866456
wandb: 	model.sklearn.max_depth: 13
wandb: 	model.sklearn.min_child_weight: 0.0023729165303517487
wandb: 	model.sklearn.n_estimators: 3411
wandb: 	model.sklearn.num_leaves: 340
wandb: 	model.sklearn.reg_alpha: 0.1266151881197213
wandb: 	model.sklearn.reg_lambda: 0.01863826529470295
wandb: 	model.sklearn.subsample: 0.5218523387655893
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125725-rpd8mobz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-23
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/rpd8mobz
2023-02-07 12:57:33.610 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 12:57:33.611 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 652 for sweep.
2023-02-07 12:57:33.611 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.003459830436117754 for sweep.
2023-02-07 12:57:33.611 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:57:33.612 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 64 for sweep.
2023-02-07 12:57:33.612 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 12:57:33.612 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.37879884699592625 for sweep.
2023-02-07 12:57:33.612 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 98 for sweep.
2023-02-07 12:57:33.612 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 2 for sweep.
2023-02-07 12:57:33.613 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0005322256775866456 for sweep.
2023-02-07 12:57:33.613 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 13 for sweep.
2023-02-07 12:57:33.613 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0023729165303517487 for sweep.
2023-02-07 12:57:33.613 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3411 for sweep.
2023-02-07 12:57:33.614 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 340 for sweep.
2023-02-07 12:57:33.614 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.1266151881197213 for sweep.
2023-02-07 12:57:33.614 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.01863826529470295 for sweep.
2023-02-07 12:57:33.614 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5218523387655893 for sweep.
2023-02-07 12:57:33.614 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:57:33.619 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125725-rpd8mobz/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 652, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 98, 'window': 2, 'min_count': 1, 'dm': 0, 'sample': 0.37879884699592625, 'workers': 4, 'alpha': 0.003459830436117754, 'epochs': 64}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3411, 'max_depth': 13, 'num_leaves': 340, 'reg_alpha': 0.1266151881197213, 'reg_lambda': 0.01863826529470295, 'subsample': 0.5218523387655893, 'min_child_weight': 0.0023729165303517487, 'n_jobs': 4, 'learning_rate': 0.0005322256775866456}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 27/3257 [00:00<00:12, 266.30it/s]  2%|‚ñè         | 54/3257 [00:00<00:12, 259.54it/s]  3%|‚ñé         | 84/3257 [00:00<00:11, 274.83it/s]  3%|‚ñé         | 112/3257 [00:00<00:11, 266.23it/s]  4%|‚ñç         | 139/3257 [00:00<00:17, 183.05it/s]  5%|‚ñå         | 167/3257 [00:00<00:14, 206.68it/s]  6%|‚ñå         | 197/3257 [00:00<00:13, 230.07it/s]  7%|‚ñã         | 230/3257 [00:00<00:11, 257.37it/s]  8%|‚ñä         | 260/3257 [00:01<00:11, 265.43it/s]  9%|‚ñâ         | 295/3257 [00:01<00:10, 288.61it/s] 10%|‚ñà         | 326/3257 [00:01<00:10, 291.69it/s] 11%|‚ñà         | 357/3257 [00:01<00:09, 291.82it/s] 12%|‚ñà‚ñè        | 387/3257 [00:01<00:10, 273.13it/s] 13%|‚ñà‚ñé        | 419/3257 [00:01<00:09, 285.83it/s] 14%|‚ñà‚ñç        | 449/3257 [00:01<00:10, 258.85it/s] 15%|‚ñà‚ñç        | 479/3257 [00:01<00:10, 268.16it/s] 16%|‚ñà‚ñå        | 513/3257 [00:01<00:09, 284.71it/s] 17%|‚ñà‚ñã        | 544/3257 [00:02<00:09, 290.21it/s] 18%|‚ñà‚ñä        | 574/3257 [00:02<00:10, 264.23it/s] 19%|‚ñà‚ñä        | 604/3257 [00:02<00:09, 272.68it/s] 20%|‚ñà‚ñâ        | 636/3257 [00:02<00:09, 285.00it/s] 20%|‚ñà‚ñà        | 666/3257 [00:02<00:09, 270.97it/s] 21%|‚ñà‚ñà‚ñè       | 695/3257 [00:02<00:09, 271.79it/s] 22%|‚ñà‚ñà‚ñè       | 723/3257 [00:02<00:09, 273.90it/s] 23%|‚ñà‚ñà‚ñé       | 751/3257 [00:02<00:09, 272.56it/s] 24%|‚ñà‚ñà‚ñç       | 780/3257 [00:02<00:08, 277.07it/s] 25%|‚ñà‚ñà‚ñç       | 810/3257 [00:03<00:08, 282.48it/s] 26%|‚ñà‚ñà‚ñå       | 839/3257 [00:03<00:08, 274.94it/s] 27%|‚ñà‚ñà‚ñã       | 867/3257 [00:03<00:08, 267.65it/s] 27%|‚ñà‚ñà‚ñã       | 894/3257 [00:03<00:08, 265.61it/s] 28%|‚ñà‚ñà‚ñä       | 926/3257 [00:03<00:08, 279.66it/s] 29%|‚ñà‚ñà‚ñâ       | 956/3257 [00:03<00:08, 283.58it/s] 30%|‚ñà‚ñà‚ñà       | 985/3257 [00:03<00:08, 282.20it/s] 31%|‚ñà‚ñà‚ñà       | 1014/3257 [00:03<00:08, 275.22it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1042/3257 [00:03<00:08, 267.43it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1073/3257 [00:03<00:07, 278.56it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1101/3257 [00:04<00:07, 273.89it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1129/3257 [00:04<00:07, 271.99it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1157/3257 [00:04<00:07, 273.29it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1185/3257 [00:04<00:07, 262.19it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1212/3257 [00:04<00:08, 250.60it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1243/3257 [00:04<00:07, 265.61it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:04<00:07, 268.02it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1298/3257 [00:04<00:07, 251.49it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1327/3257 [00:04<00:07, 260.53it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1354/3257 [00:05<00:07, 254.57it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1381/3257 [00:05<00:07, 255.18it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1412/3257 [00:05<00:06, 268.26it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1439/3257 [00:05<00:09, 187.95it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1473/3257 [00:05<00:08, 221.23it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1507/3257 [00:05<00:07, 248.16it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1535/3257 [00:05<00:07, 244.14it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1562/3257 [00:05<00:06, 247.56it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1591/3257 [00:06<00:06, 256.99it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1621/3257 [00:06<00:06, 265.17it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1649/3257 [00:06<00:06, 261.09it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1676/3257 [00:06<00:06, 257.74it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1705/3257 [00:06<00:05, 265.40it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1732/3257 [00:06<00:06, 253.99it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1760/3257 [00:06<00:05, 261.18it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1791/3257 [00:06<00:05, 274.50it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1819/3257 [00:06<00:05, 269.05it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1847/3257 [00:07<00:05, 265.43it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1877/3257 [00:07<00:05, 273.21it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1905/3257 [00:07<00:04, 273.64it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1934/3257 [00:07<00:04, 277.40it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1970/3257 [00:07<00:04, 301.13it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2001/3257 [00:07<00:04, 284.41it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2030/3257 [00:07<00:04, 285.77it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2059/3257 [00:07<00:04, 261.45it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2086/3257 [00:07<00:04, 261.68it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2113/3257 [00:07<00:04, 263.25it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2140/3257 [00:08<00:04, 251.61it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2168/3257 [00:08<00:04, 256.06it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2197/3257 [00:08<00:04, 264.44it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2224/3257 [00:08<00:03, 260.15it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2251/3257 [00:08<00:03, 262.51it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2278/3257 [00:08<00:03, 258.12it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2308/3257 [00:08<00:03, 268.88it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2346/3257 [00:08<00:03, 299.34it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2379/3257 [00:08<00:02, 307.21it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2410/3257 [00:09<00:02, 299.02it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2441/3257 [00:09<00:02, 290.74it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2473/3257 [00:09<00:02, 299.03it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2506/3257 [00:09<00:02, 306.84it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2539/3257 [00:09<00:02, 311.78it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2571/3257 [00:09<00:02, 288.64it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2601/3257 [00:09<00:02, 287.29it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2638/3257 [00:09<00:02, 309.16it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:09<00:01, 298.14it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2701/3257 [00:10<00:01, 280.25it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:10<00:01, 285.00it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2764/3257 [00:10<00:01, 293.47it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2795/3257 [00:10<00:01, 296.45it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2825/3257 [00:10<00:01, 286.66it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2854/3257 [00:10<00:02, 180.79it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2889/3257 [00:10<00:01, 214.89it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2917/3257 [00:10<00:01, 228.89it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2944/3257 [00:11<00:01, 235.64it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2974/3257 [00:11<00:01, 251.74it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3004/3257 [00:11<00:00, 261.81it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3036/3257 [00:11<00:00, 276.07it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3071/3257 [00:11<00:00, 295.52it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3104/3257 [00:11<00:00, 304.76it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3136/3257 [00:11<00:00, 303.81it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3167/3257 [00:11<00:00, 295.43it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3197/3257 [00:11<00:00, 296.28it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3227/3257 [00:12<00:00, 295.83it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:12<00:00, 268.87it/s]
2023-02-07 12:57:46.105 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:57:46,106][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d98,n5,s0.378799,t4>', 'datetime': '2023-02-07T12:57:46.106660', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:57:46,106][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:57:46,107][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:57:46,364][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 12:57:46,365][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:57:46,382][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 6662 unique words (100.00% of original 6662, drops 0)', 'datetime': '2023-02-07T12:57:46.382186', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:57:46,382][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 2911496 word corpus (100.00% of original 2911496, drops 0)', 'datetime': '2023-02-07T12:57:46.382543', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:57:46,404][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 12:57:46,404][gensim.models.word2vec][INFO] - sample=0.378799 downsamples 0 most-common words
[2023-02-07 12:57:46,404][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2911496 word corpus (100.0%% of prior 2911496)', 'datetime': '2023-02-07T12:57:46.404685', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:57:46,442][gensim.models.word2vec][INFO] - estimated required memory for 6662 words and 98 dimensions: 10482152 bytes
[2023-02-07 12:57:46,443][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:57:46,447][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 6662 vocabulary and 98 features, using sg=1 hs=0 sample=0.37879884699592625 negative=5 window=2 shrink_windows=True', 'datetime': '2023-02-07T12:57:46.446985', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:57:47,278][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2914753 effective words) took 0.8s, 3515906 effective words/s
[2023-02-07 12:57:48,023][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2914753 effective words) took 0.7s, 3920856 effective words/s
[2023-02-07 12:57:48,770][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2914753 effective words) took 0.7s, 3911232 effective words/s
[2023-02-07 12:57:49,525][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2914753 effective words) took 0.8s, 3867921 effective words/s
[2023-02-07 12:57:50,282][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2914753 effective words) took 0.8s, 3858077 effective words/s
[2023-02-07 12:57:50,983][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2914753 effective words) took 0.7s, 4164060 effective words/s
[2023-02-07 12:57:51,676][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2914753 effective words) took 0.7s, 4218957 effective words/s
[2023-02-07 12:57:52,370][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2914753 effective words) took 0.7s, 4207004 effective words/s
[2023-02-07 12:57:53,059][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2914753 effective words) took 0.7s, 4238618 effective words/s
[2023-02-07 12:57:53,754][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2914753 effective words) took 0.7s, 4202278 effective words/s
[2023-02-07 12:57:54,439][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2914753 effective words) took 0.7s, 4261212 effective words/s
[2023-02-07 12:57:55,128][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2914753 effective words) took 0.7s, 4240785 effective words/s
[2023-02-07 12:57:55,853][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2914753 effective words) took 0.7s, 4022350 effective words/s
[2023-02-07 12:57:56,620][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2914753 effective words) took 0.8s, 3812238 effective words/s
[2023-02-07 12:57:57,363][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2914753 effective words) took 0.7s, 3930240 effective words/s
[2023-02-07 12:57:58,112][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2914753 effective words) took 0.7s, 3901560 effective words/s
[2023-02-07 12:57:58,851][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2914753 effective words) took 0.7s, 3948593 effective words/s
[2023-02-07 12:57:59,583][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2914753 effective words) took 0.7s, 3988317 effective words/s
[2023-02-07 12:58:00,296][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2914753 effective words) took 0.7s, 4095758 effective words/s
[2023-02-07 12:58:01,029][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2914753 effective words) took 0.7s, 3984185 effective words/s
[2023-02-07 12:58:01,749][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2914753 effective words) took 0.7s, 4053085 effective words/s
[2023-02-07 12:58:02,457][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2914753 effective words) took 0.7s, 4127688 effective words/s
[2023-02-07 12:58:03,184][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2914753 effective words) took 0.7s, 4012923 effective words/s
[2023-02-07 12:58:03,917][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2914753 effective words) took 0.7s, 3982813 effective words/s
[2023-02-07 12:58:04,636][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2914753 effective words) took 0.7s, 4060553 effective words/s
[2023-02-07 12:58:05,350][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2914753 effective words) took 0.7s, 4090355 effective words/s
[2023-02-07 12:58:06,063][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2914753 effective words) took 0.7s, 4095485 effective words/s
[2023-02-07 12:58:06,772][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2914753 effective words) took 0.7s, 4118100 effective words/s
[2023-02-07 12:58:07,473][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2914753 effective words) took 0.7s, 4169278 effective words/s
[2023-02-07 12:58:08,182][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2914753 effective words) took 0.7s, 4113047 effective words/s
[2023-02-07 12:58:08,900][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2914753 effective words) took 0.7s, 4070376 effective words/s
[2023-02-07 12:58:09,600][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2914753 effective words) took 0.7s, 4170077 effective words/s
[2023-02-07 12:58:10,313][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2914753 effective words) took 0.7s, 4098666 effective words/s
[2023-02-07 12:58:11,022][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2914753 effective words) took 0.7s, 4119816 effective words/s
[2023-02-07 12:58:11,722][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2914753 effective words) took 0.7s, 4169952 effective words/s
[2023-02-07 12:58:12,428][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2914753 effective words) took 0.7s, 4133345 effective words/s
[2023-02-07 12:58:13,135][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2914753 effective words) took 0.7s, 4131871 effective words/s
[2023-02-07 12:58:13,846][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2914753 effective words) took 0.7s, 4104997 effective words/s
[2023-02-07 12:58:14,549][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2914753 effective words) took 0.7s, 4158004 effective words/s
[2023-02-07 12:58:15,251][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2914753 effective words) took 0.7s, 4158618 effective words/s
[2023-02-07 12:58:15,958][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2914753 effective words) took 0.7s, 4130268 effective words/s
[2023-02-07 12:58:16,663][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2914753 effective words) took 0.7s, 4142770 effective words/s
[2023-02-07 12:58:17,360][gensim.models.word2vec][INFO] - EPOCH 42: training on 2911496 raw words (2914753 effective words) took 0.7s, 4187231 effective words/s
[2023-02-07 12:58:18,061][gensim.models.word2vec][INFO] - EPOCH 43: training on 2911496 raw words (2914753 effective words) took 0.7s, 4170990 effective words/s
[2023-02-07 12:58:18,768][gensim.models.word2vec][INFO] - EPOCH 44: training on 2911496 raw words (2914753 effective words) took 0.7s, 4129262 effective words/s
[2023-02-07 12:58:19,478][gensim.models.word2vec][INFO] - EPOCH 45: training on 2911496 raw words (2914753 effective words) took 0.7s, 4113867 effective words/s
[2023-02-07 12:58:20,180][gensim.models.word2vec][INFO] - EPOCH 46: training on 2911496 raw words (2914753 effective words) took 0.7s, 4160526 effective words/s
[2023-02-07 12:58:20,889][gensim.models.word2vec][INFO] - EPOCH 47: training on 2911496 raw words (2914753 effective words) took 0.7s, 4116875 effective words/s
[2023-02-07 12:58:21,596][gensim.models.word2vec][INFO] - EPOCH 48: training on 2911496 raw words (2914753 effective words) took 0.7s, 4131879 effective words/s
[2023-02-07 12:58:22,294][gensim.models.word2vec][INFO] - EPOCH 49: training on 2911496 raw words (2914753 effective words) took 0.7s, 4182174 effective words/s
[2023-02-07 12:58:22,998][gensim.models.word2vec][INFO] - EPOCH 50: training on 2911496 raw words (2914753 effective words) took 0.7s, 4150224 effective words/s
[2023-02-07 12:58:23,699][gensim.models.word2vec][INFO] - EPOCH 51: training on 2911496 raw words (2914753 effective words) took 0.7s, 4168255 effective words/s
[2023-02-07 12:58:24,394][gensim.models.word2vec][INFO] - EPOCH 52: training on 2911496 raw words (2914753 effective words) took 0.7s, 4203054 effective words/s
[2023-02-07 12:58:25,095][gensim.models.word2vec][INFO] - EPOCH 53: training on 2911496 raw words (2914753 effective words) took 0.7s, 4161825 effective words/s
[2023-02-07 12:58:25,798][gensim.models.word2vec][INFO] - EPOCH 54: training on 2911496 raw words (2914753 effective words) took 0.7s, 4158110 effective words/s
[2023-02-07 12:58:26,503][gensim.models.word2vec][INFO] - EPOCH 55: training on 2911496 raw words (2914753 effective words) took 0.7s, 4144332 effective words/s
[2023-02-07 12:58:27,211][gensim.models.word2vec][INFO] - EPOCH 56: training on 2911496 raw words (2914753 effective words) took 0.7s, 4123147 effective words/s
[2023-02-07 12:58:27,919][gensim.models.word2vec][INFO] - EPOCH 57: training on 2911496 raw words (2914753 effective words) took 0.7s, 4123987 effective words/s
[2023-02-07 12:58:28,623][gensim.models.word2vec][INFO] - EPOCH 58: training on 2911496 raw words (2914753 effective words) took 0.7s, 4153748 effective words/s
[2023-02-07 12:58:29,319][gensim.models.word2vec][INFO] - EPOCH 59: training on 2911496 raw words (2914753 effective words) took 0.7s, 4195823 effective words/s
[2023-02-07 12:58:30,033][gensim.models.word2vec][INFO] - EPOCH 60: training on 2911496 raw words (2914753 effective words) took 0.7s, 4092215 effective words/s
[2023-02-07 12:58:30,738][gensim.models.word2vec][INFO] - EPOCH 61: training on 2911496 raw words (2914753 effective words) took 0.7s, 4138650 effective words/s
[2023-02-07 12:58:31,438][gensim.models.word2vec][INFO] - EPOCH 62: training on 2911496 raw words (2914753 effective words) took 0.7s, 4171242 effective words/s
[2023-02-07 12:58:32,150][gensim.models.word2vec][INFO] - EPOCH 63: training on 2911496 raw words (2914753 effective words) took 0.7s, 4101697 effective words/s
[2023-02-07 12:58:32,151][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 186335744 raw words (186544192 effective words) took 45.7s, 4081588 effective words/s', 'datetime': '2023-02-07T12:58:32.151140', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:58:32.151 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:58:35,427][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125725-rpd8mobz/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:58:35.427880', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:58:35,428][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:58:35,448][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125725-rpd8mobz/files/../tmp/embedding_model.pt
2023-02-07 12:58:35.448 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:58:36.433 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:58:36.817 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:58:37.507 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9994880658711824, 'test_mae': 1.0654710285091327, 'test_r2': 0.0541225956671858}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.96
wandb: percentage 0.0
wandb:   test_mae 1.06547
wandb:   test_mse 1.99949
wandb:    test_r2 0.05412
wandb: 
wandb: üöÄ View run fragrant-sweep-23 at: https://wandb.ai/xiaoqiz/mof2vec/runs/rpd8mobz
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_125725-rpd8mobz/logs
wandb: Agent Starting Run: 1h0lbdnc with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 121
wandb: 	model.gensim.alpha: 0.0014313933080970249
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 95
wandb: 	model.gensim.min_count: 9
wandb: 	model.gensim.sample: 0.3047780288603834
wandb: 	model.gensim.vector_size: 103
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.004514143896383836
wandb: 	model.sklearn.max_depth: 46
wandb: 	model.sklearn.min_child_weight: 0.02157545373352363
wandb: 	model.sklearn.n_estimators: 4083
wandb: 	model.sklearn.num_leaves: 439
wandb: 	model.sklearn.reg_alpha: 0.050343946850460064
wandb: 	model.sklearn.reg_lambda: 0.0031068105005251007
wandb: 	model.sklearn.subsample: 0.2265591508256427
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125845-1h0lbdnc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-24
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/1h0lbdnc
2023-02-07 12:58:52.241 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 12:58:52.241 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 121 for sweep.
2023-02-07 12:58:52.242 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0014313933080970249 for sweep.
2023-02-07 12:58:52.242 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 12:58:52.242 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 95 for sweep.
2023-02-07 12:58:52.242 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 9 for sweep.
2023-02-07 12:58:52.243 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3047780288603834 for sweep.
2023-02-07 12:58:52.243 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 103 for sweep.
2023-02-07 12:58:52.243 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 12:58:52.243 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.004514143896383836 for sweep.
2023-02-07 12:58:52.243 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 46 for sweep.
2023-02-07 12:58:52.244 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.02157545373352363 for sweep.
2023-02-07 12:58:52.244 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4083 for sweep.
2023-02-07 12:58:52.244 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 439 for sweep.
2023-02-07 12:58:52.244 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.050343946850460064 for sweep.
2023-02-07 12:58:52.245 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.0031068105005251007 for sweep.
2023-02-07 12:58:52.245 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2265591508256427 for sweep.
2023-02-07 12:58:52.245 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 12:58:52.249 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125845-1h0lbdnc/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 121, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 103, 'window': 3, 'min_count': 9, 'dm': 0, 'sample': 0.3047780288603834, 'workers': 4, 'alpha': 0.0014313933080970249, 'epochs': 95}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4083, 'max_depth': 46, 'num_leaves': 439, 'reg_alpha': 0.050343946850460064, 'reg_lambda': 0.0031068105005251007, 'subsample': 0.2265591508256427, 'min_child_weight': 0.02157545373352363, 'n_jobs': 4, 'learning_rate': 0.004514143896383836}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|‚ñè         | 42/3257 [00:00<00:07, 416.60it/s]  3%|‚ñé         | 90/3257 [00:00<00:07, 442.71it/s]  4%|‚ñç         | 135/3257 [00:00<00:07, 437.32it/s]  6%|‚ñå         | 181/3257 [00:00<00:06, 444.23it/s]  7%|‚ñã         | 232/3257 [00:00<00:06, 465.31it/s]  9%|‚ñä         | 282/3257 [00:00<00:06, 475.67it/s] 10%|‚ñà         | 331/3257 [00:00<00:06, 477.14it/s] 12%|‚ñà‚ñè        | 379/3257 [00:00<00:06, 464.28it/s] 13%|‚ñà‚ñé        | 426/3257 [00:00<00:06, 446.46it/s] 14%|‚ñà‚ñç        | 472/3257 [00:01<00:06, 448.11it/s] 16%|‚ñà‚ñå        | 518/3257 [00:01<00:06, 450.39it/s] 17%|‚ñà‚ñã        | 564/3257 [00:01<00:06, 434.87it/s] 19%|‚ñà‚ñä        | 609/3257 [00:01<00:06, 438.49it/s] 20%|‚ñà‚ñà        | 653/3257 [00:01<00:05, 436.93it/s] 21%|‚ñà‚ñà‚ñè       | 697/3257 [00:01<00:05, 429.51it/s] 23%|‚ñà‚ñà‚ñé       | 741/3257 [00:01<00:05, 426.00it/s] 24%|‚ñà‚ñà‚ñç       | 786/3257 [00:01<00:05, 431.81it/s] 25%|‚ñà‚ñà‚ñå       | 830/3257 [00:01<00:05, 430.98it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:01<00:05, 424.53it/s] 28%|‚ñà‚ñà‚ñä       | 917/3257 [00:02<00:08, 285.66it/s] 30%|‚ñà‚ñà‚ñâ       | 964/3257 [00:02<00:07, 325.14it/s] 31%|‚ñà‚ñà‚ñà       | 1006/3257 [00:02<00:06, 347.41it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1046/3257 [00:02<00:06, 355.80it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1090/3257 [00:02<00:05, 376.05it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1132/3257 [00:02<00:05, 386.67it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1176/3257 [00:02<00:05, 400.57it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1218/3257 [00:02<00:05, 391.05it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1265/3257 [00:03<00:04, 412.72it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1308/3257 [00:03<00:04, 404.96it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1353/3257 [00:03<00:04, 416.74it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1396/3257 [00:03<00:04, 418.60it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1443/3257 [00:03<00:04, 430.46it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1491/3257 [00:03<00:03, 442.92it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1536/3257 [00:03<00:03, 437.19it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1580/3257 [00:03<00:03, 436.78it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1630/3257 [00:03<00:03, 452.27it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1676/3257 [00:04<00:03, 430.88it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1721/3257 [00:04<00:03, 434.95it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1765/3257 [00:04<00:03, 427.67it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1808/3257 [00:04<00:03, 422.22it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1851/3257 [00:04<00:03, 419.95it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1897/3257 [00:04<00:03, 428.01it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1941/3257 [00:04<00:03, 430.61it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1987/3257 [00:04<00:02, 437.08it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2034/3257 [00:04<00:02, 446.26it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2079/3257 [00:05<00:03, 299.31it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2121/3257 [00:05<00:03, 323.24it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2168/3257 [00:05<00:03, 355.74it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2211/3257 [00:05<00:02, 372.60it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2257/3257 [00:05<00:02, 394.52it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2301/3257 [00:05<00:02, 403.83it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2358/3257 [00:05<00:01, 449.65it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:05<00:01, 449.47it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2452/3257 [00:05<00:01, 441.82it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2504/3257 [00:06<00:01, 463.36it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2552/3257 [00:06<00:01, 467.77it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2600/3257 [00:06<00:01, 448.55it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2650/3257 [00:06<00:01, 460.45it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2697/3257 [00:06<00:01, 452.93it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2743/3257 [00:06<00:01, 447.43it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2790/3257 [00:06<00:01, 453.22it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2836/3257 [00:06<00:00, 442.67it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2888/3257 [00:06<00:00, 464.06it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2935/3257 [00:06<00:00, 455.81it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2981/3257 [00:07<00:00, 442.62it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3029/3257 [00:07<00:00, 451.30it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3081/3257 [00:07<00:00, 467.99it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3130/3257 [00:07<00:00, 471.21it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3178/3257 [00:07<00:00, 449.90it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3226/3257 [00:07<00:00, 456.80it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:07<00:00, 423.56it/s]
2023-02-07 12:59:00.132 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 12:59:00,133][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d103,n5,mc9,s0.304778,t4>', 'datetime': '2023-02-07T12:59:00.133753', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 12:59:00,134][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 12:59:00,134][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 12:59:00,268][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 12:59:00,268][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 12:59:00,270][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=9 retains 462 unique words (50.00% of original 924, drops 462)', 'datetime': '2023-02-07T12:59:00.270310', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:59:00,270][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=9 leaves 1453902 word corpus (99.87% of original 1455748, drops 1846)', 'datetime': '2023-02-07T12:59:00.270470', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:59:00,273][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 12:59:00,273][gensim.models.word2vec][INFO] - sample=0.304778 downsamples 0 most-common words
[2023-02-07 12:59:00,273][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1453902 word corpus (100.0%% of prior 1453902)', 'datetime': '2023-02-07T12:59:00.273338', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 12:59:00,276][gensim.models.word2vec][INFO] - estimated required memory for 462 words and 103 dimensions: 2604972 bytes
[2023-02-07 12:59:00,276][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 12:59:00,278][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 462 vocabulary and 103 features, using sg=1 hs=0 sample=0.3047780288603834 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T12:59:00.278152', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 12:59:00,890][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1457159 effective words) took 0.6s, 2388502 effective words/s
[2023-02-07 12:59:01,497][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1457159 effective words) took 0.6s, 2403602 effective words/s
[2023-02-07 12:59:02,065][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1457159 effective words) took 0.6s, 2570861 effective words/s
[2023-02-07 12:59:02,554][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1457159 effective words) took 0.5s, 2987818 effective words/s
[2023-02-07 12:59:03,148][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1457159 effective words) took 0.6s, 2461092 effective words/s
[2023-02-07 12:59:03,770][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1457159 effective words) took 0.6s, 2345681 effective words/s
[2023-02-07 12:59:04,358][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1457159 effective words) took 0.6s, 2487718 effective words/s
[2023-02-07 12:59:05,049][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1457159 effective words) took 0.7s, 2110942 effective words/s
[2023-02-07 12:59:05,773][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1457159 effective words) took 0.7s, 2019078 effective words/s
[2023-02-07 12:59:06,369][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1457159 effective words) took 0.6s, 2450273 effective words/s
[2023-02-07 12:59:06,906][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1457159 effective words) took 0.5s, 2720437 effective words/s
[2023-02-07 12:59:07,444][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1457159 effective words) took 0.5s, 2713247 effective words/s
[2023-02-07 12:59:07,990][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1457159 effective words) took 0.5s, 2675419 effective words/s
[2023-02-07 12:59:08,558][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1457159 effective words) took 0.6s, 2572112 effective words/s
[2023-02-07 12:59:09,182][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1457159 effective words) took 0.6s, 2339061 effective words/s
[2023-02-07 12:59:09,737][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1457159 effective words) took 0.6s, 2633004 effective words/s
[2023-02-07 12:59:10,261][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1457159 effective words) took 0.5s, 2792514 effective words/s
[2023-02-07 12:59:10,785][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1457159 effective words) took 0.5s, 2790572 effective words/s
[2023-02-07 12:59:11,310][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1457159 effective words) took 0.5s, 2780279 effective words/s
[2023-02-07 12:59:11,868][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1457159 effective words) took 0.6s, 2620019 effective words/s
[2023-02-07 12:59:12,411][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1457159 effective words) took 0.5s, 2690173 effective words/s
[2023-02-07 12:59:12,973][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1457159 effective words) took 0.6s, 2599875 effective words/s
[2023-02-07 12:59:13,559][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1457159 effective words) took 0.6s, 2491591 effective words/s
[2023-02-07 12:59:14,181][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1457159 effective words) took 0.6s, 2345550 effective words/s
[2023-02-07 12:59:14,841][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1457159 effective words) took 0.7s, 2213086 effective words/s
[2023-02-07 12:59:15,460][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1457159 effective words) took 0.6s, 2363161 effective words/s
[2023-02-07 12:59:16,035][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1457159 effective words) took 0.6s, 2540753 effective words/s
[2023-02-07 12:59:16,578][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1457159 effective words) took 0.5s, 2690629 effective words/s
[2023-02-07 12:59:17,135][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1457159 effective words) took 0.6s, 2622586 effective words/s
[2023-02-07 12:59:17,658][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1457159 effective words) took 0.5s, 2791978 effective words/s
[2023-02-07 12:59:18,187][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1457159 effective words) took 0.5s, 2762371 effective words/s
[2023-02-07 12:59:18,715][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1457159 effective words) took 0.5s, 2765565 effective words/s
[2023-02-07 12:59:19,243][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1457159 effective words) took 0.5s, 2763561 effective words/s
[2023-02-07 12:59:19,780][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1457159 effective words) took 0.5s, 2720055 effective words/s
[2023-02-07 12:59:20,328][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1457159 effective words) took 0.5s, 2664874 effective words/s
[2023-02-07 12:59:20,899][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1457159 effective words) took 0.6s, 2558413 effective words/s
[2023-02-07 12:59:21,456][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1457159 effective words) took 0.6s, 2626732 effective words/s
[2023-02-07 12:59:22,019][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1457159 effective words) took 0.6s, 2593812 effective words/s
[2023-02-07 12:59:22,545][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1457159 effective words) took 0.5s, 2779964 effective words/s
[2023-02-07 12:59:23,073][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1457159 effective words) took 0.5s, 2763584 effective words/s
[2023-02-07 12:59:23,611][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1457159 effective words) took 0.5s, 2714747 effective words/s
[2023-02-07 12:59:24,161][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1457159 effective words) took 0.5s, 2660596 effective words/s
[2023-02-07 12:59:24,687][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1457159 effective words) took 0.5s, 2777306 effective words/s
[2023-02-07 12:59:25,219][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1457159 effective words) took 0.5s, 2748283 effective words/s
[2023-02-07 12:59:25,777][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1457159 effective words) took 0.6s, 2615482 effective words/s
[2023-02-07 12:59:26,372][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1457159 effective words) took 0.6s, 2456089 effective words/s
[2023-02-07 12:59:26,926][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1457159 effective words) took 0.6s, 2634836 effective words/s
[2023-02-07 12:59:27,454][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1457159 effective words) took 0.5s, 2767008 effective words/s
[2023-02-07 12:59:27,991][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1457159 effective words) took 0.5s, 2723920 effective words/s
[2023-02-07 12:59:28,536][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1457159 effective words) took 0.5s, 2676298 effective words/s
[2023-02-07 12:59:29,085][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1457159 effective words) took 0.5s, 2662073 effective words/s
[2023-02-07 12:59:29,627][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1457159 effective words) took 0.5s, 2701582 effective words/s
[2023-02-07 12:59:30,153][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1457159 effective words) took 0.5s, 2773684 effective words/s
[2023-02-07 12:59:30,676][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1457159 effective words) took 0.5s, 2796625 effective words/s
[2023-02-07 12:59:31,203][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1457159 effective words) took 0.5s, 2769323 effective words/s
[2023-02-07 12:59:31,723][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1457159 effective words) took 0.5s, 2809120 effective words/s
[2023-02-07 12:59:32,262][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1457159 effective words) took 0.5s, 2709620 effective words/s
[2023-02-07 12:59:32,817][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1457159 effective words) took 0.6s, 2633224 effective words/s
[2023-02-07 12:59:33,379][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1457159 effective words) took 0.6s, 2598598 effective words/s
[2023-02-07 12:59:33,958][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1457159 effective words) took 0.6s, 2524376 effective words/s
[2023-02-07 12:59:34,553][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1457159 effective words) took 0.6s, 2455498 effective words/s
[2023-02-07 12:59:35,077][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1457159 effective words) took 0.5s, 2787665 effective words/s
[2023-02-07 12:59:35,628][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1457159 effective words) took 0.6s, 2648624 effective words/s
[2023-02-07 12:59:36,240][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1457159 effective words) took 0.6s, 2391037 effective words/s
[2023-02-07 12:59:36,838][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1457159 effective words) took 0.6s, 2439090 effective words/s
[2023-02-07 12:59:37,405][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1457159 effective words) took 0.6s, 2580262 effective words/s
[2023-02-07 12:59:37,936][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1457159 effective words) took 0.5s, 2748961 effective words/s
[2023-02-07 12:59:38,472][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1457159 effective words) took 0.5s, 2728418 effective words/s
[2023-02-07 12:59:39,009][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1457159 effective words) took 0.5s, 2717936 effective words/s
[2023-02-07 12:59:39,537][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1457159 effective words) took 0.5s, 2766567 effective words/s
[2023-02-07 12:59:40,070][gensim.models.word2vec][INFO] - EPOCH 70: training on 1455748 raw words (1457159 effective words) took 0.5s, 2745772 effective words/s
[2023-02-07 12:59:40,609][gensim.models.word2vec][INFO] - EPOCH 71: training on 1455748 raw words (1457159 effective words) took 0.5s, 2711310 effective words/s
[2023-02-07 12:59:41,156][gensim.models.word2vec][INFO] - EPOCH 72: training on 1455748 raw words (1457159 effective words) took 0.5s, 2671822 effective words/s
[2023-02-07 12:59:41,682][gensim.models.word2vec][INFO] - EPOCH 73: training on 1455748 raw words (1457159 effective words) took 0.5s, 2776398 effective words/s
[2023-02-07 12:59:42,203][gensim.models.word2vec][INFO] - EPOCH 74: training on 1455748 raw words (1457159 effective words) took 0.5s, 2806388 effective words/s
[2023-02-07 12:59:42,723][gensim.models.word2vec][INFO] - EPOCH 75: training on 1455748 raw words (1457159 effective words) took 0.5s, 2807162 effective words/s
[2023-02-07 12:59:43,271][gensim.models.word2vec][INFO] - EPOCH 76: training on 1455748 raw words (1457159 effective words) took 0.5s, 2669396 effective words/s
[2023-02-07 12:59:43,793][gensim.models.word2vec][INFO] - EPOCH 77: training on 1455748 raw words (1457159 effective words) took 0.5s, 2798953 effective words/s
[2023-02-07 12:59:44,325][gensim.models.word2vec][INFO] - EPOCH 78: training on 1455748 raw words (1457159 effective words) took 0.5s, 2748059 effective words/s
[2023-02-07 12:59:44,852][gensim.models.word2vec][INFO] - EPOCH 79: training on 1455748 raw words (1457159 effective words) took 0.5s, 2772340 effective words/s
[2023-02-07 12:59:45,372][gensim.models.word2vec][INFO] - EPOCH 80: training on 1455748 raw words (1457159 effective words) took 0.5s, 2809036 effective words/s
[2023-02-07 12:59:45,900][gensim.models.word2vec][INFO] - EPOCH 81: training on 1455748 raw words (1457159 effective words) took 0.5s, 2762438 effective words/s
[2023-02-07 12:59:46,427][gensim.models.word2vec][INFO] - EPOCH 82: training on 1455748 raw words (1457159 effective words) took 0.5s, 2773575 effective words/s
[2023-02-07 12:59:46,950][gensim.models.word2vec][INFO] - EPOCH 83: training on 1455748 raw words (1457159 effective words) took 0.5s, 2794341 effective words/s
[2023-02-07 12:59:47,471][gensim.models.word2vec][INFO] - EPOCH 84: training on 1455748 raw words (1457159 effective words) took 0.5s, 2801768 effective words/s
[2023-02-07 12:59:47,993][gensim.models.word2vec][INFO] - EPOCH 85: training on 1455748 raw words (1457159 effective words) took 0.5s, 2801206 effective words/s
[2023-02-07 12:59:48,552][gensim.models.word2vec][INFO] - EPOCH 86: training on 1455748 raw words (1457159 effective words) took 0.6s, 2611215 effective words/s
[2023-02-07 12:59:49,117][gensim.models.word2vec][INFO] - EPOCH 87: training on 1455748 raw words (1457159 effective words) took 0.6s, 2581084 effective words/s
[2023-02-07 12:59:49,681][gensim.models.word2vec][INFO] - EPOCH 88: training on 1455748 raw words (1457159 effective words) took 0.6s, 2592108 effective words/s
[2023-02-07 12:59:50,224][gensim.models.word2vec][INFO] - EPOCH 89: training on 1455748 raw words (1457159 effective words) took 0.5s, 2690649 effective words/s
[2023-02-07 12:59:50,794][gensim.models.word2vec][INFO] - EPOCH 90: training on 1455748 raw words (1457159 effective words) took 0.6s, 2561595 effective words/s
[2023-02-07 12:59:51,329][gensim.models.word2vec][INFO] - EPOCH 91: training on 1455748 raw words (1457159 effective words) took 0.5s, 2734967 effective words/s
[2023-02-07 12:59:51,845][gensim.models.word2vec][INFO] - EPOCH 92: training on 1455748 raw words (1457159 effective words) took 0.5s, 2836536 effective words/s
[2023-02-07 12:59:52,362][gensim.models.word2vec][INFO] - EPOCH 93: training on 1455748 raw words (1457159 effective words) took 0.5s, 2825152 effective words/s
[2023-02-07 12:59:52,877][gensim.models.word2vec][INFO] - EPOCH 94: training on 1455748 raw words (1457159 effective words) took 0.5s, 2836092 effective words/s
[2023-02-07 12:59:52,877][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 138296060 raw words (138430105 effective words) took 52.6s, 2631807 effective words/s', 'datetime': '2023-02-07T12:59:52.877799', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 12:59:52.877 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 12:59:54,955][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125845-1h0lbdnc/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T12:59:54.955666', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 12:59:54,956][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 12:59:54,960][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_125845-1h0lbdnc/files/../tmp/embedding_model.pt
2023-02-07 12:59:54.961 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 12:59:56.018 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 12:59:56.439 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 12:59:57.171 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0480014929414336, 'test_mae': 1.0891917004047156, 'test_r2': 0.031172844050386228}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.46
wandb: percentage 0.5
wandb:   test_mae 1.08919
wandb:   test_mse 2.048
wandb:    test_r2 0.03117
wandb: 
wandb: üöÄ View run fancy-sweep-24 at: https://wandb.ai/xiaoqiz/mof2vec/runs/1h0lbdnc
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_125845-1h0lbdnc/logs
wandb: Agent Starting Run: aa8qya0t with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 411
wandb: 	model.gensim.alpha: 0.0008023409575617422
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.epochs: 67
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.2157385869224398
wandb: 	model.gensim.vector_size: 68
wandb: 	model.gensim.window: 20
wandb: 	model.sklearn.learning_rate: 0.004664497387516905
wandb: 	model.sklearn.max_depth: 40
wandb: 	model.sklearn.min_child_weight: 0.00948659597858413
wandb: 	model.sklearn.n_estimators: 1868
wandb: 	model.sklearn.num_leaves: 401
wandb: 	model.sklearn.reg_alpha: 0.12348587421715156
wandb: 	model.sklearn.reg_lambda: 0.002593870980079714
wandb: 	model.sklearn.subsample: 0.46218176831788976
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130009-aa8qya0t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-25
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/aa8qya0t
2023-02-07 13:00:17.226 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 13:00:17.227 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 411 for sweep.
2023-02-07 13:00:17.227 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0008023409575617422 for sweep.
2023-02-07 13:00:17.228 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 13:00:17.228 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 67 for sweep.
2023-02-07 13:00:17.228 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 13:00:17.228 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2157385869224398 for sweep.
2023-02-07 13:00:17.228 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 68 for sweep.
2023-02-07 13:00:17.229 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 20 for sweep.
2023-02-07 13:00:17.229 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.004664497387516905 for sweep.
2023-02-07 13:00:17.229 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 40 for sweep.
2023-02-07 13:00:17.229 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.00948659597858413 for sweep.
2023-02-07 13:00:17.230 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1868 for sweep.
2023-02-07 13:00:17.230 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 401 for sweep.
2023-02-07 13:00:17.230 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.12348587421715156 for sweep.
2023-02-07 13:00:17.230 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.002593870980079714 for sweep.
2023-02-07 13:00:17.230 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.46218176831788976 for sweep.
2023-02-07 13:00:17.231 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:00:17.236 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130009-aa8qya0t/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 411, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 68, 'window': 20, 'min_count': 1, 'dm': 1, 'sample': 0.2157385869224398, 'workers': 4, 'alpha': 0.0008023409575617422, 'epochs': 67}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1868, 'max_depth': 40, 'num_leaves': 401, 'reg_alpha': 0.12348587421715156, 'reg_lambda': 0.002593870980079714, 'subsample': 0.46218176831788976, 'min_child_weight': 0.00948659597858413, 'n_jobs': 4, 'learning_rate': 0.004664497387516905}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 24/3257 [00:00<00:13, 235.61it/s]  2%|‚ñè         | 50/3257 [00:00<00:13, 237.22it/s]  2%|‚ñè         | 76/3257 [00:00<00:13, 244.31it/s]  3%|‚ñé         | 102/3257 [00:00<00:12, 249.51it/s]  4%|‚ñç         | 127/3257 [00:00<00:13, 239.23it/s]  5%|‚ñç         | 155/3257 [00:00<00:12, 251.89it/s]  6%|‚ñå         | 181/3257 [00:00<00:12, 241.43it/s]  6%|‚ñã         | 208/3257 [00:00<00:12, 250.00it/s]  7%|‚ñã         | 235/3257 [00:01<00:16, 184.16it/s]  8%|‚ñä         | 259/3257 [00:01<00:15, 194.65it/s]  9%|‚ñâ         | 290/3257 [00:01<00:13, 221.79it/s] 10%|‚ñâ         | 315/3257 [00:01<00:13, 224.83it/s] 11%|‚ñà         | 342/3257 [00:01<00:12, 235.10it/s] 11%|‚ñà‚ñè        | 370/3257 [00:01<00:11, 247.36it/s] 12%|‚ñà‚ñè        | 396/3257 [00:01<00:12, 236.16it/s] 13%|‚ñà‚ñé        | 422/3257 [00:01<00:11, 242.02it/s] 14%|‚ñà‚ñé        | 447/3257 [00:01<00:12, 221.73it/s] 15%|‚ñà‚ñç        | 475/3257 [00:02<00:11, 234.99it/s] 15%|‚ñà‚ñå        | 502/3257 [00:02<00:11, 244.42it/s] 16%|‚ñà‚ñå        | 527/3257 [00:02<00:11, 243.49it/s] 17%|‚ñà‚ñã        | 555/3257 [00:02<00:10, 251.75it/s] 18%|‚ñà‚ñä        | 581/3257 [00:02<00:11, 232.37it/s] 19%|‚ñà‚ñâ        | 611/3257 [00:02<00:10, 248.49it/s] 20%|‚ñà‚ñâ        | 638/3257 [00:02<00:10, 252.53it/s] 20%|‚ñà‚ñà        | 664/3257 [00:02<00:11, 231.64it/s] 21%|‚ñà‚ñà        | 688/3257 [00:02<00:11, 233.07it/s] 22%|‚ñà‚ñà‚ñè       | 717/3257 [00:03<00:10, 245.45it/s] 23%|‚ñà‚ñà‚ñé       | 742/3257 [00:03<00:10, 230.37it/s] 24%|‚ñà‚ñà‚ñé       | 770/3257 [00:03<00:10, 242.18it/s] 24%|‚ñà‚ñà‚ñç       | 795/3257 [00:03<00:10, 239.04it/s] 25%|‚ñà‚ñà‚ñå       | 820/3257 [00:03<00:10, 238.55it/s] 26%|‚ñà‚ñà‚ñå       | 845/3257 [00:03<00:10, 221.88it/s] 27%|‚ñà‚ñà‚ñã       | 868/3257 [00:03<00:10, 223.95it/s] 27%|‚ñà‚ñà‚ñã       | 892/3257 [00:03<00:10, 227.78it/s] 28%|‚ñà‚ñà‚ñä       | 919/3257 [00:03<00:09, 239.21it/s] 29%|‚ñà‚ñà‚ñâ       | 944/3257 [00:04<00:09, 242.30it/s] 30%|‚ñà‚ñà‚ñâ       | 970/3257 [00:04<00:09, 246.06it/s] 31%|‚ñà‚ñà‚ñà       | 995/3257 [00:04<00:09, 238.10it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1022/3257 [00:04<00:09, 246.79it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:04<00:09, 230.87it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1074/3257 [00:04<00:09, 241.08it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1099/3257 [00:04<00:09, 233.15it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1124/3257 [00:04<00:09, 234.45it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1148/3257 [00:04<00:09, 227.72it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1175/3257 [00:04<00:08, 238.85it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1200/3257 [00:05<00:09, 221.87it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1224/3257 [00:05<00:08, 226.17it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:05<00:08, 231.81it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:05<00:08, 233.67it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1299/3257 [00:05<00:08, 227.54it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1325/3257 [00:05<00:08, 235.84it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:05<00:08, 233.31it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1374/3257 [00:05<00:08, 231.65it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1398/3257 [00:06<00:11, 155.87it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1429/3257 [00:06<00:09, 187.84it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1458/3257 [00:06<00:08, 211.34it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1484/3257 [00:06<00:07, 222.94it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1514/3257 [00:06<00:07, 242.55it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1541/3257 [00:06<00:07, 233.24it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1566/3257 [00:06<00:07, 233.22it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1593/3257 [00:06<00:06, 242.52it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1621/3257 [00:06<00:06, 251.96it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1647/3257 [00:07<00:06, 251.59it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1673/3257 [00:07<00:06, 249.90it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1701/3257 [00:07<00:06, 257.74it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:07<00:06, 254.55it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1755/3257 [00:07<00:05, 257.41it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1788/3257 [00:07<00:05, 276.43it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1816/3257 [00:07<00:05, 270.85it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1844/3257 [00:07<00:05, 273.21it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1874/3257 [00:07<00:04, 280.48it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1903/3257 [00:08<00:04, 279.76it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1932/3257 [00:08<00:04, 280.63it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1967/3257 [00:08<00:04, 300.79it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1998/3257 [00:08<00:04, 292.54it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2029/3257 [00:08<00:04, 296.10it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2059/3257 [00:08<00:04, 267.22it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2087/3257 [00:08<00:04, 268.62it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2117/3257 [00:08<00:04, 272.04it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2145/3257 [00:08<00:04, 254.22it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2177/3257 [00:09<00:03, 271.42it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2205/3257 [00:09<00:03, 270.87it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2233/3257 [00:09<00:03, 270.05it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2261/3257 [00:09<00:03, 264.77it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2289/3257 [00:09<00:03, 269.05it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2318/3257 [00:09<00:03, 274.25it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2355/3257 [00:09<00:02, 301.22it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2386/3257 [00:09<00:02, 298.83it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2416/3257 [00:09<00:02, 291.58it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2446/3257 [00:09<00:02, 282.37it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2476/3257 [00:10<00:02, 285.64it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2509/3257 [00:10<00:02, 296.18it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2540/3257 [00:10<00:02, 299.41it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2571/3257 [00:10<00:02, 281.24it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2600/3257 [00:10<00:02, 280.22it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2637/3257 [00:10<00:02, 303.32it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2668/3257 [00:10<00:02, 290.58it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2698/3257 [00:10<00:02, 272.05it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2726/3257 [00:10<00:01, 272.51it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2759/3257 [00:11<00:01, 287.28it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2789/3257 [00:11<00:01, 289.20it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2819/3257 [00:11<00:02, 180.02it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:11<00:02, 193.12it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2883/3257 [00:11<00:01, 236.57it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2912/3257 [00:11<00:01, 241.75it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2942/3257 [00:11<00:01, 255.21it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2970/3257 [00:12<00:01, 253.13it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2999/3257 [00:12<00:00, 261.82it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3027/3257 [00:12<00:00, 266.23it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3063/3257 [00:12<00:00, 289.46it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3093/3257 [00:12<00:00, 290.54it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3127/3257 [00:12<00:00, 302.66it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3158/3257 [00:12<00:00, 288.31it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3188/3257 [00:12<00:00, 283.07it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3217/3257 [00:12<00:00, 279.39it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:12<00:00, 291.16it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:12<00:00, 250.92it/s]
2023-02-07 13:00:30.676 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:00:30,678][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d68,n5,w20,s0.215739,t4>', 'datetime': '2023-02-07T13:00:30.678201', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:00:30,678][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:00:30,678][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:00:30,981][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 13:00:30,981][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:00:31,011][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 13061 unique words (100.00% of original 13061, drops 0)', 'datetime': '2023-02-07T13:00:31.011166', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:00:31,011][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 3639370 word corpus (100.00% of original 3639370, drops 0)', 'datetime': '2023-02-07T13:00:31.011453', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:00:31,052][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 13:00:31,053][gensim.models.word2vec][INFO] - sample=0.215739 downsamples 0 most-common words
[2023-02-07 13:00:31,053][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3639370 word corpus (100.0%% of prior 3639370)', 'datetime': '2023-02-07T13:00:31.053243', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:00:31,125][gensim.models.word2vec][INFO] - estimated required memory for 13061 words and 68 dimensions: 15172988 bytes
[2023-02-07 13:00:31,125][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:00:31,130][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 13061 vocabulary and 68 features, using sg=0 hs=0 sample=0.2157385869224398 negative=5 window=20 shrink_windows=True', 'datetime': '2023-02-07T13:00:31.130234', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:00:32,134][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 57.35% examples, 2131358 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:32,811][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3642627 effective words) took 1.7s, 2169155 effective words/s
[2023-02-07 13:00:33,814][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 59.20% examples, 2202368 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:34,453][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3642627 effective words) took 1.6s, 2220300 effective words/s
[2023-02-07 13:00:35,456][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 59.20% examples, 2202604 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:36,102][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3642627 effective words) took 1.6s, 2209799 effective words/s
[2023-02-07 13:00:37,106][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 59.66% examples, 2211219 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:37,739][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3642627 effective words) took 1.6s, 2227102 effective words/s
[2023-02-07 13:00:38,742][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 60.58% examples, 2240100 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:39,366][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3642627 effective words) took 1.6s, 2241869 effective words/s
[2023-02-07 13:00:40,372][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 59.20% examples, 2195414 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:41,022][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3642627 effective words) took 1.7s, 2201898 effective words/s
[2023-02-07 13:00:42,025][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 59.20% examples, 2203314 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:42,652][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3642627 effective words) took 1.6s, 2236287 effective words/s
[2023-02-07 13:00:43,655][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 60.06% examples, 2221792 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:44,299][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3642627 effective words) took 1.6s, 2213488 effective words/s
[2023-02-07 13:00:45,304][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 59.90% examples, 2215135 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:45,936][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3642627 effective words) took 1.6s, 2227270 effective words/s
[2023-02-07 13:00:46,943][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 60.58% examples, 2230225 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:47,579][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3642627 effective words) took 1.6s, 2219328 effective words/s
[2023-02-07 13:00:48,585][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 57.57% examples, 2135219 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:49,292][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3642627 effective words) took 1.7s, 2127232 effective words/s
[2023-02-07 13:00:50,295][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 55.14% examples, 2055882 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:51,057][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3642627 effective words) took 1.8s, 2065123 effective words/s
[2023-02-07 13:00:52,060][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 55.14% examples, 2057201 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:52,826][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3642627 effective words) took 1.8s, 2061752 effective words/s
[2023-02-07 13:00:53,831][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 58.00% examples, 2155596 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:54,539][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3642627 effective words) took 1.7s, 2128296 effective words/s
[2023-02-07 13:00:55,541][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 56.03% examples, 2091737 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:56,293][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3642627 effective words) took 1.8s, 2078504 effective words/s
[2023-02-07 13:00:57,296][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 55.76% examples, 2073749 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:58,017][gensim.models.word2vec][INFO] - EPOCH 15: training on 3639370 raw words (3642627 effective words) took 1.7s, 2115039 effective words/s
[2023-02-07 13:00:59,022][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 55.39% examples, 2058898 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:00:59,781][gensim.models.word2vec][INFO] - EPOCH 16: training on 3639370 raw words (3642627 effective words) took 1.8s, 2067405 effective words/s
[2023-02-07 13:01:00,787][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 57.35% examples, 2126075 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:01,483][gensim.models.word2vec][INFO] - EPOCH 17: training on 3639370 raw words (3642627 effective words) took 1.7s, 2141209 effective words/s
[2023-02-07 13:01:02,488][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 57.57% examples, 2139442 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:03,187][gensim.models.word2vec][INFO] - EPOCH 18: training on 3639370 raw words (3642627 effective words) took 1.7s, 2140905 effective words/s
[2023-02-07 13:01:04,189][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 56.80% examples, 2117297 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:04,925][gensim.models.word2vec][INFO] - EPOCH 19: training on 3639370 raw words (3642627 effective words) took 1.7s, 2097618 effective words/s
[2023-02-07 13:01:05,932][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 58.24% examples, 2159707 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:06,601][gensim.models.word2vec][INFO] - EPOCH 20: training on 3639370 raw words (3642627 effective words) took 1.7s, 2174541 effective words/s
[2023-02-07 13:01:07,609][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 57.57% examples, 2131857 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:08,306][gensim.models.word2vec][INFO] - EPOCH 21: training on 3639370 raw words (3642627 effective words) took 1.7s, 2138051 effective words/s
[2023-02-07 13:01:09,313][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 57.02% examples, 2116531 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:10,029][gensim.models.word2vec][INFO] - EPOCH 22: training on 3639370 raw words (3642627 effective words) took 1.7s, 2115880 effective words/s
[2023-02-07 13:01:11,035][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 56.03% examples, 2084719 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:11,756][gensim.models.word2vec][INFO] - EPOCH 23: training on 3639370 raw words (3642627 effective words) took 1.7s, 2111452 effective words/s
[2023-02-07 13:01:12,758][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 58.89% examples, 2196147 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:13,415][gensim.models.word2vec][INFO] - EPOCH 24: training on 3639370 raw words (3642627 effective words) took 1.7s, 2198620 effective words/s
[2023-02-07 13:01:14,419][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 57.35% examples, 2130223 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:15,123][gensim.models.word2vec][INFO] - EPOCH 25: training on 3639370 raw words (3642627 effective words) took 1.7s, 2133909 effective words/s
[2023-02-07 13:01:16,124][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 56.43% examples, 2109150 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:16,819][gensim.models.word2vec][INFO] - EPOCH 26: training on 3639370 raw words (3642627 effective words) took 1.7s, 2148604 effective words/s
[2023-02-07 13:01:17,822][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 57.57% examples, 2142396 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:18,507][gensim.models.word2vec][INFO] - EPOCH 27: training on 3639370 raw words (3642627 effective words) took 1.7s, 2160043 effective words/s
[2023-02-07 13:01:19,510][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 58.80% examples, 2184234 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:20,187][gensim.models.word2vec][INFO] - EPOCH 28: training on 3639370 raw words (3642627 effective words) took 1.7s, 2169853 effective words/s
[2023-02-07 13:01:21,191][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 56.43% examples, 2104998 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:21,902][gensim.models.word2vec][INFO] - EPOCH 29: training on 3639370 raw words (3642627 effective words) took 1.7s, 2125783 effective words/s
[2023-02-07 13:01:22,911][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 58.21% examples, 2152401 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:23,581][gensim.models.word2vec][INFO] - EPOCH 30: training on 3639370 raw words (3642627 effective words) took 1.7s, 2172196 effective words/s
[2023-02-07 13:01:24,589][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 58.24% examples, 2160667 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:25,260][gensim.models.word2vec][INFO] - EPOCH 31: training on 3639370 raw words (3642627 effective words) took 1.7s, 2172244 effective words/s
[2023-02-07 13:01:26,263][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 58.00% examples, 2160005 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:26,953][gensim.models.word2vec][INFO] - EPOCH 32: training on 3639370 raw words (3642627 effective words) took 1.7s, 2153484 effective words/s
[2023-02-07 13:01:27,959][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 58.21% examples, 2158960 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:28,630][gensim.models.word2vec][INFO] - EPOCH 33: training on 3639370 raw words (3642627 effective words) took 1.7s, 2174419 effective words/s
[2023-02-07 13:01:29,633][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 58.24% examples, 2168638 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:30,318][gensim.models.word2vec][INFO] - EPOCH 34: training on 3639370 raw words (3642627 effective words) took 1.7s, 2160082 effective words/s
[2023-02-07 13:01:31,323][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 57.35% examples, 2128321 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:32,019][gensim.models.word2vec][INFO] - EPOCH 35: training on 3639370 raw words (3642627 effective words) took 1.7s, 2142739 effective words/s
[2023-02-07 13:01:33,023][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 58.21% examples, 2161654 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:33,702][gensim.models.word2vec][INFO] - EPOCH 36: training on 3639370 raw words (3642627 effective words) took 1.7s, 2165713 effective words/s
[2023-02-07 13:01:34,704][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 58.46% examples, 2177359 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:35,385][gensim.models.word2vec][INFO] - EPOCH 37: training on 3639370 raw words (3642627 effective words) took 1.7s, 2166268 effective words/s
[2023-02-07 13:01:36,392][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 57.35% examples, 2125760 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:37,095][gensim.models.word2vec][INFO] - EPOCH 38: training on 3639370 raw words (3642627 effective words) took 1.7s, 2131660 effective words/s
[2023-02-07 13:01:38,098][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 59.20% examples, 2203819 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:38,774][gensim.models.word2vec][INFO] - EPOCH 39: training on 3639370 raw words (3642627 effective words) took 1.7s, 2171124 effective words/s
[2023-02-07 13:01:39,778][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 57.35% examples, 2131864 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:40,489][gensim.models.word2vec][INFO] - EPOCH 40: training on 3639370 raw words (3642627 effective words) took 1.7s, 2126497 effective words/s
[2023-02-07 13:01:41,494][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 56.03% examples, 2086594 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:42,240][gensim.models.word2vec][INFO] - EPOCH 41: training on 3639370 raw words (3642627 effective words) took 1.7s, 2082770 effective words/s
[2023-02-07 13:01:43,244][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 56.43% examples, 2107101 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:43,953][gensim.models.word2vec][INFO] - EPOCH 42: training on 3639370 raw words (3642627 effective words) took 1.7s, 2129648 effective words/s
[2023-02-07 13:01:44,963][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 57.35% examples, 2120417 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:45,686][gensim.models.word2vec][INFO] - EPOCH 43: training on 3639370 raw words (3642627 effective words) took 1.7s, 2104355 effective words/s
[2023-02-07 13:01:46,696][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 55.14% examples, 2042138 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:47,462][gensim.models.word2vec][INFO] - EPOCH 44: training on 3639370 raw words (3642627 effective words) took 1.8s, 2053680 effective words/s
[2023-02-07 13:01:48,465][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 56.03% examples, 2089467 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:49,215][gensim.models.word2vec][INFO] - EPOCH 45: training on 3639370 raw words (3642627 effective words) took 1.8s, 2080120 effective words/s
[2023-02-07 13:01:50,220][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 53.42% examples, 1993342 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:51,040][gensim.models.word2vec][INFO] - EPOCH 46: training on 3639370 raw words (3642627 effective words) took 1.8s, 1997846 effective words/s
[2023-02-07 13:01:52,047][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 53.02% examples, 1964929 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:52,864][gensim.models.word2vec][INFO] - EPOCH 47: training on 3639370 raw words (3642627 effective words) took 1.8s, 1999074 effective words/s
[2023-02-07 13:01:53,869][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 53.05% examples, 1976113 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:01:54,694][gensim.models.word2vec][INFO] - EPOCH 48: training on 3639370 raw words (3642627 effective words) took 1.8s, 1992324 effective words/s
[2023-02-07 13:01:55,698][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 60.30% examples, 2230855 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:56,297][gensim.models.word2vec][INFO] - EPOCH 49: training on 3639370 raw words (3642627 effective words) took 1.6s, 2276680 effective words/s
[2023-02-07 13:01:57,302][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 62.94% examples, 2323142 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:57,890][gensim.models.word2vec][INFO] - EPOCH 50: training on 3639370 raw words (3642627 effective words) took 1.6s, 2286921 effective words/s
[2023-02-07 13:01:58,893][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 59.20% examples, 2204912 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:01:59,544][gensim.models.word2vec][INFO] - EPOCH 51: training on 3639370 raw words (3642627 effective words) took 1.7s, 2204410 effective words/s
[2023-02-07 13:02:00,549][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 58.89% examples, 2189384 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:02:01,207][gensim.models.word2vec][INFO] - EPOCH 52: training on 3639370 raw words (3642627 effective words) took 1.7s, 2191984 effective words/s
[2023-02-07 13:02:02,213][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 58.89% examples, 2188765 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:02:02,909][gensim.models.word2vec][INFO] - EPOCH 53: training on 3639370 raw words (3642627 effective words) took 1.7s, 2142401 effective words/s
[2023-02-07 13:02:03,915][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 54.93% examples, 2040205 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:02:04,689][gensim.models.word2vec][INFO] - EPOCH 54: training on 3639370 raw words (3642627 effective words) took 1.8s, 2048740 effective words/s
[2023-02-07 13:02:05,694][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 54.59% examples, 2031068 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:02:06,482][gensim.models.word2vec][INFO] - EPOCH 55: training on 3639370 raw words (3642627 effective words) took 1.8s, 2033229 effective words/s
[2023-02-07 13:02:07,487][gensim.models.word2vec][INFO] - EPOCH 56 - PROGRESS: at 54.31% examples, 2020330 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:02:08,276][gensim.models.word2vec][INFO] - EPOCH 56: training on 3639370 raw words (3642627 effective words) took 1.8s, 2032040 effective words/s
[2023-02-07 13:02:09,279][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 57.35% examples, 2133465 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:02:09,923][gensim.models.word2vec][INFO] - EPOCH 57: training on 3639370 raw words (3642627 effective words) took 1.6s, 2214190 effective words/s
[2023-02-07 13:02:10,925][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 61.99% examples, 2294017 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:02:11,513][gensim.models.word2vec][INFO] - EPOCH 58: training on 3639370 raw words (3642627 effective words) took 1.6s, 2292993 effective words/s
[2023-02-07 13:02:12,518][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 62.82% examples, 2314103 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:02:13,089][gensim.models.word2vec][INFO] - EPOCH 59: training on 3639370 raw words (3642627 effective words) took 1.6s, 2312529 effective words/s
[2023-02-07 13:02:14,093][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 62.94% examples, 2328529 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:02:14,651][gensim.models.word2vec][INFO] - EPOCH 60: training on 3639370 raw words (3642627 effective words) took 1.6s, 2335210 effective words/s
[2023-02-07 13:02:15,653][gensim.models.word2vec][INFO] - EPOCH 61 - PROGRESS: at 63.37% examples, 2350203 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:02:16,206][gensim.models.word2vec][INFO] - EPOCH 61: training on 3639370 raw words (3642627 effective words) took 1.6s, 2344288 effective words/s
[2023-02-07 13:02:17,210][gensim.models.word2vec][INFO] - EPOCH 62 - PROGRESS: at 62.08% examples, 2294287 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:02:17,783][gensim.models.word2vec][INFO] - EPOCH 62: training on 3639370 raw words (3642627 effective words) took 1.6s, 2311532 effective words/s
[2023-02-07 13:02:18,789][gensim.models.word2vec][INFO] - EPOCH 63 - PROGRESS: at 63.92% examples, 2359333 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:02:19,322][gensim.models.word2vec][INFO] - EPOCH 63: training on 3639370 raw words (3642627 effective words) took 1.5s, 2369549 effective words/s
[2023-02-07 13:02:20,324][gensim.models.word2vec][INFO] - EPOCH 64 - PROGRESS: at 62.27% examples, 2296949 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:02:20,902][gensim.models.word2vec][INFO] - EPOCH 64: training on 3639370 raw words (3642627 effective words) took 1.6s, 2306744 effective words/s
[2023-02-07 13:02:21,903][gensim.models.word2vec][INFO] - EPOCH 65 - PROGRESS: at 61.99% examples, 2295416 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:02:22,483][gensim.models.word2vec][INFO] - EPOCH 65: training on 3639370 raw words (3642627 effective words) took 1.6s, 2306403 effective words/s
[2023-02-07 13:02:23,485][gensim.models.word2vec][INFO] - EPOCH 66 - PROGRESS: at 62.82% examples, 2321710 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:02:24,068][gensim.models.word2vec][INFO] - EPOCH 66: training on 3639370 raw words (3642627 effective words) took 1.6s, 2299837 effective words/s
[2023-02-07 13:02:24,068][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 243837790 raw words (244056009 effective words) took 112.9s, 2160972 effective words/s', 'datetime': '2023-02-07T13:02:24.068617', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:02:24.068 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:02:31,103][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130009-aa8qya0t/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:02:31.103654', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:02:31,105][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:02:31,130][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130009-aa8qya0t/files/../tmp/embedding_model.pt
2023-02-07 13:02:31.131 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:02:32.138 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:02:32.519 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:02:33.011 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2557861183625336, 'test_mae': 1.1323420874477348, 'test_r2': -0.06712170719413124}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.21
wandb: percentage 0.0
wandb:   test_mae 1.13234
wandb:   test_mse 2.25579
wandb:    test_r2 -0.06712
wandb: 
wandb: üöÄ View run valiant-sweep-25 at: https://wandb.ai/xiaoqiz/mof2vec/runs/aa8qya0t
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_130009-aa8qya0t/logs
wandb: Agent Starting Run: 62hvhr1d with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 955
wandb: 	model.gensim.alpha: 0.0008369157020266796
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 68
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.27217586690877316
wandb: 	model.gensim.vector_size: 79
wandb: 	model.gensim.window: 13
wandb: 	model.sklearn.learning_rate: 0.007343513436679285
wandb: 	model.sklearn.max_depth: 36
wandb: 	model.sklearn.min_child_weight: 0.02414841096733263
wandb: 	model.sklearn.n_estimators: 3564
wandb: 	model.sklearn.num_leaves: 436
wandb: 	model.sklearn.reg_alpha: 0.11363649147100605
wandb: 	model.sklearn.reg_lambda: 0.006441458221724924
wandb: 	model.sklearn.subsample: 0.3842771260877534
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130245-62hvhr1d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-26
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/62hvhr1d
2023-02-07 13:02:52.716 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 13:02:52.716 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 955 for sweep.
2023-02-07 13:02:52.717 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0008369157020266796 for sweep.
2023-02-07 13:02:52.717 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:02:52.717 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 68 for sweep.
2023-02-07 13:02:52.717 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 13:02:52.717 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.27217586690877316 for sweep.
2023-02-07 13:02:52.718 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 79 for sweep.
2023-02-07 13:02:52.718 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 13 for sweep.
2023-02-07 13:02:52.718 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.007343513436679285 for sweep.
2023-02-07 13:02:52.718 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 36 for sweep.
2023-02-07 13:02:52.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.02414841096733263 for sweep.
2023-02-07 13:02:52.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3564 for sweep.
2023-02-07 13:02:52.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 436 for sweep.
2023-02-07 13:02:52.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.11363649147100605 for sweep.
2023-02-07 13:02:52.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.006441458221724924 for sweep.
2023-02-07 13:02:52.720 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3842771260877534 for sweep.
2023-02-07 13:02:52.720 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:02:52.724 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130245-62hvhr1d/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 955, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 79, 'window': 13, 'min_count': 5, 'dm': 0, 'sample': 0.27217586690877316, 'workers': 4, 'alpha': 0.0008369157020266796, 'epochs': 68}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3564, 'max_depth': 36, 'num_leaves': 436, 'reg_alpha': 0.11363649147100605, 'reg_lambda': 0.006441458221724924, 'subsample': 0.3842771260877534, 'min_child_weight': 0.02414841096733263, 'n_jobs': 4, 'learning_rate': 0.007343513436679285}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 30/3257 [00:00<00:10, 295.63it/s]  2%|‚ñè         | 60/3257 [00:00<00:10, 297.00it/s]  3%|‚ñé         | 94/3257 [00:00<00:10, 310.13it/s]  4%|‚ñç         | 125/3257 [00:00<00:10, 301.39it/s]  5%|‚ñç         | 159/3257 [00:00<00:09, 313.87it/s]  6%|‚ñå         | 191/3257 [00:00<00:09, 313.30it/s]  7%|‚ñã         | 227/3257 [00:00<00:09, 327.74it/s]  8%|‚ñä         | 260/3257 [00:00<00:09, 316.29it/s]  9%|‚ñâ         | 297/3257 [00:00<00:08, 329.68it/s] 10%|‚ñà         | 331/3257 [00:01<00:08, 326.25it/s] 11%|‚ñà         | 364/3257 [00:01<00:09, 317.78it/s] 12%|‚ñà‚ñè        | 396/3257 [00:01<00:09, 305.12it/s] 13%|‚ñà‚ñé        | 427/3257 [00:01<00:09, 286.69it/s] 14%|‚ñà‚ñç        | 458/3257 [00:01<00:09, 291.68it/s] 15%|‚ñà‚ñå        | 489/3257 [00:01<00:09, 296.79it/s] 16%|‚ñà‚ñå        | 522/3257 [00:01<00:08, 305.75it/s] 17%|‚ñà‚ñã        | 553/3257 [00:01<00:08, 306.18it/s] 18%|‚ñà‚ñä        | 584/3257 [00:01<00:09, 286.57it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:02<00:08, 294.50it/s] 20%|‚ñà‚ñà        | 652/3257 [00:02<00:08, 300.16it/s] 21%|‚ñà‚ñà        | 683/3257 [00:02<00:08, 290.86it/s] 22%|‚ñà‚ñà‚ñè       | 718/3257 [00:02<00:08, 303.75it/s] 23%|‚ñà‚ñà‚ñé       | 749/3257 [00:02<00:08, 289.54it/s] 24%|‚ñà‚ñà‚ñç       | 780/3257 [00:02<00:08, 294.17it/s] 25%|‚ñà‚ñà‚ñç       | 812/3257 [00:02<00:08, 300.51it/s] 26%|‚ñà‚ñà‚ñå       | 843/3257 [00:02<00:08, 285.19it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:02<00:08, 290.18it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:03<00:07, 295.95it/s] 29%|‚ñà‚ñà‚ñä       | 936/3257 [00:03<00:07, 296.29it/s] 30%|‚ñà‚ñà‚ñâ       | 970/3257 [00:03<00:07, 304.64it/s] 31%|‚ñà‚ñà‚ñà       | 1001/3257 [00:03<00:10, 210.53it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1028/3257 [00:03<00:09, 223.15it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1056/3257 [00:03<00:09, 236.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1087/3257 [00:03<00:08, 254.53it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:03<00:08, 263.43it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1145/3257 [00:03<00:07, 265.60it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1176/3257 [00:04<00:07, 276.48it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1205/3257 [00:04<00:07, 261.32it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1239/3257 [00:04<00:07, 281.84it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1270/3257 [00:04<00:06, 286.20it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1300/3257 [00:04<00:07, 273.16it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1331/3257 [00:04<00:06, 281.29it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1361/3257 [00:04<00:06, 286.54it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1390/3257 [00:04<00:06, 277.81it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1426/3257 [00:04<00:06, 299.91it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1460/3257 [00:05<00:05, 311.39it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1494/3257 [00:05<00:05, 316.35it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1526/3257 [00:05<00:05, 307.57it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1557/3257 [00:05<00:05, 296.57it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1588/3257 [00:05<00:05, 298.53it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1620/3257 [00:05<00:05, 304.08it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1651/3257 [00:05<00:05, 291.35it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1681/3257 [00:05<00:05, 286.52it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1713/3257 [00:05<00:05, 294.83it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1743/3257 [00:06<00:05, 275.77it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1776/3257 [00:06<00:05, 290.58it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1806/3257 [00:06<00:04, 292.66it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1836/3257 [00:06<00:04, 293.26it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:06<00:04, 299.35it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1900/3257 [00:06<00:04, 302.36it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1931/3257 [00:06<00:04, 303.06it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1971/3257 [00:06<00:03, 330.08it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2005/3257 [00:06<00:03, 316.60it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2038/3257 [00:06<00:03, 319.06it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2071/3257 [00:07<00:03, 297.19it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2102/3257 [00:07<00:03, 292.56it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2132/3257 [00:07<00:03, 285.08it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2161/3257 [00:07<00:03, 284.11it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2193/3257 [00:07<00:03, 293.10it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2224/3257 [00:07<00:03, 292.12it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2254/3257 [00:07<00:03, 293.72it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2284/3257 [00:07<00:03, 290.60it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2315/3257 [00:07<00:03, 294.71it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2357/3257 [00:08<00:02, 329.72it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2391/3257 [00:08<00:02, 327.14it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2424/3257 [00:08<00:02, 317.56it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2456/3257 [00:08<00:02, 304.18it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2487/3257 [00:08<00:03, 199.69it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2524/3257 [00:08<00:03, 233.83it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2555/3257 [00:08<00:02, 249.04it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2584/3257 [00:08<00:02, 256.37it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2622/3257 [00:09<00:02, 286.96it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2654/3257 [00:09<00:02, 289.92it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2686/3257 [00:09<00:01, 297.94it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2718/3257 [00:09<00:01, 276.56it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2755/3257 [00:09<00:01, 300.78it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2787/3257 [00:09<00:01, 302.09it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2818/3257 [00:09<00:01, 299.16it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2850/3257 [00:09<00:01, 302.84it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2887/3257 [00:09<00:01, 321.78it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2920/3257 [00:10<00:01, 312.62it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2952/3257 [00:10<00:01, 298.94it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2983/3257 [00:10<00:00, 296.36it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3017/3257 [00:10<00:00, 306.93it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3052/3257 [00:10<00:00, 318.84it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3087/3257 [00:10<00:00, 324.18it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3124/3257 [00:10<00:00, 337.45it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3158/3257 [00:10<00:00, 321.75it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3191/3257 [00:10<00:00, 318.17it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3223/3257 [00:11<00:00, 309.32it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 293.26it/s]
2023-02-07 13:03:04.288 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:03:04,290][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d79,n5,mc5,s0.272176,t4>', 'datetime': '2023-02-07T13:03:04.290020', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:03:04,290][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:03:04,290][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:03:04,582][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 13:03:04,582][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:03:04,600][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 6948 unique words (53.20% of original 13061, drops 6113)', 'datetime': '2023-02-07T13:03:04.600209', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:03:04,600][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 3624596 word corpus (99.59% of original 3639370, drops 14774)', 'datetime': '2023-02-07T13:03:04.600404', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:03:04,621][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 13:03:04,622][gensim.models.word2vec][INFO] - sample=0.272176 downsamples 0 most-common words
[2023-02-07 13:03:04,622][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3624596 word corpus (100.0%% of prior 3624596)', 'datetime': '2023-02-07T13:03:04.622124', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:03:04,657][gensim.models.word2vec][INFO] - estimated required memory for 6948 words and 79 dimensions: 9545748 bytes
[2023-02-07 13:03:04,658][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:03:04,661][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 6948 vocabulary and 79 features, using sg=1 hs=0 sample=0.27217586690877316 negative=5 window=13 shrink_windows=True', 'datetime': '2023-02-07T13:03:04.661176', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:03:05,666][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 95.64% examples, 3463339 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:05,711][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3627853 effective words) took 1.0s, 3463794 effective words/s
[2023-02-07 13:03:06,715][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 92.75% examples, 3378236 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:06,783][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3627853 effective words) took 1.1s, 3389610 effective words/s
[2023-02-07 13:03:07,787][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 90.67% examples, 3304846 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:07,876][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3627853 effective words) took 1.1s, 3322293 effective words/s
[2023-02-07 13:03:08,879][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 91.65% examples, 3348508 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:08,955][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3627853 effective words) took 1.1s, 3368643 effective words/s
[2023-02-07 13:03:09,962][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 91.65% examples, 3332345 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:10,043][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3627853 effective words) took 1.1s, 3337558 effective words/s
[2023-02-07 13:03:11,047][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 90.42% examples, 3300491 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:11,143][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3627853 effective words) took 1.1s, 3305815 effective words/s
[2023-02-07 13:03:12,148][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 91.40% examples, 3331655 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:12,234][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3627853 effective words) took 1.1s, 3330383 effective words/s
[2023-02-07 13:03:13,238][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 92.05% examples, 3351955 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:13,317][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3627853 effective words) took 1.1s, 3354563 effective words/s
[2023-02-07 13:03:14,319][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 90.02% examples, 3282633 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:14,422][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3627853 effective words) took 1.1s, 3287936 effective words/s
[2023-02-07 13:03:15,426][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 91.40% examples, 3335915 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:15,512][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3627853 effective words) took 1.1s, 3334235 effective words/s
[2023-02-07 13:03:16,515][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 91.93% examples, 3356087 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:03:16,592][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3627853 effective words) took 1.1s, 3363846 effective words/s
[2023-02-07 13:03:17,594][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 91.03% examples, 3318522 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:17,685][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3627853 effective words) took 1.1s, 3320408 effective words/s
[2023-02-07 13:03:18,689][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 92.32% examples, 3365883 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:18,764][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3627853 effective words) took 1.1s, 3369194 effective words/s
[2023-02-07 13:03:19,768][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 91.65% examples, 3345665 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:19,849][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3627853 effective words) took 1.1s, 3351248 effective words/s
[2023-02-07 13:03:20,852][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 90.67% examples, 3307612 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:20,940][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3627853 effective words) took 1.1s, 3328019 effective words/s
[2023-02-07 13:03:21,945][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 97.85% examples, 3547309 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:21,963][gensim.models.word2vec][INFO] - EPOCH 15: training on 3639370 raw words (3627853 effective words) took 1.0s, 3555151 effective words/s
[2023-02-07 13:03:22,953][gensim.models.word2vec][INFO] - EPOCH 16: training on 3639370 raw words (3627853 effective words) took 1.0s, 3667156 effective words/s
[2023-02-07 13:03:23,956][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 97.85% examples, 3550609 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:23,974][gensim.models.word2vec][INFO] - EPOCH 17: training on 3639370 raw words (3627853 effective words) took 1.0s, 3558134 effective words/s
[2023-02-07 13:03:24,981][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 96.41% examples, 3480319 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:25,014][gensim.models.word2vec][INFO] - EPOCH 18: training on 3639370 raw words (3627853 effective words) took 1.0s, 3491587 effective words/s
[2023-02-07 13:03:26,017][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 91.65% examples, 3344805 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:26,095][gensim.models.word2vec][INFO] - EPOCH 19: training on 3639370 raw words (3627853 effective words) took 1.1s, 3359255 effective words/s
[2023-02-07 13:03:27,097][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 92.32% examples, 3366677 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:27,172][gensim.models.word2vec][INFO] - EPOCH 20: training on 3639370 raw words (3627853 effective words) took 1.1s, 3371492 effective words/s
[2023-02-07 13:03:28,179][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 92.75% examples, 3369975 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:28,246][gensim.models.word2vec][INFO] - EPOCH 21: training on 3639370 raw words (3627853 effective words) took 1.1s, 3384345 effective words/s
[2023-02-07 13:03:29,252][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 95.30% examples, 3446206 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:29,297][gensim.models.word2vec][INFO] - EPOCH 22: training on 3639370 raw words (3627853 effective words) took 1.1s, 3454058 effective words/s
[2023-02-07 13:03:30,302][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 94.11% examples, 3411753 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:30,360][gensim.models.word2vec][INFO] - EPOCH 23: training on 3639370 raw words (3627853 effective words) took 1.1s, 3418331 effective words/s
[2023-02-07 13:03:31,363][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 94.11% examples, 3418460 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:31,418][gensim.models.word2vec][INFO] - EPOCH 24: training on 3639370 raw words (3627853 effective words) took 1.1s, 3432616 effective words/s
[2023-02-07 13:03:32,420][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 96.90% examples, 3518310 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:32,449][gensim.models.word2vec][INFO] - EPOCH 25: training on 3639370 raw words (3627853 effective words) took 1.0s, 3523756 effective words/s
[2023-02-07 13:03:33,452][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 95.98% examples, 3475245 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:33,492][gensim.models.word2vec][INFO] - EPOCH 26: training on 3639370 raw words (3627853 effective words) took 1.0s, 3480270 effective words/s
[2023-02-07 13:03:34,494][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 93.74% examples, 3414126 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:34,556][gensim.models.word2vec][INFO] - EPOCH 27: training on 3639370 raw words (3627853 effective words) took 1.1s, 3415316 effective words/s
[2023-02-07 13:03:35,557][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 97.45% examples, 3537953 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:35,581][gensim.models.word2vec][INFO] - EPOCH 28: training on 3639370 raw words (3627853 effective words) took 1.0s, 3542708 effective words/s
[2023-02-07 13:03:36,585][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 95.64% examples, 3460760 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:36,627][gensim.models.word2vec][INFO] - EPOCH 29: training on 3639370 raw words (3627853 effective words) took 1.0s, 3470903 effective words/s
[2023-02-07 13:03:37,629][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 97.54% examples, 3543799 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:37,651][gensim.models.word2vec][INFO] - EPOCH 30: training on 3639370 raw words (3627853 effective words) took 1.0s, 3548360 effective words/s
[2023-02-07 13:03:38,653][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 98.43% examples, 3566494 words/s, in_qsize 6, out_qsize 0
[2023-02-07 13:03:38,667][gensim.models.word2vec][INFO] - EPOCH 31: training on 3639370 raw words (3627853 effective words) took 1.0s, 3571511 effective words/s
[2023-02-07 13:03:39,669][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 95.64% examples, 3468670 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:39,715][gensim.models.word2vec][INFO] - EPOCH 32: training on 3639370 raw words (3627853 effective words) took 1.0s, 3464360 effective words/s
[2023-02-07 13:03:40,719][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 90.42% examples, 3296661 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:40,816][gensim.models.word2vec][INFO] - EPOCH 33: training on 3639370 raw words (3627853 effective words) took 1.1s, 3301817 effective words/s
[2023-02-07 13:03:41,821][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 91.80% examples, 3338657 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:41,899][gensim.models.word2vec][INFO] - EPOCH 34: training on 3639370 raw words (3627853 effective words) took 1.1s, 3356433 effective words/s
[2023-02-07 13:03:42,901][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 91.40% examples, 3336954 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:42,988][gensim.models.word2vec][INFO] - EPOCH 35: training on 3639370 raw words (3627853 effective words) took 1.1s, 3332603 effective words/s
[2023-02-07 13:03:43,992][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 90.67% examples, 3306095 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:44,082][gensim.models.word2vec][INFO] - EPOCH 36: training on 3639370 raw words (3627853 effective words) took 1.1s, 3321988 effective words/s
[2023-02-07 13:03:45,087][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 91.65% examples, 3338959 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:45,164][gensim.models.word2vec][INFO] - EPOCH 37: training on 3639370 raw words (3627853 effective words) took 1.1s, 3356274 effective words/s
[2023-02-07 13:03:46,168][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 91.93% examples, 3352949 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:46,246][gensim.models.word2vec][INFO] - EPOCH 38: training on 3639370 raw words (3627853 effective words) took 1.1s, 3358773 effective words/s
[2023-02-07 13:03:47,250][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 90.67% examples, 3304294 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:47,342][gensim.models.word2vec][INFO] - EPOCH 39: training on 3639370 raw words (3627853 effective words) took 1.1s, 3311699 effective words/s
[2023-02-07 13:03:48,346][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 89.50% examples, 3259629 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:48,454][gensim.models.word2vec][INFO] - EPOCH 40: training on 3639370 raw words (3627853 effective words) took 1.1s, 3268597 effective words/s
[2023-02-07 13:03:49,459][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 88.61% examples, 3220376 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:49,582][gensim.models.word2vec][INFO] - EPOCH 41: training on 3639370 raw words (3627853 effective words) took 1.1s, 3221437 effective words/s
[2023-02-07 13:03:50,586][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 88.61% examples, 3220790 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:50,707][gensim.models.word2vec][INFO] - EPOCH 42: training on 3639370 raw words (3627853 effective words) took 1.1s, 3229235 effective words/s
[2023-02-07 13:03:51,709][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 89.19% examples, 3256871 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:51,824][gensim.models.word2vec][INFO] - EPOCH 43: training on 3639370 raw words (3627853 effective words) took 1.1s, 3254135 effective words/s
[2023-02-07 13:03:52,829][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 89.50% examples, 3252567 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:52,940][gensim.models.word2vec][INFO] - EPOCH 44: training on 3639370 raw words (3627853 effective words) took 1.1s, 3253601 effective words/s
[2023-02-07 13:03:53,943][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 87.57% examples, 3200501 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:54,072][gensim.models.word2vec][INFO] - EPOCH 45: training on 3639370 raw words (3627853 effective words) took 1.1s, 3209514 effective words/s
[2023-02-07 13:03:55,076][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 89.50% examples, 3261056 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:55,186][gensim.models.word2vec][INFO] - EPOCH 46: training on 3639370 raw words (3627853 effective words) took 1.1s, 3262238 effective words/s
[2023-02-07 13:03:56,191][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 89.87% examples, 3264794 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:56,296][gensim.models.word2vec][INFO] - EPOCH 47: training on 3639370 raw words (3627853 effective words) took 1.1s, 3272066 effective words/s
[2023-02-07 13:03:57,297][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 87.57% examples, 3203612 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:57,428][gensim.models.word2vec][INFO] - EPOCH 48: training on 3639370 raw words (3627853 effective words) took 1.1s, 3207083 effective words/s
[2023-02-07 13:03:58,432][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 88.92% examples, 3243273 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:58,547][gensim.models.word2vec][INFO] - EPOCH 49: training on 3639370 raw words (3627853 effective words) took 1.1s, 3248867 effective words/s
[2023-02-07 13:03:59,552][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 89.50% examples, 3255007 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:03:59,662][gensim.models.word2vec][INFO] - EPOCH 50: training on 3639370 raw words (3627853 effective words) took 1.1s, 3257827 effective words/s
[2023-02-07 13:04:00,665][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 88.79% examples, 3234095 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:04:00,784][gensim.models.word2vec][INFO] - EPOCH 51: training on 3639370 raw words (3627853 effective words) took 1.1s, 3240180 effective words/s
[2023-02-07 13:04:01,788][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 89.87% examples, 3268402 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:04:01,891][gensim.models.word2vec][INFO] - EPOCH 52: training on 3639370 raw words (3627853 effective words) took 1.1s, 3281364 effective words/s
[2023-02-07 13:04:02,895][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 89.50% examples, 3257658 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:04:03,000][gensim.models.word2vec][INFO] - EPOCH 53: training on 3639370 raw words (3627853 effective words) took 1.1s, 3274961 effective words/s
[2023-02-07 13:04:04,006][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 96.41% examples, 3484655 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:04:04,038][gensim.models.word2vec][INFO] - EPOCH 54: training on 3639370 raw words (3627853 effective words) took 1.0s, 3498404 effective words/s
[2023-02-07 13:04:05,040][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 98.80% examples, 3588320 words/s, in_qsize 4, out_qsize 0
[2023-02-07 13:04:05,048][gensim.models.word2vec][INFO] - EPOCH 55: training on 3639370 raw words (3627853 effective words) took 1.0s, 3596980 effective words/s
[2023-02-07 13:04:06,051][gensim.models.word2vec][INFO] - EPOCH 56 - PROGRESS: at 98.80% examples, 3586413 words/s, in_qsize 4, out_qsize 0
[2023-02-07 13:04:06,059][gensim.models.word2vec][INFO] - EPOCH 56: training on 3639370 raw words (3627853 effective words) took 1.0s, 3593773 effective words/s
[2023-02-07 13:04:07,062][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 94.90% examples, 3444498 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:04:07,113][gensim.models.word2vec][INFO] - EPOCH 57: training on 3639370 raw words (3627853 effective words) took 1.1s, 3443112 effective words/s
[2023-02-07 13:04:08,115][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 90.42% examples, 3303812 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:04:08,209][gensim.models.word2vec][INFO] - EPOCH 58: training on 3639370 raw words (3627853 effective words) took 1.1s, 3315176 effective words/s
[2023-02-07 13:04:09,213][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 91.40% examples, 3332963 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:04:09,299][gensim.models.word2vec][INFO] - EPOCH 59: training on 3639370 raw words (3627853 effective words) took 1.1s, 3331667 effective words/s
[2023-02-07 13:04:10,304][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 90.67% examples, 3306026 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:04:10,395][gensim.models.word2vec][INFO] - EPOCH 60: training on 3639370 raw words (3627853 effective words) took 1.1s, 3318399 effective words/s
[2023-02-07 13:04:11,397][gensim.models.word2vec][INFO] - EPOCH 61 - PROGRESS: at 91.25% examples, 3330372 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:04:11,482][gensim.models.word2vec][INFO] - EPOCH 61: training on 3639370 raw words (3627853 effective words) took 1.1s, 3338551 effective words/s
[2023-02-07 13:04:12,490][gensim.models.word2vec][INFO] - EPOCH 62 - PROGRESS: at 91.65% examples, 3332106 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:04:12,570][gensim.models.word2vec][INFO] - EPOCH 62: training on 3639370 raw words (3627853 effective words) took 1.1s, 3341400 effective words/s
[2023-02-07 13:04:13,572][gensim.models.word2vec][INFO] - EPOCH 63 - PROGRESS: at 91.65% examples, 3348038 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:04:13,651][gensim.models.word2vec][INFO] - EPOCH 63: training on 3639370 raw words (3627853 effective words) took 1.1s, 3358514 effective words/s
[2023-02-07 13:04:14,655][gensim.models.word2vec][INFO] - EPOCH 64 - PROGRESS: at 89.87% examples, 3266147 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:04:14,757][gensim.models.word2vec][INFO] - EPOCH 64: training on 3639370 raw words (3627853 effective words) took 1.1s, 3282634 effective words/s
[2023-02-07 13:04:15,763][gensim.models.word2vec][INFO] - EPOCH 65 - PROGRESS: at 91.65% examples, 3340709 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:04:15,839][gensim.models.word2vec][INFO] - EPOCH 65: training on 3639370 raw words (3627853 effective words) took 1.1s, 3360457 effective words/s
[2023-02-07 13:04:16,841][gensim.models.word2vec][INFO] - EPOCH 66 - PROGRESS: at 92.05% examples, 3358909 words/s, in_qsize 6, out_qsize 1
[2023-02-07 13:04:16,919][gensim.models.word2vec][INFO] - EPOCH 66: training on 3639370 raw words (3627853 effective words) took 1.1s, 3363323 effective words/s
[2023-02-07 13:04:17,923][gensim.models.word2vec][INFO] - EPOCH 67 - PROGRESS: at 92.75% examples, 3378735 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:04:17,990][gensim.models.word2vec][INFO] - EPOCH 67: training on 3639370 raw words (3627853 effective words) took 1.1s, 3390920 effective words/s
[2023-02-07 13:04:17,991][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 247477160 raw words (246694004 effective words) took 73.3s, 3364164 effective words/s', 'datetime': '2023-02-07T13:04:17.991349', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:04:17.991 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:04:23,030][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130245-62hvhr1d/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:04:23.030803', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:04:23,031][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:04:23,052][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130245-62hvhr1d/files/../tmp/embedding_model.pt
2023-02-07 13:04:23.053 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:04:24.082 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:04:24.462 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:04:25.023 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.983386349917889, 'test_mae': 1.0485169773691396, 'test_r2': 0.06173967003295333}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.044 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.044 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.88
wandb: percentage 0.46803
wandb:   test_mae 1.04852
wandb:   test_mse 1.98339
wandb:    test_r2 0.06174
wandb: 
wandb: üöÄ View run eager-sweep-26 at: https://wandb.ai/xiaoqiz/mof2vec/runs/62hvhr1d
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_130245-62hvhr1d/logs
wandb: Agent Starting Run: ne97qjz4 with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 175
wandb: 	model.gensim.alpha: 0.0032123747833285666
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 56
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.2819363622528781
wandb: 	model.gensim.vector_size: 103
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.01473716001768836
wandb: 	model.sklearn.max_depth: 5
wandb: 	model.sklearn.min_child_weight: 0.03182954951049931
wandb: 	model.sklearn.n_estimators: 3569
wandb: 	model.sklearn.num_leaves: 441
wandb: 	model.sklearn.reg_alpha: 0.0201025353043336
wandb: 	model.sklearn.reg_lambda: 0.45102522387395205
wandb: 	model.sklearn.subsample: 0.2126884750729333
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130435-ne97qjz4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-27
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/ne97qjz4
2023-02-07 13:04:43.354 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 13:04:43.355 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 175 for sweep.
2023-02-07 13:04:43.355 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0032123747833285666 for sweep.
2023-02-07 13:04:43.355 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:04:43.356 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 56 for sweep.
2023-02-07 13:04:43.356 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 13:04:43.356 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2819363622528781 for sweep.
2023-02-07 13:04:43.356 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 103 for sweep.
2023-02-07 13:04:43.356 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 13:04:43.357 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.01473716001768836 for sweep.
2023-02-07 13:04:43.357 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 5 for sweep.
2023-02-07 13:04:43.357 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03182954951049931 for sweep.
2023-02-07 13:04:43.357 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3569 for sweep.
2023-02-07 13:04:43.358 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 441 for sweep.
2023-02-07 13:04:43.358 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.0201025353043336 for sweep.
2023-02-07 13:04:43.358 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.45102522387395205 for sweep.
2023-02-07 13:04:43.358 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2126884750729333 for sweep.
2023-02-07 13:04:43.358 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:04:43.363 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130435-ne97qjz4/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 175, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 103, 'window': 3, 'min_count': 4, 'dm': 0, 'sample': 0.2819363622528781, 'workers': 4, 'alpha': 0.0032123747833285666, 'epochs': 56}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3569, 'max_depth': 5, 'num_leaves': 441, 'reg_alpha': 0.0201025353043336, 'reg_lambda': 0.45102522387395205, 'subsample': 0.2126884750729333, 'min_child_weight': 0.03182954951049931, 'n_jobs': 4, 'learning_rate': 0.01473716001768836}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 25/3257 [00:00<00:13, 245.20it/s]  2%|‚ñè         | 50/3257 [00:00<00:13, 246.44it/s]  2%|‚ñè         | 79/3257 [00:00<00:12, 262.89it/s]  3%|‚ñé         | 106/3257 [00:00<00:11, 264.85it/s]  4%|‚ñç         | 138/3257 [00:00<00:11, 282.48it/s]  5%|‚ñå         | 168/3257 [00:00<00:10, 288.20it/s]  6%|‚ñå         | 200/3257 [00:00<00:10, 296.31it/s]  7%|‚ñã         | 236/3257 [00:00<00:09, 311.99it/s]  8%|‚ñä         | 268/3257 [00:00<00:09, 304.66it/s]  9%|‚ñâ         | 304/3257 [00:01<00:09, 320.30it/s] 10%|‚ñà         | 337/3257 [00:01<00:09, 319.10it/s] 11%|‚ñà‚ñè        | 370/3257 [00:01<00:08, 321.23it/s] 12%|‚ñà‚ñè        | 403/3257 [00:01<00:09, 306.02it/s] 13%|‚ñà‚ñé        | 434/3257 [00:01<00:10, 278.73it/s] 14%|‚ñà‚ñç        | 468/3257 [00:01<00:09, 293.31it/s] 15%|‚ñà‚ñå        | 500/3257 [00:01<00:09, 298.83it/s] 16%|‚ñà‚ñã        | 531/3257 [00:01<00:09, 299.83it/s] 17%|‚ñà‚ñã        | 562/3257 [00:02<00:12, 210.11it/s] 18%|‚ñà‚ñä        | 590/3257 [00:02<00:11, 224.28it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:02<00:10, 240.92it/s] 20%|‚ñà‚ñâ        | 649/3257 [00:02<00:10, 252.68it/s] 21%|‚ñà‚ñà        | 678/3257 [00:02<00:09, 262.45it/s] 22%|‚ñà‚ñà‚ñè       | 709/3257 [00:02<00:09, 273.73it/s] 23%|‚ñà‚ñà‚ñé       | 738/3257 [00:02<00:09, 261.39it/s] 24%|‚ñà‚ñà‚ñé       | 770/3257 [00:02<00:09, 275.15it/s] 25%|‚ñà‚ñà‚ñç       | 799/3257 [00:02<00:08, 274.45it/s] 25%|‚ñà‚ñà‚ñå       | 827/3257 [00:03<00:09, 264.52it/s] 26%|‚ñà‚ñà‚ñå       | 854/3257 [00:03<00:09, 256.41it/s] 27%|‚ñà‚ñà‚ñã       | 881/3257 [00:03<00:09, 258.57it/s] 28%|‚ñà‚ñà‚ñä       | 913/3257 [00:03<00:08, 274.01it/s] 29%|‚ñà‚ñà‚ñâ       | 941/3257 [00:03<00:08, 274.26it/s] 30%|‚ñà‚ñà‚ñâ       | 971/3257 [00:03<00:08, 280.31it/s] 31%|‚ñà‚ñà‚ñà       | 1000/3257 [00:03<00:08, 273.01it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1028/3257 [00:03<00:08, 261.50it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1055/3257 [00:03<00:08, 258.91it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1082/3257 [00:03<00:08, 258.99it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1111/3257 [00:04<00:08, 265.97it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1138/3257 [00:04<00:08, 261.15it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1166/3257 [00:04<00:07, 264.30it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1193/3257 [00:04<00:08, 242.19it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1218/3257 [00:04<00:08, 238.34it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1248/3257 [00:04<00:07, 253.04it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:04<00:07, 253.63it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1301/3257 [00:04<00:07, 249.69it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:04<00:07, 254.83it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1354/3257 [00:05<00:07, 256.30it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1381/3257 [00:05<00:07, 254.54it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1411/3257 [00:05<00:06, 267.25it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1439/3257 [00:05<00:06, 269.86it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1472/3257 [00:05<00:06, 287.02it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1504/3257 [00:05<00:05, 295.70it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1534/3257 [00:05<00:06, 273.54it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1562/3257 [00:05<00:06, 268.87it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1590/3257 [00:05<00:06, 269.43it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:05<00:05, 274.98it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1647/3257 [00:06<00:06, 265.69it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1674/3257 [00:06<00:06, 257.62it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1701/3257 [00:06<00:05, 260.69it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:06<00:06, 254.76it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1754/3257 [00:06<00:05, 253.38it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1780/3257 [00:06<00:08, 168.03it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1806/3257 [00:06<00:07, 187.26it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1831/3257 [00:07<00:07, 200.85it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1861/3257 [00:07<00:06, 224.82it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1889/3257 [00:07<00:05, 237.71it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1917/3257 [00:07<00:05, 248.56it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1952/3257 [00:07<00:04, 276.54it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1981/3257 [00:07<00:04, 277.29it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2010/3257 [00:07<00:04, 280.36it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2040/3257 [00:07<00:04, 277.69it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2069/3257 [00:07<00:04, 265.07it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2096/3257 [00:07<00:04, 265.24it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2123/3257 [00:08<00:04, 255.91it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2149/3257 [00:08<00:04, 255.72it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:08<00:04, 265.19it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:08<00:03, 266.21it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2233/3257 [00:08<00:03, 266.35it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2260/3257 [00:08<00:03, 258.23it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2288/3257 [00:08<00:03, 262.77it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2315/3257 [00:08<00:03, 261.78it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2353/3257 [00:08<00:03, 295.51it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2383/3257 [00:09<00:02, 291.78it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2413/3257 [00:09<00:02, 284.28it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2442/3257 [00:09<00:03, 271.52it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2474/3257 [00:09<00:02, 283.86it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2507/3257 [00:09<00:02, 292.16it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2540/3257 [00:09<00:02, 294.34it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2570/3257 [00:09<00:02, 276.40it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2598/3257 [00:09<00:02, 275.32it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2634/3257 [00:09<00:02, 298.10it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2665/3257 [00:10<00:02, 285.17it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2694/3257 [00:10<00:02, 278.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2723/3257 [00:10<00:02, 260.22it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:10<00:01, 274.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2785/3257 [00:10<00:01, 277.40it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2814/3257 [00:10<00:01, 278.18it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2842/3257 [00:10<00:01, 264.74it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:10<00:01, 293.76it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2911/3257 [00:10<00:01, 279.08it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2940/3257 [00:11<00:01, 277.88it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2968/3257 [00:11<00:01, 271.17it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2996/3257 [00:11<00:00, 265.29it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3024/3257 [00:11<00:00, 269.29it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3058/3257 [00:11<00:00, 289.13it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3088/3257 [00:11<00:00, 289.89it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3122/3257 [00:11<00:00, 303.73it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3153/3257 [00:11<00:00, 283.65it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3182/3257 [00:11<00:00, 273.25it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3210/3257 [00:12<00:00, 165.89it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3240/3257 [00:12<00:00, 190.92it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:12<00:00, 263.03it/s]
2023-02-07 13:04:56.169 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:04:56,170][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d103,n5,mc4,s0.281936,t4>', 'datetime': '2023-02-07T13:04:56.170322', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:04:56,170][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:04:56,170][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:04:56,497][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 13:04:56,497][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:04:56,520][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 8978 unique words (68.74% of original 13061, drops 4083)', 'datetime': '2023-02-07T13:04:56.520256', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:04:56,520][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 3632716 word corpus (99.82% of original 3639370, drops 6654)', 'datetime': '2023-02-07T13:04:56.520640', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:04:56,551][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 13:04:56,552][gensim.models.word2vec][INFO] - sample=0.281936 downsamples 0 most-common words
[2023-02-07 13:04:56,552][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3632716 word corpus (100.0%% of prior 3632716)', 'datetime': '2023-02-07T13:04:56.552471', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:04:56,602][gensim.models.word2vec][INFO] - estimated required memory for 8978 words and 103 dimensions: 13880156 bytes
[2023-02-07 13:04:56,602][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:04:56,608][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 8978 vocabulary and 103 features, using sg=1 hs=0 sample=0.2819363622528781 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T13:04:56.608147', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:04:57,614][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 90.42% examples, 3300953 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:04:57,709][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3635973 effective words) took 1.1s, 3309797 effective words/s
[2023-02-07 13:04:58,711][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 98.80% examples, 3596583 words/s, in_qsize 4, out_qsize 0
[2023-02-07 13:04:58,718][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3635973 effective words) took 1.0s, 3604965 effective words/s
[2023-02-07 13:04:59,720][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 98.62% examples, 3590324 words/s, in_qsize 5, out_qsize 0
[2023-02-07 13:04:59,732][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3635973 effective words) took 1.0s, 3591518 effective words/s
[2023-02-07 13:05:00,739][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 97.24% examples, 3523460 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:05:00,761][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3635973 effective words) took 1.0s, 3537241 effective words/s
[2023-02-07 13:05:01,763][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 98.43% examples, 3576536 words/s, in_qsize 6, out_qsize 0
[2023-02-07 13:05:01,777][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3635973 effective words) took 1.0s, 3581510 effective words/s
[2023-02-07 13:05:02,780][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 98.80% examples, 3592548 words/s, in_qsize 4, out_qsize 0
[2023-02-07 13:05:02,787][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3635973 effective words) took 1.0s, 3603285 effective words/s
[2023-02-07 13:05:03,782][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3635973 effective words) took 1.0s, 3661088 effective words/s
[2023-02-07 13:05:04,783][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3635973 effective words) took 1.0s, 3636731 effective words/s
[2023-02-07 13:05:05,774][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3635973 effective words) took 1.0s, 3674786 effective words/s
[2023-02-07 13:05:06,759][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3635973 effective words) took 1.0s, 3696791 effective words/s
[2023-02-07 13:05:07,765][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 97.24% examples, 3531611 words/s, in_qsize 6, out_qsize 1
[2023-02-07 13:05:07,786][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3635973 effective words) took 1.0s, 3547938 effective words/s
[2023-02-07 13:05:08,788][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 98.43% examples, 3577005 words/s, in_qsize 6, out_qsize 0
[2023-02-07 13:05:08,802][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3635973 effective words) took 1.0s, 3583405 effective words/s
[2023-02-07 13:05:09,806][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 98.62% examples, 3580918 words/s, in_qsize 5, out_qsize 0
[2023-02-07 13:05:09,816][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3635973 effective words) took 1.0s, 3588475 effective words/s
[2023-02-07 13:05:10,821][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 98.80% examples, 3589931 words/s, in_qsize 4, out_qsize 0
[2023-02-07 13:05:10,828][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3635973 effective words) took 1.0s, 3601034 effective words/s
[2023-02-07 13:05:11,830][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 100.00% examples, 3633260 words/s, in_qsize 0, out_qsize 1
[2023-02-07 13:05:11,831][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3635973 effective words) took 1.0s, 3630190 effective words/s
[2023-02-07 13:05:12,828][gensim.models.word2vec][INFO] - EPOCH 15: training on 3639370 raw words (3635973 effective words) took 1.0s, 3653373 effective words/s
[2023-02-07 13:05:13,825][gensim.models.word2vec][INFO] - EPOCH 16: training on 3639370 raw words (3635973 effective words) took 1.0s, 3651142 effective words/s
[2023-02-07 13:05:14,827][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 100.00% examples, 3630174 words/s, in_qsize 0, out_qsize 1
[2023-02-07 13:05:14,828][gensim.models.word2vec][INFO] - EPOCH 17: training on 3639370 raw words (3635973 effective words) took 1.0s, 3628749 effective words/s
[2023-02-07 13:05:15,827][gensim.models.word2vec][INFO] - EPOCH 18: training on 3639370 raw words (3635973 effective words) took 1.0s, 3642779 effective words/s
[2023-02-07 13:05:16,828][gensim.models.word2vec][INFO] - EPOCH 19: training on 3639370 raw words (3635973 effective words) took 1.0s, 3638225 effective words/s
[2023-02-07 13:05:17,826][gensim.models.word2vec][INFO] - EPOCH 20: training on 3639370 raw words (3635973 effective words) took 1.0s, 3649576 effective words/s
[2023-02-07 13:05:18,829][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 99.85% examples, 3622813 words/s, in_qsize 1, out_qsize 1
[2023-02-07 13:05:18,831][gensim.models.word2vec][INFO] - EPOCH 21: training on 3639370 raw words (3635973 effective words) took 1.0s, 3627977 effective words/s
[2023-02-07 13:05:19,820][gensim.models.word2vec][INFO] - EPOCH 22: training on 3639370 raw words (3635973 effective words) took 1.0s, 3678005 effective words/s
[2023-02-07 13:05:20,818][gensim.models.word2vec][INFO] - EPOCH 23: training on 3639370 raw words (3635973 effective words) took 1.0s, 3652780 effective words/s
[2023-02-07 13:05:21,814][gensim.models.word2vec][INFO] - EPOCH 24: training on 3639370 raw words (3635973 effective words) took 1.0s, 3653698 effective words/s
[2023-02-07 13:05:22,819][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 98.80% examples, 3592804 words/s, in_qsize 4, out_qsize 0
[2023-02-07 13:05:22,826][gensim.models.word2vec][INFO] - EPOCH 25: training on 3639370 raw words (3635973 effective words) took 1.0s, 3604035 effective words/s
[2023-02-07 13:05:23,743][gensim.models.word2vec][INFO] - EPOCH 26: training on 3639370 raw words (3635973 effective words) took 0.9s, 3966539 effective words/s
[2023-02-07 13:05:24,677][gensim.models.word2vec][INFO] - EPOCH 27: training on 3639370 raw words (3635973 effective words) took 0.9s, 3900337 effective words/s
[2023-02-07 13:05:25,570][gensim.models.word2vec][INFO] - EPOCH 28: training on 3639370 raw words (3635973 effective words) took 0.9s, 4074866 effective words/s
[2023-02-07 13:05:26,528][gensim.models.word2vec][INFO] - EPOCH 29: training on 3639370 raw words (3635973 effective words) took 1.0s, 3800140 effective words/s
[2023-02-07 13:05:27,494][gensim.models.word2vec][INFO] - EPOCH 30: training on 3639370 raw words (3635973 effective words) took 1.0s, 3770995 effective words/s
[2023-02-07 13:05:28,484][gensim.models.word2vec][INFO] - EPOCH 31: training on 3639370 raw words (3635973 effective words) took 1.0s, 3675435 effective words/s
[2023-02-07 13:05:29,442][gensim.models.word2vec][INFO] - EPOCH 32: training on 3639370 raw words (3635973 effective words) took 1.0s, 3801654 effective words/s
[2023-02-07 13:05:30,418][gensim.models.word2vec][INFO] - EPOCH 33: training on 3639370 raw words (3635973 effective words) took 1.0s, 3730361 effective words/s
[2023-02-07 13:05:31,355][gensim.models.word2vec][INFO] - EPOCH 34: training on 3639370 raw words (3635973 effective words) took 0.9s, 3887262 effective words/s
[2023-02-07 13:05:32,335][gensim.models.word2vec][INFO] - EPOCH 35: training on 3639370 raw words (3635973 effective words) took 1.0s, 3714863 effective words/s
[2023-02-07 13:05:33,270][gensim.models.word2vec][INFO] - EPOCH 36: training on 3639370 raw words (3635973 effective words) took 0.9s, 3895117 effective words/s
[2023-02-07 13:05:34,234][gensim.models.word2vec][INFO] - EPOCH 37: training on 3639370 raw words (3635973 effective words) took 1.0s, 3778018 effective words/s
[2023-02-07 13:05:35,172][gensim.models.word2vec][INFO] - EPOCH 38: training on 3639370 raw words (3635973 effective words) took 0.9s, 3880897 effective words/s
[2023-02-07 13:05:36,124][gensim.models.word2vec][INFO] - EPOCH 39: training on 3639370 raw words (3635973 effective words) took 1.0s, 3824631 effective words/s
[2023-02-07 13:05:37,052][gensim.models.word2vec][INFO] - EPOCH 40: training on 3639370 raw words (3635973 effective words) took 0.9s, 3922773 effective words/s
[2023-02-07 13:05:38,007][gensim.models.word2vec][INFO] - EPOCH 41: training on 3639370 raw words (3635973 effective words) took 1.0s, 3811293 effective words/s
[2023-02-07 13:05:38,939][gensim.models.word2vec][INFO] - EPOCH 42: training on 3639370 raw words (3635973 effective words) took 0.9s, 3907190 effective words/s
[2023-02-07 13:05:39,874][gensim.models.word2vec][INFO] - EPOCH 43: training on 3639370 raw words (3635973 effective words) took 0.9s, 3894660 effective words/s
[2023-02-07 13:05:40,804][gensim.models.word2vec][INFO] - EPOCH 44: training on 3639370 raw words (3635973 effective words) took 0.9s, 3916415 effective words/s
[2023-02-07 13:05:41,754][gensim.models.word2vec][INFO] - EPOCH 45: training on 3639370 raw words (3635973 effective words) took 0.9s, 3829614 effective words/s
[2023-02-07 13:05:42,673][gensim.models.word2vec][INFO] - EPOCH 46: training on 3639370 raw words (3635973 effective words) took 0.9s, 3964557 effective words/s
[2023-02-07 13:05:43,598][gensim.models.word2vec][INFO] - EPOCH 47: training on 3639370 raw words (3635973 effective words) took 0.9s, 3932571 effective words/s
[2023-02-07 13:05:44,518][gensim.models.word2vec][INFO] - EPOCH 48: training on 3639370 raw words (3635973 effective words) took 0.9s, 3957897 effective words/s
[2023-02-07 13:05:45,434][gensim.models.word2vec][INFO] - EPOCH 49: training on 3639370 raw words (3635973 effective words) took 0.9s, 3976523 effective words/s
[2023-02-07 13:05:46,373][gensim.models.word2vec][INFO] - EPOCH 50: training on 3639370 raw words (3635973 effective words) took 0.9s, 3879315 effective words/s
[2023-02-07 13:05:47,338][gensim.models.word2vec][INFO] - EPOCH 51: training on 3639370 raw words (3635973 effective words) took 1.0s, 3775597 effective words/s
[2023-02-07 13:05:48,313][gensim.models.word2vec][INFO] - EPOCH 52: training on 3639370 raw words (3635973 effective words) took 1.0s, 3734796 effective words/s
[2023-02-07 13:05:49,282][gensim.models.word2vec][INFO] - EPOCH 53: training on 3639370 raw words (3635973 effective words) took 1.0s, 3755747 effective words/s
[2023-02-07 13:05:50,250][gensim.models.word2vec][INFO] - EPOCH 54: training on 3639370 raw words (3635973 effective words) took 1.0s, 3764668 effective words/s
[2023-02-07 13:05:51,216][gensim.models.word2vec][INFO] - EPOCH 55: training on 3639370 raw words (3635973 effective words) took 1.0s, 3767393 effective words/s
[2023-02-07 13:05:51,217][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 203804720 raw words (203614488 effective words) took 54.6s, 3728662 effective words/s', 'datetime': '2023-02-07T13:05:51.217137', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:05:51.217 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:05:55,506][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130435-ne97qjz4/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:05:55.506791', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:05:55,508][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:05:55,531][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130435-ne97qjz4/files/../tmp/embedding_model.pt
2023-02-07 13:05:55.531 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:05:56.652 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:05:57.072 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:05:57.813 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8445050857409775, 'test_mae': 1.0086775898499174, 'test_r2': 0.1274388116844345}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.91
wandb: percentage 0.31261
wandb:   test_mae 1.00868
wandb:   test_mse 1.84451
wandb:    test_r2 0.12744
wandb: 
wandb: üöÄ View run vocal-sweep-27 at: https://wandb.ai/xiaoqiz/mof2vec/runs/ne97qjz4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_130435-ne97qjz4/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: qb6au6ad with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 515
wandb: 	model.gensim.alpha: 0.01139572524943363
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 46
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.28384587461528493
wandb: 	model.gensim.vector_size: 175
wandb: 	model.gensim.window: 2
wandb: 	model.sklearn.learning_rate: 0.4087132961122127
wandb: 	model.sklearn.max_depth: 8
wandb: 	model.sklearn.min_child_weight: 0.015229611353640194
wandb: 	model.sklearn.n_estimators: 3934
wandb: 	model.sklearn.num_leaves: 485
wandb: 	model.sklearn.reg_alpha: 0.02024044183957904
wandb: 	model.sklearn.reg_lambda: 0.010330738734038508
wandb: 	model.sklearn.subsample: 0.3489506027229222
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130616-qb6au6ad
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-28
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/qb6au6ad
2023-02-07 13:06:24.179 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:06:24.180 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 515 for sweep.
2023-02-07 13:06:24.180 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.01139572524943363 for sweep.
2023-02-07 13:06:24.180 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:06:24.180 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 46 for sweep.
2023-02-07 13:06:24.181 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 13:06:24.181 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.28384587461528493 for sweep.
2023-02-07 13:06:24.181 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 175 for sweep.
2023-02-07 13:06:24.181 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 2 for sweep.
2023-02-07 13:06:24.182 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.4087132961122127 for sweep.
2023-02-07 13:06:24.182 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 8 for sweep.
2023-02-07 13:06:24.182 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.015229611353640194 for sweep.
2023-02-07 13:06:24.183 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3934 for sweep.
2023-02-07 13:06:24.183 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 485 for sweep.
2023-02-07 13:06:24.183 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.02024044183957904 for sweep.
2023-02-07 13:06:24.183 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.010330738734038508 for sweep.
2023-02-07 13:06:24.184 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3489506027229222 for sweep.
2023-02-07 13:06:24.184 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:06:24.188 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130616-qb6au6ad/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 515, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 175, 'window': 2, 'min_count': 2, 'dm': 0, 'sample': 0.28384587461528493, 'workers': 4, 'alpha': 0.01139572524943363, 'epochs': 46}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3934, 'max_depth': 8, 'num_leaves': 485, 'reg_alpha': 0.02024044183957904, 'reg_lambda': 0.010330738734038508, 'subsample': 0.3489506027229222, 'min_child_weight': 0.015229611353640194, 'n_jobs': 4, 'learning_rate': 0.4087132961122127}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|‚ñè         | 41/3257 [00:00<00:08, 392.73it/s]  3%|‚ñé         | 85/3257 [00:00<00:07, 417.96it/s]  4%|‚ñç         | 127/3257 [00:00<00:07, 402.51it/s]  5%|‚ñå         | 171/3257 [00:00<00:07, 415.44it/s]  7%|‚ñã         | 218/3257 [00:00<00:07, 432.18it/s]  8%|‚ñä         | 263/3257 [00:00<00:06, 437.65it/s] 10%|‚ñâ         | 312/3257 [00:00<00:06, 449.14it/s] 11%|‚ñà         | 359/3257 [00:00<00:06, 455.11it/s] 12%|‚ñà‚ñè        | 405/3257 [00:00<00:06, 435.78it/s] 14%|‚ñà‚ñç        | 449/3257 [00:01<00:06, 413.56it/s] 15%|‚ñà‚ñå        | 492/3257 [00:01<00:06, 416.40it/s] 17%|‚ñà‚ñã        | 538/3257 [00:01<00:06, 427.08it/s] 18%|‚ñà‚ñä        | 581/3257 [00:01<00:06, 408.14it/s] 19%|‚ñà‚ñâ        | 625/3257 [00:01<00:06, 416.69it/s] 20%|‚ñà‚ñà        | 667/3257 [00:01<00:06, 400.82it/s] 22%|‚ñà‚ñà‚ñè       | 709/3257 [00:01<00:06, 404.17it/s] 23%|‚ñà‚ñà‚ñé       | 750/3257 [00:01<00:06, 385.52it/s] 24%|‚ñà‚ñà‚ñç       | 789/3257 [00:01<00:06, 371.65it/s] 25%|‚ñà‚ñà‚ñå       | 827/3257 [00:02<00:06, 369.89it/s] 27%|‚ñà‚ñà‚ñã       | 865/3257 [00:02<00:06, 370.23it/s] 28%|‚ñà‚ñà‚ñä       | 903/3257 [00:02<00:06, 370.51it/s] 29%|‚ñà‚ñà‚ñâ       | 944/3257 [00:02<00:06, 379.83it/s] 30%|‚ñà‚ñà‚ñà       | 984/3257 [00:02<00:05, 383.75it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1023/3257 [00:02<00:05, 382.82it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1062/3257 [00:02<00:05, 371.94it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1102/3257 [00:02<00:05, 378.29it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1140/3257 [00:03<00:07, 265.50it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1177/3257 [00:03<00:07, 287.80it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1210/3257 [00:03<00:06, 293.34it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1248/3257 [00:03<00:06, 314.80it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1283/3257 [00:03<00:06, 322.87it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1321/3257 [00:03<00:05, 337.83it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1363/3257 [00:03<00:05, 359.18it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1400/3257 [00:03<00:05, 352.49it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1443/3257 [00:03<00:04, 373.22it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1485/3257 [00:03<00:04, 386.49it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1525/3257 [00:04<00:04, 388.40it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1565/3257 [00:04<00:04, 381.73it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1606/3257 [00:04<00:04, 388.25it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1647/3257 [00:04<00:04, 389.35it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1687/3257 [00:04<00:04, 387.95it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:04<00:03, 393.16it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1769/3257 [00:04<00:03, 396.59it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1810/3257 [00:04<00:03, 400.48it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1854/3257 [00:04<00:03, 411.86it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1897/3257 [00:04<00:03, 414.88it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1945/3257 [00:05<00:03, 433.76it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1992/3257 [00:05<00:02, 442.53it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2037/3257 [00:05<00:02, 444.11it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2082/3257 [00:05<00:02, 423.87it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2125/3257 [00:05<00:02, 406.45it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2168/3257 [00:05<00:02, 409.54it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2210/3257 [00:05<00:02, 404.61it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2253/3257 [00:05<00:02, 411.13it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2295/3257 [00:05<00:02, 412.28it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2345/3257 [00:05<00:02, 436.55it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2394/3257 [00:06<00:01, 451.59it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2440/3257 [00:06<00:01, 432.70it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2484/3257 [00:06<00:02, 300.64it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2536/3257 [00:06<00:02, 347.18it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2577/3257 [00:06<00:01, 357.59it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2627/3257 [00:06<00:01, 392.34it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:06<00:01, 400.65it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2713/3257 [00:06<00:01, 394.56it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2760/3257 [00:07<00:01, 413.38it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:07<00:01, 424.64it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2850/3257 [00:07<00:00, 421.39it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2896/3257 [00:07<00:00, 431.94it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2943/3257 [00:07<00:00, 430.77it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2987/3257 [00:07<00:00, 421.36it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3035/3257 [00:07<00:00, 437.89it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3083/3257 [00:07<00:00, 449.62it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3130/3257 [00:07<00:00, 452.87it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3176/3257 [00:08<00:00, 439.22it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3221/3257 [00:08<00:00, 430.05it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 395.91it/s]
2023-02-07 13:06:32.584 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:06:32,585][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d175,n5,mc2,s0.283846,t4>', 'datetime': '2023-02-07T13:06:32.585486', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:06:32,585][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:06:32,586][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:06:32,719][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:06:32,719][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:06:32,721][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 881 unique words (95.35% of original 924, drops 43)', 'datetime': '2023-02-07T13:06:32.721680', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:06:32,721][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 1455705 word corpus (100.00% of original 1455748, drops 43)', 'datetime': '2023-02-07T13:06:32.721926', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:06:32,724][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:06:32,725][gensim.models.word2vec][INFO] - sample=0.283846 downsamples 0 most-common words
[2023-02-07 13:06:32,725][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455705 word corpus (100.0%% of prior 1455705)', 'datetime': '2023-02-07T13:06:32.725204', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:06:32,731][gensim.models.word2vec][INFO] - estimated required memory for 881 words and 175 dimensions: 4605200 bytes
[2023-02-07 13:06:32,731][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:06:32,734][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 881 vocabulary and 175 features, using sg=1 hs=0 sample=0.28384587461528493 negative=5 window=2 shrink_windows=True', 'datetime': '2023-02-07T13:06:32.734687', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:06:33,276][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458962 effective words) took 0.5s, 2703663 effective words/s
[2023-02-07 13:06:33,704][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458962 effective words) took 0.4s, 3423185 effective words/s
[2023-02-07 13:06:34,123][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458962 effective words) took 0.4s, 3498263 effective words/s
[2023-02-07 13:06:34,535][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458962 effective words) took 0.4s, 3552236 effective words/s
[2023-02-07 13:06:34,942][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458962 effective words) took 0.4s, 3592854 effective words/s
[2023-02-07 13:06:35,348][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458962 effective words) took 0.4s, 3616380 effective words/s
[2023-02-07 13:06:35,755][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458962 effective words) took 0.4s, 3594826 effective words/s
[2023-02-07 13:06:36,160][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458962 effective words) took 0.4s, 3622669 effective words/s
[2023-02-07 13:06:36,561][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458962 effective words) took 0.4s, 3655191 effective words/s
[2023-02-07 13:06:36,961][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458962 effective words) took 0.4s, 3653858 effective words/s
[2023-02-07 13:06:37,362][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458962 effective words) took 0.4s, 3653875 effective words/s
[2023-02-07 13:06:37,766][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458962 effective words) took 0.4s, 3630562 effective words/s
[2023-02-07 13:06:38,164][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458962 effective words) took 0.4s, 3675392 effective words/s
[2023-02-07 13:06:38,561][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458962 effective words) took 0.4s, 3696979 effective words/s
[2023-02-07 13:06:38,947][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458962 effective words) took 0.4s, 3788673 effective words/s
[2023-02-07 13:06:39,314][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458962 effective words) took 0.4s, 3990363 effective words/s
[2023-02-07 13:06:39,694][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458962 effective words) took 0.4s, 3848452 effective words/s
[2023-02-07 13:06:40,064][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458962 effective words) took 0.4s, 3953327 effective words/s
[2023-02-07 13:06:40,432][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458962 effective words) took 0.4s, 3982089 effective words/s
[2023-02-07 13:06:40,799][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458962 effective words) took 0.4s, 3982691 effective words/s
[2023-02-07 13:06:41,168][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458962 effective words) took 0.4s, 3969250 effective words/s
[2023-02-07 13:06:41,564][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458962 effective words) took 0.4s, 3696924 effective words/s
[2023-02-07 13:06:41,973][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458962 effective words) took 0.4s, 3573916 effective words/s
[2023-02-07 13:06:42,375][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458962 effective words) took 0.4s, 3648049 effective words/s
[2023-02-07 13:06:42,783][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458962 effective words) took 0.4s, 3587252 effective words/s
[2023-02-07 13:06:43,185][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458962 effective words) took 0.4s, 3638854 effective words/s
[2023-02-07 13:06:43,587][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458962 effective words) took 0.4s, 3638827 effective words/s
[2023-02-07 13:06:43,995][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458962 effective words) took 0.4s, 3591838 effective words/s
[2023-02-07 13:06:44,400][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458962 effective words) took 0.4s, 3605618 effective words/s
[2023-02-07 13:06:44,804][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458962 effective words) took 0.4s, 3630153 effective words/s
[2023-02-07 13:06:45,211][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458962 effective words) took 0.4s, 3586934 effective words/s
[2023-02-07 13:06:45,633][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458962 effective words) took 0.4s, 3472650 effective words/s
[2023-02-07 13:06:46,040][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458962 effective words) took 0.4s, 3596682 effective words/s
[2023-02-07 13:06:46,452][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458962 effective words) took 0.4s, 3549164 effective words/s
[2023-02-07 13:06:46,875][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458962 effective words) took 0.4s, 3458396 effective words/s
[2023-02-07 13:06:47,327][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458962 effective words) took 0.5s, 3237478 effective words/s
[2023-02-07 13:06:47,776][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458962 effective words) took 0.4s, 3256956 effective words/s
[2023-02-07 13:06:48,223][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458962 effective words) took 0.4s, 3275317 effective words/s
[2023-02-07 13:06:48,675][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458962 effective words) took 0.4s, 3242263 effective words/s
[2023-02-07 13:06:49,125][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458962 effective words) took 0.4s, 3249009 effective words/s
[2023-02-07 13:06:49,581][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458962 effective words) took 0.5s, 3206575 effective words/s
[2023-02-07 13:06:50,034][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458962 effective words) took 0.5s, 3236183 effective words/s
[2023-02-07 13:06:50,493][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458962 effective words) took 0.5s, 3181417 effective words/s
[2023-02-07 13:06:50,946][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458962 effective words) took 0.5s, 3233010 effective words/s
[2023-02-07 13:06:51,407][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458962 effective words) took 0.5s, 3175822 effective words/s
[2023-02-07 13:06:51,867][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458962 effective words) took 0.5s, 3184095 effective words/s
[2023-02-07 13:06:51,868][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 66964408 raw words (67112252 effective words) took 19.1s, 3507671 effective words/s', 'datetime': '2023-02-07T13:06:51.868012', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:06:51.868 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:06:53,122][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130616-qb6au6ad/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:06:53.122050', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:06:53,123][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:06:53,128][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130616-qb6au6ad/files/../tmp/embedding_model.pt
2023-02-07 13:06:53.129 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:06:54.429 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:06:54.934 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:06:56.133 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8599959927736942, 'test_mae': 1.0360726938222997, 'test_r2': 0.12011068645829914}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.85
wandb: percentage 0.04654
wandb:   test_mae 1.03607
wandb:   test_mse 1.86
wandb:    test_r2 0.12011
wandb: 
wandb: üöÄ View run easy-sweep-28 at: https://wandb.ai/xiaoqiz/mof2vec/runs/qb6au6ad
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_130616-qb6au6ad/logs
wandb: Agent Starting Run: 9gowzk7k with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 353
wandb: 	model.gensim.alpha: 0.001060154980113281
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 41
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.2784585522733096
wandb: 	model.gensim.vector_size: 132
wandb: 	model.gensim.window: 9
wandb: 	model.sklearn.learning_rate: 0.0008281083411133536
wandb: 	model.sklearn.max_depth: 15
wandb: 	model.sklearn.min_child_weight: 0.020436223083739102
wandb: 	model.sklearn.n_estimators: 3390
wandb: 	model.sklearn.num_leaves: 366
wandb: 	model.sklearn.reg_alpha: 0.012175168542252096
wandb: 	model.sklearn.reg_lambda: 0.12717334958862547
wandb: 	model.sklearn.subsample: 0.2801443554908584
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130720-9gowzk7k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-sweep-29
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/9gowzk7k
2023-02-07 13:07:27.642 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:07:27.642 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 353 for sweep.
2023-02-07 13:07:27.643 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.001060154980113281 for sweep.
2023-02-07 13:07:27.643 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:07:27.643 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 41 for sweep.
2023-02-07 13:07:27.643 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 13:07:27.644 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2784585522733096 for sweep.
2023-02-07 13:07:27.644 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 132 for sweep.
2023-02-07 13:07:27.644 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 9 for sweep.
2023-02-07 13:07:27.644 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0008281083411133536 for sweep.
2023-02-07 13:07:27.645 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 15 for sweep.
2023-02-07 13:07:27.645 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.020436223083739102 for sweep.
2023-02-07 13:07:27.645 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3390 for sweep.
2023-02-07 13:07:27.646 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 366 for sweep.
2023-02-07 13:07:27.646 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.012175168542252096 for sweep.
2023-02-07 13:07:27.646 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.12717334958862547 for sweep.
2023-02-07 13:07:27.646 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2801443554908584 for sweep.
2023-02-07 13:07:27.647 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:07:27.650 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130720-9gowzk7k/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 353, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 132, 'window': 9, 'min_count': 4, 'dm': 0, 'sample': 0.2784585522733096, 'workers': 4, 'alpha': 0.001060154980113281, 'epochs': 41}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3390, 'max_depth': 15, 'num_leaves': 366, 'reg_alpha': 0.012175168542252096, 'reg_lambda': 0.12717334958862547, 'subsample': 0.2801443554908584, 'min_child_weight': 0.020436223083739102, 'n_jobs': 4, 'learning_rate': 0.0008281083411133536}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|‚ñè         | 42/3257 [00:00<00:07, 419.23it/s]  3%|‚ñé         | 90/3257 [00:00<00:07, 444.31it/s]  4%|‚ñç         | 135/3257 [00:00<00:07, 438.89it/s]  6%|‚ñå         | 180/3257 [00:00<00:06, 442.05it/s]  7%|‚ñã         | 232/3257 [00:00<00:06, 469.47it/s]  9%|‚ñä         | 284/3257 [00:00<00:06, 484.55it/s] 10%|‚ñà         | 333/3257 [00:00<00:06, 481.56it/s] 12%|‚ñà‚ñè        | 382/3257 [00:00<00:06, 465.25it/s] 13%|‚ñà‚ñé        | 429/3257 [00:00<00:06, 443.30it/s] 15%|‚ñà‚ñç        | 476/3257 [00:01<00:06, 450.18it/s] 16%|‚ñà‚ñå        | 522/3257 [00:01<00:08, 321.36it/s] 17%|‚ñà‚ñã        | 564/3257 [00:01<00:07, 342.80it/s] 19%|‚ñà‚ñä        | 610/3257 [00:01<00:07, 371.07it/s] 20%|‚ñà‚ñà        | 654/3257 [00:01<00:06, 387.86it/s] 21%|‚ñà‚ñà‚ñè       | 696/3257 [00:01<00:06, 394.76it/s] 23%|‚ñà‚ñà‚ñé       | 739/3257 [00:01<00:06, 403.83it/s] 24%|‚ñà‚ñà‚ñç       | 785/3257 [00:01<00:05, 418.87it/s] 26%|‚ñà‚ñà‚ñå       | 831/3257 [00:01<00:05, 429.58it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:02<00:05, 424.43it/s] 28%|‚ñà‚ñà‚ñä       | 923/3257 [00:02<00:05, 440.38it/s] 30%|‚ñà‚ñà‚ñâ       | 970/3257 [00:02<00:05, 446.75it/s] 31%|‚ñà‚ñà‚ñà       | 1016/3257 [00:02<00:05, 444.64it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1061/3257 [00:02<00:05, 437.90it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1107/3257 [00:02<00:04, 441.81it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1152/3257 [00:02<00:04, 435.83it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1196/3257 [00:02<00:04, 429.55it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:02<00:04, 437.67it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1286/3257 [00:03<00:04, 426.58it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1332/3257 [00:03<00:04, 435.10it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1376/3257 [00:03<00:04, 436.09it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1424/3257 [00:03<00:04, 445.92it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1475/3257 [00:03<00:03, 462.52it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1522/3257 [00:03<00:03, 461.44it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1569/3257 [00:03<00:03, 451.31it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1617/3257 [00:03<00:03, 457.13it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1663/3257 [00:03<00:03, 442.71it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1708/3257 [00:03<00:03, 442.67it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1753/3257 [00:04<00:03, 432.02it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1803/3257 [00:04<00:03, 448.85it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1848/3257 [00:04<00:04, 327.44it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1896/3257 [00:04<00:03, 362.30it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1947/3257 [00:04<00:03, 398.07it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1997/3257 [00:04<00:02, 422.93it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2045/3257 [00:04<00:02, 436.45it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2091/3257 [00:04<00:02, 433.82it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2136/3257 [00:05<00:02, 431.43it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:05<00:02, 436.03it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2228/3257 [00:05<00:02, 443.97it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2273/3257 [00:05<00:02, 430.76it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2326/3257 [00:05<00:02, 458.05it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2377/3257 [00:05<00:01, 471.00it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2425/3257 [00:05<00:01, 472.79it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2473/3257 [00:05<00:01, 466.06it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2524/3257 [00:05<00:01, 477.77it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2572/3257 [00:05<00:01, 459.00it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2624/3257 [00:06<00:01, 475.58it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2672/3257 [00:06<00:01, 470.31it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2720/3257 [00:06<00:01, 444.46it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2769/3257 [00:06<00:01, 453.65it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2817/3257 [00:06<00:00, 458.75it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2865/3257 [00:06<00:00, 464.27it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2912/3257 [00:06<00:00, 462.97it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2959/3257 [00:06<00:00, 450.90it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3008/3257 [00:06<00:00, 459.66it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3057/3257 [00:07<00:00, 468.31it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3109/3257 [00:07<00:00, 480.64it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3158/3257 [00:07<00:00, 468.55it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3205/3257 [00:07<00:00, 462.56it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:07<00:00, 461.00it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:07<00:00, 437.55it/s]
2023-02-07 13:07:35.286 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:07:35,288][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d132,n5,mc4,s0.278459,t4>', 'datetime': '2023-02-07T13:07:35.288170', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:07:35,288][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:07:35,288][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:07:35,424][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:07:35,425][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:07:35,427][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 742 unique words (80.30% of original 924, drops 182)', 'datetime': '2023-02-07T13:07:35.427158', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:07:35,427][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 1455401 word corpus (99.98% of original 1455748, drops 347)', 'datetime': '2023-02-07T13:07:35.427328', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:07:35,429][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:07:35,429][gensim.models.word2vec][INFO] - sample=0.278459 downsamples 0 most-common words
[2023-02-07 13:07:35,430][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455401 word corpus (100.0%% of prior 1455401)', 'datetime': '2023-02-07T13:07:35.430077', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:07:35,434][gensim.models.word2vec][INFO] - estimated required memory for 742 words and 132 dimensions: 3525648 bytes
[2023-02-07 13:07:35,434][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:07:35,437][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 742 vocabulary and 132 features, using sg=1 hs=0 sample=0.2784585522733096 negative=5 window=9 shrink_windows=True', 'datetime': '2023-02-07T13:07:35.437102', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:07:36,095][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458658 effective words) took 0.7s, 2224712 effective words/s
[2023-02-07 13:07:36,746][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458658 effective words) took 0.6s, 2246107 effective words/s
[2023-02-07 13:07:37,399][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458658 effective words) took 0.7s, 2235743 effective words/s
[2023-02-07 13:07:38,050][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458658 effective words) took 0.6s, 2247090 effective words/s
[2023-02-07 13:07:38,708][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458658 effective words) took 0.7s, 2224511 effective words/s
[2023-02-07 13:07:39,365][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458658 effective words) took 0.7s, 2227229 effective words/s
[2023-02-07 13:07:40,015][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458658 effective words) took 0.6s, 2247768 effective words/s
[2023-02-07 13:07:40,663][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458658 effective words) took 0.6s, 2255475 effective words/s
[2023-02-07 13:07:41,320][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458658 effective words) took 0.7s, 2225396 effective words/s
[2023-02-07 13:07:41,969][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458658 effective words) took 0.6s, 2254517 effective words/s
[2023-02-07 13:07:42,611][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458658 effective words) took 0.6s, 2274515 effective words/s
[2023-02-07 13:07:43,265][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458658 effective words) took 0.7s, 2236255 effective words/s
[2023-02-07 13:07:43,910][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458658 effective words) took 0.6s, 2267041 effective words/s
[2023-02-07 13:07:44,548][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458658 effective words) took 0.6s, 2292114 effective words/s
[2023-02-07 13:07:45,198][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458658 effective words) took 0.6s, 2247277 effective words/s
[2023-02-07 13:07:45,850][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458658 effective words) took 0.7s, 2243641 effective words/s
[2023-02-07 13:07:46,490][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458658 effective words) took 0.6s, 2281848 effective words/s
[2023-02-07 13:07:47,130][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458658 effective words) took 0.6s, 2284240 effective words/s
[2023-02-07 13:07:47,775][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458658 effective words) took 0.6s, 2268924 effective words/s
[2023-02-07 13:07:48,420][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458658 effective words) took 0.6s, 2267466 effective words/s
[2023-02-07 13:07:49,054][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458658 effective words) took 0.6s, 2304402 effective words/s
[2023-02-07 13:07:49,679][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458658 effective words) took 0.6s, 2342855 effective words/s
[2023-02-07 13:07:50,313][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458658 effective words) took 0.6s, 2308777 effective words/s
[2023-02-07 13:07:50,970][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458658 effective words) took 0.7s, 2226983 effective words/s
[2023-02-07 13:07:51,601][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458658 effective words) took 0.6s, 2317599 effective words/s
[2023-02-07 13:07:52,253][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458658 effective words) took 0.6s, 2248540 effective words/s
[2023-02-07 13:07:52,883][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458658 effective words) took 0.6s, 2324454 effective words/s
[2023-02-07 13:07:53,515][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458658 effective words) took 0.6s, 2313323 effective words/s
[2023-02-07 13:07:54,155][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458658 effective words) took 0.6s, 2284616 effective words/s
[2023-02-07 13:07:54,819][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458658 effective words) took 0.7s, 2201353 effective words/s
[2023-02-07 13:07:55,473][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458658 effective words) took 0.7s, 2240495 effective words/s
[2023-02-07 13:07:56,104][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458658 effective words) took 0.6s, 2316782 effective words/s
[2023-02-07 13:07:56,773][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458658 effective words) took 0.7s, 2185680 effective words/s
[2023-02-07 13:07:57,402][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458658 effective words) took 0.6s, 2324830 effective words/s
[2023-02-07 13:07:58,073][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458658 effective words) took 0.7s, 2180712 effective words/s
[2023-02-07 13:07:58,695][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458658 effective words) took 0.6s, 2352432 effective words/s
[2023-02-07 13:07:59,351][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458658 effective words) took 0.7s, 2228734 effective words/s
[2023-02-07 13:07:59,977][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458658 effective words) took 0.6s, 2337146 effective words/s
[2023-02-07 13:08:00,640][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458658 effective words) took 0.7s, 2206153 effective words/s
[2023-02-07 13:08:01,259][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458658 effective words) took 0.6s, 2364662 effective words/s
[2023-02-07 13:08:01,893][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458658 effective words) took 0.6s, 2308517 effective words/s
[2023-02-07 13:08:01,894][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 59685668 raw words (59804978 effective words) took 26.5s, 2260538 effective words/s', 'datetime': '2023-02-07T13:08:01.894168', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:08:01.894 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:08:02,968][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130720-9gowzk7k/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:08:02.968611', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:08:02,969][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:08:02,975][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130720-9gowzk7k/files/../tmp/embedding_model.pt
2023-02-07 13:08:02.975 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:08:04.112 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:08:04.568 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:08:05.398 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9770795015794318, 'test_mae': 1.0564544072111874, 'test_r2': 0.06472318638282537}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.68
wandb: percentage 0.19697
wandb:   test_mae 1.05645
wandb:   test_mse 1.97708
wandb:    test_r2 0.06472
wandb: 
wandb: üöÄ View run lilac-sweep-29 at: https://wandb.ai/xiaoqiz/mof2vec/runs/9gowzk7k
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_130720-9gowzk7k/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: cm4p1qja with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 182
wandb: 	model.gensim.alpha: 0.10879846733275804
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 59
wandb: 	model.gensim.min_count: 10
wandb: 	model.gensim.sample: 0.4985818368303579
wandb: 	model.gensim.vector_size: 128
wandb: 	model.gensim.window: 6
wandb: 	model.sklearn.learning_rate: 0.10700332441268284
wandb: 	model.sklearn.max_depth: 85
wandb: 	model.sklearn.min_child_weight: 0.007339918006278501
wandb: 	model.sklearn.n_estimators: 3256
wandb: 	model.sklearn.num_leaves: 493
wandb: 	model.sklearn.reg_alpha: 0.024010160631637096
wandb: 	model.sklearn.reg_lambda: 0.6160487059359099
wandb: 	model.sklearn.subsample: 0.2406841110942419
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130822-cm4p1qja
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-sweep-30
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/cm4p1qja
2023-02-07 13:08:29.854 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:08:29.854 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 182 for sweep.
2023-02-07 13:08:29.855 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.10879846733275804 for sweep.
2023-02-07 13:08:29.855 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:08:29.855 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 59 for sweep.
2023-02-07 13:08:29.855 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 10 for sweep.
2023-02-07 13:08:29.856 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4985818368303579 for sweep.
2023-02-07 13:08:29.856 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 128 for sweep.
2023-02-07 13:08:29.856 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 6 for sweep.
2023-02-07 13:08:29.856 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.10700332441268284 for sweep.
2023-02-07 13:08:29.856 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 85 for sweep.
2023-02-07 13:08:29.857 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.007339918006278501 for sweep.
2023-02-07 13:08:29.857 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3256 for sweep.
2023-02-07 13:08:29.857 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 493 for sweep.
2023-02-07 13:08:29.857 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.024010160631637096 for sweep.
2023-02-07 13:08:29.858 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.6160487059359099 for sweep.
2023-02-07 13:08:29.858 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2406841110942419 for sweep.
2023-02-07 13:08:29.858 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:08:29.864 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130822-cm4p1qja/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 182, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 128, 'window': 6, 'min_count': 10, 'dm': 0, 'sample': 0.4985818368303579, 'workers': 4, 'alpha': 0.10879846733275804, 'epochs': 59}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3256, 'max_depth': 85, 'num_leaves': 493, 'reg_alpha': 0.024010160631637096, 'reg_lambda': 0.6160487059359099, 'subsample': 0.2406841110942419, 'min_child_weight': 0.007339918006278501, 'n_jobs': 4, 'learning_rate': 0.10700332441268284}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:31, 103.18it/s]  2%|‚ñè         | 61/3257 [00:00<00:12, 257.58it/s]  3%|‚ñé         | 106/3257 [00:00<00:09, 330.03it/s]  5%|‚ñç         | 155/3257 [00:00<00:08, 386.98it/s]  6%|‚ñå         | 197/3257 [00:00<00:07, 395.19it/s]  8%|‚ñä         | 245/3257 [00:00<00:07, 421.24it/s]  9%|‚ñâ         | 293/3257 [00:00<00:06, 439.40it/s] 10%|‚ñà         | 338/3257 [00:00<00:06, 421.39it/s] 12%|‚ñà‚ñè        | 381/3257 [00:00<00:06, 419.53it/s] 13%|‚ñà‚ñé        | 424/3257 [00:01<00:06, 413.02it/s] 14%|‚ñà‚ñç        | 466/3257 [00:01<00:07, 393.31it/s] 16%|‚ñà‚ñå        | 507/3257 [00:01<00:06, 397.14it/s] 17%|‚ñà‚ñã        | 550/3257 [00:01<00:06, 406.55it/s] 18%|‚ñà‚ñä        | 594/3257 [00:01<00:06, 414.78it/s] 20%|‚ñà‚ñâ        | 639/3257 [00:01<00:06, 420.10it/s] 21%|‚ñà‚ñà        | 682/3257 [00:01<00:06, 401.89it/s] 22%|‚ñà‚ñà‚ñè       | 723/3257 [00:01<00:06, 382.14it/s] 24%|‚ñà‚ñà‚ñé       | 768/3257 [00:01<00:06, 398.27it/s] 25%|‚ñà‚ñà‚ñç       | 811/3257 [00:02<00:06, 405.67it/s] 26%|‚ñà‚ñà‚ñå       | 852/3257 [00:02<00:06, 364.43it/s] 27%|‚ñà‚ñà‚ñã       | 890/3257 [00:02<00:07, 296.86it/s] 28%|‚ñà‚ñà‚ñä       | 923/3257 [00:02<00:08, 281.58it/s] 29%|‚ñà‚ñà‚ñâ       | 953/3257 [00:02<00:08, 267.08it/s] 30%|‚ñà‚ñà‚ñà       | 981/3257 [00:02<00:08, 261.83it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1023/3257 [00:02<00:07, 301.33it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1061/3257 [00:02<00:06, 320.87it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1103/3257 [00:03<00:06, 347.10it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1144/3257 [00:03<00:05, 363.87it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1182/3257 [00:03<00:06, 341.80it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1218/3257 [00:03<00:05, 345.66it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1259/3257 [00:03<00:05, 361.90it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1296/3257 [00:03<00:05, 327.49it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1330/3257 [00:03<00:05, 324.87it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1374/3257 [00:03<00:05, 354.60it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1411/3257 [00:04<00:07, 261.46it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1460/3257 [00:04<00:05, 312.22it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1509/3257 [00:04<00:04, 353.46it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1549/3257 [00:04<00:04, 360.65it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1592/3257 [00:04<00:04, 378.37it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1635/3257 [00:04<00:04, 392.48it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1676/3257 [00:04<00:04, 388.50it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1720/3257 [00:04<00:03, 401.96it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1762/3257 [00:04<00:03, 395.94it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:05<00:03, 402.40it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1846/3257 [00:05<00:03, 401.23it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1888/3257 [00:05<00:03, 406.51it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1929/3257 [00:05<00:03, 407.28it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:05<00:02, 427.44it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2020/3257 [00:05<00:03, 387.67it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2060/3257 [00:05<00:03, 366.17it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2098/3257 [00:05<00:03, 365.87it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2138/3257 [00:05<00:03, 372.90it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2177/3257 [00:05<00:02, 377.43it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2219/3257 [00:06<00:02, 389.39it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2261/3257 [00:06<00:02, 396.44it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2305/3257 [00:06<00:02, 408.62it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2359/3257 [00:06<00:02, 445.53it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:06<00:01, 441.26it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2450/3257 [00:06<00:01, 422.31it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2499/3257 [00:06<00:01, 441.55it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2546/3257 [00:06<00:01, 448.19it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2592/3257 [00:06<00:01, 422.00it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:07<00:01, 434.23it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2684/3257 [00:07<00:01, 431.65it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2728/3257 [00:07<00:01, 414.25it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2770/3257 [00:07<00:01, 414.97it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2812/3257 [00:07<00:01, 387.99it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2852/3257 [00:07<00:01, 373.57it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2890/3257 [00:07<00:01, 362.11it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2927/3257 [00:07<00:01, 253.72it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2966/3257 [00:08<00:01, 282.70it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3010/3257 [00:08<00:00, 318.99it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3046/3257 [00:08<00:00, 319.52it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3083/3257 [00:08<00:00, 332.07it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3125/3257 [00:08<00:00, 353.81it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3163/3257 [00:08<00:00, 351.66it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3201/3257 [00:08<00:00, 358.21it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:08<00:00, 375.77it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 367.73it/s]
2023-02-07 13:08:38.884 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:08:38,885][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d128,n5,mc10,s0.498582,t4>', 'datetime': '2023-02-07T13:08:38.885257', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:08:38,886][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:08:38,886][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:08:39,020][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:08:39,020][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:08:39,022][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=10 retains 456 unique words (49.35% of original 924, drops 468)', 'datetime': '2023-02-07T13:08:39.022343', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:08:39,022][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 1453848 word corpus (99.87% of original 1455748, drops 1900)', 'datetime': '2023-02-07T13:08:39.022505', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:08:39,024][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:08:39,024][gensim.models.word2vec][INFO] - sample=0.498582 downsamples 0 most-common words
[2023-02-07 13:08:39,024][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1453848 word corpus (100.0%% of prior 1453848)', 'datetime': '2023-02-07T13:08:39.024316', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:08:39,027][gensim.models.word2vec][INFO] - estimated required memory for 456 words and 128 dimensions: 3013928 bytes
[2023-02-07 13:08:39,027][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:08:39,029][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 456 vocabulary and 128 features, using sg=1 hs=0 sample=0.4985818368303579 negative=5 window=6 shrink_windows=True', 'datetime': '2023-02-07T13:08:39.029262', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:08:39,340][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1457105 effective words) took 0.3s, 4714318 effective words/s
[2023-02-07 13:08:39,638][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1457105 effective words) took 0.3s, 4893174 effective words/s
[2023-02-07 13:08:39,936][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1457105 effective words) took 0.3s, 4924633 effective words/s
[2023-02-07 13:08:40,227][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1457105 effective words) took 0.3s, 5020175 effective words/s
[2023-02-07 13:08:40,518][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1457105 effective words) took 0.3s, 5030906 effective words/s
[2023-02-07 13:08:40,808][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1457105 effective words) took 0.3s, 5049778 effective words/s
[2023-02-07 13:08:41,097][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1457105 effective words) took 0.3s, 5059834 effective words/s
[2023-02-07 13:08:41,384][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1457105 effective words) took 0.3s, 5087229 effective words/s
[2023-02-07 13:08:41,674][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1457105 effective words) took 0.3s, 5044093 effective words/s
[2023-02-07 13:08:41,976][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1457105 effective words) took 0.3s, 4851971 effective words/s
[2023-02-07 13:08:42,289][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1457105 effective words) took 0.3s, 4682150 effective words/s
[2023-02-07 13:08:42,599][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1457105 effective words) took 0.3s, 4726267 effective words/s
[2023-02-07 13:08:42,909][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1457105 effective words) took 0.3s, 4722738 effective words/s
[2023-02-07 13:08:43,219][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1457105 effective words) took 0.3s, 4721558 effective words/s
[2023-02-07 13:08:43,527][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1457105 effective words) took 0.3s, 4750740 effective words/s
[2023-02-07 13:08:43,841][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1457105 effective words) took 0.3s, 4657159 effective words/s
[2023-02-07 13:08:44,152][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1457105 effective words) took 0.3s, 4703002 effective words/s
[2023-02-07 13:08:44,463][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1457105 effective words) took 0.3s, 4703948 effective words/s
[2023-02-07 13:08:44,770][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1457105 effective words) took 0.3s, 4773942 effective words/s
[2023-02-07 13:08:45,079][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1457105 effective words) took 0.3s, 4726433 effective words/s
[2023-02-07 13:08:45,388][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1457105 effective words) took 0.3s, 4743345 effective words/s
[2023-02-07 13:08:45,703][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1457105 effective words) took 0.3s, 4638693 effective words/s
[2023-02-07 13:08:46,019][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1457105 effective words) took 0.3s, 4629136 effective words/s
[2023-02-07 13:08:46,333][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1457105 effective words) took 0.3s, 4666244 effective words/s
[2023-02-07 13:08:46,645][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1457105 effective words) took 0.3s, 4683808 effective words/s
[2023-02-07 13:08:46,963][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1457105 effective words) took 0.3s, 4604653 effective words/s
[2023-02-07 13:08:47,283][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1457105 effective words) took 0.3s, 4566363 effective words/s
[2023-02-07 13:08:47,603][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1457105 effective words) took 0.3s, 4573696 effective words/s
[2023-02-07 13:08:47,923][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1457105 effective words) took 0.3s, 4582453 effective words/s
[2023-02-07 13:08:48,245][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1457105 effective words) took 0.3s, 4539730 effective words/s
[2023-02-07 13:08:48,567][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1457105 effective words) took 0.3s, 4548416 effective words/s
[2023-02-07 13:08:48,887][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1457105 effective words) took 0.3s, 4567473 effective words/s
[2023-02-07 13:08:49,212][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1457105 effective words) took 0.3s, 4493325 effective words/s
[2023-02-07 13:08:49,531][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1457105 effective words) took 0.3s, 4591548 effective words/s
[2023-02-07 13:08:49,858][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1457105 effective words) took 0.3s, 4482011 effective words/s
[2023-02-07 13:08:50,177][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1457105 effective words) took 0.3s, 4582987 effective words/s
[2023-02-07 13:08:50,499][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1457105 effective words) took 0.3s, 4560063 effective words/s
[2023-02-07 13:08:50,825][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1457105 effective words) took 0.3s, 4482929 effective words/s
[2023-02-07 13:08:51,146][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1457105 effective words) took 0.3s, 4563647 effective words/s
[2023-02-07 13:08:51,473][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1457105 effective words) took 0.3s, 4469211 effective words/s
[2023-02-07 13:08:51,799][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1457105 effective words) took 0.3s, 4483301 effective words/s
[2023-02-07 13:08:52,128][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1457105 effective words) took 0.3s, 4449980 effective words/s
[2023-02-07 13:08:52,458][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1457105 effective words) took 0.3s, 4431607 effective words/s
[2023-02-07 13:08:52,789][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1457105 effective words) took 0.3s, 4421212 effective words/s
[2023-02-07 13:08:53,118][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1457105 effective words) took 0.3s, 4447036 effective words/s
[2023-02-07 13:08:53,438][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1457105 effective words) took 0.3s, 4576434 effective words/s
[2023-02-07 13:08:53,766][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1457105 effective words) took 0.3s, 4465343 effective words/s
[2023-02-07 13:08:54,099][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1457105 effective words) took 0.3s, 4389174 effective words/s
[2023-02-07 13:08:54,436][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1457105 effective words) took 0.3s, 4339564 effective words/s
[2023-02-07 13:08:54,761][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1457105 effective words) took 0.3s, 4497442 effective words/s
[2023-02-07 13:08:55,096][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1457105 effective words) took 0.3s, 4367321 effective words/s
[2023-02-07 13:08:55,431][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1457105 effective words) took 0.3s, 4372214 effective words/s
[2023-02-07 13:08:55,770][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1457105 effective words) took 0.3s, 4313971 effective words/s
[2023-02-07 13:08:56,107][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1457105 effective words) took 0.3s, 4345924 effective words/s
[2023-02-07 13:08:56,448][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1457105 effective words) took 0.3s, 4287495 effective words/s
[2023-02-07 13:08:56,791][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1457105 effective words) took 0.3s, 4258014 effective words/s
[2023-02-07 13:08:57,134][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1457105 effective words) took 0.3s, 4257387 effective words/s
[2023-02-07 13:08:57,482][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1457105 effective words) took 0.3s, 4211986 effective words/s
[2023-02-07 13:08:57,826][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1457105 effective words) took 0.3s, 4242479 effective words/s
[2023-02-07 13:08:57,827][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 85889132 raw words (85969195 effective words) took 18.8s, 4573409 effective words/s', 'datetime': '2023-02-07T13:08:57.827221', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:08:57.828 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:08:59,024][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130822-cm4p1qja/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:08:59.024300', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:08:59,024][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:08:59,032][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130822-cm4p1qja/files/../tmp/embedding_model.pt
2023-02-07 13:08:59.033 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:09:00.187 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:09:00.642 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:09:01.535 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0319267883675227, 'test_mae': 1.0999099745592273, 'test_r2': 0.038777140418698286}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.35
wandb: percentage 0.50649
wandb:   test_mae 1.09991
wandb:   test_mse 2.03193
wandb:    test_r2 0.03878
wandb: 
wandb: üöÄ View run rosy-sweep-30 at: https://wandb.ai/xiaoqiz/mof2vec/runs/cm4p1qja
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_130822-cm4p1qja/logs
wandb: Agent Starting Run: 83ab1wmt with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 875
wandb: 	model.gensim.alpha: 0.004555340819121709
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 73
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.5154253799117479
wandb: 	model.gensim.vector_size: 95
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.5115629372407825
wandb: 	model.sklearn.max_depth: 20
wandb: 	model.sklearn.min_child_weight: 0.032264439411532224
wandb: 	model.sklearn.n_estimators: 1965
wandb: 	model.sklearn.num_leaves: 386
wandb: 	model.sklearn.reg_alpha: 0.5419430348728784
wandb: 	model.sklearn.reg_lambda: 0.02130610702533657
wandb: 	model.sklearn.subsample: 0.24018672572783456
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130913-83ab1wmt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-31
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/83ab1wmt
2023-02-07 13:09:21.755 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 13:09:21.756 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 875 for sweep.
2023-02-07 13:09:21.756 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.004555340819121709 for sweep.
2023-02-07 13:09:21.757 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:09:21.757 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 73 for sweep.
2023-02-07 13:09:21.757 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 13:09:21.758 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5154253799117479 for sweep.
2023-02-07 13:09:21.758 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 95 for sweep.
2023-02-07 13:09:21.758 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 13:09:21.758 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.5115629372407825 for sweep.
2023-02-07 13:09:21.759 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 20 for sweep.
2023-02-07 13:09:21.759 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.032264439411532224 for sweep.
2023-02-07 13:09:21.759 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1965 for sweep.
2023-02-07 13:09:21.759 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 386 for sweep.
2023-02-07 13:09:21.759 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.5419430348728784 for sweep.
2023-02-07 13:09:21.760 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.02130610702533657 for sweep.
2023-02-07 13:09:21.760 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.24018672572783456 for sweep.
2023-02-07 13:09:21.760 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:09:21.768 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130913-83ab1wmt/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 875, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 95, 'window': 3, 'min_count': 3, 'dm': 0, 'sample': 0.5154253799117479, 'workers': 4, 'alpha': 0.004555340819121709, 'epochs': 73}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1965, 'max_depth': 20, 'num_leaves': 386, 'reg_alpha': 0.5419430348728784, 'reg_lambda': 0.02130610702533657, 'subsample': 0.24018672572783456, 'min_child_weight': 0.032264439411532224, 'n_jobs': 4, 'learning_rate': 0.5115629372407825}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 29/3257 [00:00<00:11, 282.49it/s]  2%|‚ñè         | 58/3257 [00:00<00:11, 278.22it/s]  3%|‚ñé         | 90/3257 [00:00<00:10, 292.72it/s]  4%|‚ñé         | 120/3257 [00:00<00:10, 288.08it/s]  5%|‚ñç         | 152/3257 [00:00<00:10, 296.26it/s]  6%|‚ñå         | 182/3257 [00:00<00:10, 286.73it/s]  7%|‚ñã         | 215/3257 [00:00<00:10, 299.48it/s]  8%|‚ñä         | 248/3257 [00:00<00:09, 307.21it/s]  9%|‚ñä         | 283/3257 [00:00<00:09, 318.28it/s] 10%|‚ñâ         | 315/3257 [00:01<00:09, 311.30it/s] 11%|‚ñà         | 347/3257 [00:01<00:09, 310.40it/s] 12%|‚ñà‚ñè        | 379/3257 [00:01<00:09, 304.57it/s] 13%|‚ñà‚ñé        | 410/3257 [00:01<00:09, 304.90it/s] 14%|‚ñà‚ñé        | 441/3257 [00:01<00:10, 273.41it/s] 14%|‚ñà‚ñç        | 472/3257 [00:01<00:09, 283.00it/s] 15%|‚ñà‚ñå        | 503/3257 [00:01<00:09, 289.55it/s] 16%|‚ñà‚ñã        | 533/3257 [00:01<00:09, 285.80it/s] 17%|‚ñà‚ñã        | 562/3257 [00:01<00:09, 275.52it/s] 18%|‚ñà‚ñä        | 590/3257 [00:02<00:09, 272.58it/s] 19%|‚ñà‚ñâ        | 621/3257 [00:02<00:09, 280.88it/s] 20%|‚ñà‚ñâ        | 650/3257 [00:02<00:09, 279.76it/s] 21%|‚ñà‚ñà        | 679/3257 [00:02<00:09, 270.60it/s] 22%|‚ñà‚ñà‚ñè       | 707/3257 [00:02<00:09, 272.19it/s] 23%|‚ñà‚ñà‚ñé       | 736/3257 [00:02<00:09, 269.35it/s] 24%|‚ñà‚ñà‚ñé       | 768/3257 [00:02<00:08, 281.32it/s] 24%|‚ñà‚ñà‚ñç       | 797/3257 [00:02<00:09, 273.05it/s] 25%|‚ñà‚ñà‚ñå       | 825/3257 [00:02<00:09, 257.20it/s] 26%|‚ñà‚ñà‚ñå       | 851/3257 [00:03<00:09, 247.92it/s] 27%|‚ñà‚ñà‚ñã       | 877/3257 [00:03<00:09, 246.41it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:03<00:09, 257.68it/s] 29%|‚ñà‚ñà‚ñä       | 933/3257 [00:03<00:08, 260.05it/s] 30%|‚ñà‚ñà‚ñâ       | 962/3257 [00:03<00:08, 268.33it/s] 30%|‚ñà‚ñà‚ñà       | 989/3257 [00:03<00:08, 267.20it/s] 31%|‚ñà‚ñà‚ñà       | 1016/3257 [00:03<00:11, 188.03it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1041/3257 [00:03<00:11, 198.73it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1072/3257 [00:03<00:09, 224.48it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1098/3257 [00:04<00:09, 233.15it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1127/3257 [00:04<00:08, 244.07it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1156/3257 [00:04<00:08, 255.85it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1183/3257 [00:04<00:08, 248.85it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1209/3257 [00:04<00:08, 235.43it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:04<00:07, 257.10it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1272/3257 [00:04<00:07, 266.23it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1300/3257 [00:04<00:07, 258.42it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:04<00:07, 259.71it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1355/3257 [00:05<00:07, 256.56it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1381/3257 [00:05<00:07, 253.21it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1414/3257 [00:05<00:06, 274.47it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1445/3257 [00:05<00:06, 282.26it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1477/3257 [00:05<00:06, 291.84it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:05<00:05, 304.07it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1542/3257 [00:05<00:06, 282.77it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1571/3257 [00:05<00:05, 283.18it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1601/3257 [00:05<00:05, 287.54it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1631/3257 [00:06<00:05, 289.72it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1661/3257 [00:06<00:05, 283.44it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1691/3257 [00:06<00:05, 285.38it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1721/3257 [00:06<00:05, 288.34it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1750/3257 [00:06<00:05, 282.46it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1784/3257 [00:06<00:04, 297.42it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1814/3257 [00:06<00:04, 295.46it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1845/3257 [00:06<00:04, 297.57it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1878/3257 [00:06<00:04, 306.97it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1909/3257 [00:06<00:04, 304.84it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1940/3257 [00:07<00:04, 304.55it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:07<00:03, 320.65it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2011/3257 [00:07<00:03, 324.29it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2044/3257 [00:07<00:03, 319.83it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2076/3257 [00:07<00:03, 310.39it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2108/3257 [00:07<00:03, 305.89it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2139/3257 [00:07<00:03, 301.57it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2170/3257 [00:07<00:03, 293.75it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2201/3257 [00:07<00:03, 297.09it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2231/3257 [00:08<00:05, 198.30it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2260/3257 [00:08<00:04, 216.87it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2291/3257 [00:08<00:04, 238.61it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2329/3257 [00:08<00:03, 272.71it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2365/3257 [00:08<00:03, 295.47it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2399/3257 [00:08<00:02, 307.33it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2432/3257 [00:08<00:02, 294.08it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2463/3257 [00:08<00:02, 298.06it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2496/3257 [00:09<00:02, 304.28it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2530/3257 [00:09<00:02, 314.20it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2562/3257 [00:09<00:02, 306.73it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2594/3257 [00:09<00:02, 292.10it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2633/3257 [00:09<00:01, 318.76it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2666/3257 [00:09<00:01, 305.33it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2697/3257 [00:09<00:01, 301.26it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2728/3257 [00:09<00:01, 285.12it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:09<00:01, 291.80it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2794/3257 [00:10<00:01, 302.06it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2825/3257 [00:10<00:01, 297.01it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2860/3257 [00:10<00:01, 310.56it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2894/3257 [00:10<00:01, 314.18it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:10<00:01, 317.51it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2960/3257 [00:10<00:00, 305.72it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2991/3257 [00:10<00:00, 304.58it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3025/3257 [00:10<00:00, 313.19it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3063/3257 [00:10<00:00, 329.97it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3097/3257 [00:10<00:00, 326.70it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3130/3257 [00:11<00:00, 326.77it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3163/3257 [00:11<00:00, 308.23it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3195/3257 [00:11<00:00, 303.42it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3226/3257 [00:11<00:00, 294.64it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 283.35it/s]
2023-02-07 13:09:33.589 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:09:33,590][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d95,n5,mc3,s0.515425,t4>', 'datetime': '2023-02-07T13:09:33.590697', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:09:33,592][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:09:33,592][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:09:33,836][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 13:09:33,836][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:09:33,847][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 4875 unique words (73.18% of original 6662, drops 1787)', 'datetime': '2023-02-07T13:09:33.847805', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:09:33,848][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 2908813 word corpus (99.91% of original 2911496, drops 2683)', 'datetime': '2023-02-07T13:09:33.848004', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:09:33,863][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 13:09:33,863][gensim.models.word2vec][INFO] - sample=0.515425 downsamples 0 most-common words
[2023-02-07 13:09:33,863][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2908813 word corpus (100.0%% of prior 2908813)', 'datetime': '2023-02-07T13:09:33.863707', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:09:33,889][gensim.models.word2vec][INFO] - estimated required memory for 4875 words and 95 dimensions: 8031560 bytes
[2023-02-07 13:09:33,889][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:09:33,892][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 4875 vocabulary and 95 features, using sg=1 hs=0 sample=0.5154253799117479 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T13:09:33.892804', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:09:34,851][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2912070 effective words) took 1.0s, 3041284 effective words/s
[2023-02-07 13:09:35,707][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2912070 effective words) took 0.9s, 3409193 effective words/s
[2023-02-07 13:09:36,592][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2912070 effective words) took 0.9s, 3294617 effective words/s
[2023-02-07 13:09:37,465][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2912070 effective words) took 0.9s, 3339998 effective words/s
[2023-02-07 13:09:38,339][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2912070 effective words) took 0.9s, 3337071 effective words/s
[2023-02-07 13:09:39,222][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2912070 effective words) took 0.9s, 3304059 effective words/s
[2023-02-07 13:09:40,084][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2912070 effective words) took 0.9s, 3380255 effective words/s
[2023-02-07 13:09:40,947][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2912070 effective words) took 0.9s, 3378129 effective words/s
[2023-02-07 13:09:41,823][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2912070 effective words) took 0.9s, 3332290 effective words/s
[2023-02-07 13:09:42,701][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2912070 effective words) took 0.9s, 3322837 effective words/s
[2023-02-07 13:09:43,558][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2912070 effective words) took 0.9s, 3401783 effective words/s
[2023-02-07 13:09:44,416][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2912070 effective words) took 0.9s, 3400120 effective words/s
[2023-02-07 13:09:45,269][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2912070 effective words) took 0.9s, 3420214 effective words/s
[2023-02-07 13:09:46,127][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2912070 effective words) took 0.9s, 3398233 effective words/s
[2023-02-07 13:09:46,986][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2912070 effective words) took 0.9s, 3394427 effective words/s
[2023-02-07 13:09:47,850][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2912070 effective words) took 0.9s, 3373631 effective words/s
[2023-02-07 13:09:48,708][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2912070 effective words) took 0.9s, 3398063 effective words/s
[2023-02-07 13:09:49,563][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2912070 effective words) took 0.9s, 3408839 effective words/s
[2023-02-07 13:09:50,408][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2912070 effective words) took 0.8s, 3451315 effective words/s
[2023-02-07 13:09:51,261][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2912070 effective words) took 0.9s, 3417939 effective words/s
[2023-02-07 13:09:52,104][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2912070 effective words) took 0.8s, 3457391 effective words/s
[2023-02-07 13:09:52,947][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2912070 effective words) took 0.8s, 3460300 effective words/s
[2023-02-07 13:09:53,785][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2912070 effective words) took 0.8s, 3479808 effective words/s
[2023-02-07 13:09:54,616][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2912070 effective words) took 0.8s, 3509195 effective words/s
[2023-02-07 13:09:55,452][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2912070 effective words) took 0.8s, 3488256 effective words/s
[2023-02-07 13:09:56,289][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2912070 effective words) took 0.8s, 3483025 effective words/s
[2023-02-07 13:09:57,131][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2912070 effective words) took 0.8s, 3462567 effective words/s
[2023-02-07 13:09:57,953][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2912070 effective words) took 0.8s, 3547538 effective words/s
[2023-02-07 13:09:58,801][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2912070 effective words) took 0.8s, 3441397 effective words/s
[2023-02-07 13:09:59,658][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2912070 effective words) took 0.9s, 3403374 effective words/s
[2023-02-07 13:10:00,519][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2912070 effective words) took 0.9s, 3387551 effective words/s
[2023-02-07 13:10:01,369][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2912070 effective words) took 0.8s, 3428879 effective words/s
[2023-02-07 13:10:02,217][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2912070 effective words) took 0.8s, 3441871 effective words/s
[2023-02-07 13:10:03,064][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2912070 effective words) took 0.8s, 3439189 effective words/s
[2023-02-07 13:10:03,908][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2912070 effective words) took 0.8s, 3455542 effective words/s
[2023-02-07 13:10:04,754][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2912070 effective words) took 0.8s, 3447261 effective words/s
[2023-02-07 13:10:05,600][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2912070 effective words) took 0.8s, 3451742 effective words/s
[2023-02-07 13:10:06,450][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2912070 effective words) took 0.8s, 3428523 effective words/s
[2023-02-07 13:10:07,296][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2912070 effective words) took 0.8s, 3446571 effective words/s
[2023-02-07 13:10:08,155][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2912070 effective words) took 0.9s, 3394503 effective words/s
[2023-02-07 13:10:09,000][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2912070 effective words) took 0.8s, 3454518 effective words/s
[2023-02-07 13:10:09,847][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2912070 effective words) took 0.8s, 3440656 effective words/s
[2023-02-07 13:10:10,690][gensim.models.word2vec][INFO] - EPOCH 42: training on 2911496 raw words (2912070 effective words) took 0.8s, 3462419 effective words/s
[2023-02-07 13:10:11,531][gensim.models.word2vec][INFO] - EPOCH 43: training on 2911496 raw words (2912070 effective words) took 0.8s, 3466684 effective words/s
[2023-02-07 13:10:12,369][gensim.models.word2vec][INFO] - EPOCH 44: training on 2911496 raw words (2912070 effective words) took 0.8s, 3480490 effective words/s
[2023-02-07 13:10:13,212][gensim.models.word2vec][INFO] - EPOCH 45: training on 2911496 raw words (2912070 effective words) took 0.8s, 3458593 effective words/s
[2023-02-07 13:10:14,063][gensim.models.word2vec][INFO] - EPOCH 46: training on 2911496 raw words (2912070 effective words) took 0.9s, 3425919 effective words/s
[2023-02-07 13:10:14,896][gensim.models.word2vec][INFO] - EPOCH 47: training on 2911496 raw words (2912070 effective words) took 0.8s, 3500112 effective words/s
[2023-02-07 13:10:15,734][gensim.models.word2vec][INFO] - EPOCH 48: training on 2911496 raw words (2912070 effective words) took 0.8s, 3480361 effective words/s
[2023-02-07 13:10:16,542][gensim.models.word2vec][INFO] - EPOCH 49: training on 2911496 raw words (2912070 effective words) took 0.8s, 3608596 effective words/s
[2023-02-07 13:10:17,341][gensim.models.word2vec][INFO] - EPOCH 50: training on 2911496 raw words (2912070 effective words) took 0.8s, 3651336 effective words/s
[2023-02-07 13:10:18,146][gensim.models.word2vec][INFO] - EPOCH 51: training on 2911496 raw words (2912070 effective words) took 0.8s, 3622148 effective words/s
[2023-02-07 13:10:18,939][gensim.models.word2vec][INFO] - EPOCH 52: training on 2911496 raw words (2912070 effective words) took 0.8s, 3676001 effective words/s
[2023-02-07 13:10:19,760][gensim.models.word2vec][INFO] - EPOCH 53: training on 2911496 raw words (2912070 effective words) took 0.8s, 3557568 effective words/s
[2023-02-07 13:10:20,567][gensim.models.word2vec][INFO] - EPOCH 54: training on 2911496 raw words (2912070 effective words) took 0.8s, 3611324 effective words/s
[2023-02-07 13:10:21,396][gensim.models.word2vec][INFO] - EPOCH 55: training on 2911496 raw words (2912070 effective words) took 0.8s, 3516089 effective words/s
[2023-02-07 13:10:22,228][gensim.models.word2vec][INFO] - EPOCH 56: training on 2911496 raw words (2912070 effective words) took 0.8s, 3505287 effective words/s
[2023-02-07 13:10:23,062][gensim.models.word2vec][INFO] - EPOCH 57: training on 2911496 raw words (2912070 effective words) took 0.8s, 3496873 effective words/s
[2023-02-07 13:10:23,896][gensim.models.word2vec][INFO] - EPOCH 58: training on 2911496 raw words (2912070 effective words) took 0.8s, 3493992 effective words/s
[2023-02-07 13:10:24,729][gensim.models.word2vec][INFO] - EPOCH 59: training on 2911496 raw words (2912070 effective words) took 0.8s, 3501076 effective words/s
[2023-02-07 13:10:25,543][gensim.models.word2vec][INFO] - EPOCH 60: training on 2911496 raw words (2912070 effective words) took 0.8s, 3583812 effective words/s
[2023-02-07 13:10:26,371][gensim.models.word2vec][INFO] - EPOCH 61: training on 2911496 raw words (2912070 effective words) took 0.8s, 3520848 effective words/s
[2023-02-07 13:10:27,199][gensim.models.word2vec][INFO] - EPOCH 62: training on 2911496 raw words (2912070 effective words) took 0.8s, 3523843 effective words/s
[2023-02-07 13:10:28,012][gensim.models.word2vec][INFO] - EPOCH 63: training on 2911496 raw words (2912070 effective words) took 0.8s, 3587563 effective words/s
[2023-02-07 13:10:28,836][gensim.models.word2vec][INFO] - EPOCH 64: training on 2911496 raw words (2912070 effective words) took 0.8s, 3539052 effective words/s
[2023-02-07 13:10:29,660][gensim.models.word2vec][INFO] - EPOCH 65: training on 2911496 raw words (2912070 effective words) took 0.8s, 3536493 effective words/s
[2023-02-07 13:10:30,490][gensim.models.word2vec][INFO] - EPOCH 66: training on 2911496 raw words (2912070 effective words) took 0.8s, 3516826 effective words/s
[2023-02-07 13:10:31,300][gensim.models.word2vec][INFO] - EPOCH 67: training on 2911496 raw words (2912070 effective words) took 0.8s, 3599497 effective words/s
[2023-02-07 13:10:32,121][gensim.models.word2vec][INFO] - EPOCH 68: training on 2911496 raw words (2912070 effective words) took 0.8s, 3550660 effective words/s
[2023-02-07 13:10:32,937][gensim.models.word2vec][INFO] - EPOCH 69: training on 2911496 raw words (2912070 effective words) took 0.8s, 3575706 effective words/s
[2023-02-07 13:10:33,751][gensim.models.word2vec][INFO] - EPOCH 70: training on 2911496 raw words (2912070 effective words) took 0.8s, 3579728 effective words/s
[2023-02-07 13:10:34,562][gensim.models.word2vec][INFO] - EPOCH 71: training on 2911496 raw words (2912070 effective words) took 0.8s, 3598046 effective words/s
[2023-02-07 13:10:35,382][gensim.models.word2vec][INFO] - EPOCH 72: training on 2911496 raw words (2912070 effective words) took 0.8s, 3556648 effective words/s
[2023-02-07 13:10:35,382][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 212539208 raw words (212581110 effective words) took 61.5s, 3457173 effective words/s', 'datetime': '2023-02-07T13:10:35.382866', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:10:35.383 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:10:40,214][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130913-83ab1wmt/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:10:40.214425', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:10:40,216][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:10:40,237][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_130913-83ab1wmt/files/../tmp/embedding_model.pt
2023-02-07 13:10:40.237 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:10:41.302 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:10:41.704 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:10:42.349 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8226125468384684, 'test_mae': 1.0346742753522253, 'test_r2': 0.1377952915378603}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.035 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.035 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.97
wandb: percentage 0.26824
wandb:   test_mae 1.03467
wandb:   test_mse 1.82261
wandb:    test_r2 0.1378
wandb: 
wandb: üöÄ View run dry-sweep-31 at: https://wandb.ai/xiaoqiz/mof2vec/runs/83ab1wmt
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_130913-83ab1wmt/logs
wandb: Agent Starting Run: xl0jdwik with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 596
wandb: 	model.gensim.alpha: 0.1941627586509181
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 82
wandb: 	model.gensim.min_count: 7
wandb: 	model.gensim.sample: 0.3504861339668189
wandb: 	model.gensim.vector_size: 143
wandb: 	model.gensim.window: 10
wandb: 	model.sklearn.learning_rate: 0.07795114486191983
wandb: 	model.sklearn.max_depth: 28
wandb: 	model.sklearn.min_child_weight: 0.002267884569176615
wandb: 	model.sklearn.n_estimators: 452
wandb: 	model.sklearn.num_leaves: 344
wandb: 	model.sklearn.reg_alpha: 0.047018908015931
wandb: 	model.sklearn.reg_lambda: 0.061893704777099184
wandb: 	model.sklearn.subsample: 0.29548098511620974
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131053-xl0jdwik
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-32
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/xl0jdwik
2023-02-07 13:11:01.790 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 13:11:01.790 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 596 for sweep.
2023-02-07 13:11:01.791 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.1941627586509181 for sweep.
2023-02-07 13:11:01.791 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:11:01.791 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 82 for sweep.
2023-02-07 13:11:01.791 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 7 for sweep.
2023-02-07 13:11:01.792 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3504861339668189 for sweep.
2023-02-07 13:11:01.792 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 143 for sweep.
2023-02-07 13:11:01.792 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 10 for sweep.
2023-02-07 13:11:01.792 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.07795114486191983 for sweep.
2023-02-07 13:11:01.792 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 28 for sweep.
2023-02-07 13:11:01.793 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.002267884569176615 for sweep.
2023-02-07 13:11:01.793 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 452 for sweep.
2023-02-07 13:11:01.793 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 344 for sweep.
2023-02-07 13:11:01.793 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.047018908015931 for sweep.
2023-02-07 13:11:01.793 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.061893704777099184 for sweep.
2023-02-07 13:11:01.795 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.29548098511620974 for sweep.
2023-02-07 13:11:01.795 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:11:01.803 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131053-xl0jdwik/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 596, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 143, 'window': 10, 'min_count': 7, 'dm': 0, 'sample': 0.3504861339668189, 'workers': 4, 'alpha': 0.1941627586509181, 'epochs': 82}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 452, 'max_depth': 28, 'num_leaves': 344, 'reg_alpha': 0.047018908015931, 'reg_lambda': 0.061893704777099184, 'subsample': 0.29548098511620974, 'min_child_weight': 0.002267884569176615, 'n_jobs': 4, 'learning_rate': 0.07795114486191983}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 29/3257 [00:00<00:11, 281.35it/s]  2%|‚ñè         | 58/3257 [00:00<00:11, 276.51it/s]  3%|‚ñé         | 90/3257 [00:00<00:10, 291.49it/s]  4%|‚ñé         | 120/3257 [00:00<00:10, 291.10it/s]  5%|‚ñç         | 151/3257 [00:00<00:10, 296.34it/s]  6%|‚ñå         | 181/3257 [00:00<00:10, 284.92it/s]  7%|‚ñã         | 213/3257 [00:00<00:10, 293.72it/s]  8%|‚ñä         | 245/3257 [00:00<00:09, 301.39it/s]  8%|‚ñä         | 276/3257 [00:00<00:09, 299.40it/s]  9%|‚ñâ         | 307/3257 [00:01<00:09, 300.21it/s] 10%|‚ñà         | 338/3257 [00:01<00:09, 299.36it/s] 11%|‚ñà‚ñè        | 368/3257 [00:01<00:09, 296.37it/s] 12%|‚ñà‚ñè        | 398/3257 [00:01<00:09, 291.56it/s] 13%|‚ñà‚ñé        | 428/3257 [00:01<00:13, 202.84it/s] 14%|‚ñà‚ñç        | 458/3257 [00:01<00:12, 224.15it/s] 15%|‚ñà‚ñç        | 488/3257 [00:01<00:11, 242.28it/s] 16%|‚ñà‚ñå        | 520/3257 [00:01<00:10, 262.12it/s] 17%|‚ñà‚ñã        | 553/3257 [00:02<00:09, 278.93it/s] 18%|‚ñà‚ñä        | 583/3257 [00:02<00:10, 266.35it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:02<00:09, 280.66it/s] 20%|‚ñà‚ñâ        | 645/3257 [00:02<00:09, 274.98it/s] 21%|‚ñà‚ñà        | 674/3257 [00:02<00:09, 269.10it/s] 22%|‚ñà‚ñà‚ñè       | 702/3257 [00:02<00:09, 262.38it/s] 22%|‚ñà‚ñà‚ñè       | 729/3257 [00:02<00:09, 254.38it/s] 23%|‚ñà‚ñà‚ñé       | 755/3257 [00:02<00:09, 255.35it/s] 24%|‚ñà‚ñà‚ñç       | 783/3257 [00:02<00:09, 260.62it/s] 25%|‚ñà‚ñà‚ñç       | 810/3257 [00:03<00:09, 259.38it/s] 26%|‚ñà‚ñà‚ñå       | 837/3257 [00:03<00:09, 252.61it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:03<00:09, 257.10it/s] 27%|‚ñà‚ñà‚ñã       | 890/3257 [00:03<00:09, 255.77it/s] 28%|‚ñà‚ñà‚ñä       | 918/3257 [00:03<00:08, 261.73it/s] 29%|‚ñà‚ñà‚ñâ       | 946/3257 [00:03<00:08, 266.13it/s] 30%|‚ñà‚ñà‚ñâ       | 976/3257 [00:03<00:08, 275.20it/s] 31%|‚ñà‚ñà‚ñà       | 1004/3257 [00:03<00:08, 268.81it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1031/3257 [00:03<00:08, 261.31it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1058/3257 [00:03<00:08, 256.90it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1086/3257 [00:04<00:08, 262.15it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1115/3257 [00:04<00:08, 266.17it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1142/3257 [00:04<00:08, 260.55it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:04<00:08, 258.88it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1196/3257 [00:04<00:08, 239.34it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1221/3257 [00:04<00:08, 237.55it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1248/3257 [00:04<00:08, 243.70it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:04<00:08, 246.76it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1300/3257 [00:04<00:07, 245.84it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:05<00:07, 251.42it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1354/3257 [00:05<00:07, 250.88it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1381/3257 [00:05<00:07, 249.79it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1409/3257 [00:05<00:07, 258.08it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1438/3257 [00:05<00:06, 261.59it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1470/3257 [00:05<00:06, 277.88it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1501/3257 [00:05<00:06, 285.77it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1530/3257 [00:05<00:06, 273.12it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1558/3257 [00:05<00:06, 267.69it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1587/3257 [00:05<00:06, 270.31it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1616/3257 [00:06<00:06, 272.71it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1644/3257 [00:06<00:08, 181.32it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1669/3257 [00:06<00:08, 195.73it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1693/3257 [00:06<00:07, 205.98it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1720/3257 [00:06<00:06, 220.73it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1745/3257 [00:06<00:06, 219.21it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1772/3257 [00:06<00:06, 232.41it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1800/3257 [00:06<00:06, 242.61it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1826/3257 [00:07<00:05, 239.35it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1852/3257 [00:07<00:05, 242.17it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1881/3257 [00:07<00:05, 254.29it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1910/3257 [00:07<00:05, 261.13it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1937/3257 [00:07<00:05, 258.36it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1973/3257 [00:07<00:04, 287.44it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2003/3257 [00:07<00:04, 286.02it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2033/3257 [00:07<00:04, 289.11it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2063/3257 [00:07<00:04, 269.18it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2092/3257 [00:08<00:04, 273.19it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:08<00:04, 263.34it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2147/3257 [00:08<00:04, 257.79it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2177/3257 [00:08<00:04, 265.71it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2204/3257 [00:08<00:03, 263.77it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2231/3257 [00:08<00:03, 263.54it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2258/3257 [00:08<00:03, 264.00it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2285/3257 [00:08<00:03, 265.04it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2312/3257 [00:08<00:03, 266.20it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2344/3257 [00:09<00:03, 281.93it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2375/3257 [00:09<00:03, 290.01it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:09<00:03, 277.30it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2433/3257 [00:09<00:03, 263.16it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2462/3257 [00:09<00:02, 269.96it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2490/3257 [00:09<00:02, 272.53it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2522/3257 [00:09<00:02, 285.14it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:09<00:02, 285.62it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2580/3257 [00:09<00:02, 256.75it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2608/3257 [00:09<00:02, 262.84it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2641/3257 [00:10<00:02, 278.91it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:10<00:02, 268.55it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2698/3257 [00:10<00:02, 256.88it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2724/3257 [00:10<00:02, 256.77it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:10<00:01, 273.16it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2786/3257 [00:10<00:01, 280.51it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2815/3257 [00:10<00:01, 280.08it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:10<00:01, 266.30it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:10<00:01, 293.01it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2911/3257 [00:11<00:01, 277.64it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2940/3257 [00:11<00:01, 274.91it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2968/3257 [00:11<00:01, 270.31it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2996/3257 [00:11<00:00, 266.97it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3024/3257 [00:11<00:00, 269.19it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3055/3257 [00:11<00:00, 277.90it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3083/3257 [00:11<00:00, 177.29it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3116/3257 [00:12<00:00, 207.84it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3143/3257 [00:12<00:00, 218.32it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3171/3257 [00:12<00:00, 232.96it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3199/3257 [00:12<00:00, 245.04it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3226/3257 [00:12<00:00, 247.62it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3255/3257 [00:12<00:00, 257.77it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:12<00:00, 259.77it/s]
2023-02-07 13:11:14.682 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:11:14,683][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d143,n5,mc7,s0.350486,t4>', 'datetime': '2023-02-07T13:11:14.683092', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:11:14,684][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:11:14,684][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:11:14,941][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 13:11:14,942][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:11:14,951][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 retains 3228 unique words (48.45% of original 6662, drops 3434)', 'datetime': '2023-02-07T13:11:14.951325', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:11:14,951][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 leaves 2901608 word corpus (99.66% of original 2911496, drops 9888)', 'datetime': '2023-02-07T13:11:14.951626', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:11:14,962][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 13:11:14,963][gensim.models.word2vec][INFO] - sample=0.350486 downsamples 0 most-common words
[2023-02-07 13:11:14,963][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2901608 word corpus (100.0%% of prior 2901608)', 'datetime': '2023-02-07T13:11:14.963697', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:11:14,982][gensim.models.word2vec][INFO] - estimated required memory for 3228 words and 143 dimensions: 7821236 bytes
[2023-02-07 13:11:14,982][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:11:14,986][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 3228 vocabulary and 143 features, using sg=1 hs=0 sample=0.3504861339668189 negative=5 window=10 shrink_windows=True', 'datetime': '2023-02-07T13:11:14.986180', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:11:15,612][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2904865 effective words) took 0.6s, 4651313 effective words/s
[2023-02-07 13:11:16,162][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2904865 effective words) took 0.5s, 5294890 effective words/s
[2023-02-07 13:11:16,709][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2904865 effective words) took 0.5s, 5317161 effective words/s
[2023-02-07 13:11:17,255][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2904865 effective words) took 0.5s, 5334313 effective words/s
[2023-02-07 13:11:17,796][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2904865 effective words) took 0.5s, 5382572 effective words/s
[2023-02-07 13:11:18,341][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2904865 effective words) took 0.5s, 5340693 effective words/s
[2023-02-07 13:11:18,878][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2904865 effective words) took 0.5s, 5427262 effective words/s
[2023-02-07 13:11:19,417][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2904865 effective words) took 0.5s, 5396301 effective words/s
[2023-02-07 13:11:19,955][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2904865 effective words) took 0.5s, 5409798 effective words/s
[2023-02-07 13:11:20,499][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2904865 effective words) took 0.5s, 5354132 effective words/s
[2023-02-07 13:11:21,041][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2904865 effective words) took 0.5s, 5363272 effective words/s
[2023-02-07 13:11:21,584][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2904865 effective words) took 0.5s, 5361219 effective words/s
[2023-02-07 13:11:22,124][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2904865 effective words) took 0.5s, 5393206 effective words/s
[2023-02-07 13:11:22,660][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2904865 effective words) took 0.5s, 5437394 effective words/s
[2023-02-07 13:11:23,197][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2904865 effective words) took 0.5s, 5421012 effective words/s
[2023-02-07 13:11:23,741][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2904865 effective words) took 0.5s, 5347063 effective words/s
[2023-02-07 13:11:24,284][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2904865 effective words) took 0.5s, 5361627 effective words/s
[2023-02-07 13:11:24,832][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2904865 effective words) took 0.5s, 5318476 effective words/s
[2023-02-07 13:11:25,373][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2904865 effective words) took 0.5s, 5374232 effective words/s
[2023-02-07 13:11:25,921][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2904865 effective words) took 0.5s, 5320286 effective words/s
[2023-02-07 13:11:26,468][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2904865 effective words) took 0.5s, 5319474 effective words/s
[2023-02-07 13:11:27,011][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2904865 effective words) took 0.5s, 5363049 effective words/s
[2023-02-07 13:11:27,556][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2904865 effective words) took 0.5s, 5340021 effective words/s
[2023-02-07 13:11:28,098][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2904865 effective words) took 0.5s, 5367856 effective words/s
[2023-02-07 13:11:28,642][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2904865 effective words) took 0.5s, 5347288 effective words/s
[2023-02-07 13:11:29,211][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2904865 effective words) took 0.6s, 5119332 effective words/s
[2023-02-07 13:11:29,780][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2904865 effective words) took 0.6s, 5116712 effective words/s
[2023-02-07 13:11:30,354][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2904865 effective words) took 0.6s, 5072308 effective words/s
[2023-02-07 13:11:30,923][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2904865 effective words) took 0.6s, 5109908 effective words/s
[2023-02-07 13:11:31,492][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2904865 effective words) took 0.6s, 5116681 effective words/s
[2023-02-07 13:11:32,065][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2904865 effective words) took 0.6s, 5083454 effective words/s
[2023-02-07 13:11:32,639][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2904865 effective words) took 0.6s, 5078806 effective words/s
[2023-02-07 13:11:33,221][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2904865 effective words) took 0.6s, 5000719 effective words/s
[2023-02-07 13:11:33,804][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2904865 effective words) took 0.6s, 4996677 effective words/s
[2023-02-07 13:11:34,374][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2904865 effective words) took 0.6s, 5106680 effective words/s
[2023-02-07 13:11:34,953][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2904865 effective words) took 0.6s, 5029794 effective words/s
[2023-02-07 13:11:35,540][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2904865 effective words) took 0.6s, 4954783 effective words/s
[2023-02-07 13:11:36,110][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2904865 effective words) took 0.6s, 5108945 effective words/s
[2023-02-07 13:11:36,698][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2904865 effective words) took 0.6s, 4955518 effective words/s
[2023-02-07 13:11:37,291][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2904865 effective words) took 0.6s, 4902379 effective words/s
[2023-02-07 13:11:37,862][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2904865 effective words) took 0.6s, 5111294 effective words/s
[2023-02-07 13:11:38,451][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2904865 effective words) took 0.6s, 4936611 effective words/s
[2023-02-07 13:11:39,048][gensim.models.word2vec][INFO] - EPOCH 42: training on 2911496 raw words (2904865 effective words) took 0.6s, 4881487 effective words/s
[2023-02-07 13:11:39,623][gensim.models.word2vec][INFO] - EPOCH 43: training on 2911496 raw words (2904865 effective words) took 0.6s, 5062707 effective words/s
[2023-02-07 13:11:40,204][gensim.models.word2vec][INFO] - EPOCH 44: training on 2911496 raw words (2904865 effective words) took 0.6s, 5013158 effective words/s
[2023-02-07 13:11:40,810][gensim.models.word2vec][INFO] - EPOCH 45: training on 2911496 raw words (2904865 effective words) took 0.6s, 4802107 effective words/s
[2023-02-07 13:11:41,391][gensim.models.word2vec][INFO] - EPOCH 46: training on 2911496 raw words (2904865 effective words) took 0.6s, 5005620 effective words/s
[2023-02-07 13:11:41,978][gensim.models.word2vec][INFO] - EPOCH 47: training on 2911496 raw words (2904865 effective words) took 0.6s, 4962199 effective words/s
[2023-02-07 13:11:42,590][gensim.models.word2vec][INFO] - EPOCH 48: training on 2911496 raw words (2904865 effective words) took 0.6s, 4760988 effective words/s
[2023-02-07 13:11:43,183][gensim.models.word2vec][INFO] - EPOCH 49: training on 2911496 raw words (2904865 effective words) took 0.6s, 4908077 effective words/s
[2023-02-07 13:11:43,767][gensim.models.word2vec][INFO] - EPOCH 50: training on 2911496 raw words (2904865 effective words) took 0.6s, 4989451 effective words/s
[2023-02-07 13:11:44,368][gensim.models.word2vec][INFO] - EPOCH 51: training on 2911496 raw words (2904865 effective words) took 0.6s, 4841366 effective words/s
[2023-02-07 13:11:44,986][gensim.models.word2vec][INFO] - EPOCH 52: training on 2911496 raw words (2904865 effective words) took 0.6s, 4711472 effective words/s
[2023-02-07 13:11:45,563][gensim.models.word2vec][INFO] - EPOCH 53: training on 2911496 raw words (2904865 effective words) took 0.6s, 5045384 effective words/s
[2023-02-07 13:11:46,168][gensim.models.word2vec][INFO] - EPOCH 54: training on 2911496 raw words (2904865 effective words) took 0.6s, 4812483 effective words/s
[2023-02-07 13:11:46,744][gensim.models.word2vec][INFO] - EPOCH 55: training on 2911496 raw words (2904865 effective words) took 0.6s, 5055893 effective words/s
[2023-02-07 13:11:47,291][gensim.models.word2vec][INFO] - EPOCH 56: training on 2911496 raw words (2904865 effective words) took 0.5s, 5326202 effective words/s
[2023-02-07 13:11:47,852][gensim.models.word2vec][INFO] - EPOCH 57: training on 2911496 raw words (2904865 effective words) took 0.6s, 5188150 effective words/s
[2023-02-07 13:11:48,394][gensim.models.word2vec][INFO] - EPOCH 58: training on 2911496 raw words (2904865 effective words) took 0.5s, 5366345 effective words/s
[2023-02-07 13:11:48,944][gensim.models.word2vec][INFO] - EPOCH 59: training on 2911496 raw words (2904865 effective words) took 0.5s, 5296858 effective words/s
[2023-02-07 13:11:49,535][gensim.models.word2vec][INFO] - EPOCH 60: training on 2911496 raw words (2904865 effective words) took 0.6s, 4921297 effective words/s
[2023-02-07 13:11:50,138][gensim.models.word2vec][INFO] - EPOCH 61: training on 2911496 raw words (2904865 effective words) took 0.6s, 4833321 effective words/s
[2023-02-07 13:11:50,731][gensim.models.word2vec][INFO] - EPOCH 62: training on 2911496 raw words (2904865 effective words) took 0.6s, 4902982 effective words/s
[2023-02-07 13:11:51,326][gensim.models.word2vec][INFO] - EPOCH 63: training on 2911496 raw words (2904865 effective words) took 0.6s, 4890752 effective words/s
[2023-02-07 13:11:51,920][gensim.models.word2vec][INFO] - EPOCH 64: training on 2911496 raw words (2904865 effective words) took 0.6s, 4900304 effective words/s
[2023-02-07 13:11:52,511][gensim.models.word2vec][INFO] - EPOCH 65: training on 2911496 raw words (2904865 effective words) took 0.6s, 4926591 effective words/s
[2023-02-07 13:11:53,112][gensim.models.word2vec][INFO] - EPOCH 66: training on 2911496 raw words (2904865 effective words) took 0.6s, 4843707 effective words/s
[2023-02-07 13:11:53,716][gensim.models.word2vec][INFO] - EPOCH 67: training on 2911496 raw words (2904865 effective words) took 0.6s, 4819173 effective words/s
[2023-02-07 13:11:54,341][gensim.models.word2vec][INFO] - EPOCH 68: training on 2911496 raw words (2904865 effective words) took 0.6s, 4655557 effective words/s
[2023-02-07 13:11:54,946][gensim.models.word2vec][INFO] - EPOCH 69: training on 2911496 raw words (2904865 effective words) took 0.6s, 4812039 effective words/s
[2023-02-07 13:11:55,545][gensim.models.word2vec][INFO] - EPOCH 70: training on 2911496 raw words (2904865 effective words) took 0.6s, 4864411 effective words/s
[2023-02-07 13:11:56,150][gensim.models.word2vec][INFO] - EPOCH 71: training on 2911496 raw words (2904865 effective words) took 0.6s, 4810801 effective words/s
[2023-02-07 13:11:56,752][gensim.models.word2vec][INFO] - EPOCH 72: training on 2911496 raw words (2904865 effective words) took 0.6s, 4835248 effective words/s
[2023-02-07 13:11:57,349][gensim.models.word2vec][INFO] - EPOCH 73: training on 2911496 raw words (2904865 effective words) took 0.6s, 4875644 effective words/s
[2023-02-07 13:11:57,959][gensim.models.word2vec][INFO] - EPOCH 74: training on 2911496 raw words (2904865 effective words) took 0.6s, 4774948 effective words/s
[2023-02-07 13:11:58,569][gensim.models.word2vec][INFO] - EPOCH 75: training on 2911496 raw words (2904865 effective words) took 0.6s, 4771153 effective words/s
[2023-02-07 13:11:59,183][gensim.models.word2vec][INFO] - EPOCH 76: training on 2911496 raw words (2904865 effective words) took 0.6s, 4735972 effective words/s
[2023-02-07 13:11:59,791][gensim.models.word2vec][INFO] - EPOCH 77: training on 2911496 raw words (2904865 effective words) took 0.6s, 4793732 effective words/s
[2023-02-07 13:12:00,394][gensim.models.word2vec][INFO] - EPOCH 78: training on 2911496 raw words (2904865 effective words) took 0.6s, 4818785 effective words/s
[2023-02-07 13:12:01,001][gensim.models.word2vec][INFO] - EPOCH 79: training on 2911496 raw words (2904865 effective words) took 0.6s, 4800071 effective words/s
[2023-02-07 13:12:01,603][gensim.models.word2vec][INFO] - EPOCH 80: training on 2911496 raw words (2904865 effective words) took 0.6s, 4836174 effective words/s
[2023-02-07 13:12:02,215][gensim.models.word2vec][INFO] - EPOCH 81: training on 2911496 raw words (2904865 effective words) took 0.6s, 4756450 effective words/s
[2023-02-07 13:12:02,215][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 238742672 raw words (238198930 effective words) took 47.2s, 5043488 effective words/s', 'datetime': '2023-02-07T13:12:02.215459', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:12:02.215 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:12:06,128][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131053-xl0jdwik/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:12:06.128790', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:12:06,129][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:12:06,151][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131053-xl0jdwik/files/../tmp/embedding_model.pt
2023-02-07 13:12:06.151 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:12:07.411 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:12:07.878 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:12:08.825 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.24030921172151, 'test_mae': 1.144486832712417, 'test_r2': -0.05980020499034855}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.63
wandb: percentage 0.51546
wandb:   test_mae 1.14449
wandb:   test_mse 2.24031
wandb:    test_r2 -0.0598
wandb: 
wandb: üöÄ View run legendary-sweep-32 at: https://wandb.ai/xiaoqiz/mof2vec/runs/xl0jdwik
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_131053-xl0jdwik/logs
wandb: Agent Starting Run: rf6v302w with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 36
wandb: 	model.gensim.alpha: 0.0005047784297648574
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 68
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.4027809519887683
wandb: 	model.gensim.vector_size: 86
wandb: 	model.gensim.window: 9
wandb: 	model.sklearn.learning_rate: 0.7155514417406688
wandb: 	model.sklearn.max_depth: 16
wandb: 	model.sklearn.min_child_weight: 0.01797356042945734
wandb: 	model.sklearn.n_estimators: 3333
wandb: 	model.sklearn.num_leaves: 426
wandb: 	model.sklearn.reg_alpha: 0.014599764636841804
wandb: 	model.sklearn.reg_lambda: 0.01411753042735901
wandb: 	model.sklearn.subsample: 0.7634698965992186
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131223-rf6v302w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-33
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/rf6v302w
2023-02-07 13:12:31.868 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 13:12:31.868 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 36 for sweep.
2023-02-07 13:12:31.869 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0005047784297648574 for sweep.
2023-02-07 13:12:31.869 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:12:31.869 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 68 for sweep.
2023-02-07 13:12:31.869 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 13:12:31.870 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4027809519887683 for sweep.
2023-02-07 13:12:31.870 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 86 for sweep.
2023-02-07 13:12:31.870 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 9 for sweep.
2023-02-07 13:12:31.870 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.7155514417406688 for sweep.
2023-02-07 13:12:31.870 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 16 for sweep.
2023-02-07 13:12:31.871 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.01797356042945734 for sweep.
2023-02-07 13:12:31.871 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3333 for sweep.
2023-02-07 13:12:31.871 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 426 for sweep.
2023-02-07 13:12:31.871 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.014599764636841804 for sweep.
2023-02-07 13:12:31.871 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.01411753042735901 for sweep.
2023-02-07 13:12:31.872 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.7634698965992186 for sweep.
2023-02-07 13:12:31.872 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:12:31.878 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131223-rf6v302w/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 36, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 86, 'window': 9, 'min_count': 1, 'dm': 0, 'sample': 0.4027809519887683, 'workers': 4, 'alpha': 0.0005047784297648574, 'epochs': 68}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3333, 'max_depth': 16, 'num_leaves': 426, 'reg_alpha': 0.014599764636841804, 'reg_lambda': 0.01411753042735901, 'subsample': 0.7634698965992186, 'min_child_weight': 0.01797356042945734, 'n_jobs': 4, 'learning_rate': 0.7155514417406688}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 34/3257 [00:00<00:09, 324.72it/s]  2%|‚ñè         | 68/3257 [00:00<00:09, 330.42it/s]  3%|‚ñé         | 103/3257 [00:00<00:09, 337.93it/s]  4%|‚ñç         | 137/3257 [00:00<00:10, 309.67it/s]  5%|‚ñå         | 171/3257 [00:00<00:09, 318.95it/s]  6%|‚ñã         | 204/3257 [00:00<00:09, 322.14it/s]  8%|‚ñä         | 245/3257 [00:00<00:08, 347.71it/s]  9%|‚ñä         | 284/3257 [00:00<00:08, 358.94it/s] 10%|‚ñâ         | 321/3257 [00:00<00:08, 357.47it/s] 11%|‚ñà         | 357/3257 [00:01<00:08, 344.22it/s] 12%|‚ñà‚ñè        | 392/3257 [00:01<00:08, 330.46it/s] 13%|‚ñà‚ñé        | 426/3257 [00:01<00:08, 330.87it/s] 14%|‚ñà‚ñç        | 460/3257 [00:01<00:08, 329.06it/s] 15%|‚ñà‚ñå        | 493/3257 [00:01<00:08, 326.62it/s] 16%|‚ñà‚ñå        | 527/3257 [00:01<00:08, 329.52it/s] 17%|‚ñà‚ñã        | 560/3257 [00:01<00:08, 321.39it/s] 18%|‚ñà‚ñä        | 593/3257 [00:01<00:08, 308.72it/s] 19%|‚ñà‚ñâ        | 624/3257 [00:01<00:08, 309.06it/s] 20%|‚ñà‚ñà        | 655/3257 [00:02<00:08, 305.32it/s] 21%|‚ñà‚ñà        | 686/3257 [00:02<00:08, 302.33it/s] 22%|‚ñà‚ñà‚ñè       | 720/3257 [00:02<00:08, 313.20it/s] 23%|‚ñà‚ñà‚ñé       | 752/3257 [00:02<00:08, 310.03it/s] 24%|‚ñà‚ñà‚ñç       | 784/3257 [00:02<00:07, 310.63it/s] 25%|‚ñà‚ñà‚ñå       | 818/3257 [00:02<00:07, 318.45it/s] 26%|‚ñà‚ñà‚ñå       | 850/3257 [00:02<00:07, 304.91it/s] 27%|‚ñà‚ñà‚ñã       | 881/3257 [00:02<00:07, 306.13it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:02<00:07, 308.65it/s] 29%|‚ñà‚ñà‚ñâ       | 951/3257 [00:02<00:07, 326.16it/s] 30%|‚ñà‚ñà‚ñà       | 984/3257 [00:03<00:07, 314.06it/s] 31%|‚ñà‚ñà‚ñà       | 1016/3257 [00:03<00:07, 304.07it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:03<00:07, 292.30it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:03<00:07, 295.55it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1112/3257 [00:03<00:07, 300.21it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1143/3257 [00:03<00:07, 294.60it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:03<00:10, 206.42it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1198/3257 [00:03<00:09, 215.05it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1228/3257 [00:04<00:08, 234.23it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1260/3257 [00:04<00:07, 253.63it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1288/3257 [00:04<00:07, 250.94it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1321/3257 [00:04<00:07, 269.24it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1353/3257 [00:04<00:06, 282.86it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1383/3257 [00:04<00:06, 283.52it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1417/3257 [00:04<00:06, 298.97it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1450/3257 [00:04<00:05, 307.06it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1482/3257 [00:04<00:05, 303.98it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1517/3257 [00:05<00:05, 314.81it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1549/3257 [00:05<00:05, 296.76it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1580/3257 [00:05<00:05, 300.30it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1613/3257 [00:05<00:05, 308.32it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1645/3257 [00:05<00:05, 300.22it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1676/3257 [00:05<00:05, 294.07it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1706/3257 [00:05<00:05, 292.27it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1736/3257 [00:05<00:05, 282.17it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1768/3257 [00:05<00:05, 291.84it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1800/3257 [00:05<00:04, 299.60it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1831/3257 [00:06<00:04, 290.23it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1866/3257 [00:06<00:04, 306.83it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1897/3257 [00:06<00:04, 305.37it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1928/3257 [00:06<00:04, 297.42it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1962/3257 [00:06<00:04, 308.36it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1993/3257 [00:06<00:04, 303.08it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2025/3257 [00:06<00:04, 303.67it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2056/3257 [00:06<00:04, 298.68it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2086/3257 [00:06<00:03, 297.58it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:07<00:03, 298.11it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2150/3257 [00:07<00:03, 288.34it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2184/3257 [00:07<00:03, 300.82it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2218/3257 [00:07<00:03, 310.94it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2250/3257 [00:07<00:03, 307.26it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2281/3257 [00:07<00:03, 296.65it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2315/3257 [00:07<00:03, 303.68it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2355/3257 [00:07<00:02, 330.16it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2389/3257 [00:07<00:02, 330.56it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2423/3257 [00:08<00:02, 319.96it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2456/3257 [00:08<00:02, 295.97it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2492/3257 [00:08<00:02, 312.15it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2528/3257 [00:08<00:02, 325.32it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2561/3257 [00:08<00:02, 321.25it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2594/3257 [00:08<00:02, 307.37it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2630/3257 [00:08<00:01, 320.95it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2663/3257 [00:08<00:02, 207.32it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2696/3257 [00:09<00:02, 231.56it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2724/3257 [00:09<00:02, 240.20it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2757/3257 [00:09<00:01, 261.01it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2790/3257 [00:09<00:01, 277.74it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2822/3257 [00:09<00:01, 287.08it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2853/3257 [00:09<00:01, 283.92it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2890/3257 [00:09<00:01, 307.50it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2922/3257 [00:09<00:01, 293.62it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2953/3257 [00:09<00:01, 288.35it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2983/3257 [00:10<00:00, 289.03it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3013/3257 [00:10<00:00, 292.06it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3046/3257 [00:10<00:00, 302.11it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3083/3257 [00:10<00:00, 321.24it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3122/3257 [00:10<00:00, 340.74it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3157/3257 [00:10<00:00, 324.47it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3190/3257 [00:10<00:00, 320.53it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3223/3257 [00:10<00:00, 308.83it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:10<00:00, 299.79it/s]
2023-02-07 13:12:43.000 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:12:43,001][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d86,n5,s0.402781,t4>', 'datetime': '2023-02-07T13:12:43.001664', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:12:43,002][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:12:43,002][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:12:43,197][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 13:12:43,197][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:12:43,204][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 2819 unique words (100.00% of original 2819, drops 0)', 'datetime': '2023-02-07T13:12:43.204256', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:12:43,204][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 2183622 word corpus (100.00% of original 2183622, drops 0)', 'datetime': '2023-02-07T13:12:43.204519', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:12:43,213][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 13:12:43,214][gensim.models.word2vec][INFO] - sample=0.402781 downsamples 0 most-common words
[2023-02-07 13:12:43,214][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2183622 word corpus (100.0%% of prior 2183622)', 'datetime': '2023-02-07T13:12:43.214154', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:12:43,230][gensim.models.word2vec][INFO] - estimated required memory for 2819 words and 86 dimensions: 5120780 bytes
[2023-02-07 13:12:43,230][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:12:43,232][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2819 vocabulary and 86 features, using sg=1 hs=0 sample=0.4027809519887683 negative=5 window=9 shrink_windows=True', 'datetime': '2023-02-07T13:12:43.232946', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:12:44,011][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2186879 effective words) took 0.8s, 2817847 effective words/s
[2023-02-07 13:12:44,788][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2186879 effective words) took 0.8s, 2821152 effective words/s
[2023-02-07 13:12:45,560][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2186879 effective words) took 0.8s, 2834424 effective words/s
[2023-02-07 13:12:46,333][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2186879 effective words) took 0.8s, 2832920 effective words/s
[2023-02-07 13:12:47,108][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2186879 effective words) took 0.8s, 2828925 effective words/s
[2023-02-07 13:12:47,879][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2186879 effective words) took 0.8s, 2839299 effective words/s
[2023-02-07 13:12:48,653][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2186879 effective words) took 0.8s, 2832467 effective words/s
[2023-02-07 13:12:49,421][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2186879 effective words) took 0.8s, 2851880 effective words/s
[2023-02-07 13:12:50,192][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2186879 effective words) took 0.8s, 2839507 effective words/s
[2023-02-07 13:12:50,961][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2186879 effective words) took 0.8s, 2846600 effective words/s
[2023-02-07 13:12:51,738][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2186879 effective words) took 0.8s, 2820672 effective words/s
[2023-02-07 13:12:52,517][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2186879 effective words) took 0.8s, 2811234 effective words/s
[2023-02-07 13:12:53,296][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2186879 effective words) took 0.8s, 2811580 effective words/s
[2023-02-07 13:12:54,074][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2186879 effective words) took 0.8s, 2817155 effective words/s
[2023-02-07 13:12:54,846][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2186879 effective words) took 0.8s, 2835577 effective words/s
[2023-02-07 13:12:55,637][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2186879 effective words) took 0.8s, 2768380 effective words/s
[2023-02-07 13:12:56,444][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2186879 effective words) took 0.8s, 2715206 effective words/s
[2023-02-07 13:12:57,255][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2186879 effective words) took 0.8s, 2700727 effective words/s
[2023-02-07 13:12:58,056][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2186879 effective words) took 0.8s, 2734707 effective words/s
[2023-02-07 13:12:58,864][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2186879 effective words) took 0.8s, 2711312 effective words/s
[2023-02-07 13:12:59,670][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2186879 effective words) took 0.8s, 2718922 effective words/s
[2023-02-07 13:13:00,466][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2186879 effective words) took 0.8s, 2751748 effective words/s
[2023-02-07 13:13:01,268][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2186879 effective words) took 0.8s, 2730477 effective words/s
[2023-02-07 13:13:02,062][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2186879 effective words) took 0.8s, 2756320 effective words/s
[2023-02-07 13:13:02,856][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2186879 effective words) took 0.8s, 2760780 effective words/s
[2023-02-07 13:13:03,650][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2186879 effective words) took 0.8s, 2760659 effective words/s
[2023-02-07 13:13:04,445][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2186879 effective words) took 0.8s, 2752943 effective words/s
[2023-02-07 13:13:05,245][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2186879 effective words) took 0.8s, 2738398 effective words/s
[2023-02-07 13:13:06,038][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2186879 effective words) took 0.8s, 2764589 effective words/s
[2023-02-07 13:13:06,830][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2186879 effective words) took 0.8s, 2766042 effective words/s
[2023-02-07 13:13:07,615][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2186879 effective words) took 0.8s, 2788079 effective words/s
[2023-02-07 13:13:08,403][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2186879 effective words) took 0.8s, 2781213 effective words/s
[2023-02-07 13:13:09,189][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2186879 effective words) took 0.8s, 2785753 effective words/s
[2023-02-07 13:13:09,984][gensim.models.word2vec][INFO] - EPOCH 33: training on 2183622 raw words (2186879 effective words) took 0.8s, 2756307 effective words/s
[2023-02-07 13:13:10,775][gensim.models.word2vec][INFO] - EPOCH 34: training on 2183622 raw words (2186879 effective words) took 0.8s, 2770588 effective words/s
[2023-02-07 13:13:11,567][gensim.models.word2vec][INFO] - EPOCH 35: training on 2183622 raw words (2186879 effective words) took 0.8s, 2764560 effective words/s
[2023-02-07 13:13:12,364][gensim.models.word2vec][INFO] - EPOCH 36: training on 2183622 raw words (2186879 effective words) took 0.8s, 2748349 effective words/s
[2023-02-07 13:13:13,128][gensim.models.word2vec][INFO] - EPOCH 37: training on 2183622 raw words (2186879 effective words) took 0.8s, 2869034 effective words/s
[2023-02-07 13:13:13,864][gensim.models.word2vec][INFO] - EPOCH 38: training on 2183622 raw words (2186879 effective words) took 0.7s, 2973090 effective words/s
[2023-02-07 13:13:14,592][gensim.models.word2vec][INFO] - EPOCH 39: training on 2183622 raw words (2186879 effective words) took 0.7s, 3010205 effective words/s
[2023-02-07 13:13:15,314][gensim.models.word2vec][INFO] - EPOCH 40: training on 2183622 raw words (2186879 effective words) took 0.7s, 3033518 effective words/s
[2023-02-07 13:13:16,095][gensim.models.word2vec][INFO] - EPOCH 41: training on 2183622 raw words (2186879 effective words) took 0.8s, 2808308 effective words/s
[2023-02-07 13:13:16,876][gensim.models.word2vec][INFO] - EPOCH 42: training on 2183622 raw words (2186879 effective words) took 0.8s, 2801414 effective words/s
[2023-02-07 13:13:17,670][gensim.models.word2vec][INFO] - EPOCH 43: training on 2183622 raw words (2186879 effective words) took 0.8s, 2758866 effective words/s
[2023-02-07 13:13:18,449][gensim.models.word2vec][INFO] - EPOCH 44: training on 2183622 raw words (2186879 effective words) took 0.8s, 2813034 effective words/s
[2023-02-07 13:13:19,232][gensim.models.word2vec][INFO] - EPOCH 45: training on 2183622 raw words (2186879 effective words) took 0.8s, 2797853 effective words/s
[2023-02-07 13:13:20,011][gensim.models.word2vec][INFO] - EPOCH 46: training on 2183622 raw words (2186879 effective words) took 0.8s, 2809080 effective words/s
[2023-02-07 13:13:20,773][gensim.models.word2vec][INFO] - EPOCH 47: training on 2183622 raw words (2186879 effective words) took 0.8s, 2876651 effective words/s
[2023-02-07 13:13:21,557][gensim.models.word2vec][INFO] - EPOCH 48: training on 2183622 raw words (2186879 effective words) took 0.8s, 2794042 effective words/s
[2023-02-07 13:13:22,340][gensim.models.word2vec][INFO] - EPOCH 49: training on 2183622 raw words (2186879 effective words) took 0.8s, 2797609 effective words/s
[2023-02-07 13:13:23,103][gensim.models.word2vec][INFO] - EPOCH 50: training on 2183622 raw words (2186879 effective words) took 0.8s, 2869931 effective words/s
[2023-02-07 13:13:23,872][gensim.models.word2vec][INFO] - EPOCH 51: training on 2183622 raw words (2186879 effective words) took 0.8s, 2849193 effective words/s
[2023-02-07 13:13:24,648][gensim.models.word2vec][INFO] - EPOCH 52: training on 2183622 raw words (2186879 effective words) took 0.8s, 2825853 effective words/s
[2023-02-07 13:13:25,406][gensim.models.word2vec][INFO] - EPOCH 53: training on 2183622 raw words (2186879 effective words) took 0.8s, 2889813 effective words/s
[2023-02-07 13:13:26,174][gensim.models.word2vec][INFO] - EPOCH 54: training on 2183622 raw words (2186879 effective words) took 0.8s, 2851167 effective words/s
[2023-02-07 13:13:26,931][gensim.models.word2vec][INFO] - EPOCH 55: training on 2183622 raw words (2186879 effective words) took 0.8s, 2891332 effective words/s
[2023-02-07 13:13:27,694][gensim.models.word2vec][INFO] - EPOCH 56: training on 2183622 raw words (2186879 effective words) took 0.8s, 2869327 effective words/s
[2023-02-07 13:13:28,461][gensim.models.word2vec][INFO] - EPOCH 57: training on 2183622 raw words (2186879 effective words) took 0.8s, 2856899 effective words/s
[2023-02-07 13:13:29,232][gensim.models.word2vec][INFO] - EPOCH 58: training on 2183622 raw words (2186879 effective words) took 0.8s, 2843260 effective words/s
[2023-02-07 13:13:30,007][gensim.models.word2vec][INFO] - EPOCH 59: training on 2183622 raw words (2186879 effective words) took 0.8s, 2824578 effective words/s
[2023-02-07 13:13:30,780][gensim.models.word2vec][INFO] - EPOCH 60: training on 2183622 raw words (2186879 effective words) took 0.8s, 2835511 effective words/s
[2023-02-07 13:13:31,544][gensim.models.word2vec][INFO] - EPOCH 61: training on 2183622 raw words (2186879 effective words) took 0.8s, 2864626 effective words/s
[2023-02-07 13:13:32,306][gensim.models.word2vec][INFO] - EPOCH 62: training on 2183622 raw words (2186879 effective words) took 0.8s, 2876428 effective words/s
[2023-02-07 13:13:33,069][gensim.models.word2vec][INFO] - EPOCH 63: training on 2183622 raw words (2186879 effective words) took 0.8s, 2869559 effective words/s
[2023-02-07 13:13:33,851][gensim.models.word2vec][INFO] - EPOCH 64: training on 2183622 raw words (2186879 effective words) took 0.8s, 2802872 effective words/s
[2023-02-07 13:13:34,618][gensim.models.word2vec][INFO] - EPOCH 65: training on 2183622 raw words (2186879 effective words) took 0.8s, 2856820 effective words/s
[2023-02-07 13:13:35,380][gensim.models.word2vec][INFO] - EPOCH 66: training on 2183622 raw words (2186879 effective words) took 0.8s, 2874933 effective words/s
[2023-02-07 13:13:36,154][gensim.models.word2vec][INFO] - EPOCH 67: training on 2183622 raw words (2186879 effective words) took 0.8s, 2827947 effective words/s
[2023-02-07 13:13:36,155][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 148486296 raw words (148707772 effective words) took 52.9s, 2809968 effective words/s', 'datetime': '2023-02-07T13:13:36.155218', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:13:36.155 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:13:39,285][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131223-rf6v302w/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:13:39.284940', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:13:39,285][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:13:39,299][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131223-rf6v302w/files/../tmp/embedding_model.pt
2023-02-07 13:13:39.300 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:13:40.411 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:13:40.837 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:13:41.475 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9521748854274392, 'test_mae': 1.0556645395662392, 'test_r2': 0.07650455886703067}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.035 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.035 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.8
wandb: percentage 0.0
wandb:   test_mae 1.05566
wandb:   test_mse 1.95217
wandb:    test_r2 0.0765
wandb: 
wandb: üöÄ View run driven-sweep-33 at: https://wandb.ai/xiaoqiz/mof2vec/runs/rf6v302w
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_131223-rf6v302w/logs
wandb: Agent Starting Run: 8vobc1z0 with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 520
wandb: 	model.gensim.alpha: 0.0006608439416833212
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 42
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.23794300233384247
wandb: 	model.gensim.vector_size: 70
wandb: 	model.gensim.window: 2
wandb: 	model.sklearn.learning_rate: 0.13706751330894854
wandb: 	model.sklearn.max_depth: 45
wandb: 	model.sklearn.min_child_weight: 0.009190200017175142
wandb: 	model.sklearn.n_estimators: 3554
wandb: 	model.sklearn.num_leaves: 413
wandb: 	model.sklearn.reg_alpha: 0.06377536506049448
wandb: 	model.sklearn.reg_lambda: 0.11812906847695864
wandb: 	model.sklearn.subsample: 0.38530359581425255
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131352-8vobc1z0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-34
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/8vobc1z0
2023-02-07 13:14:00.578 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 13:14:00.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 520 for sweep.
2023-02-07 13:14:00.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0006608439416833212 for sweep.
2023-02-07 13:14:00.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:14:00.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 42 for sweep.
2023-02-07 13:14:00.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 13:14:00.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.23794300233384247 for sweep.
2023-02-07 13:14:00.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 70 for sweep.
2023-02-07 13:14:00.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 2 for sweep.
2023-02-07 13:14:00.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.13706751330894854 for sweep.
2023-02-07 13:14:00.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 45 for sweep.
2023-02-07 13:14:00.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.009190200017175142 for sweep.
2023-02-07 13:14:00.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3554 for sweep.
2023-02-07 13:14:00.582 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 413 for sweep.
2023-02-07 13:14:00.582 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.06377536506049448 for sweep.
2023-02-07 13:14:00.582 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.11812906847695864 for sweep.
2023-02-07 13:14:00.582 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.38530359581425255 for sweep.
2023-02-07 13:14:00.582 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:14:00.592 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131352-8vobc1z0/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 520, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 70, 'window': 2, 'min_count': 2, 'dm': 0, 'sample': 0.23794300233384247, 'workers': 4, 'alpha': 0.0006608439416833212, 'epochs': 42}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3554, 'max_depth': 45, 'num_leaves': 413, 'reg_alpha': 0.06377536506049448, 'reg_lambda': 0.11812906847695864, 'subsample': 0.38530359581425255, 'min_child_weight': 0.009190200017175142, 'n_jobs': 4, 'learning_rate': 0.13706751330894854}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 34/3257 [00:00<00:09, 335.04it/s]  2%|‚ñè         | 68/3257 [00:00<00:09, 322.85it/s]  3%|‚ñé         | 104/3257 [00:00<00:09, 332.90it/s]  4%|‚ñç         | 138/3257 [00:00<00:09, 314.58it/s]  5%|‚ñå         | 171/3257 [00:00<00:09, 318.33it/s]  6%|‚ñå         | 203/3257 [00:00<00:09, 317.41it/s]  7%|‚ñã         | 244/3257 [00:00<00:08, 344.22it/s]  9%|‚ñä         | 282/3257 [00:00<00:08, 353.61it/s] 10%|‚ñâ         | 318/3257 [00:00<00:08, 348.96it/s] 11%|‚ñà         | 355/3257 [00:01<00:08, 352.70it/s] 12%|‚ñà‚ñè        | 391/3257 [00:01<00:08, 334.93it/s] 13%|‚ñà‚ñé        | 425/3257 [00:01<00:08, 324.81it/s] 14%|‚ñà‚ñç        | 458/3257 [00:01<00:08, 322.92it/s] 15%|‚ñà‚ñå        | 495/3257 [00:01<00:08, 335.45it/s] 16%|‚ñà‚ñã        | 532/3257 [00:01<00:07, 345.04it/s] 17%|‚ñà‚ñã        | 567/3257 [00:01<00:08, 329.07it/s] 18%|‚ñà‚ñä        | 601/3257 [00:01<00:08, 328.61it/s] 20%|‚ñà‚ñâ        | 636/3257 [00:01<00:07, 332.75it/s] 21%|‚ñà‚ñà        | 670/3257 [00:02<00:10, 239.55it/s] 22%|‚ñà‚ñà‚ñè       | 702/3257 [00:02<00:09, 257.86it/s] 23%|‚ñà‚ñà‚ñé       | 736/3257 [00:02<00:09, 272.90it/s] 24%|‚ñà‚ñà‚ñé       | 770/3257 [00:02<00:08, 289.81it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:02<00:08, 304.11it/s] 26%|‚ñà‚ñà‚ñå       | 840/3257 [00:02<00:07, 311.32it/s] 27%|‚ñà‚ñà‚ñã       | 873/3257 [00:02<00:07, 307.37it/s] 28%|‚ñà‚ñà‚ñä       | 909/3257 [00:02<00:07, 320.30it/s] 29%|‚ñà‚ñà‚ñâ       | 943/3257 [00:02<00:07, 325.34it/s] 30%|‚ñà‚ñà‚ñà       | 978/3257 [00:03<00:06, 332.29it/s] 31%|‚ñà‚ñà‚ñà       | 1012/3257 [00:03<00:07, 319.90it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1045/3257 [00:03<00:07, 310.25it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1077/3257 [00:03<00:06, 311.50it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1109/3257 [00:03<00:06, 306.90it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1140/3257 [00:03<00:06, 306.36it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1175/3257 [00:03<00:06, 310.47it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1207/3257 [00:03<00:06, 304.02it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1243/3257 [00:03<00:06, 318.97it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1276/3257 [00:04<00:06, 309.99it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1309/3257 [00:04<00:06, 314.76it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1344/3257 [00:04<00:05, 324.60it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1377/3257 [00:04<00:06, 309.39it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1409/3257 [00:04<00:06, 298.37it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1442/3257 [00:04<00:05, 307.14it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1477/3257 [00:04<00:05, 315.87it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:04<00:05, 327.67it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1546/3257 [00:04<00:05, 302.37it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1577/3257 [00:05<00:05, 297.46it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1608/3257 [00:05<00:05, 289.73it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1640/3257 [00:05<00:05, 297.29it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1670/3257 [00:05<00:05, 290.88it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1702/3257 [00:05<00:05, 297.07it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1732/3257 [00:05<00:05, 290.58it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1766/3257 [00:05<00:04, 303.34it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1803/3257 [00:05<00:04, 318.86it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1835/3257 [00:05<00:04, 311.68it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:05<00:04, 313.34it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1902/3257 [00:06<00:04, 315.77it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1935/3257 [00:06<00:04, 319.81it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1969/3257 [00:06<00:03, 323.01it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2002/3257 [00:06<00:03, 323.43it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2037/3257 [00:06<00:03, 330.75it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2071/3257 [00:06<00:05, 215.04it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2100/3257 [00:06<00:05, 231.02it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2132/3257 [00:06<00:04, 250.24it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2162/3257 [00:07<00:04, 260.09it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2193/3257 [00:07<00:03, 272.92it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2224/3257 [00:07<00:03, 281.52it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2256/3257 [00:07<00:03, 291.71it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2289/3257 [00:07<00:03, 302.19it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2321/3257 [00:07<00:03, 306.91it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2358/3257 [00:07<00:02, 324.45it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2391/3257 [00:07<00:02, 315.72it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2423/3257 [00:07<00:02, 295.14it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2454/3257 [00:08<00:02, 279.18it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2485/3257 [00:08<00:02, 285.91it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2522/3257 [00:08<00:02, 308.66it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2554/3257 [00:08<00:02, 309.55it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2586/3257 [00:08<00:02, 304.14it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2625/3257 [00:08<00:01, 327.94it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2659/3257 [00:08<00:01, 314.79it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2694/3257 [00:08<00:01, 321.14it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2727/3257 [00:08<00:01, 309.41it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2760/3257 [00:09<00:01, 313.33it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2792/3257 [00:09<00:01, 302.96it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2823/3257 [00:09<00:01, 299.59it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2854/3257 [00:09<00:01, 291.54it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2892/3257 [00:09<00:01, 314.09it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2924/3257 [00:09<00:01, 306.37it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2955/3257 [00:09<00:01, 294.83it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:09<00:00, 286.76it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3018/3257 [00:09<00:00, 295.90it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3052/3257 [00:09<00:00, 307.95it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3087/3257 [00:10<00:00, 315.78it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3119/3257 [00:10<00:00, 309.86it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3151/3257 [00:10<00:00, 292.12it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3181/3257 [00:10<00:00, 276.47it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3212/3257 [00:10<00:00, 285.06it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:10<00:00, 291.45it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:10<00:00, 304.53it/s]
2023-02-07 13:14:11.576 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:14:11,577][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d70,n5,mc2,s0.237943,t4>', 'datetime': '2023-02-07T13:14:11.577845', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:14:11,579][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:14:11,579][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:14:11,781][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 13:14:11,781][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:14:11,787][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 2542 unique words (90.17% of original 2819, drops 277)', 'datetime': '2023-02-07T13:14:11.787450', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:14:11,787][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 2183345 word corpus (99.99% of original 2183622, drops 277)', 'datetime': '2023-02-07T13:14:11.787680', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:14:11,796][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 13:14:11,796][gensim.models.word2vec][INFO] - sample=0.237943 downsamples 0 most-common words
[2023-02-07 13:14:11,796][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2183345 word corpus (100.0%% of prior 2183345)', 'datetime': '2023-02-07T13:14:11.796527', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:14:11,812][gensim.models.word2vec][INFO] - estimated required memory for 2542 words and 70 dimensions: 4257880 bytes
[2023-02-07 13:14:11,812][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:14:11,814][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2542 vocabulary and 70 features, using sg=1 hs=0 sample=0.23794300233384247 negative=5 window=2 shrink_windows=True', 'datetime': '2023-02-07T13:14:11.814783', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:14:12,504][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2186602 effective words) took 0.7s, 3180344 effective words/s
[2023-02-07 13:14:13,195][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2186602 effective words) took 0.7s, 3170057 effective words/s
[2023-02-07 13:14:13,886][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2186602 effective words) took 0.7s, 3168629 effective words/s
[2023-02-07 13:14:14,574][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2186602 effective words) took 0.7s, 3184048 effective words/s
[2023-02-07 13:14:15,262][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2186602 effective words) took 0.7s, 3181036 effective words/s
[2023-02-07 13:14:15,953][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2186602 effective words) took 0.7s, 3173513 effective words/s
[2023-02-07 13:14:16,650][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2186602 effective words) took 0.7s, 3138808 effective words/s
[2023-02-07 13:14:17,338][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2186602 effective words) took 0.7s, 3185729 effective words/s
[2023-02-07 13:14:18,044][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2186602 effective words) took 0.7s, 3101615 effective words/s
[2023-02-07 13:14:18,746][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2186602 effective words) took 0.7s, 3121610 effective words/s
[2023-02-07 13:14:19,435][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2186602 effective words) took 0.7s, 3182740 effective words/s
[2023-02-07 13:14:20,122][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2186602 effective words) took 0.7s, 3188864 effective words/s
[2023-02-07 13:14:20,818][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2186602 effective words) took 0.7s, 3147378 effective words/s
[2023-02-07 13:14:21,522][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2186602 effective words) took 0.7s, 3110514 effective words/s
[2023-02-07 13:14:22,217][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2186602 effective words) took 0.7s, 3149432 effective words/s
[2023-02-07 13:14:22,907][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2186602 effective words) took 0.7s, 3174195 effective words/s
[2023-02-07 13:14:23,589][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2186602 effective words) took 0.7s, 3214100 effective words/s
[2023-02-07 13:14:24,274][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2186602 effective words) took 0.7s, 3197738 effective words/s
[2023-02-07 13:14:24,956][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2186602 effective words) took 0.7s, 3208108 effective words/s
[2023-02-07 13:14:25,645][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2186602 effective words) took 0.7s, 3180645 effective words/s
[2023-02-07 13:14:26,350][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2186602 effective words) took 0.7s, 3109880 effective words/s
[2023-02-07 13:14:27,048][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2186602 effective words) took 0.7s, 3138681 effective words/s
[2023-02-07 13:14:27,733][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2186602 effective words) took 0.7s, 3193354 effective words/s
[2023-02-07 13:14:28,421][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2186602 effective words) took 0.7s, 3186442 effective words/s
[2023-02-07 13:14:29,112][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2186602 effective words) took 0.7s, 3169344 effective words/s
[2023-02-07 13:14:29,805][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2186602 effective words) took 0.7s, 3159297 effective words/s
[2023-02-07 13:14:30,502][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2186602 effective words) took 0.7s, 3146169 effective words/s
[2023-02-07 13:14:31,199][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2186602 effective words) took 0.7s, 3139202 effective words/s
[2023-02-07 13:14:31,883][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2186602 effective words) took 0.7s, 3208562 effective words/s
[2023-02-07 13:14:32,569][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2186602 effective words) took 0.7s, 3190153 effective words/s
[2023-02-07 13:14:33,251][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2186602 effective words) took 0.7s, 3212101 effective words/s
[2023-02-07 13:14:33,941][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2186602 effective words) took 0.7s, 3174369 effective words/s
[2023-02-07 13:14:34,628][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2186602 effective words) took 0.7s, 3188002 effective words/s
[2023-02-07 13:14:35,339][gensim.models.word2vec][INFO] - EPOCH 33: training on 2183622 raw words (2186602 effective words) took 0.7s, 3081555 effective words/s
[2023-02-07 13:14:36,025][gensim.models.word2vec][INFO] - EPOCH 34: training on 2183622 raw words (2186602 effective words) took 0.7s, 3189862 effective words/s
[2023-02-07 13:14:36,716][gensim.models.word2vec][INFO] - EPOCH 35: training on 2183622 raw words (2186602 effective words) took 0.7s, 3174680 effective words/s
[2023-02-07 13:14:37,427][gensim.models.word2vec][INFO] - EPOCH 36: training on 2183622 raw words (2186602 effective words) took 0.7s, 3077713 effective words/s
[2023-02-07 13:14:38,123][gensim.models.word2vec][INFO] - EPOCH 37: training on 2183622 raw words (2186602 effective words) took 0.7s, 3148867 effective words/s
[2023-02-07 13:14:38,812][gensim.models.word2vec][INFO] - EPOCH 38: training on 2183622 raw words (2186602 effective words) took 0.7s, 3179322 effective words/s
[2023-02-07 13:14:39,490][gensim.models.word2vec][INFO] - EPOCH 39: training on 2183622 raw words (2186602 effective words) took 0.7s, 3231398 effective words/s
[2023-02-07 13:14:40,171][gensim.models.word2vec][INFO] - EPOCH 40: training on 2183622 raw words (2186602 effective words) took 0.7s, 3214292 effective words/s
[2023-02-07 13:14:40,852][gensim.models.word2vec][INFO] - EPOCH 41: training on 2183622 raw words (2186602 effective words) took 0.7s, 3214222 effective words/s
[2023-02-07 13:14:40,853][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 91712124 raw words (91837284 effective words) took 29.0s, 3162627 effective words/s', 'datetime': '2023-02-07T13:14:40.853308', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:14:40.853 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:14:42,493][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131352-8vobc1z0/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:14:42.493424', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:14:42,494][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:14:42,500][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131352-8vobc1z0/files/../tmp/embedding_model.pt
2023-02-07 13:14:42.500 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:14:43.524 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:14:43.932 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:14:44.466 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8137592253899382, 'test_mae': 1.030935477042582, 'test_r2': 0.1419834419221495}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.78
wandb: percentage 0.09826
wandb:   test_mae 1.03094
wandb:   test_mse 1.81376
wandb:    test_r2 0.14198
wandb: 
wandb: üöÄ View run fancy-sweep-34 at: https://wandb.ai/xiaoqiz/mof2vec/runs/8vobc1z0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_131352-8vobc1z0/logs
wandb: Agent Starting Run: bwm8xhuf with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 1013
wandb: 	model.gensim.alpha: 0.0005867399360132891
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 35
wandb: 	model.gensim.min_count: 10
wandb: 	model.gensim.sample: 0.3349277964685092
wandb: 	model.gensim.vector_size: 229
wandb: 	model.gensim.window: 2
wandb: 	model.sklearn.learning_rate: 0.24223181027301804
wandb: 	model.sklearn.max_depth: 5
wandb: 	model.sklearn.min_child_weight: 0.012725474719631494
wandb: 	model.sklearn.n_estimators: 999
wandb: 	model.sklearn.num_leaves: 479
wandb: 	model.sklearn.reg_alpha: 0.05021545611222542
wandb: 	model.sklearn.reg_lambda: 0.005492651476873009
wandb: 	model.sklearn.subsample: 0.3254413131272427
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131456-bwm8xhuf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-35
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/bwm8xhuf
2023-02-07 13:15:04.516 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:15:04.516 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 1013 for sweep.
2023-02-07 13:15:04.517 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0005867399360132891 for sweep.
2023-02-07 13:15:04.517 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:15:04.517 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 35 for sweep.
2023-02-07 13:15:04.517 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 10 for sweep.
2023-02-07 13:15:04.518 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3349277964685092 for sweep.
2023-02-07 13:15:04.518 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 229 for sweep.
2023-02-07 13:15:04.518 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 2 for sweep.
2023-02-07 13:15:04.518 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.24223181027301804 for sweep.
2023-02-07 13:15:04.518 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 5 for sweep.
2023-02-07 13:15:04.519 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.012725474719631494 for sweep.
2023-02-07 13:15:04.519 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 999 for sweep.
2023-02-07 13:15:04.519 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 479 for sweep.
2023-02-07 13:15:04.519 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.05021545611222542 for sweep.
2023-02-07 13:15:04.519 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.005492651476873009 for sweep.
2023-02-07 13:15:04.520 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3254413131272427 for sweep.
2023-02-07 13:15:04.520 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:15:04.529 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131456-bwm8xhuf/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 1013, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 229, 'window': 2, 'min_count': 10, 'dm': 0, 'sample': 0.3349277964685092, 'workers': 4, 'alpha': 0.0005867399360132891, 'epochs': 35}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 999, 'max_depth': 5, 'num_leaves': 479, 'reg_alpha': 0.05021545611222542, 'reg_lambda': 0.005492651476873009, 'subsample': 0.3254413131272427, 'min_child_weight': 0.012725474719631494, 'n_jobs': 4, 'learning_rate': 0.24223181027301804}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 34/3257 [00:00<00:09, 334.09it/s]  2%|‚ñè         | 68/3257 [00:00<00:10, 317.96it/s]  3%|‚ñé         | 101/3257 [00:00<00:09, 323.00it/s]  4%|‚ñç         | 134/3257 [00:00<00:09, 322.53it/s]  5%|‚ñå         | 167/3257 [00:00<00:09, 324.43it/s]  6%|‚ñå         | 200/3257 [00:00<00:09, 325.20it/s]  7%|‚ñã         | 241/3257 [00:00<00:08, 352.56it/s]  9%|‚ñä         | 279/3257 [00:00<00:08, 360.37it/s] 10%|‚ñâ         | 316/3257 [00:00<00:08, 360.36it/s] 11%|‚ñà         | 353/3257 [00:01<00:10, 270.29it/s] 12%|‚ñà‚ñè        | 388/3257 [00:01<00:09, 289.48it/s] 13%|‚ñà‚ñé        | 426/3257 [00:01<00:09, 307.60it/s] 14%|‚ñà‚ñç        | 467/3257 [00:01<00:08, 333.41it/s] 16%|‚ñà‚ñå        | 505/3257 [00:01<00:07, 344.64it/s] 17%|‚ñà‚ñã        | 545/3257 [00:01<00:07, 358.30it/s] 18%|‚ñà‚ñä        | 582/3257 [00:01<00:08, 331.31it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:01<00:07, 338.23it/s] 20%|‚ñà‚ñà        | 655/3257 [00:01<00:07, 339.78it/s] 21%|‚ñà‚ñà        | 690/3257 [00:02<00:07, 342.09it/s] 22%|‚ñà‚ñà‚ñè       | 727/3257 [00:02<00:07, 346.41it/s] 24%|‚ñà‚ñà‚ñé       | 766/3257 [00:02<00:06, 357.77it/s] 25%|‚ñà‚ñà‚ñç       | 803/3257 [00:02<00:06, 357.54it/s] 26%|‚ñà‚ñà‚ñå       | 839/3257 [00:02<00:07, 344.89it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:02<00:06, 341.11it/s] 28%|‚ñà‚ñà‚ñä       | 912/3257 [00:02<00:06, 351.93it/s] 29%|‚ñà‚ñà‚ñâ       | 948/3257 [00:02<00:06, 347.92it/s] 30%|‚ñà‚ñà‚ñà       | 983/3257 [00:02<00:06, 346.48it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1018/3257 [00:03<00:06, 343.22it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1053/3257 [00:03<00:06, 325.01it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:03<00:06, 336.63it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1127/3257 [00:03<00:06, 341.43it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1166/3257 [00:03<00:05, 354.63it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1202/3257 [00:03<00:06, 333.31it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1243/3257 [00:03<00:05, 353.81it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1279/3257 [00:03<00:05, 344.84it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1314/3257 [00:03<00:05, 340.66it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1352/3257 [00:03<00:05, 351.59it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1388/3257 [00:04<00:05, 350.58it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1430/3257 [00:04<00:04, 369.02it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1468/3257 [00:04<00:04, 370.60it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1506/3257 [00:04<00:04, 371.90it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1544/3257 [00:04<00:05, 333.54it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1581/3257 [00:04<00:04, 342.16it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1621/3257 [00:04<00:04, 358.14it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1658/3257 [00:04<00:04, 337.81it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1693/3257 [00:05<00:06, 245.41it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:05<00:05, 266.26it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1764/3257 [00:05<00:05, 286.14it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1800/3257 [00:05<00:04, 304.63it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1833/3257 [00:05<00:04, 308.54it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1873/3257 [00:05<00:04, 330.51it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1910/3257 [00:05<00:03, 341.47it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1950/3257 [00:05<00:03, 355.22it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1988/3257 [00:05<00:03, 360.57it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2025/3257 [00:06<00:03, 346.11it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2061/3257 [00:06<00:03, 339.51it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2096/3257 [00:06<00:03, 336.30it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2132/3257 [00:06<00:03, 342.41it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2168/3257 [00:06<00:03, 347.35it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2203/3257 [00:06<00:03, 344.27it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2238/3257 [00:06<00:02, 344.09it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2274/3257 [00:06<00:02, 347.12it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2317/3257 [00:06<00:02, 370.38it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2362/3257 [00:06<00:02, 391.67it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2404/3257 [00:07<00:02, 398.21it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2444/3257 [00:07<00:02, 374.84it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2483/3257 [00:07<00:02, 378.57it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2522/3257 [00:07<00:01, 378.42it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2560/3257 [00:07<00:01, 378.87it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2598/3257 [00:07<00:01, 365.33it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:07<00:01, 380.74it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2679/3257 [00:07<00:01, 365.03it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2716/3257 [00:07<00:01, 332.54it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:08<00:01, 348.75it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2796/3257 [00:08<00:01, 359.56it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2833/3257 [00:08<00:01, 358.29it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2871/3257 [00:08<00:01, 363.89it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2908/3257 [00:08<00:00, 355.89it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2944/3257 [00:08<00:00, 351.25it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2980/3257 [00:08<00:00, 347.08it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3015/3257 [00:08<00:00, 344.51it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3050/3257 [00:08<00:00, 343.14it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3087/3257 [00:09<00:00, 350.23it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3123/3257 [00:09<00:00, 350.06it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3159/3257 [00:09<00:00, 350.46it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3195/3257 [00:09<00:00, 237.82it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3234/3257 [00:09<00:00, 270.37it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 337.94it/s]
2023-02-07 13:15:14.343 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:15:14,344][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d229,n5,mc10,s0.334928,t4>', 'datetime': '2023-02-07T13:15:14.344509', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:15:14,345][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:15:14,345][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:15:14,480][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:15:14,480][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:15:14,481][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=10 retains 456 unique words (49.35% of original 924, drops 468)', 'datetime': '2023-02-07T13:15:14.481686', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:15:14,481][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 1453848 word corpus (99.87% of original 1455748, drops 1900)', 'datetime': '2023-02-07T13:15:14.481865', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:15:14,483][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:15:14,483][gensim.models.word2vec][INFO] - sample=0.334928 downsamples 0 most-common words
[2023-02-07 13:15:14,484][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1453848 word corpus (100.0%% of prior 1453848)', 'datetime': '2023-02-07T13:15:14.484919', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:15:14,487][gensim.models.word2vec][INFO] - estimated required memory for 456 words and 229 dimensions: 4698204 bytes
[2023-02-07 13:15:14,488][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:15:14,491][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 456 vocabulary and 229 features, using sg=1 hs=0 sample=0.3349277964685092 negative=5 window=2 shrink_windows=True', 'datetime': '2023-02-07T13:15:14.491320', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:15:15,361][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1457105 effective words) took 0.9s, 1678783 effective words/s
[2023-02-07 13:15:16,232][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1457105 effective words) took 0.9s, 1674650 effective words/s
[2023-02-07 13:15:17,104][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1457105 effective words) took 0.9s, 1673420 effective words/s
[2023-02-07 13:15:17,986][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1457105 effective words) took 0.9s, 1654386 effective words/s
[2023-02-07 13:15:18,860][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1457105 effective words) took 0.9s, 1669017 effective words/s
[2023-02-07 13:15:19,739][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1457105 effective words) took 0.9s, 1660020 effective words/s
[2023-02-07 13:15:20,617][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1457105 effective words) took 0.9s, 1662175 effective words/s
[2023-02-07 13:15:21,506][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1457105 effective words) took 0.9s, 1642048 effective words/s
[2023-02-07 13:15:22,396][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1457105 effective words) took 0.9s, 1638268 effective words/s
[2023-02-07 13:15:23,279][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1457105 effective words) took 0.9s, 1653038 effective words/s
[2023-02-07 13:15:24,163][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1457105 effective words) took 0.9s, 1652905 effective words/s
[2023-02-07 13:15:25,042][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1457105 effective words) took 0.9s, 1659311 effective words/s
[2023-02-07 13:15:25,922][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1457105 effective words) took 0.9s, 1658081 effective words/s
[2023-02-07 13:15:26,795][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1457105 effective words) took 0.9s, 1673764 effective words/s
[2023-02-07 13:15:27,672][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1457105 effective words) took 0.9s, 1664298 effective words/s
[2023-02-07 13:15:28,548][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1457105 effective words) took 0.9s, 1664539 effective words/s
[2023-02-07 13:15:29,418][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1457105 effective words) took 0.9s, 1679985 effective words/s
[2023-02-07 13:15:30,239][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1457105 effective words) took 0.8s, 1776322 effective words/s
[2023-02-07 13:15:31,036][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1457105 effective words) took 0.8s, 1830504 effective words/s
[2023-02-07 13:15:31,834][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1457105 effective words) took 0.8s, 1829877 effective words/s
[2023-02-07 13:15:32,644][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1457105 effective words) took 0.8s, 1803553 effective words/s
[2023-02-07 13:15:33,441][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1457105 effective words) took 0.8s, 1829796 effective words/s
[2023-02-07 13:15:34,237][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1457105 effective words) took 0.8s, 1834254 effective words/s
[2023-02-07 13:15:35,046][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1457105 effective words) took 0.8s, 1804262 effective words/s
[2023-02-07 13:15:35,864][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1457105 effective words) took 0.8s, 1784526 effective words/s
[2023-02-07 13:15:36,717][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1457105 effective words) took 0.9s, 1710200 effective words/s
[2023-02-07 13:15:37,553][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1457105 effective words) took 0.8s, 1746717 effective words/s
[2023-02-07 13:15:38,413][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1457105 effective words) took 0.9s, 1696579 effective words/s
[2023-02-07 13:15:39,278][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1457105 effective words) took 0.9s, 1686525 effective words/s
[2023-02-07 13:15:40,095][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1457105 effective words) took 0.8s, 1786652 effective words/s
[2023-02-07 13:15:40,878][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1457105 effective words) took 0.8s, 1864252 effective words/s
[2023-02-07 13:15:41,681][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1457105 effective words) took 0.8s, 1818174 effective words/s
[2023-02-07 13:15:42,477][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1457105 effective words) took 0.8s, 1838051 effective words/s
[2023-02-07 13:15:43,274][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1457105 effective words) took 0.8s, 1830366 effective words/s
[2023-02-07 13:15:44,082][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1457105 effective words) took 0.8s, 1806561 effective words/s
[2023-02-07 13:15:44,082][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 50951180 raw words (50998675 effective words) took 29.6s, 1723439 effective words/s', 'datetime': '2023-02-07T13:15:44.082852', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:15:44.083 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:15:45,126][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131456-bwm8xhuf/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:15:45.126826', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:15:45,127][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:15:45,135][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131456-bwm8xhuf/files/../tmp/embedding_model.pt
2023-02-07 13:15:45.135 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:15:46.650 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:15:47.227 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:15:48.742 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.823840038289534, 'test_mae': 1.011795061131928, 'test_r2': 0.1372146146899249}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.48
wandb: percentage 0.50649
wandb:   test_mae 1.0118
wandb:   test_mse 1.82384
wandb:    test_r2 0.13721
wandb: 
wandb: üöÄ View run upbeat-sweep-35 at: https://wandb.ai/xiaoqiz/mof2vec/runs/bwm8xhuf
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_131456-bwm8xhuf/logs
wandb: Agent Starting Run: 5du27hxh with config:
wandb: 	data.data.wl_step: 5
wandb: 	data.nn.batch_size: 663
wandb: 	model.gensim.alpha: 0.002384303309675533
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 55
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.48765482320895376
wandb: 	model.gensim.vector_size: 102
wandb: 	model.gensim.window: 7
wandb: 	model.sklearn.learning_rate: 0.12160094784517136
wandb: 	model.sklearn.max_depth: 41
wandb: 	model.sklearn.min_child_weight: 0.03359318042168885
wandb: 	model.sklearn.n_estimators: 3773
wandb: 	model.sklearn.num_leaves: 483
wandb: 	model.sklearn.reg_alpha: 0.2525611570527275
wandb: 	model.sklearn.reg_lambda: 0.003048836898298384
wandb: 	model.sklearn.subsample: 0.2633340695647274
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131559-5du27hxh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-sweep-36
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/5du27hxh
2023-02-07 13:16:08.333 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 5 for sweep.
2023-02-07 13:16:08.334 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 663 for sweep.
2023-02-07 13:16:08.334 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.002384303309675533 for sweep.
2023-02-07 13:16:08.334 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:16:08.334 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 55 for sweep.
2023-02-07 13:16:08.335 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 13:16:08.335 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.48765482320895376 for sweep.
2023-02-07 13:16:08.335 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 102 for sweep.
2023-02-07 13:16:08.335 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 7 for sweep.
2023-02-07 13:16:08.335 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.12160094784517136 for sweep.
2023-02-07 13:16:08.336 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 41 for sweep.
2023-02-07 13:16:08.336 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03359318042168885 for sweep.
2023-02-07 13:16:08.336 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3773 for sweep.
2023-02-07 13:16:08.336 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 483 for sweep.
2023-02-07 13:16:08.337 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.2525611570527275 for sweep.
2023-02-07 13:16:08.337 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.003048836898298384 for sweep.
2023-02-07 13:16:08.337 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2633340695647274 for sweep.
2023-02-07 13:16:08.337 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:16:08.345 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 5}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131559-5du27hxh/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 663, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 102, 'window': 7, 'min_count': 1, 'dm': 0, 'sample': 0.48765482320895376, 'workers': 4, 'alpha': 0.002384303309675533, 'epochs': 55}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3773, 'max_depth': 41, 'num_leaves': 483, 'reg_alpha': 0.2525611570527275, 'reg_lambda': 0.003048836898298384, 'subsample': 0.2633340695647274, 'min_child_weight': 0.03359318042168885, 'n_jobs': 4, 'learning_rate': 0.12160094784517136}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 19/3257 [00:00<00:17, 185.29it/s]  1%|‚ñè         | 41/3257 [00:00<00:16, 200.05it/s]  2%|‚ñè         | 64/3257 [00:00<00:15, 212.25it/s]  3%|‚ñé         | 89/3257 [00:00<00:14, 222.44it/s]  3%|‚ñé         | 112/3257 [00:00<00:15, 203.80it/s]  4%|‚ñç         | 136/3257 [00:00<00:14, 212.94it/s]  5%|‚ñç         | 159/3257 [00:00<00:14, 216.71it/s]  6%|‚ñå         | 181/3257 [00:00<00:14, 210.03it/s]  6%|‚ñã         | 204/3257 [00:00<00:14, 214.97it/s]  7%|‚ñã         | 231/3257 [00:01<00:13, 228.18it/s]  8%|‚ñä         | 254/3257 [00:01<00:13, 226.45it/s]  9%|‚ñä         | 281/3257 [00:01<00:12, 239.11it/s]  9%|‚ñâ         | 305/3257 [00:01<00:12, 233.34it/s] 10%|‚ñà         | 330/3257 [00:01<00:12, 237.24it/s] 11%|‚ñà         | 354/3257 [00:01<00:12, 233.49it/s] 12%|‚ñà‚ñè        | 378/3257 [00:01<00:13, 217.40it/s] 12%|‚ñà‚ñè        | 400/3257 [00:01<00:13, 216.61it/s] 13%|‚ñà‚ñé        | 422/3257 [00:01<00:13, 214.54it/s] 14%|‚ñà‚ñé        | 444/3257 [00:02<00:14, 197.27it/s] 14%|‚ñà‚ñç        | 468/3257 [00:02<00:13, 208.20it/s] 15%|‚ñà‚ñå        | 490/3257 [00:02<00:13, 210.86it/s] 16%|‚ñà‚ñå        | 516/3257 [00:02<00:12, 224.74it/s] 17%|‚ñà‚ñã        | 539/3257 [00:02<00:12, 225.02it/s] 17%|‚ñà‚ñã        | 562/3257 [00:02<00:12, 215.58it/s] 18%|‚ñà‚ñä        | 584/3257 [00:02<00:12, 207.78it/s] 19%|‚ñà‚ñä        | 609/3257 [00:02<00:12, 219.46it/s] 19%|‚ñà‚ñâ        | 633/3257 [00:02<00:11, 224.38it/s] 20%|‚ñà‚ñà        | 656/3257 [00:03<00:12, 207.07it/s] 21%|‚ñà‚ñà        | 679/3257 [00:03<00:12, 212.30it/s] 22%|‚ñà‚ñà‚ñè       | 701/3257 [00:03<00:12, 204.76it/s] 22%|‚ñà‚ñà‚ñè       | 723/3257 [00:03<00:12, 207.46it/s] 23%|‚ñà‚ñà‚ñé       | 744/3257 [00:03<00:12, 205.98it/s] 24%|‚ñà‚ñà‚ñé       | 769/3257 [00:03<00:11, 216.18it/s] 24%|‚ñà‚ñà‚ñç       | 791/3257 [00:03<00:11, 208.11it/s] 25%|‚ñà‚ñà‚ñç       | 812/3257 [00:03<00:11, 207.25it/s] 26%|‚ñà‚ñà‚ñå       | 833/3257 [00:03<00:11, 202.97it/s] 26%|‚ñà‚ñà‚ñå       | 854/3257 [00:03<00:12, 196.99it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:04<00:11, 199.11it/s] 28%|‚ñà‚ñà‚ñä       | 897/3257 [00:04<00:11, 202.54it/s] 28%|‚ñà‚ñà‚ñä       | 920/3257 [00:04<00:11, 208.05it/s] 29%|‚ñà‚ñà‚ñâ       | 941/3257 [00:04<00:11, 206.95it/s] 30%|‚ñà‚ñà‚ñâ       | 964/3257 [00:04<00:10, 211.75it/s] 30%|‚ñà‚ñà‚ñà       | 986/3257 [00:04<00:11, 204.76it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:04<00:11, 201.62it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1028/3257 [00:04<00:11, 198.41it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1048/3257 [00:04<00:11, 192.87it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1072/3257 [00:05<00:10, 204.41it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1093/3257 [00:05<00:10, 197.84it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:05<00:10, 206.58it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1138/3257 [00:05<00:10, 203.71it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1159/3257 [00:05<00:14, 139.90it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1178/3257 [00:05<00:14, 147.19it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1198/3257 [00:05<00:12, 158.86it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1217/3257 [00:05<00:12, 165.48it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:06<00:10, 187.47it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1265/3257 [00:06<00:10, 197.87it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1286/3257 [00:06<00:10, 189.23it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1309/3257 [00:06<00:09, 195.70it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1333/3257 [00:06<00:09, 207.71it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1355/3257 [00:06<00:09, 203.93it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1376/3257 [00:06<00:09, 204.06it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1397/3257 [00:06<00:09, 203.35it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:06<00:08, 217.11it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1445/3257 [00:07<00:08, 215.90it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1470/3257 [00:07<00:07, 224.17it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1494/3257 [00:07<00:07, 228.69it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:07<00:07, 226.10it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1542/3257 [00:07<00:07, 221.09it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1565/3257 [00:07<00:07, 213.77it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1587/3257 [00:07<00:08, 206.79it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1612/3257 [00:07<00:07, 217.01it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:07<00:07, 207.12it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1655/3257 [00:08<00:07, 205.87it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1676/3257 [00:08<00:07, 200.81it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1699/3257 [00:08<00:07, 207.31it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1720/3257 [00:08<00:07, 204.50it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1741/3257 [00:08<00:07, 189.63it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:08<00:07, 197.79it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1786/3257 [00:08<00:07, 205.86it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1807/3257 [00:08<00:07, 201.16it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1828/3257 [00:08<00:07, 202.48it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1850/3257 [00:08<00:06, 206.44it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1874/3257 [00:09<00:06, 213.69it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1896/3257 [00:09<00:06, 209.66it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1918/3257 [00:09<00:06, 205.98it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1946/3257 [00:09<00:05, 226.65it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1974/3257 [00:09<00:05, 235.71it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1998/3257 [00:09<00:05, 234.65it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2022/3257 [00:09<00:05, 235.93it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2046/3257 [00:09<00:05, 224.80it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2069/3257 [00:09<00:05, 205.20it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2091/3257 [00:10<00:05, 208.73it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2114/3257 [00:10<00:05, 214.53it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2136/3257 [00:10<00:05, 202.44it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2157/3257 [00:10<00:05, 201.58it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2179/3257 [00:10<00:05, 205.82it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2202/3257 [00:10<00:05, 210.91it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2224/3257 [00:10<00:04, 208.44it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2245/3257 [00:10<00:04, 207.55it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2267/3257 [00:10<00:04, 209.93it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2289/3257 [00:11<00:04, 208.64it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2310/3257 [00:11<00:04, 205.59it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2339/3257 [00:11<00:03, 229.91it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2367/3257 [00:11<00:03, 238.49it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2394/3257 [00:11<00:03, 246.34it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2419/3257 [00:11<00:03, 223.12it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2442/3257 [00:11<00:03, 215.57it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2464/3257 [00:11<00:03, 216.43it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2486/3257 [00:11<00:03, 212.84it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2508/3257 [00:12<00:05, 138.57it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2535/3257 [00:12<00:04, 164.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2555/3257 [00:12<00:04, 171.63it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2575/3257 [00:12<00:03, 177.23it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2597/3257 [00:12<00:03, 187.01it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:12<00:02, 216.92it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2652/3257 [00:12<00:02, 212.21it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2675/3257 [00:12<00:02, 215.80it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2698/3257 [00:13<00:02, 207.06it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2720/3257 [00:13<00:02, 204.88it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2746/3257 [00:13<00:02, 219.87it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2769/3257 [00:13<00:02, 214.67it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2796/3257 [00:13<00:02, 229.40it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2820/3257 [00:13<00:01, 226.18it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2843/3257 [00:13<00:01, 218.91it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2872/3257 [00:13<00:01, 238.67it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2897/3257 [00:13<00:01, 229.65it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2921/3257 [00:14<00:01, 232.42it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2945/3257 [00:14<00:01, 223.99it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2969/3257 [00:14<00:01, 227.57it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2992/3257 [00:14<00:01, 212.86it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3018/3257 [00:14<00:01, 223.49it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3046/3257 [00:14<00:00, 238.15it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3073/3257 [00:14<00:00, 246.17it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3098/3257 [00:14<00:00, 236.79it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3125/3257 [00:14<00:00, 245.24it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3150/3257 [00:15<00:00, 229.19it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3175/3257 [00:15<00:00, 233.00it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3199/3257 [00:15<00:00, 230.50it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3223/3257 [00:15<00:00, 223.76it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:15<00:00, 234.17it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:15<00:00, 210.43it/s]
2023-02-07 13:16:24.391 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:16:24,391][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d102,n5,s0.487655,t4>', 'datetime': '2023-02-07T13:16:24.391942', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:16:24,393][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:16:24,394][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:16:24,789][gensim.models.doc2vec][INFO] - collected 21699 word types and 3257 unique tags from a corpus of 3257 examples and 4367244 words
[2023-02-07 13:16:24,790][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:16:24,843][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 21699 unique words (100.00% of original 21699, drops 0)', 'datetime': '2023-02-07T13:16:24.843895', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:16:24,844][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 4367244 word corpus (100.00% of original 4367244, drops 0)', 'datetime': '2023-02-07T13:16:24.844246', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:16:24,917][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 21699 items
[2023-02-07 13:16:24,917][gensim.models.word2vec][INFO] - sample=0.487655 downsamples 0 most-common words
[2023-02-07 13:16:24,918][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4367244 word corpus (100.0%% of prior 4367244)', 'datetime': '2023-02-07T13:16:24.918046', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:16:25,045][gensim.models.word2vec][INFO] - estimated required memory for 21699 words and 102 dimensions: 30536140 bytes
[2023-02-07 13:16:25,046][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:16:25,056][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 21699 vocabulary and 102 features, using sg=1 hs=0 sample=0.48765482320895376 negative=5 window=7 shrink_windows=True', 'datetime': '2023-02-07T13:16:25.056561', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:16:26,063][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 69.36% examples, 3086090 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:26,473][gensim.models.word2vec][INFO] - EPOCH 0: training on 4367244 raw words (4368669 effective words) took 1.4s, 3091320 effective words/s
[2023-02-07 13:16:27,479][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 71.66% examples, 3176508 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:27,840][gensim.models.word2vec][INFO] - EPOCH 1: training on 4367244 raw words (4368669 effective words) took 1.4s, 3198677 effective words/s
[2023-02-07 13:16:28,845][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 72.64% examples, 3217030 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:29,193][gensim.models.word2vec][INFO] - EPOCH 2: training on 4367244 raw words (4368669 effective words) took 1.4s, 3231807 effective words/s
[2023-02-07 13:16:30,196][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 72.92% examples, 3233514 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:30,555][gensim.models.word2vec][INFO] - EPOCH 3: training on 4367244 raw words (4368669 effective words) took 1.4s, 3210615 effective words/s
[2023-02-07 13:16:31,558][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 72.46% examples, 3215758 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:31,909][gensim.models.word2vec][INFO] - EPOCH 4: training on 4367244 raw words (4368669 effective words) took 1.4s, 3231004 effective words/s
[2023-02-07 13:16:32,910][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 71.91% examples, 3199370 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:33,267][gensim.models.word2vec][INFO] - EPOCH 5: training on 4367244 raw words (4368669 effective words) took 1.4s, 3221114 effective words/s
[2023-02-07 13:16:34,269][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 73.23% examples, 3242414 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:34,609][gensim.models.word2vec][INFO] - EPOCH 6: training on 4367244 raw words (4368669 effective words) took 1.3s, 3257840 effective words/s
[2023-02-07 13:16:35,613][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 73.90% examples, 3270074 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:35,952][gensim.models.word2vec][INFO] - EPOCH 7: training on 4367244 raw words (4368669 effective words) took 1.3s, 3256927 effective words/s
[2023-02-07 13:16:36,955][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 73.29% examples, 3243202 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:37,295][gensim.models.word2vec][INFO] - EPOCH 8: training on 4367244 raw words (4368669 effective words) took 1.3s, 3255946 effective words/s
[2023-02-07 13:16:38,296][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 72.98% examples, 3238870 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:38,634][gensim.models.word2vec][INFO] - EPOCH 9: training on 4367244 raw words (4368669 effective words) took 1.3s, 3264879 effective words/s
[2023-02-07 13:16:39,639][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 74.79% examples, 3302151 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:39,950][gensim.models.word2vec][INFO] - EPOCH 10: training on 4367244 raw words (4368669 effective words) took 1.3s, 3323770 effective words/s
[2023-02-07 13:16:40,954][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 75.10% examples, 3324685 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:41,268][gensim.models.word2vec][INFO] - EPOCH 11: training on 4367244 raw words (4368669 effective words) took 1.3s, 3319504 effective words/s
[2023-02-07 13:16:42,270][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 77.31% examples, 3411346 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:42,529][gensim.models.word2vec][INFO] - EPOCH 12: training on 4367244 raw words (4368669 effective words) took 1.3s, 3467767 effective words/s
[2023-02-07 13:16:43,533][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 82.84% examples, 3647170 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:43,721][gensim.models.word2vec][INFO] - EPOCH 13: training on 4367244 raw words (4368669 effective words) took 1.2s, 3668598 effective words/s
[2023-02-07 13:16:44,725][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 84.59% examples, 3717835 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:44,887][gensim.models.word2vec][INFO] - EPOCH 14: training on 4367244 raw words (4368669 effective words) took 1.2s, 3749701 effective words/s
[2023-02-07 13:16:45,892][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 78.35% examples, 3442033 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:46,162][gensim.models.word2vec][INFO] - EPOCH 15: training on 4367244 raw words (4368669 effective words) took 1.3s, 3429095 effective words/s
[2023-02-07 13:16:47,165][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 76.51% examples, 3383341 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:47,446][gensim.models.word2vec][INFO] - EPOCH 16: training on 4367244 raw words (4368669 effective words) took 1.3s, 3406132 effective words/s
[2023-02-07 13:16:48,448][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 76.85% examples, 3394111 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:48,728][gensim.models.word2vec][INFO] - EPOCH 17: training on 4367244 raw words (4368669 effective words) took 1.3s, 3412201 effective words/s
[2023-02-07 13:16:49,732][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 78.94% examples, 3480345 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:49,986][gensim.models.word2vec][INFO] - EPOCH 18: training on 4367244 raw words (4368669 effective words) took 1.3s, 3476266 effective words/s
[2023-02-07 13:16:50,989][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 78.35% examples, 3449865 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:51,258][gensim.models.word2vec][INFO] - EPOCH 19: training on 4367244 raw words (4368669 effective words) took 1.3s, 3439006 effective words/s
[2023-02-07 13:16:52,262][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 78.94% examples, 3480871 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:52,509][gensim.models.word2vec][INFO] - EPOCH 20: training on 4367244 raw words (4368669 effective words) took 1.2s, 3496190 effective words/s
[2023-02-07 13:16:53,512][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 78.94% examples, 3481289 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:16:53,764][gensim.models.word2vec][INFO] - EPOCH 21: training on 4367244 raw words (4368669 effective words) took 1.3s, 3483441 effective words/s
[2023-02-07 13:16:54,767][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 78.35% examples, 3450990 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:55,022][gensim.models.word2vec][INFO] - EPOCH 22: training on 4367244 raw words (4368669 effective words) took 1.3s, 3478050 effective words/s
[2023-02-07 13:16:56,026][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 79.71% examples, 3513261 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:56,272][gensim.models.word2vec][INFO] - EPOCH 23: training on 4367244 raw words (4368669 effective words) took 1.2s, 3498254 effective words/s
[2023-02-07 13:16:57,275][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 76.24% examples, 3369893 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:57,562][gensim.models.word2vec][INFO] - EPOCH 24: training on 4367244 raw words (4368669 effective words) took 1.3s, 3388377 effective words/s
[2023-02-07 13:16:58,564][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 79.15% examples, 3495846 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:16:58,808][gensim.models.word2vec][INFO] - EPOCH 25: training on 4367244 raw words (4368669 effective words) took 1.2s, 3511661 effective words/s
[2023-02-07 13:16:59,810][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 78.94% examples, 3486783 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:00,059][gensim.models.word2vec][INFO] - EPOCH 26: training on 4367244 raw words (4368669 effective words) took 1.3s, 3494179 effective words/s
[2023-02-07 13:17:01,063][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 80.87% examples, 3552430 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:01,291][gensim.models.word2vec][INFO] - EPOCH 27: training on 4367244 raw words (4368669 effective words) took 1.2s, 3549586 effective words/s
[2023-02-07 13:17:02,292][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 78.14% examples, 3446261 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:02,562][gensim.models.word2vec][INFO] - EPOCH 28: training on 4367244 raw words (4368669 effective words) took 1.3s, 3440428 effective words/s
[2023-02-07 13:17:03,563][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 80.04% examples, 3533215 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:03,795][gensim.models.word2vec][INFO] - EPOCH 29: training on 4367244 raw words (4368669 effective words) took 1.2s, 3547476 effective words/s
[2023-02-07 13:17:04,800][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 81.61% examples, 3586689 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:05,015][gensim.models.word2vec][INFO] - EPOCH 30: training on 4367244 raw words (4368669 effective words) took 1.2s, 3586525 effective words/s
[2023-02-07 13:17:06,018][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 81.24% examples, 3573405 words/s, in_qsize 6, out_qsize 1
[2023-02-07 13:17:06,243][gensim.models.word2vec][INFO] - EPOCH 31: training on 4367244 raw words (4368669 effective words) took 1.2s, 3560055 effective words/s
[2023-02-07 13:17:07,245][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 81.24% examples, 3575677 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:07,465][gensim.models.word2vec][INFO] - EPOCH 32: training on 4367244 raw words (4368669 effective words) took 1.2s, 3577503 effective words/s
[2023-02-07 13:17:08,472][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 83.51% examples, 3673854 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:08,643][gensim.models.word2vec][INFO] - EPOCH 33: training on 4367244 raw words (4368669 effective words) took 1.2s, 3717962 effective words/s
[2023-02-07 13:17:09,646][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 88.89% examples, 3900928 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:09,761][gensim.models.word2vec][INFO] - EPOCH 34: training on 4367244 raw words (4368669 effective words) took 1.1s, 3910544 effective words/s
[2023-02-07 13:17:10,765][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 88.89% examples, 3899521 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:10,886][gensim.models.word2vec][INFO] - EPOCH 35: training on 4367244 raw words (4368669 effective words) took 1.1s, 3887399 effective words/s
[2023-02-07 13:17:11,888][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 87.38% examples, 3849386 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:12,022][gensim.models.word2vec][INFO] - EPOCH 36: training on 4367244 raw words (4368669 effective words) took 1.1s, 3850356 effective words/s
[2023-02-07 13:17:13,024][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 86.77% examples, 3820265 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:13,162][gensim.models.word2vec][INFO] - EPOCH 37: training on 4367244 raw words (4368669 effective words) took 1.1s, 3836489 effective words/s
[2023-02-07 13:17:14,165][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 89.93% examples, 3948001 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:14,268][gensim.models.word2vec][INFO] - EPOCH 38: training on 4367244 raw words (4368669 effective words) took 1.1s, 3954550 effective words/s
[2023-02-07 13:17:15,270][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 88.30% examples, 3876579 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:15,400][gensim.models.word2vec][INFO] - EPOCH 39: training on 4367244 raw words (4368669 effective words) took 1.1s, 3863510 effective words/s
[2023-02-07 13:17:16,403][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 84.74% examples, 3729995 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:16,591][gensim.models.word2vec][INFO] - EPOCH 40: training on 4367244 raw words (4368669 effective words) took 1.2s, 3674461 effective words/s
[2023-02-07 13:17:17,593][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 76.51% examples, 3385730 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:17,870][gensim.models.word2vec][INFO] - EPOCH 41: training on 4367244 raw words (4368669 effective words) took 1.3s, 3419431 effective words/s
[2023-02-07 13:17:18,873][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 78.26% examples, 3449583 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:19,130][gensim.models.word2vec][INFO] - EPOCH 42: training on 4367244 raw words (4368669 effective words) took 1.3s, 3469764 effective words/s
[2023-02-07 13:17:20,133][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 78.14% examples, 3443697 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:20,403][gensim.models.word2vec][INFO] - EPOCH 43: training on 4367244 raw words (4368669 effective words) took 1.3s, 3435357 effective words/s
[2023-02-07 13:17:21,406][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 77.43% examples, 3412867 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:21,679][gensim.models.word2vec][INFO] - EPOCH 44: training on 4367244 raw words (4368669 effective words) took 1.3s, 3428867 effective words/s
[2023-02-07 13:17:22,682][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 77.31% examples, 3409902 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:22,957][gensim.models.word2vec][INFO] - EPOCH 45: training on 4367244 raw words (4368669 effective words) took 1.3s, 3423105 effective words/s
[2023-02-07 13:17:23,962][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 85.23% examples, 3748059 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:24,116][gensim.models.word2vec][INFO] - EPOCH 46: training on 4367244 raw words (4368669 effective words) took 1.2s, 3775596 effective words/s
[2023-02-07 13:17:25,118][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 87.69% examples, 3858543 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:25,251][gensim.models.word2vec][INFO] - EPOCH 47: training on 4367244 raw words (4368669 effective words) took 1.1s, 3853538 effective words/s
[2023-02-07 13:17:26,255][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 87.69% examples, 3853937 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:26,385][gensim.models.word2vec][INFO] - EPOCH 48: training on 4367244 raw words (4368669 effective words) took 1.1s, 3856728 effective words/s
[2023-02-07 13:17:27,387][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 87.84% examples, 3864904 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:27,512][gensim.models.word2vec][INFO] - EPOCH 49: training on 4367244 raw words (4368669 effective words) took 1.1s, 3880435 effective words/s
[2023-02-07 13:17:28,517][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 89.53% examples, 3926091 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:28,626][gensim.models.word2vec][INFO] - EPOCH 50: training on 4367244 raw words (4368669 effective words) took 1.1s, 3930623 effective words/s
[2023-02-07 13:17:29,629][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 89.07% examples, 3914240 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:29,745][gensim.models.word2vec][INFO] - EPOCH 51: training on 4367244 raw words (4368669 effective words) took 1.1s, 3909705 effective words/s
[2023-02-07 13:17:30,746][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 86.52% examples, 3813291 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:30,890][gensim.models.word2vec][INFO] - EPOCH 52: training on 4367244 raw words (4368669 effective words) took 1.1s, 3818170 effective words/s
[2023-02-07 13:17:31,893][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 87.69% examples, 3856708 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:32,022][gensim.models.word2vec][INFO] - EPOCH 53: training on 4367244 raw words (4368669 effective words) took 1.1s, 3864785 effective words/s
[2023-02-07 13:17:33,024][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 89.07% examples, 3913865 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:17:33,138][gensim.models.word2vec][INFO] - EPOCH 54: training on 4367244 raw words (4368669 effective words) took 1.1s, 3916816 effective words/s
[2023-02-07 13:17:33,139][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 240198420 raw words (240276795 effective words) took 68.1s, 3529209 effective words/s', 'datetime': '2023-02-07T13:17:33.139346', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:17:33.139 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:17:37,884][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131559-5du27hxh/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:17:37.884004', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:17:37,885][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:17:37,936][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131559-5du27hxh/files/../tmp/embedding_model.pt
2023-02-07 13:17:37.936 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:17:39.013 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:17:39.414 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:17:40.114 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.952888842346535, 'test_mae': 1.047099042555283, 'test_r2': 0.07616681455689123}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.96
wandb: percentage 0.0
wandb:   test_mae 1.0471
wandb:   test_mse 1.95289
wandb:    test_r2 0.07617
wandb: 
wandb: üöÄ View run vivid-sweep-36 at: https://wandb.ai/xiaoqiz/mof2vec/runs/5du27hxh
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_131559-5du27hxh/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: yuwe38iz with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 167
wandb: 	model.gensim.alpha: 0.0021430623338039006
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 58
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.4790200425900086
wandb: 	model.gensim.vector_size: 16
wandb: 	model.gensim.window: 14
wandb: 	model.sklearn.learning_rate: 0.044378024015402695
wandb: 	model.sklearn.max_depth: 62
wandb: 	model.sklearn.min_child_weight: 0.009709081744192397
wandb: 	model.sklearn.n_estimators: 4833
wandb: 	model.sklearn.num_leaves: 497
wandb: 	model.sklearn.reg_alpha: 0.5130372923408316
wandb: 	model.sklearn.reg_lambda: 0.1134396071923335
wandb: 	model.sklearn.subsample: 0.3586285115176622
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131801-yuwe38iz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-37
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/yuwe38iz
2023-02-07 13:18:09.350 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 13:18:09.350 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 167 for sweep.
2023-02-07 13:18:09.351 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0021430623338039006 for sweep.
2023-02-07 13:18:09.351 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:18:09.351 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 58 for sweep.
2023-02-07 13:18:09.351 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 13:18:09.352 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4790200425900086 for sweep.
2023-02-07 13:18:09.352 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 16 for sweep.
2023-02-07 13:18:09.352 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 14 for sweep.
2023-02-07 13:18:09.353 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.044378024015402695 for sweep.
2023-02-07 13:18:09.353 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 62 for sweep.
2023-02-07 13:18:09.353 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.009709081744192397 for sweep.
2023-02-07 13:18:09.353 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4833 for sweep.
2023-02-07 13:18:09.354 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 497 for sweep.
2023-02-07 13:18:09.354 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.5130372923408316 for sweep.
2023-02-07 13:18:09.354 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.1134396071923335 for sweep.
2023-02-07 13:18:09.354 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3586285115176622 for sweep.
2023-02-07 13:18:09.354 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:18:09.361 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131801-yuwe38iz/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 167, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 16, 'window': 14, 'min_count': 6, 'dm': 0, 'sample': 0.4790200425900086, 'workers': 4, 'alpha': 0.0021430623338039006, 'epochs': 58}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4833, 'max_depth': 62, 'num_leaves': 497, 'reg_alpha': 0.5130372923408316, 'reg_lambda': 0.1134396071923335, 'subsample': 0.3586285115176622, 'min_child_weight': 0.009709081744192397, 'n_jobs': 4, 'learning_rate': 0.044378024015402695}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 29/3257 [00:00<00:11, 286.69it/s]  2%|‚ñè         | 58/3257 [00:00<00:11, 275.32it/s]  3%|‚ñé         | 90/3257 [00:00<00:11, 287.29it/s]  4%|‚ñé         | 119/3257 [00:00<00:11, 277.62it/s]  5%|‚ñç         | 151/3257 [00:00<00:10, 287.76it/s]  6%|‚ñå         | 180/3257 [00:00<00:11, 273.19it/s]  6%|‚ñã         | 211/3257 [00:00<00:10, 283.30it/s]  7%|‚ñã         | 244/3257 [00:00<00:10, 295.19it/s]  8%|‚ñä         | 274/3257 [00:00<00:10, 288.26it/s]  9%|‚ñâ         | 308/3257 [00:01<00:09, 301.32it/s] 10%|‚ñà         | 339/3257 [00:01<00:09, 301.57it/s] 11%|‚ñà‚ñè        | 370/3257 [00:01<00:09, 302.09it/s] 12%|‚ñà‚ñè        | 401/3257 [00:01<00:09, 291.47it/s] 13%|‚ñà‚ñé        | 431/3257 [00:01<00:10, 273.18it/s] 14%|‚ñà‚ñç        | 459/3257 [00:01<00:10, 272.51it/s] 15%|‚ñà‚ñå        | 489/3257 [00:01<00:09, 279.55it/s] 16%|‚ñà‚ñå        | 522/3257 [00:01<00:09, 292.50it/s] 17%|‚ñà‚ñã        | 552/3257 [00:01<00:09, 293.01it/s] 18%|‚ñà‚ñä        | 582/3257 [00:02<00:10, 261.25it/s] 19%|‚ñà‚ñä        | 610/3257 [00:02<00:09, 265.32it/s] 20%|‚ñà‚ñâ        | 639/3257 [00:02<00:09, 270.66it/s] 20%|‚ñà‚ñà        | 667/3257 [00:02<00:09, 262.93it/s] 21%|‚ñà‚ñà‚ñè       | 694/3257 [00:02<00:13, 190.05it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:02<00:12, 206.95it/s] 23%|‚ñà‚ñà‚ñé       | 750/3257 [00:02<00:11, 223.77it/s] 24%|‚ñà‚ñà‚ñç       | 779/3257 [00:02<00:10, 239.37it/s] 25%|‚ñà‚ñà‚ñç       | 808/3257 [00:03<00:09, 252.64it/s] 26%|‚ñà‚ñà‚ñå       | 836/3257 [00:03<00:09, 258.03it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:03<00:09, 258.78it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:03<00:09, 260.84it/s] 28%|‚ñà‚ñà‚ñä       | 923/3257 [00:03<00:08, 277.09it/s] 29%|‚ñà‚ñà‚ñâ       | 952/3257 [00:03<00:08, 278.29it/s] 30%|‚ñà‚ñà‚ñà       | 981/3257 [00:03<00:08, 274.61it/s] 31%|‚ñà‚ñà‚ñà       | 1009/3257 [00:03<00:08, 271.95it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1038/3257 [00:03<00:08, 272.96it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:03<00:07, 275.89it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1095/3257 [00:04<00:07, 275.33it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1124/3257 [00:04<00:07, 277.71it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1152/3257 [00:04<00:07, 271.66it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1180/3257 [00:04<00:07, 269.74it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1208/3257 [00:04<00:07, 258.58it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:04<00:07, 276.10it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:04<00:07, 276.75it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1299/3257 [00:04<00:07, 267.69it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:04<00:07, 273.41it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1356/3257 [00:05<00:06, 275.11it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:05<00:06, 271.35it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1416/3257 [00:05<00:06, 285.02it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1446/3257 [00:05<00:06, 287.02it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1477/3257 [00:05<00:06, 292.15it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:05<00:05, 304.55it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1542/3257 [00:05<00:06, 282.75it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1571/3257 [00:05<00:05, 281.90it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1600/3257 [00:05<00:05, 277.09it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1631/3257 [00:05<00:05, 285.09it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1660/3257 [00:06<00:05, 273.23it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1688/3257 [00:06<00:05, 267.78it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1716/3257 [00:06<00:05, 269.41it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1744/3257 [00:06<00:05, 264.07it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1776/3257 [00:06<00:05, 278.46it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:06<00:05, 276.96it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1833/3257 [00:06<00:05, 268.87it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1864/3257 [00:06<00:04, 279.23it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1893/3257 [00:06<00:04, 279.05it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1922/3257 [00:07<00:04, 282.03it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1956/3257 [00:07<00:04, 298.59it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1986/3257 [00:07<00:06, 195.77it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2018/3257 [00:07<00:05, 221.74it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2046/3257 [00:07<00:05, 235.42it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2073/3257 [00:07<00:04, 237.48it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2100/3257 [00:07<00:04, 245.83it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2129/3257 [00:07<00:04, 252.37it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2159/3257 [00:08<00:04, 263.56it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2188/3257 [00:08<00:03, 270.12it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2218/3257 [00:08<00:03, 276.61it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2247/3257 [00:08<00:03, 275.77it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2275/3257 [00:08<00:03, 267.10it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2306/3257 [00:08<00:03, 277.33it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2341/3257 [00:08<00:03, 297.69it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2373/3257 [00:08<00:02, 302.65it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:08<00:02, 304.93it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2436/3257 [00:09<00:02, 290.83it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2467/3257 [00:09<00:02, 293.53it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2500/3257 [00:09<00:02, 303.21it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2534/3257 [00:09<00:02, 313.66it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2566/3257 [00:09<00:02, 288.62it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2596/3257 [00:09<00:02, 280.32it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2633/3257 [00:09<00:02, 304.55it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2664/3257 [00:09<00:02, 293.90it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2694/3257 [00:09<00:01, 293.48it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2724/3257 [00:09<00:01, 277.22it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2758/3257 [00:10<00:01, 292.43it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2790/3257 [00:10<00:01, 299.98it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2821/3257 [00:10<00:01, 295.84it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2851/3257 [00:10<00:01, 295.09it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2888/3257 [00:10<00:01, 314.50it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2920/3257 [00:10<00:01, 303.39it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2951/3257 [00:10<00:01, 283.60it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2980/3257 [00:10<00:00, 284.72it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:10<00:00, 294.55it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3047/3257 [00:11<00:00, 308.90it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3082/3257 [00:11<00:00, 319.76it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3118/3257 [00:11<00:00, 330.77it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3152/3257 [00:11<00:00, 308.36it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3184/3257 [00:11<00:00, 299.05it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3215/3257 [00:11<00:00, 299.70it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:11<00:00, 317.93it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 277.92it/s]
2023-02-07 13:18:21.423 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:18:21,425][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d16,n5,mc6,s0.47902,t4>', 'datetime': '2023-02-07T13:18:21.425009', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:18:21,426][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:18:21,426][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:18:21,675][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 13:18:21,675][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:18:21,684][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 3569 unique words (53.57% of original 6662, drops 3093)', 'datetime': '2023-02-07T13:18:21.684793', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:18:21,684][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 2903654 word corpus (99.73% of original 2911496, drops 7842)', 'datetime': '2023-02-07T13:18:21.684968', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:18:21,696][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 13:18:21,696][gensim.models.word2vec][INFO] - sample=0.47902 downsamples 0 most-common words
[2023-02-07 13:18:21,696][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2903654 word corpus (100.0%% of prior 2903654)', 'datetime': '2023-02-07T13:18:21.696707', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:18:21,716][gensim.models.word2vec][INFO] - estimated required memory for 3569 words and 16 dimensions: 3101180 bytes
[2023-02-07 13:18:21,716][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:18:21,717][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 3569 vocabulary and 16 features, using sg=1 hs=0 sample=0.4790200425900086 negative=5 window=14 shrink_windows=True', 'datetime': '2023-02-07T13:18:21.717390', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:18:22,364][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2906911 effective words) took 0.6s, 4504002 effective words/s
[2023-02-07 13:18:22,978][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2906911 effective words) took 0.6s, 4744169 effective words/s
[2023-02-07 13:18:23,606][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2906911 effective words) took 0.6s, 4634219 effective words/s
[2023-02-07 13:18:24,238][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2906911 effective words) took 0.6s, 4607070 effective words/s
[2023-02-07 13:18:24,870][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2906911 effective words) took 0.6s, 4608406 effective words/s
[2023-02-07 13:18:25,503][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2906911 effective words) took 0.6s, 4599271 effective words/s
[2023-02-07 13:18:26,136][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2906911 effective words) took 0.6s, 4599036 effective words/s
[2023-02-07 13:18:26,777][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2906911 effective words) took 0.6s, 4540511 effective words/s
[2023-02-07 13:18:27,422][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2906911 effective words) took 0.6s, 4514193 effective words/s
[2023-02-07 13:18:28,062][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2906911 effective words) took 0.6s, 4552508 effective words/s
[2023-02-07 13:18:28,694][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2906911 effective words) took 0.6s, 4605327 effective words/s
[2023-02-07 13:18:29,327][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2906911 effective words) took 0.6s, 4603191 effective words/s
[2023-02-07 13:18:29,960][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2906911 effective words) took 0.6s, 4597610 effective words/s
[2023-02-07 13:18:30,598][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2906911 effective words) took 0.6s, 4567551 effective words/s
[2023-02-07 13:18:31,237][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2906911 effective words) took 0.6s, 4554274 effective words/s
[2023-02-07 13:18:31,866][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2906911 effective words) took 0.6s, 4635456 effective words/s
[2023-02-07 13:18:32,495][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2906911 effective words) took 0.6s, 4621960 effective words/s
[2023-02-07 13:18:33,123][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2906911 effective words) took 0.6s, 4641595 effective words/s
[2023-02-07 13:18:33,751][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2906911 effective words) took 0.6s, 4633686 effective words/s
[2023-02-07 13:18:34,379][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2906911 effective words) took 0.6s, 4637051 effective words/s
[2023-02-07 13:18:35,002][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2906911 effective words) took 0.6s, 4673830 effective words/s
[2023-02-07 13:18:35,648][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2906911 effective words) took 0.6s, 4506598 effective words/s
[2023-02-07 13:18:36,279][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2906911 effective words) took 0.6s, 4619679 effective words/s
[2023-02-07 13:18:36,919][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2906911 effective words) took 0.6s, 4549849 effective words/s
[2023-02-07 13:18:37,544][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2906911 effective words) took 0.6s, 4656448 effective words/s
[2023-02-07 13:18:38,169][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2906911 effective words) took 0.6s, 4661909 effective words/s
[2023-02-07 13:18:38,791][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2906911 effective words) took 0.6s, 4675460 effective words/s
[2023-02-07 13:18:39,420][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2906911 effective words) took 0.6s, 4633884 effective words/s
[2023-02-07 13:18:40,046][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2906911 effective words) took 0.6s, 4646307 effective words/s
[2023-02-07 13:18:40,672][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2906911 effective words) took 0.6s, 4654806 effective words/s
[2023-02-07 13:18:41,296][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2906911 effective words) took 0.6s, 4666264 effective words/s
[2023-02-07 13:18:41,922][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2906911 effective words) took 0.6s, 4650323 effective words/s
[2023-02-07 13:18:42,544][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2906911 effective words) took 0.6s, 4678540 effective words/s
[2023-02-07 13:18:43,167][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2906911 effective words) took 0.6s, 4675049 effective words/s
[2023-02-07 13:18:43,789][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2906911 effective words) took 0.6s, 4681537 effective words/s
[2023-02-07 13:18:44,411][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2906911 effective words) took 0.6s, 4684758 effective words/s
[2023-02-07 13:18:45,039][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2906911 effective words) took 0.6s, 4639092 effective words/s
[2023-02-07 13:18:45,659][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2906911 effective words) took 0.6s, 4692962 effective words/s
[2023-02-07 13:18:46,277][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2906911 effective words) took 0.6s, 4712906 effective words/s
[2023-02-07 13:18:46,896][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2906911 effective words) took 0.6s, 4705616 effective words/s
[2023-02-07 13:18:47,511][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2906911 effective words) took 0.6s, 4738727 effective words/s
[2023-02-07 13:18:48,144][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2906911 effective words) took 0.6s, 4595340 effective words/s
[2023-02-07 13:18:48,775][gensim.models.word2vec][INFO] - EPOCH 42: training on 2911496 raw words (2906911 effective words) took 0.6s, 4617082 effective words/s
[2023-02-07 13:18:49,413][gensim.models.word2vec][INFO] - EPOCH 43: training on 2911496 raw words (2906911 effective words) took 0.6s, 4560959 effective words/s
[2023-02-07 13:18:50,051][gensim.models.word2vec][INFO] - EPOCH 44: training on 2911496 raw words (2906911 effective words) took 0.6s, 4570343 effective words/s
[2023-02-07 13:18:50,686][gensim.models.word2vec][INFO] - EPOCH 45: training on 2911496 raw words (2906911 effective words) took 0.6s, 4578717 effective words/s
[2023-02-07 13:18:51,320][gensim.models.word2vec][INFO] - EPOCH 46: training on 2911496 raw words (2906911 effective words) took 0.6s, 4597937 effective words/s
[2023-02-07 13:18:51,953][gensim.models.word2vec][INFO] - EPOCH 47: training on 2911496 raw words (2906911 effective words) took 0.6s, 4598775 effective words/s
[2023-02-07 13:18:52,583][gensim.models.word2vec][INFO] - EPOCH 48: training on 2911496 raw words (2906911 effective words) took 0.6s, 4624465 effective words/s
[2023-02-07 13:18:53,216][gensim.models.word2vec][INFO] - EPOCH 49: training on 2911496 raw words (2906911 effective words) took 0.6s, 4599130 effective words/s
[2023-02-07 13:18:53,850][gensim.models.word2vec][INFO] - EPOCH 50: training on 2911496 raw words (2906911 effective words) took 0.6s, 4595260 effective words/s
[2023-02-07 13:18:54,482][gensim.models.word2vec][INFO] - EPOCH 51: training on 2911496 raw words (2906911 effective words) took 0.6s, 4614305 effective words/s
[2023-02-07 13:18:55,112][gensim.models.word2vec][INFO] - EPOCH 52: training on 2911496 raw words (2906911 effective words) took 0.6s, 4619199 effective words/s
[2023-02-07 13:18:55,742][gensim.models.word2vec][INFO] - EPOCH 53: training on 2911496 raw words (2906911 effective words) took 0.6s, 4623736 effective words/s
[2023-02-07 13:18:56,379][gensim.models.word2vec][INFO] - EPOCH 54: training on 2911496 raw words (2906911 effective words) took 0.6s, 4568967 effective words/s
[2023-02-07 13:18:57,042][gensim.models.word2vec][INFO] - EPOCH 55: training on 2911496 raw words (2906911 effective words) took 0.7s, 4399161 effective words/s
[2023-02-07 13:18:57,702][gensim.models.word2vec][INFO] - EPOCH 56: training on 2911496 raw words (2906911 effective words) took 0.7s, 4405641 effective words/s
[2023-02-07 13:18:58,367][gensim.models.word2vec][INFO] - EPOCH 57: training on 2911496 raw words (2906911 effective words) took 0.7s, 4382873 effective words/s
[2023-02-07 13:18:58,368][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 168866768 raw words (168600838 effective words) took 36.7s, 4600232 effective words/s', 'datetime': '2023-02-07T13:18:58.368084', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:18:58.368 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:19:01,578][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131801-yuwe38iz/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:19:01.577994', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:19:01,580][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:19:01,587][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131801-yuwe38iz/files/../tmp/embedding_model.pt
2023-02-07 13:19:01.588 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:19:02.470 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:19:02.821 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:19:02.992 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1041998690815995, 'test_mae': 1.1125125736398107, 'test_r2': 0.004587652041241341}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.65
wandb: percentage 0.46427
wandb:   test_mae 1.11251
wandb:   test_mse 2.1042
wandb:    test_r2 0.00459
wandb: 
wandb: üöÄ View run silvery-sweep-37 at: https://wandb.ai/xiaoqiz/mof2vec/runs/yuwe38iz
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_131801-yuwe38iz/logs
wandb: Agent Starting Run: vgcjs6et with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 626
wandb: 	model.gensim.alpha: 0.0009500369165265772
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 45
wandb: 	model.gensim.min_count: 8
wandb: 	model.gensim.sample: 0.2943154046909183
wandb: 	model.gensim.vector_size: 76
wandb: 	model.gensim.window: 1
wandb: 	model.sklearn.learning_rate: 0.007874779223797385
wandb: 	model.sklearn.max_depth: 38
wandb: 	model.sklearn.min_child_weight: 0.06858421304692043
wandb: 	model.sklearn.n_estimators: 1727
wandb: 	model.sklearn.num_leaves: 498
wandb: 	model.sklearn.reg_alpha: 0.009532620149992592
wandb: 	model.sklearn.reg_lambda: 0.06465155123856708
wandb: 	model.sklearn.subsample: 0.5783567778978035
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131914-vgcjs6et
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-38
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/vgcjs6et
2023-02-07 13:19:23.368 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 13:19:23.368 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 626 for sweep.
2023-02-07 13:19:23.368 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0009500369165265772 for sweep.
2023-02-07 13:19:23.369 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:19:23.369 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 45 for sweep.
2023-02-07 13:19:23.369 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 8 for sweep.
2023-02-07 13:19:23.369 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2943154046909183 for sweep.
2023-02-07 13:19:23.369 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 76 for sweep.
2023-02-07 13:19:23.370 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 1 for sweep.
2023-02-07 13:19:23.370 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.007874779223797385 for sweep.
2023-02-07 13:19:23.370 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 38 for sweep.
2023-02-07 13:19:23.371 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.06858421304692043 for sweep.
2023-02-07 13:19:23.371 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1727 for sweep.
2023-02-07 13:19:23.371 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 498 for sweep.
2023-02-07 13:19:23.371 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.009532620149992592 for sweep.
2023-02-07 13:19:23.371 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.06465155123856708 for sweep.
2023-02-07 13:19:23.372 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5783567778978035 for sweep.
2023-02-07 13:19:23.372 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:19:23.380 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131914-vgcjs6et/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 626, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 76, 'window': 1, 'min_count': 8, 'dm': 0, 'sample': 0.2943154046909183, 'workers': 4, 'alpha': 0.0009500369165265772, 'epochs': 45}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1727, 'max_depth': 38, 'num_leaves': 498, 'reg_alpha': 0.009532620149992592, 'reg_lambda': 0.06465155123856708, 'subsample': 0.5783567778978035, 'min_child_weight': 0.06858421304692043, 'n_jobs': 4, 'learning_rate': 0.007874779223797385}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 28/3257 [00:00<00:11, 275.90it/s]  2%|‚ñè         | 56/3257 [00:00<00:11, 273.81it/s]  3%|‚ñé         | 90/3257 [00:00<00:10, 293.69it/s]  4%|‚ñé         | 120/3257 [00:00<00:10, 285.64it/s]  5%|‚ñç         | 151/3257 [00:00<00:10, 290.78it/s]  6%|‚ñå         | 181/3257 [00:00<00:15, 201.75it/s]  6%|‚ñã         | 210/3257 [00:00<00:13, 222.77it/s]  7%|‚ñã         | 244/3257 [00:00<00:11, 252.10it/s]  8%|‚ñä         | 275/3257 [00:01<00:11, 265.77it/s]  9%|‚ñâ         | 305/3257 [00:01<00:10, 273.78it/s] 10%|‚ñà         | 336/3257 [00:01<00:10, 280.91it/s] 11%|‚ñà         | 366/3257 [00:01<00:10, 283.52it/s] 12%|‚ñà‚ñè        | 396/3257 [00:01<00:10, 277.94it/s] 13%|‚ñà‚ñé        | 425/3257 [00:01<00:10, 274.58it/s] 14%|‚ñà‚ñç        | 453/3257 [00:01<00:10, 270.56it/s] 15%|‚ñà‚ñç        | 481/3257 [00:01<00:10, 272.22it/s] 16%|‚ñà‚ñå        | 514/3257 [00:01<00:09, 287.38it/s] 17%|‚ñà‚ñã        | 544/3257 [00:02<00:09, 288.93it/s] 18%|‚ñà‚ñä        | 574/3257 [00:02<00:10, 266.78it/s] 19%|‚ñà‚ñä        | 606/3257 [00:02<00:09, 279.20it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:02<00:09, 282.09it/s] 20%|‚ñà‚ñà        | 666/3257 [00:02<00:09, 266.55it/s] 21%|‚ñà‚ñà‚ñè       | 695/3257 [00:02<00:09, 272.78it/s] 22%|‚ñà‚ñà‚ñè       | 723/3257 [00:02<00:09, 274.37it/s] 23%|‚ñà‚ñà‚ñé       | 751/3257 [00:02<00:09, 274.43it/s] 24%|‚ñà‚ñà‚ñç       | 780/3257 [00:02<00:08, 275.43it/s] 25%|‚ñà‚ñà‚ñç       | 809/3257 [00:02<00:08, 278.51it/s] 26%|‚ñà‚ñà‚ñå       | 837/3257 [00:03<00:08, 272.59it/s] 27%|‚ñà‚ñà‚ñã       | 865/3257 [00:03<00:08, 268.01it/s] 27%|‚ñà‚ñà‚ñã       | 894/3257 [00:03<00:08, 270.94it/s] 28%|‚ñà‚ñà‚ñä       | 927/3257 [00:03<00:08, 286.44it/s] 29%|‚ñà‚ñà‚ñâ       | 956/3257 [00:03<00:08, 283.97it/s] 30%|‚ñà‚ñà‚ñà       | 985/3257 [00:03<00:08, 283.46it/s] 31%|‚ñà‚ñà‚ñà       | 1014/3257 [00:03<00:08, 273.00it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1042/3257 [00:03<00:08, 267.33it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1073/3257 [00:03<00:07, 278.71it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1101/3257 [00:04<00:07, 277.38it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1129/3257 [00:04<00:07, 272.96it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1158/3257 [00:04<00:07, 276.06it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1186/3257 [00:04<00:07, 268.78it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1213/3257 [00:04<00:08, 248.86it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1246/3257 [00:04<00:07, 270.79it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1274/3257 [00:04<00:07, 271.73it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1302/3257 [00:04<00:07, 259.73it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1329/3257 [00:04<00:07, 260.62it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1357/3257 [00:05<00:07, 265.40it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:05<00:07, 266.27it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1417/3257 [00:05<00:06, 283.89it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1446/3257 [00:05<00:06, 284.05it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1476/3257 [00:05<00:06, 287.25it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1505/3257 [00:05<00:06, 287.78it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1534/3257 [00:05<00:06, 271.59it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1562/3257 [00:05<00:06, 268.35it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1591/3257 [00:05<00:06, 273.52it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:06<00:08, 193.50it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1646/3257 [00:06<00:07, 209.98it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1671/3257 [00:06<00:07, 218.73it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1702/3257 [00:06<00:06, 241.17it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:06<00:06, 242.99it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1755/3257 [00:06<00:06, 249.84it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1786/3257 [00:06<00:05, 264.24it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1814/3257 [00:06<00:05, 263.63it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1846/3257 [00:06<00:05, 275.04it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1879/3257 [00:07<00:04, 288.20it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1910/3257 [00:07<00:04, 292.57it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1940/3257 [00:07<00:04, 293.99it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1973/3257 [00:07<00:04, 304.35it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:07<00:04, 285.60it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2033/3257 [00:07<00:04, 280.84it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2062/3257 [00:07<00:04, 252.16it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2088/3257 [00:07<00:04, 251.64it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2114/3257 [00:07<00:04, 250.08it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2140/3257 [00:08<00:04, 238.48it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2166/3257 [00:08<00:04, 243.79it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2193/3257 [00:08<00:04, 249.11it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2219/3257 [00:08<00:04, 250.33it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2245/3257 [00:08<00:04, 245.39it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2270/3257 [00:08<00:04, 242.08it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2297/3257 [00:08<00:03, 248.69it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2326/3257 [00:08<00:03, 258.95it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:08<00:03, 275.47it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2389/3257 [00:08<00:03, 278.03it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2417/3257 [00:09<00:03, 265.97it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2444/3257 [00:09<00:03, 258.04it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2473/3257 [00:09<00:02, 264.21it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2502/3257 [00:09<00:02, 269.74it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2531/3257 [00:09<00:02, 273.89it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2559/3257 [00:09<00:02, 257.52it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2585/3257 [00:09<00:02, 249.61it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2616/3257 [00:09<00:02, 264.92it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2643/3257 [00:09<00:02, 266.34it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:10<00:02, 264.80it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2697/3257 [00:10<00:02, 254.13it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2723/3257 [00:10<00:02, 239.56it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2751/3257 [00:10<00:02, 248.42it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2777/3257 [00:10<00:01, 247.92it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2807/3257 [00:10<00:01, 261.08it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2834/3257 [00:10<00:01, 250.79it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2862/3257 [00:10<00:01, 257.16it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2891/3257 [00:10<00:01, 263.89it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2918/3257 [00:11<00:01, 257.14it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2944/3257 [00:11<00:01, 245.97it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2969/3257 [00:11<00:01, 246.96it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2994/3257 [00:11<00:01, 234.69it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3018/3257 [00:11<00:01, 227.90it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3041/3257 [00:11<00:01, 145.32it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3070/3257 [00:11<00:01, 173.51it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3096/3257 [00:12<00:00, 191.24it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3127/3257 [00:12<00:00, 219.11it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3152/3257 [00:12<00:00, 222.85it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3178/3257 [00:12<00:00, 228.51it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:12<00:00, 241.01it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3237/3257 [00:12<00:00, 254.33it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:12<00:00, 258.40it/s]
2023-02-07 13:19:36.346 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:19:36,348][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d76,n5,mc8,s0.294315,t4>', 'datetime': '2023-02-07T13:19:36.347992', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:19:36,349][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:19:36,349][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:19:36,609][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 13:19:36,610][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:19:36,619][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 retains 3146 unique words (47.22% of original 6662, drops 3516)', 'datetime': '2023-02-07T13:19:36.619140', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:19:36,619][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=8 leaves 2901034 word corpus (99.64% of original 2911496, drops 10462)', 'datetime': '2023-02-07T13:19:36.619495', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:19:36,630][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 13:19:36,630][gensim.models.word2vec][INFO] - sample=0.294315 downsamples 0 most-common words
[2023-02-07 13:19:36,630][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2901034 word corpus (100.0%% of prior 2901034)', 'datetime': '2023-02-07T13:19:36.630664', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:19:36,649][gensim.models.word2vec][INFO] - estimated required memory for 3146 words and 76 dimensions: 5127296 bytes
[2023-02-07 13:19:36,649][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:19:36,651][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 3146 vocabulary and 76 features, using sg=1 hs=0 sample=0.2943154046909183 negative=5 window=1 shrink_windows=True', 'datetime': '2023-02-07T13:19:36.651750', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:19:37,545][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2904291 effective words) took 0.9s, 3257279 effective words/s
[2023-02-07 13:19:38,432][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2904291 effective words) took 0.9s, 3278667 effective words/s
[2023-02-07 13:19:39,335][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2904291 effective words) took 0.9s, 3222341 effective words/s
[2023-02-07 13:19:40,224][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2904291 effective words) took 0.9s, 3272948 effective words/s
[2023-02-07 13:19:41,114][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2904291 effective words) took 0.9s, 3269710 effective words/s
[2023-02-07 13:19:42,006][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2904291 effective words) took 0.9s, 3258804 effective words/s
[2023-02-07 13:19:42,896][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2904291 effective words) took 0.9s, 3270345 effective words/s
[2023-02-07 13:19:43,784][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2904291 effective words) took 0.9s, 3273795 effective words/s
[2023-02-07 13:19:44,665][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2904291 effective words) took 0.9s, 3300829 effective words/s
[2023-02-07 13:19:45,556][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2904291 effective words) took 0.9s, 3266593 effective words/s
[2023-02-07 13:19:46,446][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2904291 effective words) took 0.9s, 3268327 effective words/s
[2023-02-07 13:19:47,339][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2904291 effective words) took 0.9s, 3255161 effective words/s
[2023-02-07 13:19:48,224][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2904291 effective words) took 0.9s, 3291030 effective words/s
[2023-02-07 13:19:49,070][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2904291 effective words) took 0.8s, 3437203 effective words/s
[2023-02-07 13:19:49,872][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2904291 effective words) took 0.8s, 3625319 effective words/s
[2023-02-07 13:19:50,686][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2904291 effective words) took 0.8s, 3576125 effective words/s
[2023-02-07 13:19:51,516][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2904291 effective words) took 0.8s, 3502889 effective words/s
[2023-02-07 13:19:52,397][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2904291 effective words) took 0.9s, 3300684 effective words/s
[2023-02-07 13:19:53,287][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2904291 effective words) took 0.9s, 3267982 effective words/s
[2023-02-07 13:19:54,153][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2904291 effective words) took 0.9s, 3359630 effective words/s
[2023-02-07 13:19:55,032][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2904291 effective words) took 0.9s, 3308341 effective words/s
[2023-02-07 13:19:55,901][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2904291 effective words) took 0.9s, 3345217 effective words/s
[2023-02-07 13:19:56,753][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2904291 effective words) took 0.9s, 3414287 effective words/s
[2023-02-07 13:19:57,605][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2904291 effective words) took 0.9s, 3414851 effective words/s
[2023-02-07 13:19:58,467][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2904291 effective words) took 0.9s, 3373465 effective words/s
[2023-02-07 13:19:59,300][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2904291 effective words) took 0.8s, 3488841 effective words/s
[2023-02-07 13:20:00,156][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2904291 effective words) took 0.9s, 3398941 effective words/s
[2023-02-07 13:20:01,023][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2904291 effective words) took 0.9s, 3357404 effective words/s
[2023-02-07 13:20:01,868][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2904291 effective words) took 0.8s, 3440756 effective words/s
[2023-02-07 13:20:02,713][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2904291 effective words) took 0.8s, 3446438 effective words/s
[2023-02-07 13:20:03,554][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2904291 effective words) took 0.8s, 3458016 effective words/s
[2023-02-07 13:20:04,398][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2904291 effective words) took 0.8s, 3447077 effective words/s
[2023-02-07 13:20:05,238][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2904291 effective words) took 0.8s, 3460891 effective words/s
[2023-02-07 13:20:06,079][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2904291 effective words) took 0.8s, 3458465 effective words/s
[2023-02-07 13:20:06,933][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2904291 effective words) took 0.9s, 3406978 effective words/s
[2023-02-07 13:20:07,781][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2904291 effective words) took 0.8s, 3429563 effective words/s
[2023-02-07 13:20:08,617][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2904291 effective words) took 0.8s, 3483714 effective words/s
[2023-02-07 13:20:09,458][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2904291 effective words) took 0.8s, 3458375 effective words/s
[2023-02-07 13:20:10,293][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2904291 effective words) took 0.8s, 3483566 effective words/s
[2023-02-07 13:20:11,136][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2904291 effective words) took 0.8s, 3449883 effective words/s
[2023-02-07 13:20:11,980][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2904291 effective words) took 0.8s, 3444918 effective words/s
[2023-02-07 13:20:12,812][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2904291 effective words) took 0.8s, 3496298 effective words/s
[2023-02-07 13:20:13,634][gensim.models.word2vec][INFO] - EPOCH 42: training on 2911496 raw words (2904291 effective words) took 0.8s, 3537703 effective words/s
[2023-02-07 13:20:14,490][gensim.models.word2vec][INFO] - EPOCH 43: training on 2911496 raw words (2904291 effective words) took 0.9s, 3398567 effective words/s
[2023-02-07 13:20:15,352][gensim.models.word2vec][INFO] - EPOCH 44: training on 2911496 raw words (2904291 effective words) took 0.9s, 3372901 effective words/s
[2023-02-07 13:20:15,353][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 131017320 raw words (130693095 effective words) took 38.7s, 3376995 effective words/s', 'datetime': '2023-02-07T13:20:15.353077', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:20:15.353 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:20:17,946][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131914-vgcjs6et/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:20:17.946026', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:20:17,946][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:20:17,958][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_131914-vgcjs6et/files/../tmp/embedding_model.pt
2023-02-07 13:20:17.958 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:20:19.038 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:20:19.450 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:20:20.033 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.001766108088864, 'test_mae': 1.0589755589450978, 'test_r2': 0.053044945494324036}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.89
wandb: percentage 0.52777
wandb:   test_mae 1.05898
wandb:   test_mse 2.00177
wandb:    test_r2 0.05304
wandb: 
wandb: üöÄ View run serene-sweep-38 at: https://wandb.ai/xiaoqiz/mof2vec/runs/vgcjs6et
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_131914-vgcjs6et/logs
wandb: Agent Starting Run: 4fvlpeuf with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 722
wandb: 	model.gensim.alpha: 0.0018693592589968096
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 67
wandb: 	model.gensim.min_count: 9
wandb: 	model.gensim.sample: 0.6053859173335816
wandb: 	model.gensim.vector_size: 122
wandb: 	model.gensim.window: 17
wandb: 	model.sklearn.learning_rate: 0.19420611662764575
wandb: 	model.sklearn.max_depth: 19
wandb: 	model.sklearn.min_child_weight: 0.02022431248188856
wandb: 	model.sklearn.n_estimators: 4184
wandb: 	model.sklearn.num_leaves: 496
wandb: 	model.sklearn.reg_alpha: 0.0151055217182482
wandb: 	model.sklearn.reg_lambda: 0.012193053083639918
wandb: 	model.sklearn.subsample: 0.2809153582788909
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132033-4fvlpeuf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-39
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/4fvlpeuf
2023-02-07 13:20:41.176 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:20:41.176 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 722 for sweep.
2023-02-07 13:20:41.176 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0018693592589968096 for sweep.
2023-02-07 13:20:41.177 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:20:41.177 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 67 for sweep.
2023-02-07 13:20:41.177 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 9 for sweep.
2023-02-07 13:20:41.177 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6053859173335816 for sweep.
2023-02-07 13:20:41.178 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 122 for sweep.
2023-02-07 13:20:41.178 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 17 for sweep.
2023-02-07 13:20:41.178 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.19420611662764575 for sweep.
2023-02-07 13:20:41.178 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 19 for sweep.
2023-02-07 13:20:41.178 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.02022431248188856 for sweep.
2023-02-07 13:20:41.179 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4184 for sweep.
2023-02-07 13:20:41.179 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 496 for sweep.
2023-02-07 13:20:41.179 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.0151055217182482 for sweep.
2023-02-07 13:20:41.179 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.012193053083639918 for sweep.
2023-02-07 13:20:41.179 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2809153582788909 for sweep.
2023-02-07 13:20:41.180 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:20:41.185 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132033-4fvlpeuf/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 722, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 122, 'window': 17, 'min_count': 9, 'dm': 0, 'sample': 0.6053859173335816, 'workers': 4, 'alpha': 0.0018693592589968096, 'epochs': 67}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4184, 'max_depth': 19, 'num_leaves': 496, 'reg_alpha': 0.0151055217182482, 'reg_lambda': 0.012193053083639918, 'subsample': 0.2809153582788909, 'min_child_weight': 0.02022431248188856, 'n_jobs': 4, 'learning_rate': 0.19420611662764575}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 40/3257 [00:00<00:08, 398.61it/s]  2%|‚ñè         | 81/3257 [00:00<00:07, 399.93it/s]  4%|‚ñé         | 121/3257 [00:00<00:08, 388.36it/s]  5%|‚ñå         | 163/3257 [00:00<00:07, 393.78it/s]  6%|‚ñã         | 205/3257 [00:00<00:07, 402.17it/s]  8%|‚ñä         | 251/3257 [00:00<00:07, 421.04it/s]  9%|‚ñâ         | 294/3257 [00:00<00:07, 423.13it/s] 10%|‚ñà         | 337/3257 [00:00<00:06, 419.20it/s] 12%|‚ñà‚ñè        | 379/3257 [00:00<00:06, 413.02it/s] 13%|‚ñà‚ñé        | 421/3257 [00:01<00:06, 412.81it/s] 14%|‚ñà‚ñç        | 463/3257 [00:01<00:07, 396.18it/s] 16%|‚ñà‚ñå        | 506/3257 [00:01<00:06, 405.45it/s] 17%|‚ñà‚ñã        | 547/3257 [00:01<00:06, 405.47it/s] 18%|‚ñà‚ñä        | 588/3257 [00:01<00:06, 396.18it/s] 19%|‚ñà‚ñâ        | 633/3257 [00:01<00:06, 411.23it/s] 21%|‚ñà‚ñà        | 675/3257 [00:01<00:06, 400.71it/s] 22%|‚ñà‚ñà‚ñè       | 716/3257 [00:01<00:06, 390.34it/s] 23%|‚ñà‚ñà‚ñé       | 756/3257 [00:01<00:06, 385.55it/s] 24%|‚ñà‚ñà‚ñç       | 796/3257 [00:01<00:06, 388.74it/s] 26%|‚ñà‚ñà‚ñå       | 836/3257 [00:02<00:06, 389.08it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:02<00:06, 377.67it/s] 28%|‚ñà‚ñà‚ñä       | 916/3257 [00:02<00:06, 380.01it/s] 29%|‚ñà‚ñà‚ñâ       | 957/3257 [00:02<00:05, 387.41it/s] 31%|‚ñà‚ñà‚ñà       | 996/3257 [00:02<00:05, 382.72it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1035/3257 [00:02<00:05, 376.02it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1075/3257 [00:02<00:05, 380.55it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1115/3257 [00:02<00:05, 382.10it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1154/3257 [00:03<00:07, 268.39it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1189/3257 [00:03<00:07, 285.92it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1231/3257 [00:03<00:06, 318.46it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:03<00:05, 336.29it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1308/3257 [00:03<00:05, 343.29it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:03<00:05, 360.16it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1388/3257 [00:03<00:05, 363.64it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1429/3257 [00:03<00:04, 375.31it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1473/3257 [00:03<00:04, 392.15it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1517/3257 [00:03<00:04, 405.10it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1558/3257 [00:04<00:04, 394.18it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1602/3257 [00:04<00:04, 405.00it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1643/3257 [00:04<00:04, 401.59it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1684/3257 [00:04<00:03, 395.84it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1727/3257 [00:04<00:03, 404.22it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1768/3257 [00:04<00:03, 389.36it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1808/3257 [00:04<00:03, 387.07it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1850/3257 [00:04<00:03, 394.69it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1891/3257 [00:04<00:03, 399.01it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1935/3257 [00:05<00:03, 409.81it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1980/3257 [00:05<00:03, 419.70it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2025/3257 [00:05<00:02, 427.67it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2068/3257 [00:05<00:02, 410.06it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2110/3257 [00:05<00:02, 410.73it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2152/3257 [00:05<00:02, 391.72it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2193/3257 [00:05<00:02, 396.29it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2234/3257 [00:05<00:02, 398.25it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2274/3257 [00:05<00:02, 384.18it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2321/3257 [00:05<00:02, 407.56it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2367/3257 [00:06<00:02, 420.88it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2410/3257 [00:06<00:02, 420.56it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2453/3257 [00:06<00:02, 386.37it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2493/3257 [00:06<00:01, 384.33it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2532/3257 [00:06<00:01, 384.83it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2571/3257 [00:06<00:02, 259.54it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2610/3257 [00:06<00:02, 286.98it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2649/3257 [00:07<00:01, 310.31it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2685/3257 [00:07<00:01, 322.66it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2721/3257 [00:07<00:01, 320.96it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:07<00:01, 326.13it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2794/3257 [00:07<00:01, 339.86it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2830/3257 [00:07<00:01, 335.15it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2867/3257 [00:07<00:01, 344.71it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2905/3257 [00:07<00:00, 353.64it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2943/3257 [00:07<00:00, 357.83it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2980/3257 [00:07<00:00, 360.65it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3023/3257 [00:08<00:00, 380.85it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3073/3257 [00:08<00:00, 414.15it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3117/3257 [00:08<00:00, 421.41it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3160/3257 [00:08<00:00, 412.19it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3202/3257 [00:08<00:00, 413.51it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:08<00:00, 415.13it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 379.25it/s]
2023-02-07 13:20:49.946 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:20:49,947][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d122,n5,mc9,s0.605386,t4>', 'datetime': '2023-02-07T13:20:49.947791', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:20:49,949][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:20:49,949][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:20:50,083][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:20:50,083][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:20:50,084][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=9 retains 462 unique words (50.00% of original 924, drops 462)', 'datetime': '2023-02-07T13:20:50.084828', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:20:50,085][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=9 leaves 1453902 word corpus (99.87% of original 1455748, drops 1846)', 'datetime': '2023-02-07T13:20:50.084999', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:20:50,086][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:20:50,086][gensim.models.word2vec][INFO] - sample=0.605386 downsamples 0 most-common words
[2023-02-07 13:20:50,086][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1453902 word corpus (100.0%% of prior 1453902)', 'datetime': '2023-02-07T13:20:50.086821', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:20:50,089][gensim.models.word2vec][INFO] - estimated required memory for 462 words and 122 dimensions: 2922728 bytes
[2023-02-07 13:20:50,089][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:20:50,091][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 462 vocabulary and 122 features, using sg=1 hs=0 sample=0.6053859173335816 negative=5 window=17 shrink_windows=True', 'datetime': '2023-02-07T13:20:50.091755', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:20:50,721][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1457159 effective words) took 0.6s, 2325432 effective words/s
[2023-02-07 13:20:51,279][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1457159 effective words) took 0.6s, 2624361 effective words/s
[2023-02-07 13:20:51,824][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1457159 effective words) took 0.5s, 2679389 effective words/s
[2023-02-07 13:20:52,350][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1457159 effective words) took 0.5s, 2780467 effective words/s
[2023-02-07 13:20:52,895][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1457159 effective words) took 0.5s, 2680679 effective words/s
[2023-02-07 13:20:53,477][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1457159 effective words) took 0.6s, 2506516 effective words/s
[2023-02-07 13:20:54,053][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1457159 effective words) took 0.6s, 2535885 effective words/s
[2023-02-07 13:20:54,626][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1457159 effective words) took 0.6s, 2549323 effective words/s
[2023-02-07 13:20:55,194][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1457159 effective words) took 0.6s, 2573827 effective words/s
[2023-02-07 13:20:55,768][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1457159 effective words) took 0.6s, 2544692 effective words/s
[2023-02-07 13:20:56,344][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1457159 effective words) took 0.6s, 2541332 effective words/s
[2023-02-07 13:20:56,917][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1457159 effective words) took 0.6s, 2548473 effective words/s
[2023-02-07 13:20:57,489][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1457159 effective words) took 0.6s, 2557720 effective words/s
[2023-02-07 13:20:58,049][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1457159 effective words) took 0.6s, 2608197 effective words/s
[2023-02-07 13:20:58,617][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1457159 effective words) took 0.6s, 2571734 effective words/s
[2023-02-07 13:20:59,190][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1457159 effective words) took 0.6s, 2547983 effective words/s
[2023-02-07 13:20:59,773][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1457159 effective words) took 0.6s, 2505361 effective words/s
[2023-02-07 13:21:00,349][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1457159 effective words) took 0.6s, 2533543 effective words/s
[2023-02-07 13:21:00,924][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1457159 effective words) took 0.6s, 2542751 effective words/s
[2023-02-07 13:21:01,502][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1457159 effective words) took 0.6s, 2523231 effective words/s
[2023-02-07 13:21:02,095][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1457159 effective words) took 0.6s, 2465410 effective words/s
[2023-02-07 13:21:02,684][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1457159 effective words) took 0.6s, 2479326 effective words/s
[2023-02-07 13:21:03,267][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1457159 effective words) took 0.6s, 2507670 effective words/s
[2023-02-07 13:21:03,848][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1457159 effective words) took 0.6s, 2515508 effective words/s
[2023-02-07 13:21:04,440][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1457159 effective words) took 0.6s, 2468031 effective words/s
[2023-02-07 13:21:05,021][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1457159 effective words) took 0.6s, 2513344 effective words/s
[2023-02-07 13:21:05,612][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1457159 effective words) took 0.6s, 2474733 effective words/s
[2023-02-07 13:21:06,185][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1457159 effective words) took 0.6s, 2548362 effective words/s
[2023-02-07 13:21:06,758][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1457159 effective words) took 0.6s, 2555157 effective words/s
[2023-02-07 13:21:07,335][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1457159 effective words) took 0.6s, 2529464 effective words/s
[2023-02-07 13:21:07,907][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1457159 effective words) took 0.6s, 2553783 effective words/s
[2023-02-07 13:21:08,482][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1457159 effective words) took 0.6s, 2537539 effective words/s
[2023-02-07 13:21:09,063][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1457159 effective words) took 0.6s, 2515966 effective words/s
[2023-02-07 13:21:09,636][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1457159 effective words) took 0.6s, 2548526 effective words/s
[2023-02-07 13:21:10,218][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1457159 effective words) took 0.6s, 2512291 effective words/s
[2023-02-07 13:21:10,799][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1457159 effective words) took 0.6s, 2515309 effective words/s
[2023-02-07 13:21:11,374][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1457159 effective words) took 0.6s, 2540447 effective words/s
[2023-02-07 13:21:11,945][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1457159 effective words) took 0.6s, 2560025 effective words/s
[2023-02-07 13:21:12,527][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1457159 effective words) took 0.6s, 2515404 effective words/s
[2023-02-07 13:21:13,110][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1457159 effective words) took 0.6s, 2509434 effective words/s
[2023-02-07 13:21:13,690][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1457159 effective words) took 0.6s, 2514865 effective words/s
[2023-02-07 13:21:14,271][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1457159 effective words) took 0.6s, 2516993 effective words/s
[2023-02-07 13:21:14,846][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1457159 effective words) took 0.6s, 2542636 effective words/s
[2023-02-07 13:21:15,428][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1457159 effective words) took 0.6s, 2510898 effective words/s
[2023-02-07 13:21:16,006][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1457159 effective words) took 0.6s, 2524570 effective words/s
[2023-02-07 13:21:16,579][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1457159 effective words) took 0.6s, 2553363 effective words/s
[2023-02-07 13:21:17,160][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1457159 effective words) took 0.6s, 2512921 effective words/s
[2023-02-07 13:21:17,732][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1457159 effective words) took 0.6s, 2555591 effective words/s
[2023-02-07 13:21:18,305][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1457159 effective words) took 0.6s, 2550713 effective words/s
[2023-02-07 13:21:18,885][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1457159 effective words) took 0.6s, 2515772 effective words/s
[2023-02-07 13:21:19,454][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1457159 effective words) took 0.6s, 2569321 effective words/s
[2023-02-07 13:21:20,028][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1457159 effective words) took 0.6s, 2545279 effective words/s
[2023-02-07 13:21:20,596][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1457159 effective words) took 0.6s, 2571670 effective words/s
[2023-02-07 13:21:21,168][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1457159 effective words) took 0.6s, 2551870 effective words/s
[2023-02-07 13:21:21,740][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1457159 effective words) took 0.6s, 2555567 effective words/s
[2023-02-07 13:21:22,308][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1457159 effective words) took 0.6s, 2573200 effective words/s
[2023-02-07 13:21:22,842][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1457159 effective words) took 0.5s, 2735736 effective words/s
[2023-02-07 13:21:23,368][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1457159 effective words) took 0.5s, 2778223 effective words/s
[2023-02-07 13:21:23,888][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1457159 effective words) took 0.5s, 2806159 effective words/s
[2023-02-07 13:21:24,408][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1457159 effective words) took 0.5s, 2813708 effective words/s
[2023-02-07 13:21:24,925][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1457159 effective words) took 0.5s, 2833185 effective words/s
[2023-02-07 13:21:25,441][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1457159 effective words) took 0.5s, 2830663 effective words/s
[2023-02-07 13:21:25,966][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1457159 effective words) took 0.5s, 2785273 effective words/s
[2023-02-07 13:21:26,491][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1457159 effective words) took 0.5s, 2778773 effective words/s
[2023-02-07 13:21:27,013][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1457159 effective words) took 0.5s, 2798155 effective words/s
[2023-02-07 13:21:27,542][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1457159 effective words) took 0.5s, 2765734 effective words/s
[2023-02-07 13:21:28,059][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1457159 effective words) took 0.5s, 2822582 effective words/s
[2023-02-07 13:21:28,060][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 97535116 raw words (97629653 effective words) took 38.0s, 2571355 effective words/s', 'datetime': '2023-02-07T13:21:28.060137', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:21:28.060 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:21:30,009][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132033-4fvlpeuf/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:21:30.009442', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:21:30,010][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:21:30,018][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132033-4fvlpeuf/files/../tmp/embedding_model.pt
2023-02-07 13:21:30.018 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:21:31.107 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:21:31.529 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:21:32.328 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8714958166070432, 'test_mae': 1.018337146967558, 'test_r2': 0.11467058221189885}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.8
wandb: percentage 0.5
wandb:   test_mae 1.01834
wandb:   test_mse 1.8715
wandb:    test_r2 0.11467
wandb: 
wandb: üöÄ View run solar-sweep-39 at: https://wandb.ai/xiaoqiz/mof2vec/runs/4fvlpeuf
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_132033-4fvlpeuf/logs
wandb: Agent Starting Run: 6n9jqnnn with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 956
wandb: 	model.gensim.alpha: 0.0006274932383263002
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 64
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.4512073744386957
wandb: 	model.gensim.vector_size: 249
wandb: 	model.gensim.window: 1
wandb: 	model.sklearn.learning_rate: 0.2546180934084179
wandb: 	model.sklearn.max_depth: 11
wandb: 	model.sklearn.min_child_weight: 0.021715589685276423
wandb: 	model.sklearn.n_estimators: 2572
wandb: 	model.sklearn.num_leaves: 483
wandb: 	model.sklearn.reg_alpha: 0.2553218201839251
wandb: 	model.sklearn.reg_lambda: 0.9181495394248472
wandb: 	model.sklearn.subsample: 0.37299599039368714
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132142-6n9jqnnn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/6n9jqnnn
2023-02-07 13:21:50.951 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 13:21:50.951 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 956 for sweep.
2023-02-07 13:21:50.952 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0006274932383263002 for sweep.
2023-02-07 13:21:50.952 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:21:50.952 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 64 for sweep.
2023-02-07 13:21:50.952 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 13:21:50.953 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4512073744386957 for sweep.
2023-02-07 13:21:50.953 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 249 for sweep.
2023-02-07 13:21:50.953 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 1 for sweep.
2023-02-07 13:21:50.953 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.2546180934084179 for sweep.
2023-02-07 13:21:50.953 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 11 for sweep.
2023-02-07 13:21:50.954 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.021715589685276423 for sweep.
2023-02-07 13:21:50.954 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2572 for sweep.
2023-02-07 13:21:50.954 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 483 for sweep.
2023-02-07 13:21:50.954 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.2553218201839251 for sweep.
2023-02-07 13:21:50.955 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.9181495394248472 for sweep.
2023-02-07 13:21:50.955 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.37299599039368714 for sweep.
2023-02-07 13:21:50.955 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:21:50.960 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132142-6n9jqnnn/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 956, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 249, 'window': 1, 'min_count': 1, 'dm': 0, 'sample': 0.4512073744386957, 'workers': 4, 'alpha': 0.0006274932383263002, 'epochs': 64}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2572, 'max_depth': 11, 'num_leaves': 483, 'reg_alpha': 0.2553218201839251, 'reg_lambda': 0.9181495394248472, 'subsample': 0.37299599039368714, 'min_child_weight': 0.021715589685276423, 'n_jobs': 4, 'learning_rate': 0.2546180934084179}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 17/3257 [00:00<00:19, 162.72it/s]  1%|          | 35/3257 [00:00<00:18, 171.30it/s]  2%|‚ñè         | 53/3257 [00:00<00:19, 168.45it/s]  2%|‚ñè         | 72/3257 [00:00<00:18, 176.22it/s]  3%|‚ñé         | 90/3257 [00:00<00:18, 175.89it/s]  3%|‚ñé         | 108/3257 [00:00<00:18, 167.13it/s]  4%|‚ñç         | 126/3257 [00:00<00:18, 169.36it/s]  5%|‚ñç         | 149/3257 [00:00<00:16, 184.08it/s]  5%|‚ñå         | 168/3257 [00:00<00:17, 177.75it/s]  6%|‚ñå         | 186/3257 [00:01<00:17, 177.44it/s]  6%|‚ñã         | 204/3257 [00:01<00:17, 174.71it/s]  7%|‚ñã         | 230/3257 [00:01<00:15, 194.35it/s]  8%|‚ñä         | 250/3257 [00:01<00:15, 189.47it/s]  8%|‚ñä         | 269/3257 [00:01<00:16, 181.63it/s]  9%|‚ñâ         | 295/3257 [00:01<00:14, 202.97it/s] 10%|‚ñâ         | 316/3257 [00:01<00:15, 188.06it/s] 10%|‚ñà         | 336/3257 [00:01<00:15, 190.02it/s] 11%|‚ñà         | 356/3257 [00:01<00:15, 190.91it/s] 12%|‚ñà‚ñè        | 376/3257 [00:02<00:16, 177.69it/s] 12%|‚ñà‚ñè        | 395/3257 [00:02<00:16, 171.54it/s] 13%|‚ñà‚ñé        | 414/3257 [00:02<00:16, 176.31it/s] 13%|‚ñà‚ñé        | 432/3257 [00:02<00:18, 154.06it/s] 14%|‚ñà‚ñç        | 448/3257 [00:02<00:18, 153.65it/s] 14%|‚ñà‚ñç        | 468/3257 [00:02<00:16, 165.88it/s] 15%|‚ñà‚ñç        | 486/3257 [00:02<00:16, 163.07it/s] 16%|‚ñà‚ñå        | 505/3257 [00:02<00:16, 170.15it/s] 16%|‚ñà‚ñå        | 524/3257 [00:02<00:15, 174.35it/s] 17%|‚ñà‚ñã        | 543/3257 [00:03<00:15, 174.91it/s] 17%|‚ñà‚ñã        | 561/3257 [00:03<00:16, 158.66it/s] 18%|‚ñà‚ñä        | 578/3257 [00:03<00:17, 150.96it/s] 18%|‚ñà‚ñä        | 599/3257 [00:03<00:16, 161.68it/s] 19%|‚ñà‚ñâ        | 619/3257 [00:03<00:21, 121.35it/s] 20%|‚ñà‚ñâ        | 636/3257 [00:03<00:20, 130.52it/s] 20%|‚ñà‚ñâ        | 651/3257 [00:03<00:20, 129.99it/s] 20%|‚ñà‚ñà        | 666/3257 [00:04<00:19, 132.89it/s] 21%|‚ñà‚ñà        | 683/3257 [00:04<00:18, 139.52it/s] 21%|‚ñà‚ñà‚ñè       | 700/3257 [00:04<00:17, 146.49it/s] 22%|‚ñà‚ñà‚ñè       | 720/3257 [00:04<00:16, 157.78it/s] 23%|‚ñà‚ñà‚ñé       | 737/3257 [00:04<00:16, 152.44it/s] 23%|‚ñà‚ñà‚ñé       | 756/3257 [00:04<00:15, 159.67it/s] 24%|‚ñà‚ñà‚ñé       | 773/3257 [00:04<00:15, 161.30it/s] 24%|‚ñà‚ñà‚ñç       | 790/3257 [00:04<00:15, 160.68it/s] 25%|‚ñà‚ñà‚ñç       | 807/3257 [00:04<00:15, 161.60it/s] 25%|‚ñà‚ñà‚ñå       | 824/3257 [00:05<00:15, 159.56it/s] 26%|‚ñà‚ñà‚ñå       | 841/3257 [00:05<00:15, 151.12it/s] 26%|‚ñà‚ñà‚ñã       | 859/3257 [00:05<00:15, 157.42it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:05<00:15, 156.97it/s] 27%|‚ñà‚ñà‚ñã       | 892/3257 [00:05<00:14, 159.11it/s] 28%|‚ñà‚ñà‚ñä       | 912/3257 [00:05<00:13, 170.15it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:05<00:13, 168.94it/s] 29%|‚ñà‚ñà‚ñâ       | 949/3257 [00:05<00:13, 170.24it/s] 30%|‚ñà‚ñà‚ñâ       | 968/3257 [00:05<00:13, 173.55it/s] 30%|‚ñà‚ñà‚ñà       | 986/3257 [00:05<00:13, 164.19it/s] 31%|‚ñà‚ñà‚ñà       | 1004/3257 [00:06<00:13, 168.08it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1021/3257 [00:06<00:13, 167.92it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1038/3257 [00:06<00:14, 154.31it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1054/3257 [00:06<00:14, 155.18it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1074/3257 [00:06<00:13, 166.61it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:06<00:13, 156.53it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1111/3257 [00:06<00:12, 166.91it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1128/3257 [00:06<00:13, 156.96it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1145/3257 [00:06<00:13, 156.01it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1165/3257 [00:07<00:12, 167.07it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1182/3257 [00:07<00:13, 154.60it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1198/3257 [00:07<00:13, 150.65it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1214/3257 [00:07<00:14, 142.82it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1236/3257 [00:07<00:12, 161.95it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1253/3257 [00:07<00:12, 156.44it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:07<00:12, 159.30it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1288/3257 [00:07<00:13, 145.43it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1305/3257 [00:08<00:12, 151.42it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1324/3257 [00:08<00:12, 161.00it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1342/3257 [00:08<00:11, 165.78it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1359/3257 [00:08<00:11, 160.93it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1376/3257 [00:08<00:11, 157.95it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1392/3257 [00:08<00:11, 157.03it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1414/3257 [00:08<00:10, 174.86it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:08<00:10, 174.07it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1452/3257 [00:08<00:10, 179.28it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1473/3257 [00:08<00:09, 185.66it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1492/3257 [00:09<00:09, 186.07it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1512/3257 [00:09<00:09, 189.47it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1531/3257 [00:09<00:10, 170.39it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1549/3257 [00:09<00:10, 163.96it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1566/3257 [00:09<00:10, 162.91it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1583/3257 [00:09<00:10, 162.72it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1603/3257 [00:09<00:09, 172.04it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1621/3257 [00:09<00:09, 171.70it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1639/3257 [00:09<00:09, 170.92it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1657/3257 [00:10<00:09, 164.82it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1674/3257 [00:10<00:09, 163.44it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1691/3257 [00:10<00:09, 160.12it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1709/3257 [00:10<00:09, 164.52it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:10<00:09, 157.11it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1744/3257 [00:10<00:09, 152.80it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:10<00:09, 162.62it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1783/3257 [00:10<00:08, 172.79it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1801/3257 [00:10<00:08, 169.85it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1819/3257 [00:11<00:08, 168.11it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1836/3257 [00:11<00:08, 163.50it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1855/3257 [00:11<00:08, 167.15it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1872/3257 [00:11<00:13, 102.69it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1889/3257 [00:11<00:11, 114.91it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1908/3257 [00:11<00:10, 130.16it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1924/3257 [00:11<00:09, 134.49it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1948/3257 [00:12<00:08, 159.94it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1971/3257 [00:12<00:07, 176.10it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1990/3257 [00:12<00:07, 169.34it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2008/3257 [00:12<00:07, 170.38it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2030/3257 [00:12<00:06, 181.67it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2049/3257 [00:12<00:07, 163.98it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2067/3257 [00:12<00:07, 156.58it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2085/3257 [00:12<00:07, 160.24it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2102/3257 [00:12<00:07, 158.55it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:13<00:07, 157.59it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2136/3257 [00:13<00:07, 157.07it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2152/3257 [00:13<00:07, 153.86it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2170/3257 [00:13<00:06, 160.91it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2187/3257 [00:13<00:06, 159.75it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:13<00:06, 166.86it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2224/3257 [00:13<00:06, 165.42it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2241/3257 [00:13<00:06, 161.68it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2260/3257 [00:13<00:06, 163.48it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2277/3257 [00:14<00:06, 155.90it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2298/3257 [00:14<00:05, 170.00it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2316/3257 [00:14<00:05, 168.62it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2339/3257 [00:14<00:04, 185.66it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:14<00:04, 192.20it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2381/3257 [00:14<00:04, 195.09it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2402/3257 [00:14<00:04, 196.87it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2422/3257 [00:14<00:04, 180.56it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2441/3257 [00:14<00:04, 168.67it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2459/3257 [00:15<00:04, 171.35it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2478/3257 [00:15<00:04, 172.70it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2499/3257 [00:15<00:04, 182.54it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2519/3257 [00:15<00:03, 186.55it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2540/3257 [00:15<00:03, 187.14it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2559/3257 [00:15<00:03, 177.75it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2577/3257 [00:15<00:04, 169.53it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2595/3257 [00:15<00:04, 163.95it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2620/3257 [00:15<00:03, 185.52it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:16<00:03, 187.42it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2659/3257 [00:16<00:03, 173.31it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2677/3257 [00:16<00:03, 175.08it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2695/3257 [00:16<00:03, 168.41it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2713/3257 [00:16<00:03, 157.21it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:16<00:03, 163.72it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2752/3257 [00:16<00:02, 173.18it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2770/3257 [00:16<00:02, 166.47it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2792/3257 [00:16<00:02, 179.30it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2811/3257 [00:17<00:02, 179.37it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2830/3257 [00:17<00:02, 165.52it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2850/3257 [00:17<00:02, 173.24it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2874/3257 [00:17<00:02, 190.87it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2894/3257 [00:17<00:02, 177.16it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2914/3257 [00:17<00:01, 182.47it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2933/3257 [00:17<00:01, 174.81it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2951/3257 [00:17<00:01, 163.44it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2970/3257 [00:17<00:01, 168.62it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2988/3257 [00:18<00:01, 159.94it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3011/3257 [00:18<00:01, 175.33it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3029/3257 [00:18<00:01, 168.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:18<00:01, 176.73it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3072/3257 [00:18<00:00, 190.38it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3092/3257 [00:18<00:00, 183.91it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3115/3257 [00:18<00:00, 195.17it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3135/3257 [00:18<00:00, 184.09it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3154/3257 [00:19<00:00, 176.00it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3172/3257 [00:19<00:00, 174.93it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3190/3257 [00:19<00:00, 169.14it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3208/3257 [00:19<00:00, 169.90it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3226/3257 [00:19<00:00, 167.66it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3248/3257 [00:19<00:00, 182.13it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:19<00:00, 166.21it/s]
2023-02-07 13:22:11.471 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:22:11,472][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d249,n5,s0.451207,t4>', 'datetime': '2023-02-07T13:22:11.472368', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:22:11,472][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:22:11,472][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:22:12,075][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 13:22:12,075][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:22:12,214][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 54054 unique words (100.00% of original 54054, drops 0)', 'datetime': '2023-02-07T13:22:12.214452', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:22:12,214][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 6550866 word corpus (100.00% of original 6550866, drops 0)', 'datetime': '2023-02-07T13:22:12.214817', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:22:12,400][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 13:22:12,401][gensim.models.word2vec][INFO] - sample=0.451207 downsamples 0 most-common words
[2023-02-07 13:22:12,402][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6550866 word corpus (100.0%% of prior 6550866)', 'datetime': '2023-02-07T13:22:12.402402', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:22:12,730][gensim.models.word2vec][INFO] - estimated required memory for 54054 words and 249 dimensions: 138597940 bytes
[2023-02-07 13:22:12,730][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:22:12,786][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 54054 vocabulary and 249 features, using sg=1 hs=0 sample=0.4512073744386957 negative=5 window=1 shrink_windows=True', 'datetime': '2023-02-07T13:22:12.786628', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:22:13,794][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 29.66% examples, 1914241 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:14,795][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 58.89% examples, 1943552 words/s, in_qsize 7, out_qsize 1
[2023-02-07 13:22:15,797][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 89.87% examples, 1950018 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:16,116][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6494971 effective words) took 3.3s, 1952003 effective words/s
[2023-02-07 13:22:17,121][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 30.06% examples, 1949364 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:18,127][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 58.89% examples, 1940278 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:19,129][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 89.50% examples, 1941160 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:19,460][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6494971 effective words) took 3.3s, 1942863 effective words/s
[2023-02-07 13:22:20,464][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 29.90% examples, 1935287 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:21,472][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 59.26% examples, 1954335 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:22,482][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 90.45% examples, 1955040 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:22,778][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6494971 effective words) took 3.3s, 1958065 effective words/s
[2023-02-07 13:22:23,783][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 30.43% examples, 1978532 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:24,786][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 59.47% examples, 1963605 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:25,786][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 90.18% examples, 1959777 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:26,093][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6494971 effective words) took 3.3s, 1960374 effective words/s
[2023-02-07 13:22:27,097][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 30.03% examples, 1945477 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:28,099][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 59.53% examples, 1964757 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:29,100][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 90.45% examples, 1964887 words/s, in_qsize 8, out_qsize 2
[2023-02-07 13:22:29,392][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6494971 effective words) took 3.3s, 1969636 effective words/s
[2023-02-07 13:22:30,396][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 30.12% examples, 1959296 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:31,398][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 59.26% examples, 1960561 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:32,409][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 90.45% examples, 1958168 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:22:32,697][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6494971 effective words) took 3.3s, 1965924 effective words/s
[2023-02-07 13:22:33,701][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 30.43% examples, 1979390 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:34,702][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 60.42% examples, 1986420 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:35,708][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 91.46% examples, 1986205 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:35,960][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6494971 effective words) took 3.3s, 1990881 effective words/s
[2023-02-07 13:22:36,965][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 30.61% examples, 1989505 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:37,965][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 60.58% examples, 1991849 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:38,965][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 91.34% examples, 1988002 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:39,232][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6494971 effective words) took 3.3s, 1986125 effective words/s
[2023-02-07 13:22:40,239][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 30.61% examples, 1987890 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:41,243][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 60.98% examples, 2004208 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:42,249][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 92.54% examples, 2006290 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:22:42,462][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6494971 effective words) took 3.2s, 2012584 effective words/s
[2023-02-07 13:22:43,465][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 30.12% examples, 1964448 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:44,470][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 60.09% examples, 1976216 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:45,481][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 91.46% examples, 1981193 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:45,731][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6494971 effective words) took 3.3s, 1987898 effective words/s
[2023-02-07 13:22:46,736][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 30.12% examples, 1959744 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:47,744][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 60.18% examples, 1972862 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:48,754][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 90.97% examples, 1966939 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:49,030][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6494971 effective words) took 3.3s, 1970055 effective words/s
[2023-02-07 13:22:50,033][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 30.61% examples, 1992213 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:51,041][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 60.58% examples, 1985204 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:52,047][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 91.46% examples, 1982474 words/s, in_qsize 7, out_qsize 1
[2023-02-07 13:22:52,299][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6494971 effective words) took 3.3s, 1987784 effective words/s
[2023-02-07 13:22:53,309][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 29.87% examples, 1933087 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:54,314][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 58.89% examples, 1940079 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:55,317][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 89.16% examples, 1936067 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:55,665][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6494971 effective words) took 3.4s, 1932295 effective words/s
[2023-02-07 13:22:56,672][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 29.66% examples, 1915584 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:57,673][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 58.27% examples, 1926165 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:58,678][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 89.25% examples, 1938844 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:22:58,999][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6494971 effective words) took 3.3s, 1949220 effective words/s
[2023-02-07 13:23:00,005][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 30.06% examples, 1950393 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:01,006][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 59.26% examples, 1960347 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:02,018][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 90.97% examples, 1968796 words/s, in_qsize 7, out_qsize 1
[2023-02-07 13:23:02,289][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6494971 effective words) took 3.3s, 1975475 effective words/s
[2023-02-07 13:23:03,293][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 31.99% examples, 2095408 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:04,294][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 64.05% examples, 2108310 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:05,294][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 98.28% examples, 2126879 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:05,344][gensim.models.word2vec][INFO] - EPOCH 15: training on 6550866 raw words (6494971 effective words) took 3.1s, 2127143 effective words/s
[2023-02-07 13:23:06,345][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 32.55% examples, 2135453 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:07,347][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 65.12% examples, 2149922 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:08,351][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 99.26% examples, 2146311 words/s, in_qsize 6, out_qsize 0
[2023-02-07 13:23:08,369][gensim.models.word2vec][INFO] - EPOCH 16: training on 6550866 raw words (6494971 effective words) took 3.0s, 2147768 effective words/s
[2023-02-07 13:23:09,373][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 32.88% examples, 2149717 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:10,377][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 65.09% examples, 2146439 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:11,379][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 100.00% examples, 2158720 words/s, in_qsize 0, out_qsize 1
[2023-02-07 13:23:11,379][gensim.models.word2vec][INFO] - EPOCH 17: training on 6550866 raw words (6494971 effective words) took 3.0s, 2158471 effective words/s
[2023-02-07 13:23:12,383][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 33.13% examples, 2158659 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:13,387][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 65.64% examples, 2165329 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:14,371][gensim.models.word2vec][INFO] - EPOCH 18: training on 6550866 raw words (6494971 effective words) took 3.0s, 2171340 effective words/s
[2023-02-07 13:23:15,375][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 33.34% examples, 2182401 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:16,383][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 65.80% examples, 2173223 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:17,382][gensim.models.word2vec][INFO] - EPOCH 19: training on 6550866 raw words (6494971 effective words) took 3.0s, 2159747 effective words/s
[2023-02-07 13:23:18,386][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 32.55% examples, 2131062 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:19,389][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 64.66% examples, 2129659 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:20,392][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 98.68% examples, 2135904 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:20,423][gensim.models.word2vec][INFO] - EPOCH 20: training on 6550866 raw words (6494971 effective words) took 3.0s, 2136469 effective words/s
[2023-02-07 13:23:21,430][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 32.27% examples, 2107410 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:22,437][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 63.22% examples, 2075967 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:23,440][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 97.67% examples, 2109043 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:23,499][gensim.models.word2vec][INFO] - EPOCH 21: training on 6550866 raw words (6494971 effective words) took 3.1s, 2112367 effective words/s
[2023-02-07 13:23:24,502][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 32.88% examples, 2151647 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:25,506][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 65.09% examples, 2147824 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:26,506][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 99.97% examples, 2157864 words/s, in_qsize 1, out_qsize 1
[2023-02-07 13:23:26,511][gensim.models.word2vec][INFO] - EPOCH 22: training on 6550866 raw words (6494971 effective words) took 3.0s, 2157715 effective words/s
[2023-02-07 13:23:27,513][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 33.13% examples, 2162409 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:28,516][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 65.64% examples, 2167884 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:29,518][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 100.00% examples, 2160331 words/s, in_qsize 0, out_qsize 1
[2023-02-07 13:23:29,518][gensim.models.word2vec][INFO] - EPOCH 23: training on 6550866 raw words (6494971 effective words) took 3.0s, 2160084 effective words/s
[2023-02-07 13:23:30,524][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 31.81% examples, 2073261 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:31,531][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 62.39% examples, 2039567 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:32,531][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 93.58% examples, 2028059 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:32,724][gensim.models.word2vec][INFO] - EPOCH 24: training on 6550866 raw words (6494971 effective words) took 3.2s, 2026733 effective words/s
[2023-02-07 13:23:33,733][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 30.12% examples, 1951558 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:34,742][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 60.09% examples, 1965452 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:35,747][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 90.82% examples, 1966856 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:36,024][gensim.models.word2vec][INFO] - EPOCH 25: training on 6550866 raw words (6494971 effective words) took 3.3s, 1969148 effective words/s
[2023-02-07 13:23:37,027][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 30.30% examples, 1975004 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:38,031][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 59.26% examples, 1961327 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:39,046][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 90.97% examples, 1968229 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:39,309][gensim.models.word2vec][INFO] - EPOCH 26: training on 6550866 raw words (6494971 effective words) took 3.3s, 1978863 effective words/s
[2023-02-07 13:23:40,318][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 30.92% examples, 2008455 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:41,329][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 61.74% examples, 2016383 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:23:42,331][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 93.12% examples, 2014590 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:42,532][gensim.models.word2vec][INFO] - EPOCH 27: training on 6550866 raw words (6494971 effective words) took 3.2s, 2016907 effective words/s
[2023-02-07 13:23:43,540][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 31.04% examples, 2016058 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:44,541][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 61.59% examples, 2023477 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:45,542][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 92.75% examples, 2016808 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:45,753][gensim.models.word2vec][INFO] - EPOCH 28: training on 6550866 raw words (6494971 effective words) took 3.2s, 2018181 effective words/s
[2023-02-07 13:23:46,755][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 30.24% examples, 1966384 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:47,758][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 60.42% examples, 1986281 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:48,768][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 91.86% examples, 1995449 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:49,003][gensim.models.word2vec][INFO] - EPOCH 29: training on 6550866 raw words (6494971 effective words) took 3.2s, 1999356 effective words/s
[2023-02-07 13:23:50,006][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 30.92% examples, 2018852 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:51,009][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 61.13% examples, 2012033 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:52,015][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 93.06% examples, 2020012 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:52,209][gensim.models.word2vec][INFO] - EPOCH 30: training on 6550866 raw words (6494971 effective words) took 3.2s, 2026896 effective words/s
[2023-02-07 13:23:53,218][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 30.92% examples, 2007848 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:54,218][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 61.77% examples, 2027105 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:23:55,219][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 93.46% examples, 2027239 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:55,415][gensim.models.word2vec][INFO] - EPOCH 31: training on 6550866 raw words (6494971 effective words) took 3.2s, 2027096 effective words/s
[2023-02-07 13:23:56,422][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 30.92% examples, 2010620 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:57,430][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 61.74% examples, 2020486 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:58,432][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 93.58% examples, 2025054 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:23:58,616][gensim.models.word2vec][INFO] - EPOCH 32: training on 6550866 raw words (6494971 effective words) took 3.2s, 2029785 effective words/s
[2023-02-07 13:23:59,620][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 31.44% examples, 2039899 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:00,627][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 62.39% examples, 2041054 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:01,628][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 94.50% examples, 2043242 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:01,791][gensim.models.word2vec][INFO] - EPOCH 33: training on 6550866 raw words (6494971 effective words) took 3.2s, 2046140 effective words/s
[2023-02-07 13:24:02,797][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 31.50% examples, 2050850 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:03,801][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 62.39% examples, 2045153 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:04,808][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 94.32% examples, 2038894 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:04,976][gensim.models.word2vec][INFO] - EPOCH 34: training on 6550866 raw words (6494971 effective words) took 3.2s, 2041715 effective words/s
[2023-02-07 13:24:05,982][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 31.53% examples, 2059416 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:06,986][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 62.91% examples, 2064552 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:07,986][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 95.43% examples, 2063694 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:08,126][gensim.models.word2vec][INFO] - EPOCH 35: training on 6550866 raw words (6494971 effective words) took 3.1s, 2063975 effective words/s
[2023-02-07 13:24:09,129][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 31.50% examples, 2052441 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:10,132][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 62.67% examples, 2061475 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:11,135][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 93.77% examples, 2034214 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:11,322][gensim.models.word2vec][INFO] - EPOCH 36: training on 6550866 raw words (6494971 effective words) took 3.2s, 2033080 effective words/s
[2023-02-07 13:24:12,327][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 29.90% examples, 1936318 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:13,329][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 59.10% examples, 1956184 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:14,329][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 89.90% examples, 1956001 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:24:14,638][gensim.models.word2vec][INFO] - EPOCH 37: training on 6550866 raw words (6494971 effective words) took 3.3s, 1960480 effective words/s
[2023-02-07 13:24:15,647][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 30.06% examples, 1942776 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:16,651][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 58.83% examples, 1939412 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:17,653][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 89.87% examples, 1946653 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:17,971][gensim.models.word2vec][INFO] - EPOCH 38: training on 6550866 raw words (6494971 effective words) took 3.3s, 1949462 effective words/s
[2023-02-07 13:24:18,974][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 30.06% examples, 1954274 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:19,975][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 59.26% examples, 1962667 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:24:20,986][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 90.45% examples, 1959760 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:21,280][gensim.models.word2vec][INFO] - EPOCH 39: training on 6550866 raw words (6494971 effective words) took 3.3s, 1963662 effective words/s
[2023-02-07 13:24:22,283][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 30.03% examples, 1949391 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:23,285][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 58.92% examples, 1952624 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:24,288][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 89.62% examples, 1948882 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:24,610][gensim.models.word2vec][INFO] - EPOCH 40: training on 6550866 raw words (6494971 effective words) took 3.3s, 1951844 effective words/s
[2023-02-07 13:24:25,615][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 30.30% examples, 1969969 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:26,615][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 60.09% examples, 1978770 words/s, in_qsize 6, out_qsize 1
[2023-02-07 13:24:27,624][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 91.37% examples, 1984935 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:27,878][gensim.models.word2vec][INFO] - EPOCH 41: training on 6550866 raw words (6494971 effective words) took 3.3s, 1988681 effective words/s
[2023-02-07 13:24:28,888][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 30.92% examples, 2006825 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:29,893][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 63.40% examples, 2079263 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:30,894][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 98.28% examples, 2120169 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:30,940][gensim.models.word2vec][INFO] - EPOCH 42: training on 6550866 raw words (6494971 effective words) took 3.1s, 2122374 effective words/s
[2023-02-07 13:24:31,942][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 33.16% examples, 2174821 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:32,945][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 63.99% examples, 2109944 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:33,945][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 96.19% examples, 2081835 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:34,063][gensim.models.word2vec][INFO] - EPOCH 43: training on 6550866 raw words (6494971 effective words) took 3.1s, 2080580 effective words/s
[2023-02-07 13:24:35,066][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 31.23% examples, 2034586 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:36,068][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 61.77% examples, 2031162 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:37,068][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 93.83% examples, 2036163 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:37,247][gensim.models.word2vec][INFO] - EPOCH 44: training on 6550866 raw words (6494971 effective words) took 3.2s, 2041131 effective words/s
[2023-02-07 13:24:38,250][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 31.04% examples, 2023297 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:39,252][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 61.77% examples, 2030321 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:40,255][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 93.83% examples, 2033921 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:40,436][gensim.models.word2vec][INFO] - EPOCH 45: training on 6550866 raw words (6494971 effective words) took 3.2s, 2037164 effective words/s
[2023-02-07 13:24:41,440][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 32.39% examples, 2121458 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:42,443][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 65.12% examples, 2146971 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:43,413][gensim.models.word2vec][INFO] - EPOCH 46: training on 6550866 raw words (6494971 effective words) took 3.0s, 2182944 effective words/s
[2023-02-07 13:24:44,421][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 33.68% examples, 2194488 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:45,426][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 66.38% examples, 2186751 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:46,377][gensim.models.word2vec][INFO] - EPOCH 47: training on 6550866 raw words (6494971 effective words) took 3.0s, 2191966 effective words/s
[2023-02-07 13:24:47,385][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 34.45% examples, 2250297 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:48,387][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 67.58% examples, 2229685 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:49,303][gensim.models.word2vec][INFO] - EPOCH 48: training on 6550866 raw words (6494971 effective words) took 2.9s, 2222482 effective words/s
[2023-02-07 13:24:50,309][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 31.44% examples, 2038296 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:51,317][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 62.39% examples, 2038997 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:52,317][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 95.03% examples, 2054141 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:52,461][gensim.models.word2vec][INFO] - EPOCH 49: training on 6550866 raw words (6494971 effective words) took 3.2s, 2058134 effective words/s
[2023-02-07 13:24:53,463][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 31.04% examples, 2026853 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:54,469][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 62.27% examples, 2040660 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:55,474][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 93.83% examples, 2030768 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:55,655][gensim.models.word2vec][INFO] - EPOCH 50: training on 6550866 raw words (6494971 effective words) took 3.2s, 2033980 effective words/s
[2023-02-07 13:24:56,663][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 30.92% examples, 2013984 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:57,666][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 65.34% examples, 2153867 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:24:58,591][gensim.models.word2vec][INFO] - EPOCH 51: training on 6550866 raw words (6494971 effective words) took 2.9s, 2214580 effective words/s
[2023-02-07 13:24:59,593][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 35.19% examples, 2318520 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:00,598][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 69.88% examples, 2316656 words/s, in_qsize 8, out_qsize 1
[2023-02-07 13:25:01,407][gensim.models.word2vec][INFO] - EPOCH 52: training on 6550866 raw words (6494971 effective words) took 2.8s, 2307985 effective words/s
[2023-02-07 13:25:02,413][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 35.19% examples, 2308437 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:03,413][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 69.57% examples, 2304507 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:04,220][gensim.models.word2vec][INFO] - EPOCH 53: training on 6550866 raw words (6494971 effective words) took 2.8s, 2309912 effective words/s
[2023-02-07 13:25:05,225][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 33.13% examples, 2162118 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:06,229][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 65.58% examples, 2167721 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:07,230][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 99.51% examples, 2150203 words/s, in_qsize 4, out_qsize 0
[2023-02-07 13:25:07,240][gensim.models.word2vec][INFO] - EPOCH 54: training on 6550866 raw words (6494971 effective words) took 3.0s, 2153072 effective words/s
[2023-02-07 13:25:08,242][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 32.73% examples, 2143640 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:09,246][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 65.12% examples, 2147787 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:10,248][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 99.66% examples, 2152104 words/s, in_qsize 3, out_qsize 1
[2023-02-07 13:25:10,258][gensim.models.word2vec][INFO] - EPOCH 55: training on 6550866 raw words (6494971 effective words) took 3.0s, 2153096 effective words/s
[2023-02-07 13:25:11,259][gensim.models.word2vec][INFO] - EPOCH 56 - PROGRESS: at 31.93% examples, 2091518 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:12,260][gensim.models.word2vec][INFO] - EPOCH 56 - PROGRESS: at 63.77% examples, 2102881 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:13,264][gensim.models.word2vec][INFO] - EPOCH 56 - PROGRESS: at 97.30% examples, 2105868 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:13,338][gensim.models.word2vec][INFO] - EPOCH 56: training on 6550866 raw words (6494971 effective words) took 3.1s, 2109761 effective words/s
[2023-02-07 13:25:14,341][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 32.39% examples, 2122831 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:15,342][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 64.91% examples, 2140577 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:16,344][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 98.62% examples, 2134610 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:16,377][gensim.models.word2vec][INFO] - EPOCH 57: training on 6550866 raw words (6494971 effective words) took 3.0s, 2137903 effective words/s
[2023-02-07 13:25:17,380][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 31.93% examples, 2089084 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:18,389][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 64.42% examples, 2115100 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:19,394][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 98.83% examples, 2133594 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:19,417][gensim.models.word2vec][INFO] - EPOCH 58: training on 6550866 raw words (6494971 effective words) took 3.0s, 2136997 effective words/s
[2023-02-07 13:25:20,427][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 32.55% examples, 2120040 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:21,427][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 65.12% examples, 2144037 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:22,428][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 98.34% examples, 2126754 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:22,467][gensim.models.word2vec][INFO] - EPOCH 59: training on 6550866 raw words (6494971 effective words) took 3.0s, 2131170 effective words/s
[2023-02-07 13:25:23,470][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 32.27% examples, 2115669 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:24,471][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 63.77% examples, 2100943 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:25,473][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 96.65% examples, 2092435 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:25,566][gensim.models.word2vec][INFO] - EPOCH 60: training on 6550866 raw words (6494971 effective words) took 3.1s, 2096760 effective words/s
[2023-02-07 13:25:26,571][gensim.models.word2vec][INFO] - EPOCH 61 - PROGRESS: at 33.87% examples, 2214782 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:27,573][gensim.models.word2vec][INFO] - EPOCH 61 - PROGRESS: at 68.62% examples, 2267111 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:28,425][gensim.models.word2vec][INFO] - EPOCH 61: training on 6550866 raw words (6494971 effective words) took 2.9s, 2274036 effective words/s
[2023-02-07 13:25:29,428][gensim.models.word2vec][INFO] - EPOCH 62 - PROGRESS: at 35.34% examples, 2323202 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:30,430][gensim.models.word2vec][INFO] - EPOCH 62 - PROGRESS: at 68.84% examples, 2278804 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:31,272][gensim.models.word2vec][INFO] - EPOCH 62: training on 6550866 raw words (6494971 effective words) took 2.8s, 2281532 effective words/s
[2023-02-07 13:25:32,274][gensim.models.word2vec][INFO] - EPOCH 63 - PROGRESS: at 33.68% examples, 2207999 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:33,280][gensim.models.word2vec][INFO] - EPOCH 63 - PROGRESS: at 66.87% examples, 2207687 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:25:34,212][gensim.models.word2vec][INFO] - EPOCH 63: training on 6550866 raw words (6494971 effective words) took 2.9s, 2209985 effective words/s
[2023-02-07 13:25:34,213][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 419255424 raw words (415678144 effective words) took 201.4s, 2063673 effective words/s', 'datetime': '2023-02-07T13:25:34.213419', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:25:34.214 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:25:50,169][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132142-6n9jqnnn/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:25:50.169085', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:25:50,170][gensim.utils][INFO] - storing np array 'vectors' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132142-6n9jqnnn/files/../tmp/embedding_model.pt.wv.vectors.npy
[2023-02-07 13:25:50,250][gensim.utils][INFO] - storing np array 'syn1neg' to /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132142-6n9jqnnn/files/../tmp/embedding_model.pt.syn1neg.npy
[2023-02-07 13:25:50,314][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:25:50,357][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132142-6n9jqnnn/files/../tmp/embedding_model.pt
2023-02-07 13:25:50.357 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:25:51.967 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:25:52.502 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:25:54.074 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9735871163548997, 'test_mae': 1.0256500523080307, 'test_r2': 0.06637529340336479}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.059 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.059 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.91
wandb: percentage 0.0
wandb:   test_mae 1.02565
wandb:   test_mse 1.97359
wandb:    test_r2 0.06638
wandb: 
wandb: üöÄ View run faithful-sweep-40 at: https://wandb.ai/xiaoqiz/mof2vec/runs/6n9jqnnn
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_132142-6n9jqnnn/logs
wandb: Agent Starting Run: bt43atww with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 615
wandb: 	model.gensim.alpha: 0.0010829018283140892
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 56
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.5056781829579895
wandb: 	model.gensim.vector_size: 215
wandb: 	model.gensim.window: 8
wandb: 	model.sklearn.learning_rate: 0.6433907665738126
wandb: 	model.sklearn.max_depth: 12
wandb: 	model.sklearn.min_child_weight: 0.031374000937807664
wandb: 	model.sklearn.n_estimators: 4749
wandb: 	model.sklearn.num_leaves: 364
wandb: 	model.sklearn.reg_alpha: 0.006929163843453242
wandb: 	model.sklearn.reg_lambda: 0.07984160766160725
wandb: 	model.sklearn.subsample: 0.37297836630216913
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132601-bt43atww
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-41
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/bt43atww
2023-02-07 13:26:09.935 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:26:09.936 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 615 for sweep.
2023-02-07 13:26:09.936 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0010829018283140892 for sweep.
2023-02-07 13:26:09.937 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:26:09.937 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 56 for sweep.
2023-02-07 13:26:09.937 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 13:26:09.937 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5056781829579895 for sweep.
2023-02-07 13:26:09.937 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 215 for sweep.
2023-02-07 13:26:09.938 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 8 for sweep.
2023-02-07 13:26:09.938 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.6433907665738126 for sweep.
2023-02-07 13:26:09.938 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 12 for sweep.
2023-02-07 13:26:09.939 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.031374000937807664 for sweep.
2023-02-07 13:26:09.939 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4749 for sweep.
2023-02-07 13:26:09.939 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 364 for sweep.
2023-02-07 13:26:09.939 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.006929163843453242 for sweep.
2023-02-07 13:26:09.939 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.07984160766160725 for sweep.
2023-02-07 13:26:09.940 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.37297836630216913 for sweep.
2023-02-07 13:26:09.940 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:26:09.948 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132601-bt43atww/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 615, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 215, 'window': 8, 'min_count': 4, 'dm': 0, 'sample': 0.5056781829579895, 'workers': 4, 'alpha': 0.0010829018283140892, 'epochs': 56}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4749, 'max_depth': 12, 'num_leaves': 364, 'reg_alpha': 0.006929163843453242, 'reg_lambda': 0.07984160766160725, 'subsample': 0.37297836630216913, 'min_child_weight': 0.031374000937807664, 'n_jobs': 4, 'learning_rate': 0.6433907665738126}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 3/3257 [00:00<02:23, 22.71it/s]  1%|‚ñè         | 42/3257 [00:00<00:15, 212.37it/s]  3%|‚ñé         | 86/3257 [00:00<00:10, 308.48it/s]  4%|‚ñç         | 124/3257 [00:00<00:09, 334.54it/s]  5%|‚ñå         | 163/3257 [00:00<00:08, 348.02it/s]  6%|‚ñã         | 205/3257 [00:00<00:08, 370.35it/s]  8%|‚ñä         | 251/3257 [00:00<00:07, 397.77it/s]  9%|‚ñâ         | 297/3257 [00:00<00:07, 414.05it/s] 10%|‚ñà         | 339/3257 [00:00<00:07, 414.36it/s] 12%|‚ñà‚ñè        | 381/3257 [00:01<00:06, 411.92it/s] 13%|‚ñà‚ñé        | 423/3257 [00:01<00:06, 412.31it/s] 14%|‚ñà‚ñç        | 465/3257 [00:01<00:07, 396.72it/s] 16%|‚ñà‚ñå        | 509/3257 [00:01<00:06, 409.27it/s] 17%|‚ñà‚ñã        | 551/3257 [00:01<00:06, 408.67it/s] 18%|‚ñà‚ñä        | 592/3257 [00:01<00:06, 395.95it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:01<00:06, 408.66it/s] 21%|‚ñà‚ñà        | 679/3257 [00:01<00:06, 403.28it/s] 22%|‚ñà‚ñà‚ñè       | 720/3257 [00:01<00:06, 401.95it/s] 23%|‚ñà‚ñà‚ñé       | 761/3257 [00:01<00:06, 401.79it/s] 25%|‚ñà‚ñà‚ñç       | 802/3257 [00:02<00:06, 396.86it/s] 26%|‚ñà‚ñà‚ñå       | 842/3257 [00:02<00:06, 384.13it/s] 27%|‚ñà‚ñà‚ñã       | 884/3257 [00:02<00:06, 389.70it/s] 29%|‚ñà‚ñà‚ñä       | 929/3257 [00:02<00:05, 405.69it/s] 30%|‚ñà‚ñà‚ñâ       | 970/3257 [00:02<00:05, 405.90it/s] 31%|‚ñà‚ñà‚ñà       | 1011/3257 [00:02<00:05, 395.35it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1051/3257 [00:02<00:05, 387.78it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:02<00:05, 390.47it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1131/3257 [00:02<00:05, 391.34it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1174/3257 [00:03<00:05, 402.53it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1215/3257 [00:03<00:05, 383.70it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1262/3257 [00:03<00:04, 405.87it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1303/3257 [00:03<00:04, 403.94it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:03<00:04, 416.52it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1392/3257 [00:03<00:05, 316.54it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1441/3257 [00:03<00:05, 356.87it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1493/3257 [00:03<00:04, 397.19it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1536/3257 [00:03<00:04, 404.45it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1580/3257 [00:04<00:04, 412.74it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1630/3257 [00:04<00:03, 434.72it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1675/3257 [00:04<00:03, 419.99it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1720/3257 [00:04<00:03, 425.99it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1764/3257 [00:04<00:03, 425.24it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1809/3257 [00:04<00:03, 431.47it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1856/3257 [00:04<00:03, 440.65it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1903/3257 [00:04<00:03, 447.01it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1954/3257 [00:04<00:02, 465.08it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2002/3257 [00:05<00:02, 467.51it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2049/3257 [00:05<00:02, 456.44it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2095/3257 [00:05<00:02, 451.94it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2141/3257 [00:05<00:02, 439.23it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2186/3257 [00:05<00:02, 439.83it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2233/3257 [00:05<00:02, 445.40it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2278/3257 [00:05<00:02, 423.22it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2328/3257 [00:05<00:02, 443.05it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2379/3257 [00:05<00:01, 461.88it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2427/3257 [00:05<00:01, 466.33it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2474/3257 [00:06<00:01, 466.92it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2525/3257 [00:06<00:01, 477.29it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2573/3257 [00:06<00:01, 457.22it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2623/3257 [00:06<00:01, 469.14it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2671/3257 [00:06<00:01, 470.25it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2719/3257 [00:06<00:01, 453.08it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:06<00:01, 459.00it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2815/3257 [00:06<00:00, 460.61it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2862/3257 [00:07<00:01, 335.18it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2908/3257 [00:07<00:00, 362.87it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2952/3257 [00:07<00:00, 380.49it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2997/3257 [00:07<00:00, 398.28it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3046/3257 [00:07<00:00, 421.06it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3095/3257 [00:07<00:00, 438.08it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3143/3257 [00:07<00:00, 444.77it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3189/3257 [00:07<00:00, 441.67it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3236/3257 [00:07<00:00, 448.07it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:07<00:00, 412.32it/s]
2023-02-07 13:26:18.023 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:26:18,024][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d215,n5,mc4,s0.505678,t4>', 'datetime': '2023-02-07T13:26:18.024333', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:26:18,024][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:26:18,024][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:26:18,153][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:26:18,153][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:26:18,155][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 742 unique words (80.30% of original 924, drops 182)', 'datetime': '2023-02-07T13:26:18.155263', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:26:18,155][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 1455401 word corpus (99.98% of original 1455748, drops 347)', 'datetime': '2023-02-07T13:26:18.155421', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:26:18,157][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:26:18,158][gensim.models.word2vec][INFO] - sample=0.505678 downsamples 0 most-common words
[2023-02-07 13:26:18,158][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455401 word corpus (100.0%% of prior 1455401)', 'datetime': '2023-02-07T13:26:18.158152', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:26:18,162][gensim.models.word2vec][INFO] - estimated required memory for 742 words and 215 dimensions: 5099660 bytes
[2023-02-07 13:26:18,162][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:26:18,165][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 742 vocabulary and 215 features, using sg=1 hs=0 sample=0.5056781829579895 negative=5 window=8 shrink_windows=True', 'datetime': '2023-02-07T13:26:18.165834', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:26:18,975][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458658 effective words) took 0.8s, 1806970 effective words/s
[2023-02-07 13:26:19,766][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458658 effective words) took 0.8s, 1845876 effective words/s
[2023-02-07 13:26:20,567][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458658 effective words) took 0.8s, 1823339 effective words/s
[2023-02-07 13:26:21,370][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458658 effective words) took 0.8s, 1821557 effective words/s
[2023-02-07 13:26:22,167][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458658 effective words) took 0.8s, 1833028 effective words/s
[2023-02-07 13:26:22,966][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458658 effective words) took 0.8s, 1827471 effective words/s
[2023-02-07 13:26:23,794][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458658 effective words) took 0.8s, 1764540 effective words/s
[2023-02-07 13:26:24,633][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458658 effective words) took 0.8s, 1742118 effective words/s
[2023-02-07 13:26:25,447][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458658 effective words) took 0.8s, 1794939 effective words/s
[2023-02-07 13:26:26,265][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458658 effective words) took 0.8s, 1785979 effective words/s
[2023-02-07 13:26:27,090][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458658 effective words) took 0.8s, 1770720 effective words/s
[2023-02-07 13:26:27,912][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458658 effective words) took 0.8s, 1778135 effective words/s
[2023-02-07 13:26:28,711][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458658 effective words) took 0.8s, 1827445 effective words/s
[2023-02-07 13:26:29,530][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458658 effective words) took 0.8s, 1784679 effective words/s
[2023-02-07 13:26:30,337][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458658 effective words) took 0.8s, 1811780 effective words/s
[2023-02-07 13:26:31,183][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458658 effective words) took 0.8s, 1727911 effective words/s
[2023-02-07 13:26:32,032][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458658 effective words) took 0.8s, 1720545 effective words/s
[2023-02-07 13:26:32,898][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458658 effective words) took 0.9s, 1688601 effective words/s
[2023-02-07 13:26:33,756][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458658 effective words) took 0.9s, 1701483 effective words/s
[2023-02-07 13:26:34,612][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458658 effective words) took 0.9s, 1707762 effective words/s
[2023-02-07 13:26:35,471][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458658 effective words) took 0.9s, 1701868 effective words/s
[2023-02-07 13:26:36,323][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458658 effective words) took 0.8s, 1716111 effective words/s
[2023-02-07 13:26:37,177][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458658 effective words) took 0.9s, 1711151 effective words/s
[2023-02-07 13:26:38,020][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458658 effective words) took 0.8s, 1734309 effective words/s
[2023-02-07 13:26:38,899][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458658 effective words) took 0.9s, 1661780 effective words/s
[2023-02-07 13:26:39,749][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458658 effective words) took 0.8s, 1721687 effective words/s
[2023-02-07 13:26:40,606][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458658 effective words) took 0.9s, 1707883 effective words/s
[2023-02-07 13:26:41,456][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458658 effective words) took 0.8s, 1717616 effective words/s
[2023-02-07 13:26:42,321][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458658 effective words) took 0.9s, 1689932 effective words/s
[2023-02-07 13:26:43,196][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458658 effective words) took 0.9s, 1669266 effective words/s
[2023-02-07 13:26:44,068][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458658 effective words) took 0.9s, 1675731 effective words/s
[2023-02-07 13:26:44,948][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458658 effective words) took 0.9s, 1662714 effective words/s
[2023-02-07 13:26:45,815][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458658 effective words) took 0.9s, 1686780 effective words/s
[2023-02-07 13:26:46,679][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458658 effective words) took 0.9s, 1691641 effective words/s
[2023-02-07 13:26:47,554][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458658 effective words) took 0.9s, 1670163 effective words/s
[2023-02-07 13:26:48,431][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458658 effective words) took 0.9s, 1665619 effective words/s
[2023-02-07 13:26:49,299][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458658 effective words) took 0.9s, 1683634 effective words/s
[2023-02-07 13:26:50,162][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458658 effective words) took 0.9s, 1692759 effective words/s
[2023-02-07 13:26:51,028][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458658 effective words) took 0.9s, 1687731 effective words/s
[2023-02-07 13:26:51,908][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458658 effective words) took 0.9s, 1660570 effective words/s
[2023-02-07 13:26:52,771][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458658 effective words) took 0.9s, 1696381 effective words/s
[2023-02-07 13:26:53,635][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458658 effective words) took 0.9s, 1691785 effective words/s
[2023-02-07 13:26:54,495][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458658 effective words) took 0.9s, 1701605 effective words/s
[2023-02-07 13:26:55,353][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458658 effective words) took 0.9s, 1701326 effective words/s
[2023-02-07 13:26:56,228][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458658 effective words) took 0.9s, 1670732 effective words/s
[2023-02-07 13:26:57,108][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458658 effective words) took 0.9s, 1662480 effective words/s
[2023-02-07 13:26:57,990][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458658 effective words) took 0.9s, 1655968 effective words/s
[2023-02-07 13:26:58,859][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458658 effective words) took 0.9s, 1681825 effective words/s
[2023-02-07 13:26:59,726][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458658 effective words) took 0.9s, 1684625 effective words/s
[2023-02-07 13:27:00,553][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458658 effective words) took 0.8s, 1768589 effective words/s
[2023-02-07 13:27:01,335][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458658 effective words) took 0.8s, 1867746 effective words/s
[2023-02-07 13:27:02,124][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458658 effective words) took 0.8s, 1852547 effective words/s
[2023-02-07 13:27:02,909][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458658 effective words) took 0.8s, 1862184 effective words/s
[2023-02-07 13:27:03,687][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458658 effective words) took 0.8s, 1876759 effective words/s
[2023-02-07 13:27:04,540][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458658 effective words) took 0.9s, 1715590 effective words/s
[2023-02-07 13:27:05,411][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458658 effective words) took 0.9s, 1676474 effective words/s
[2023-02-07 13:27:05,412][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 81521888 raw words (81684848 effective words) took 47.2s, 1728941 effective words/s', 'datetime': '2023-02-07T13:27:05.412248', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:27:05.412 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:27:07,366][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132601-bt43atww/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:27:07.366597', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:27:07,367][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:27:07,381][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132601-bt43atww/files/../tmp/embedding_model.pt
2023-02-07 13:27:07.382 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:27:08.779 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:27:09.321 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:27:10.746 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9002159874450635, 'test_mae': 1.015820752440786, 'test_r2': 0.10108422422959928}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.73
wandb: percentage 0.19697
wandb:   test_mae 1.01582
wandb:   test_mse 1.90022
wandb:    test_r2 0.10108
wandb: 
wandb: üöÄ View run legendary-sweep-41 at: https://wandb.ai/xiaoqiz/mof2vec/runs/bt43atww
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_132601-bt43atww/logs
wandb: Agent Starting Run: ygmw7t4j with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 225
wandb: 	model.gensim.alpha: 0.0004072619108261779
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 40
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.2468035208633217
wandb: 	model.gensim.vector_size: 186
wandb: 	model.gensim.window: 9
wandb: 	model.sklearn.learning_rate: 0.07687411756829805
wandb: 	model.sklearn.max_depth: 50
wandb: 	model.sklearn.min_child_weight: 0.038953263202159104
wandb: 	model.sklearn.n_estimators: 1894
wandb: 	model.sklearn.num_leaves: 415
wandb: 	model.sklearn.reg_alpha: 0.0029793409916914777
wandb: 	model.sklearn.reg_lambda: 0.9766971877704244
wandb: 	model.sklearn.subsample: 0.4124959815036743
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132725-ygmw7t4j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-sweep-42
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/ygmw7t4j
2023-02-07 13:27:33.740 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:27:33.740 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 225 for sweep.
2023-02-07 13:27:33.741 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0004072619108261779 for sweep.
2023-02-07 13:27:33.741 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:27:33.741 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 40 for sweep.
2023-02-07 13:27:33.741 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 13:27:33.741 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2468035208633217 for sweep.
2023-02-07 13:27:33.742 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 186 for sweep.
2023-02-07 13:27:33.742 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 9 for sweep.
2023-02-07 13:27:33.742 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.07687411756829805 for sweep.
2023-02-07 13:27:33.742 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 50 for sweep.
2023-02-07 13:27:33.743 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.038953263202159104 for sweep.
2023-02-07 13:27:33.743 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1894 for sweep.
2023-02-07 13:27:33.743 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 415 for sweep.
2023-02-07 13:27:33.743 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.0029793409916914777 for sweep.
2023-02-07 13:27:33.743 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.9766971877704244 for sweep.
2023-02-07 13:27:33.744 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.4124959815036743 for sweep.
2023-02-07 13:27:33.744 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:27:33.749 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132725-ygmw7t4j/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 225, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 186, 'window': 9, 'min_count': 5, 'dm': 0, 'sample': 0.2468035208633217, 'workers': 4, 'alpha': 0.0004072619108261779, 'epochs': 40}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1894, 'max_depth': 50, 'num_leaves': 415, 'reg_alpha': 0.0029793409916914777, 'reg_lambda': 0.9766971877704244, 'subsample': 0.4124959815036743, 'min_child_weight': 0.038953263202159104, 'n_jobs': 4, 'learning_rate': 0.07687411756829805}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 36/3257 [00:00<00:09, 353.42it/s]  2%|‚ñè         | 76/3257 [00:00<00:08, 379.23it/s]  4%|‚ñé         | 114/3257 [00:00<00:08, 374.23it/s]  5%|‚ñç         | 157/3257 [00:00<00:07, 393.34it/s]  6%|‚ñå         | 197/3257 [00:00<00:07, 387.19it/s]  7%|‚ñã         | 242/3257 [00:00<00:07, 407.24it/s]  9%|‚ñâ         | 285/3257 [00:00<00:07, 414.51it/s] 10%|‚ñà         | 328/3257 [00:00<00:07, 414.04it/s] 11%|‚ñà‚ñè        | 370/3257 [00:00<00:07, 409.54it/s] 13%|‚ñà‚ñé        | 411/3257 [00:01<00:07, 396.07it/s] 14%|‚ñà‚ñç        | 451/3257 [00:01<00:07, 372.39it/s] 15%|‚ñà‚ñå        | 491/3257 [00:01<00:07, 379.54it/s] 16%|‚ñà‚ñã        | 532/3257 [00:01<00:07, 385.61it/s] 18%|‚ñà‚ñä        | 571/3257 [00:01<00:07, 371.22it/s] 19%|‚ñà‚ñâ        | 614/3257 [00:01<00:06, 387.61it/s] 20%|‚ñà‚ñà        | 654/3257 [00:01<00:06, 378.37it/s] 21%|‚ñà‚ñà‚ñè       | 693/3257 [00:01<00:06, 373.11it/s] 22%|‚ñà‚ñà‚ñè       | 731/3257 [00:01<00:06, 372.61it/s] 24%|‚ñà‚ñà‚ñé       | 770/3257 [00:01<00:06, 376.37it/s] 25%|‚ñà‚ñà‚ñç       | 808/3257 [00:02<00:06, 375.05it/s] 26%|‚ñà‚ñà‚ñå       | 846/3257 [00:02<00:06, 365.26it/s] 27%|‚ñà‚ñà‚ñã       | 883/3257 [00:02<00:09, 261.41it/s] 28%|‚ñà‚ñà‚ñä       | 924/3257 [00:02<00:07, 294.42it/s] 30%|‚ñà‚ñà‚ñâ       | 964/3257 [00:02<00:07, 319.38it/s] 31%|‚ñà‚ñà‚ñà       | 1002/3257 [00:02<00:06, 333.64it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1039/3257 [00:02<00:06, 341.14it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:02<00:06, 353.84it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1119/3257 [00:03<00:05, 362.65it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1158/3257 [00:03<00:05, 370.32it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1196/3257 [00:03<00:05, 361.18it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1236/3257 [00:03<00:05, 371.61it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:03<00:05, 370.86it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1313/3257 [00:03<00:05, 370.73it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1353/3257 [00:03<00:05, 377.84it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1391/3257 [00:03<00:04, 375.91it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1435/3257 [00:03<00:04, 393.44it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1480/3257 [00:03<00:04, 409.14it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1522/3257 [00:04<00:04, 407.34it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1563/3257 [00:04<00:04, 395.12it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1604/3257 [00:04<00:04, 398.99it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1644/3257 [00:04<00:04, 391.84it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1684/3257 [00:04<00:04, 383.32it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1726/3257 [00:04<00:03, 393.19it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1766/3257 [00:04<00:03, 377.11it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1807/3257 [00:04<00:03, 380.71it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1848/3257 [00:04<00:03, 388.96it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1890/3257 [00:05<00:03, 395.85it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1930/3257 [00:05<00:03, 391.95it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:05<00:03, 414.34it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2019/3257 [00:05<00:02, 412.99it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2061/3257 [00:05<00:03, 393.10it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2101/3257 [00:05<00:02, 390.10it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2141/3257 [00:05<00:02, 382.49it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2180/3257 [00:05<00:02, 382.78it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2219/3257 [00:06<00:03, 273.39it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2258/3257 [00:06<00:03, 299.73it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2297/3257 [00:06<00:02, 321.24it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2341/3257 [00:06<00:02, 351.53it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2387/3257 [00:06<00:02, 378.88it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2428/3257 [00:06<00:02, 381.10it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2469/3257 [00:06<00:02, 387.46it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2510/3257 [00:06<00:01, 393.77it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:06<00:01, 385.61it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2591/3257 [00:06<00:01, 363.81it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2636/3257 [00:07<00:01, 386.04it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2676/3257 [00:07<00:01, 388.67it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2716/3257 [00:07<00:01, 377.29it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:07<00:01, 397.32it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2803/3257 [00:07<00:01, 403.52it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2844/3257 [00:07<00:01, 389.87it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2892/3257 [00:07<00:00, 413.15it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2934/3257 [00:07<00:00, 404.78it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2975/3257 [00:07<00:00, 388.72it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3015/3257 [00:08<00:00, 385.73it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3057/3257 [00:08<00:00, 395.30it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3097/3257 [00:08<00:00, 384.77it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3136/3257 [00:08<00:00, 375.93it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3174/3257 [00:08<00:00, 364.11it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3211/3257 [00:08<00:00, 354.19it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3254/3257 [00:08<00:00, 371.79it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 374.48it/s]
2023-02-07 13:27:42.633 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:27:42,634][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d186,n5,mc5,s0.246804,t4>', 'datetime': '2023-02-07T13:27:42.634441', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:27:42,634][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:27:42,634][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:27:42,769][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:27:42,769][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:27:42,771][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 599 unique words (64.83% of original 924, drops 325)', 'datetime': '2023-02-07T13:27:42.771106', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:27:42,771][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1454829 word corpus (99.94% of original 1455748, drops 919)', 'datetime': '2023-02-07T13:27:42.771283', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:27:42,775][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:27:42,775][gensim.models.word2vec][INFO] - sample=0.246804 downsamples 0 most-common words
[2023-02-07 13:27:42,775][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1454829 word corpus (100.0%% of prior 1454829)', 'datetime': '2023-02-07T13:27:42.775600', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:27:42,780][gensim.models.word2vec][INFO] - estimated required memory for 599 words and 186 dimensions: 4265420 bytes
[2023-02-07 13:27:42,780][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:27:42,783][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 599 vocabulary and 186 features, using sg=1 hs=0 sample=0.2468035208633217 negative=5 window=9 shrink_windows=True', 'datetime': '2023-02-07T13:27:42.783253', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:27:43,599][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458086 effective words) took 0.8s, 1789981 effective words/s
[2023-02-07 13:27:44,421][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458086 effective words) took 0.8s, 1777386 effective words/s
[2023-02-07 13:27:45,183][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458086 effective words) took 0.8s, 1917775 effective words/s
[2023-02-07 13:27:45,921][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458086 effective words) took 0.7s, 1980402 effective words/s
[2023-02-07 13:27:46,666][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458086 effective words) took 0.7s, 1960975 effective words/s
[2023-02-07 13:27:47,398][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458086 effective words) took 0.7s, 1995975 effective words/s
[2023-02-07 13:27:48,171][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458086 effective words) took 0.8s, 1889270 effective words/s
[2023-02-07 13:27:48,972][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458086 effective words) took 0.8s, 1823256 effective words/s
[2023-02-07 13:27:49,794][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458086 effective words) took 0.8s, 1777544 effective words/s
[2023-02-07 13:27:50,612][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458086 effective words) took 0.8s, 1786254 effective words/s
[2023-02-07 13:27:51,424][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458086 effective words) took 0.8s, 1798107 effective words/s
[2023-02-07 13:27:52,234][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458086 effective words) took 0.8s, 1802861 effective words/s
[2023-02-07 13:27:53,043][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458086 effective words) took 0.8s, 1805760 effective words/s
[2023-02-07 13:27:53,847][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458086 effective words) took 0.8s, 1816258 effective words/s
[2023-02-07 13:27:54,669][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458086 effective words) took 0.8s, 1777916 effective words/s
[2023-02-07 13:27:55,465][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458086 effective words) took 0.8s, 1836121 effective words/s
[2023-02-07 13:27:56,280][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458086 effective words) took 0.8s, 1792362 effective words/s
[2023-02-07 13:27:57,096][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458086 effective words) took 0.8s, 1791338 effective words/s
[2023-02-07 13:27:57,901][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458086 effective words) took 0.8s, 1815771 effective words/s
[2023-02-07 13:27:58,696][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458086 effective words) took 0.8s, 1837638 effective words/s
[2023-02-07 13:27:59,498][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458086 effective words) took 0.8s, 1821438 effective words/s
[2023-02-07 13:28:00,293][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458086 effective words) took 0.8s, 1838683 effective words/s
[2023-02-07 13:28:01,089][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458086 effective words) took 0.8s, 1835381 effective words/s
[2023-02-07 13:28:01,893][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458086 effective words) took 0.8s, 1817651 effective words/s
[2023-02-07 13:28:02,709][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458086 effective words) took 0.8s, 1789678 effective words/s
[2023-02-07 13:28:03,514][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458086 effective words) took 0.8s, 1816710 effective words/s
[2023-02-07 13:28:04,326][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458086 effective words) took 0.8s, 1800350 effective words/s
[2023-02-07 13:28:05,117][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458086 effective words) took 0.8s, 1845021 effective words/s
[2023-02-07 13:28:05,911][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458086 effective words) took 0.8s, 1840367 effective words/s
[2023-02-07 13:28:06,712][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458086 effective words) took 0.8s, 1822303 effective words/s
[2023-02-07 13:28:07,510][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458086 effective words) took 0.8s, 1831018 effective words/s
[2023-02-07 13:28:08,302][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458086 effective words) took 0.8s, 1845130 effective words/s
[2023-02-07 13:28:09,094][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458086 effective words) took 0.8s, 1843714 effective words/s
[2023-02-07 13:28:09,911][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458086 effective words) took 0.8s, 1786667 effective words/s
[2023-02-07 13:28:10,735][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458086 effective words) took 0.8s, 1772369 effective words/s
[2023-02-07 13:28:11,564][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458086 effective words) took 0.8s, 1762161 effective words/s
[2023-02-07 13:28:12,383][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458086 effective words) took 0.8s, 1786692 effective words/s
[2023-02-07 13:28:13,197][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458086 effective words) took 0.8s, 1793633 effective words/s
[2023-02-07 13:28:14,021][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458086 effective words) took 0.8s, 1773772 effective words/s
[2023-02-07 13:28:14,849][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458086 effective words) took 0.8s, 1762908 effective words/s
[2023-02-07 13:28:14,850][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 58229920 raw words (58323440 effective words) took 32.1s, 1818829 effective words/s', 'datetime': '2023-02-07T13:28:14.850080', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:28:14.850 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:28:16,467][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132725-ygmw7t4j/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:28:16.467367', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:28:16,468][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:28:16,475][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132725-ygmw7t4j/files/../tmp/embedding_model.pt
2023-02-07 13:28:16.475 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:28:17.856 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:28:18.387 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:28:19.640 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.926555052127183, 'test_mae': 1.0386561549788604, 'test_r2': 0.08862427182512089}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.35
wandb: percentage 0.35173
wandb:   test_mae 1.03866
wandb:   test_mse 1.92656
wandb:    test_r2 0.08862
wandb: 
wandb: üöÄ View run crimson-sweep-42 at: https://wandb.ai/xiaoqiz/mof2vec/runs/ygmw7t4j
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_132725-ygmw7t4j/logs
wandb: Agent Starting Run: hjj464r7 with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 435
wandb: 	model.gensim.alpha: 0.008286431112530098
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 28
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.2482263776387379
wandb: 	model.gensim.vector_size: 129
wandb: 	model.gensim.window: 4
wandb: 	model.sklearn.learning_rate: 0.3098599861413002
wandb: 	model.sklearn.max_depth: 14
wandb: 	model.sklearn.min_child_weight: 0.01611196063139996
wandb: 	model.sklearn.n_estimators: 1346
wandb: 	model.sklearn.num_leaves: 461
wandb: 	model.sklearn.reg_alpha: 0.11087591614616772
wandb: 	model.sklearn.reg_lambda: 0.022824521636638825
wandb: 	model.sklearn.subsample: 0.6578188925228079
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132829-hjj464r7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-43
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/hjj464r7
2023-02-07 13:28:37.688 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:28:37.689 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 435 for sweep.
2023-02-07 13:28:37.689 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.008286431112530098 for sweep.
2023-02-07 13:28:37.689 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:28:37.689 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 28 for sweep.
2023-02-07 13:28:37.690 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 13:28:37.690 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2482263776387379 for sweep.
2023-02-07 13:28:37.690 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 129 for sweep.
2023-02-07 13:28:37.691 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 4 for sweep.
2023-02-07 13:28:37.691 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.3098599861413002 for sweep.
2023-02-07 13:28:37.691 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 14 for sweep.
2023-02-07 13:28:37.691 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.01611196063139996 for sweep.
2023-02-07 13:28:37.691 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1346 for sweep.
2023-02-07 13:28:37.692 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 461 for sweep.
2023-02-07 13:28:37.692 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.11087591614616772 for sweep.
2023-02-07 13:28:37.692 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.022824521636638825 for sweep.
2023-02-07 13:28:37.692 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6578188925228079 for sweep.
2023-02-07 13:28:37.693 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:28:37.698 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132829-hjj464r7/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 435, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 129, 'window': 4, 'min_count': 6, 'dm': 0, 'sample': 0.2482263776387379, 'workers': 4, 'alpha': 0.008286431112530098, 'epochs': 28}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1346, 'max_depth': 14, 'num_leaves': 461, 'reg_alpha': 0.11087591614616772, 'reg_lambda': 0.022824521636638825, 'subsample': 0.6578188925228079, 'min_child_weight': 0.01611196063139996, 'n_jobs': 4, 'learning_rate': 0.3098599861413002}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 38/3257 [00:00<00:08, 378.88it/s]  2%|‚ñè         | 77/3257 [00:00<00:08, 382.53it/s]  4%|‚ñé         | 116/3257 [00:00<00:08, 373.50it/s]  5%|‚ñç         | 158/3257 [00:00<00:08, 386.91it/s]  6%|‚ñå         | 197/3257 [00:00<00:07, 382.63it/s]  7%|‚ñã         | 240/3257 [00:00<00:07, 396.87it/s]  9%|‚ñä         | 283/3257 [00:00<00:07, 406.43it/s] 10%|‚ñâ         | 324/3257 [00:00<00:07, 406.74it/s] 11%|‚ñà         | 365/3257 [00:00<00:07, 405.03it/s] 12%|‚ñà‚ñè        | 406/3257 [00:01<00:07, 395.15it/s] 14%|‚ñà‚ñé        | 446/3257 [00:01<00:09, 281.89it/s] 15%|‚ñà‚ñç        | 481/3257 [00:01<00:09, 297.14it/s] 16%|‚ñà‚ñå        | 522/3257 [00:01<00:08, 323.93it/s] 17%|‚ñà‚ñã        | 558/3257 [00:01<00:08, 329.00it/s] 18%|‚ñà‚ñä        | 594/3257 [00:01<00:07, 336.38it/s] 20%|‚ñà‚ñâ        | 636/3257 [00:01<00:07, 357.22it/s] 21%|‚ñà‚ñà        | 673/3257 [00:01<00:07, 339.73it/s] 22%|‚ñà‚ñà‚ñè       | 708/3257 [00:02<00:07, 324.53it/s] 23%|‚ñà‚ñà‚ñé       | 742/3257 [00:02<00:08, 309.89it/s] 24%|‚ñà‚ñà‚ñç       | 774/3257 [00:02<00:08, 307.48it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:02<00:08, 302.16it/s] 26%|‚ñà‚ñà‚ñå       | 837/3257 [00:02<00:08, 296.46it/s] 27%|‚ñà‚ñà‚ñã       | 867/3257 [00:02<00:08, 295.84it/s] 28%|‚ñà‚ñà‚ñä       | 897/3257 [00:02<00:08, 293.57it/s] 29%|‚ñà‚ñà‚ñä       | 929/3257 [00:02<00:07, 300.55it/s] 29%|‚ñà‚ñà‚ñâ       | 960/3257 [00:02<00:07, 302.92it/s] 30%|‚ñà‚ñà‚ñà       | 991/3257 [00:02<00:07, 296.13it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1021/3257 [00:03<00:07, 295.19it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1051/3257 [00:03<00:07, 283.54it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1081/3257 [00:03<00:07, 287.51it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1111/3257 [00:03<00:07, 290.29it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1141/3257 [00:03<00:07, 288.96it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:03<00:07, 291.33it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1201/3257 [00:03<00:07, 277.89it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1234/3257 [00:03<00:06, 292.00it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1264/3257 [00:03<00:06, 290.27it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1294/3257 [00:04<00:06, 283.41it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1324/3257 [00:04<00:06, 288.01it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1353/3257 [00:04<00:06, 287.06it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1382/3257 [00:04<00:06, 281.78it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1412/3257 [00:04<00:06, 286.90it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1443/3257 [00:04<00:06, 291.02it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1475/3257 [00:04<00:05, 297.66it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:04<00:05, 319.89it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1546/3257 [00:04<00:05, 314.64it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1581/3257 [00:04<00:05, 321.83it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1617/3257 [00:05<00:04, 330.10it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1651/3257 [00:05<00:05, 320.42it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1684/3257 [00:05<00:05, 310.41it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1716/3257 [00:05<00:05, 307.33it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1747/3257 [00:05<00:04, 304.48it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1779/3257 [00:05<00:04, 307.59it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1813/3257 [00:05<00:04, 315.56it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1845/3257 [00:05<00:06, 228.44it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1881/3257 [00:06<00:05, 257.33it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:06<00:04, 279.89it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1955/3257 [00:06<00:04, 307.61it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1990/3257 [00:06<00:03, 317.45it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2029/3257 [00:06<00:03, 336.96it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2065/3257 [00:06<00:03, 327.54it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2100/3257 [00:06<00:03, 333.54it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2135/3257 [00:06<00:03, 332.51it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:06<00:03, 332.78it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:06<00:03, 340.33it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2241/3257 [00:07<00:03, 331.87it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2275/3257 [00:07<00:02, 329.70it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2314/3257 [00:07<00:02, 345.73it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2356/3257 [00:07<00:02, 365.28it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2395/3257 [00:07<00:02, 370.74it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2433/3257 [00:07<00:02, 351.48it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2469/3257 [00:07<00:02, 349.93it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2506/3257 [00:07<00:02, 355.40it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2542/3257 [00:07<00:02, 356.29it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2578/3257 [00:08<00:01, 348.73it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2616/3257 [00:08<00:01, 357.30it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2652/3257 [00:08<00:01, 358.04it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2690/3257 [00:08<00:01, 364.33it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2727/3257 [00:08<00:01, 364.16it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2769/3257 [00:08<00:01, 380.26it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2812/3257 [00:08<00:01, 394.11it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2852/3257 [00:08<00:01, 387.87it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2896/3257 [00:08<00:00, 399.42it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2940/3257 [00:08<00:00, 408.72it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2981/3257 [00:09<00:00, 397.43it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3025/3257 [00:09<00:00, 409.56it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3070/3257 [00:09<00:00, 420.11it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3113/3257 [00:09<00:00, 422.80it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3156/3257 [00:09<00:00, 409.05it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3198/3257 [00:09<00:00, 407.90it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3239/3257 [00:09<00:00, 404.00it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 334.16it/s]
2023-02-07 13:28:47.631 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:28:47,632][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d129,n5,mc6,s0.248226,t4>', 'datetime': '2023-02-07T13:28:47.632464', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:28:47,632][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:28:47,632][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:28:47,766][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:28:47,767][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:28:47,768][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 585 unique words (63.31% of original 924, drops 339)', 'datetime': '2023-02-07T13:28:47.768706', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:28:47,768][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 1454759 word corpus (99.93% of original 1455748, drops 989)', 'datetime': '2023-02-07T13:28:47.768866', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:28:47,771][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:28:47,771][gensim.models.word2vec][INFO] - sample=0.248226 downsamples 0 most-common words
[2023-02-07 13:28:47,771][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1454759 word corpus (100.0%% of prior 1454759)', 'datetime': '2023-02-07T13:28:47.771609', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:28:47,775][gensim.models.word2vec][INFO] - estimated required memory for 585 words and 129 dimensions: 3228232 bytes
[2023-02-07 13:28:47,775][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:28:47,777][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 585 vocabulary and 129 features, using sg=1 hs=0 sample=0.2482263776387379 negative=5 window=4 shrink_windows=True', 'datetime': '2023-02-07T13:28:47.777285', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:28:48,280][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458016 effective words) took 0.5s, 2905207 effective words/s
[2023-02-07 13:28:48,655][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458016 effective words) took 0.4s, 3906077 effective words/s
[2023-02-07 13:28:49,022][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458016 effective words) took 0.4s, 3989214 effective words/s
[2023-02-07 13:28:49,404][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458016 effective words) took 0.4s, 3822117 effective words/s
[2023-02-07 13:28:49,793][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458016 effective words) took 0.4s, 3767698 effective words/s
[2023-02-07 13:28:50,181][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458016 effective words) took 0.4s, 3769332 effective words/s
[2023-02-07 13:28:50,574][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458016 effective words) took 0.4s, 3725569 effective words/s
[2023-02-07 13:28:50,965][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458016 effective words) took 0.4s, 3737821 effective words/s
[2023-02-07 13:28:51,356][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458016 effective words) took 0.4s, 3743351 effective words/s
[2023-02-07 13:28:51,758][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458016 effective words) took 0.4s, 3642828 effective words/s
[2023-02-07 13:28:52,158][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458016 effective words) took 0.4s, 3654115 effective words/s
[2023-02-07 13:28:52,557][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458016 effective words) took 0.4s, 3662932 effective words/s
[2023-02-07 13:28:52,962][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458016 effective words) took 0.4s, 3617907 effective words/s
[2023-02-07 13:28:53,361][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458016 effective words) took 0.4s, 3662527 effective words/s
[2023-02-07 13:28:53,756][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458016 effective words) took 0.4s, 3706162 effective words/s
[2023-02-07 13:28:54,151][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458016 effective words) took 0.4s, 3698835 effective words/s
[2023-02-07 13:28:54,538][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458016 effective words) took 0.4s, 3787315 effective words/s
[2023-02-07 13:28:54,924][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458016 effective words) took 0.4s, 3786112 effective words/s
[2023-02-07 13:28:55,317][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458016 effective words) took 0.4s, 3736261 effective words/s
[2023-02-07 13:28:55,712][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458016 effective words) took 0.4s, 3696278 effective words/s
[2023-02-07 13:28:56,106][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458016 effective words) took 0.4s, 3717197 effective words/s
[2023-02-07 13:28:56,504][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458016 effective words) took 0.4s, 3671544 effective words/s
[2023-02-07 13:28:56,901][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458016 effective words) took 0.4s, 3688785 effective words/s
[2023-02-07 13:28:57,298][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458016 effective words) took 0.4s, 3683060 effective words/s
[2023-02-07 13:28:57,702][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458016 effective words) took 0.4s, 3622816 effective words/s
[2023-02-07 13:28:58,096][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458016 effective words) took 0.4s, 3705166 effective words/s
[2023-02-07 13:28:58,499][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458016 effective words) took 0.4s, 3635560 effective words/s
[2023-02-07 13:28:58,894][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458016 effective words) took 0.4s, 3705934 effective words/s
[2023-02-07 13:28:58,894][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 40760944 raw words (40824448 effective words) took 11.1s, 3672289 effective words/s', 'datetime': '2023-02-07T13:28:58.894371', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:28:58.894 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:28:59,629][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132829-hjj464r7/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:28:59.629492', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:28:59,631][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:28:59,636][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132829-hjj464r7/files/../tmp/embedding_model.pt
2023-02-07 13:28:59.637 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:29:00.773 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:29:01.225 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:29:02.147 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.886228437415978, 'test_mae': 1.0387670788114056, 'test_r2': 0.10770117170746396}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.7
wandb: percentage 0.36688
wandb:   test_mae 1.03877
wandb:   test_mse 1.88623
wandb:    test_r2 0.1077
wandb: 
wandb: üöÄ View run ruby-sweep-43 at: https://wandb.ai/xiaoqiz/mof2vec/runs/hjj464r7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_132829-hjj464r7/logs
wandb: Agent Starting Run: a813ddrb with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 853
wandb: 	model.gensim.alpha: 0.0003407897935471895
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 27
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.5395607014973709
wandb: 	model.gensim.vector_size: 141
wandb: 	model.gensim.window: 2
wandb: 	model.sklearn.learning_rate: 0.3686331626818018
wandb: 	model.sklearn.max_depth: 44
wandb: 	model.sklearn.min_child_weight: 0.02668494796137443
wandb: 	model.sklearn.n_estimators: 2203
wandb: 	model.sklearn.num_leaves: 213
wandb: 	model.sklearn.reg_alpha: 0.46116879179944387
wandb: 	model.sklearn.reg_lambda: 0.0028055898119680013
wandb: 	model.sklearn.subsample: 0.2915208144880337
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132913-a813ddrb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-44
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/a813ddrb
2023-02-07 13:29:21.502 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:29:21.503 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 853 for sweep.
2023-02-07 13:29:21.503 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0003407897935471895 for sweep.
2023-02-07 13:29:21.504 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:29:21.504 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 27 for sweep.
2023-02-07 13:29:21.504 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 13:29:21.504 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5395607014973709 for sweep.
2023-02-07 13:29:21.504 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 141 for sweep.
2023-02-07 13:29:21.505 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 2 for sweep.
2023-02-07 13:29:21.505 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.3686331626818018 for sweep.
2023-02-07 13:29:21.505 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 44 for sweep.
2023-02-07 13:29:21.505 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.02668494796137443 for sweep.
2023-02-07 13:29:21.505 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2203 for sweep.
2023-02-07 13:29:21.506 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 213 for sweep.
2023-02-07 13:29:21.506 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.46116879179944387 for sweep.
2023-02-07 13:29:21.506 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.0028055898119680013 for sweep.
2023-02-07 13:29:21.506 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2915208144880337 for sweep.
2023-02-07 13:29:21.507 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:29:21.515 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132913-a813ddrb/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 853, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 141, 'window': 2, 'min_count': 5, 'dm': 0, 'sample': 0.5395607014973709, 'workers': 4, 'alpha': 0.0003407897935471895, 'epochs': 27}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2203, 'max_depth': 44, 'num_leaves': 213, 'reg_alpha': 0.46116879179944387, 'reg_lambda': 0.0028055898119680013, 'subsample': 0.2915208144880337, 'min_child_weight': 0.02668494796137443, 'n_jobs': 4, 'learning_rate': 0.3686331626818018}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 27/3257 [00:00<00:25, 128.32it/s]  2%|‚ñè         | 61/3257 [00:00<00:15, 212.14it/s]  3%|‚ñé         | 95/3257 [00:00<00:12, 256.81it/s]  4%|‚ñç         | 130/3257 [00:00<00:10, 288.10it/s]  5%|‚ñå         | 167/3257 [00:00<00:09, 312.64it/s]  6%|‚ñå         | 203/3257 [00:00<00:09, 326.37it/s]  8%|‚ñä         | 246/3257 [00:00<00:08, 358.16it/s]  9%|‚ñâ         | 288/3257 [00:00<00:07, 376.19it/s] 10%|‚ñà         | 331/3257 [00:01<00:07, 389.38it/s] 11%|‚ñà‚ñè        | 373/3257 [00:01<00:07, 395.70it/s] 13%|‚ñà‚ñé        | 413/3257 [00:01<00:07, 391.13it/s] 14%|‚ñà‚ñç        | 453/3257 [00:01<00:07, 375.02it/s] 15%|‚ñà‚ñå        | 494/3257 [00:01<00:07, 384.02it/s] 16%|‚ñà‚ñã        | 536/3257 [00:01<00:06, 392.20it/s] 18%|‚ñà‚ñä        | 576/3257 [00:01<00:06, 383.31it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:01<00:06, 393.08it/s] 20%|‚ñà‚ñà        | 660/3257 [00:01<00:06, 389.43it/s] 21%|‚ñà‚ñà‚ñè       | 700/3257 [00:01<00:06, 390.35it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:02<00:06, 389.77it/s] 24%|‚ñà‚ñà‚ñç       | 780/3257 [00:02<00:06, 386.96it/s] 25%|‚ñà‚ñà‚ñå       | 821/3257 [00:02<00:06, 389.62it/s] 26%|‚ñà‚ñà‚ñã       | 860/3257 [00:02<00:06, 370.66it/s] 28%|‚ñà‚ñà‚ñä       | 898/3257 [00:02<00:06, 362.05it/s] 29%|‚ñà‚ñà‚ñä       | 935/3257 [00:02<00:06, 361.24it/s] 30%|‚ñà‚ñà‚ñâ       | 975/3257 [00:02<00:06, 370.60it/s] 31%|‚ñà‚ñà‚ñà       | 1013/3257 [00:02<00:06, 354.79it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1049/3257 [00:02<00:06, 350.71it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1085/3257 [00:03<00:06, 348.24it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1120/3257 [00:03<00:06, 344.31it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1156/3257 [00:03<00:06, 347.93it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:03<00:06, 333.53it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1227/3257 [00:03<00:05, 339.53it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1262/3257 [00:03<00:05, 332.63it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1296/3257 [00:03<00:08, 236.62it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1329/3257 [00:03<00:07, 256.79it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1364/3257 [00:04<00:06, 278.02it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1398/3257 [00:04<00:06, 292.10it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1433/3257 [00:04<00:05, 306.39it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1467/3257 [00:04<00:05, 313.92it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1502/3257 [00:04<00:05, 321.81it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1536/3257 [00:04<00:05, 304.78it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1568/3257 [00:04<00:05, 305.27it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1602/3257 [00:04<00:05, 312.81it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:04<00:05, 309.64it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1666/3257 [00:04<00:05, 310.38it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1698/3257 [00:05<00:05, 311.37it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1730/3257 [00:05<00:05, 302.52it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1762/3257 [00:05<00:04, 306.01it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1796/3257 [00:05<00:04, 312.95it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1828/3257 [00:05<00:04, 310.02it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1866/3257 [00:05<00:04, 329.85it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1900/3257 [00:05<00:04, 329.14it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1934/3257 [00:05<00:03, 331.53it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1971/3257 [00:05<00:03, 341.03it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2006/3257 [00:06<00:03, 339.48it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2042/3257 [00:06<00:03, 345.09it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2077/3257 [00:06<00:03, 345.65it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2113/3257 [00:06<00:03, 349.05it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2148/3257 [00:06<00:03, 338.29it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2186/3257 [00:06<00:03, 348.61it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2225/3257 [00:06<00:02, 359.41it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2268/3257 [00:06<00:02, 377.86it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2311/3257 [00:06<00:02, 391.53it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2362/3257 [00:06<00:02, 425.98it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2407/3257 [00:07<00:01, 432.06it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2451/3257 [00:07<00:01, 423.82it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2499/3257 [00:07<00:01, 438.62it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2543/3257 [00:07<00:01, 436.38it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2587/3257 [00:07<00:01, 415.09it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2629/3257 [00:07<00:02, 304.76it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:07<00:01, 328.12it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2708/3257 [00:07<00:01, 340.76it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2752/3257 [00:07<00:01, 365.23it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2791/3257 [00:08<00:01, 355.52it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2829/3257 [00:08<00:01, 337.62it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2865/3257 [00:08<00:01, 339.10it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2900/3257 [00:08<00:01, 330.15it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2934/3257 [00:08<00:00, 324.62it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2967/3257 [00:08<00:00, 315.25it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3001/3257 [00:08<00:00, 320.92it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3034/3257 [00:08<00:00, 314.77it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3069/3257 [00:08<00:00, 324.33it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3102/3257 [00:09<00:00, 322.83it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3135/3257 [00:09<00:00, 314.73it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3172/3257 [00:09<00:00, 329.91it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3208/3257 [00:09<00:00, 334.56it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3245/3257 [00:09<00:00, 343.52it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 341.36it/s]
2023-02-07 13:29:31.239 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:29:31,241][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d141,n5,mc5,s0.539561,t4>', 'datetime': '2023-02-07T13:29:31.241112', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:29:31,241][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:29:31,241][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:29:31,374][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:29:31,374][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:29:31,376][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 599 unique words (64.83% of original 924, drops 325)', 'datetime': '2023-02-07T13:29:31.376005', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:29:31,376][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1454829 word corpus (99.94% of original 1455748, drops 919)', 'datetime': '2023-02-07T13:29:31.376163', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:29:31,378][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:29:31,378][gensim.models.word2vec][INFO] - sample=0.539561 downsamples 0 most-common words
[2023-02-07 13:29:31,378][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1454829 word corpus (100.0%% of prior 1454829)', 'datetime': '2023-02-07T13:29:31.378421', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:29:31,382][gensim.models.word2vec][INFO] - estimated required memory for 599 words and 141 dimensions: 3463520 bytes
[2023-02-07 13:29:31,382][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:29:31,384][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 599 vocabulary and 141 features, using sg=1 hs=0 sample=0.5395607014973709 negative=5 window=2 shrink_windows=True', 'datetime': '2023-02-07T13:29:31.384368', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:29:32,070][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458086 effective words) took 0.7s, 2131539 effective words/s
[2023-02-07 13:29:32,753][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458086 effective words) took 0.7s, 2137516 effective words/s
[2023-02-07 13:29:33,443][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458086 effective words) took 0.7s, 2116014 effective words/s
[2023-02-07 13:29:34,125][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458086 effective words) took 0.7s, 2143404 effective words/s
[2023-02-07 13:29:34,804][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458086 effective words) took 0.7s, 2155591 effective words/s
[2023-02-07 13:29:35,484][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458086 effective words) took 0.7s, 2148258 effective words/s
[2023-02-07 13:29:36,155][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458086 effective words) took 0.7s, 2181971 effective words/s
[2023-02-07 13:29:36,831][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458086 effective words) took 0.7s, 2160093 effective words/s
[2023-02-07 13:29:37,503][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458086 effective words) took 0.7s, 2173009 effective words/s
[2023-02-07 13:29:38,173][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458086 effective words) took 0.7s, 2182631 effective words/s
[2023-02-07 13:29:38,847][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458086 effective words) took 0.7s, 2165691 effective words/s
[2023-02-07 13:29:39,523][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458086 effective words) took 0.7s, 2160874 effective words/s
[2023-02-07 13:29:40,200][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458086 effective words) took 0.7s, 2159428 effective words/s
[2023-02-07 13:29:40,877][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458086 effective words) took 0.7s, 2158967 effective words/s
[2023-02-07 13:29:41,554][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458086 effective words) took 0.7s, 2158270 effective words/s
[2023-02-07 13:29:42,220][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458086 effective words) took 0.7s, 2190924 effective words/s
[2023-02-07 13:29:42,894][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458086 effective words) took 0.7s, 2168698 effective words/s
[2023-02-07 13:29:43,536][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458086 effective words) took 0.6s, 2277190 effective words/s
[2023-02-07 13:29:44,199][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458086 effective words) took 0.7s, 2203883 effective words/s
[2023-02-07 13:29:44,855][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458086 effective words) took 0.7s, 2227760 effective words/s
[2023-02-07 13:29:45,517][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458086 effective words) took 0.7s, 2207943 effective words/s
[2023-02-07 13:29:46,176][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458086 effective words) took 0.7s, 2218210 effective words/s
[2023-02-07 13:29:46,828][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458086 effective words) took 0.7s, 2238419 effective words/s
[2023-02-07 13:29:47,493][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458086 effective words) took 0.7s, 2198915 effective words/s
[2023-02-07 13:29:48,146][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458086 effective words) took 0.7s, 2242335 effective words/s
[2023-02-07 13:29:48,807][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458086 effective words) took 0.7s, 2211428 effective words/s
[2023-02-07 13:29:49,470][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458086 effective words) took 0.7s, 2204375 effective words/s
[2023-02-07 13:29:49,470][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 39305196 raw words (39368322 effective words) took 18.1s, 2176693 effective words/s', 'datetime': '2023-02-07T13:29:49.470857', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:29:49.471 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:29:50,362][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132913-a813ddrb/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:29:50.362842', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:29:50,364][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:29:50,368][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_132913-a813ddrb/files/../tmp/embedding_model.pt
2023-02-07 13:29:50.368 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:29:51.528 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:29:51.986 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:29:52.917 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1075512328790533, 'test_mae': 1.1103149904338128, 'test_r2': 0.003002256587365748}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.27
wandb: percentage 0.35173
wandb:   test_mae 1.11031
wandb:   test_mse 2.10755
wandb:    test_r2 0.003
wandb: 
wandb: üöÄ View run celestial-sweep-44 at: https://wandb.ai/xiaoqiz/mof2vec/runs/a813ddrb
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_132913-a813ddrb/logs
wandb: Agent Starting Run: yn8cluu7 with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 652
wandb: 	model.gensim.alpha: 0.01343158234286868
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 90
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.4415681610365531
wandb: 	model.gensim.vector_size: 79
wandb: 	model.gensim.window: 7
wandb: 	model.sklearn.learning_rate: 0.12594153576748252
wandb: 	model.sklearn.max_depth: 49
wandb: 	model.sklearn.min_child_weight: 0.038119003669590215
wandb: 	model.sklearn.n_estimators: 3261
wandb: 	model.sklearn.num_leaves: 458
wandb: 	model.sklearn.reg_alpha: 0.010779347821900253
wandb: 	model.sklearn.reg_lambda: 0.6817086492892097
wandb: 	model.sklearn.subsample: 0.4347818054646702
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133006-yn8cluu7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-45
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/yn8cluu7
2023-02-07 13:30:13.275 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 13:30:13.276 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 652 for sweep.
2023-02-07 13:30:13.276 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.01343158234286868 for sweep.
2023-02-07 13:30:13.276 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:30:13.277 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 90 for sweep.
2023-02-07 13:30:13.277 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 13:30:13.277 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4415681610365531 for sweep.
2023-02-07 13:30:13.277 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 79 for sweep.
2023-02-07 13:30:13.278 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 7 for sweep.
2023-02-07 13:30:13.278 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.12594153576748252 for sweep.
2023-02-07 13:30:13.278 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 49 for sweep.
2023-02-07 13:30:13.278 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.038119003669590215 for sweep.
2023-02-07 13:30:13.279 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3261 for sweep.
2023-02-07 13:30:13.279 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 458 for sweep.
2023-02-07 13:30:13.279 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.010779347821900253 for sweep.
2023-02-07 13:30:13.279 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.6817086492892097 for sweep.
2023-02-07 13:30:13.280 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.4347818054646702 for sweep.
2023-02-07 13:30:13.280 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:30:13.284 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133006-yn8cluu7/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 652, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 79, 'window': 7, 'min_count': 4, 'dm': 0, 'sample': 0.4415681610365531, 'workers': 4, 'alpha': 0.01343158234286868, 'epochs': 90}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3261, 'max_depth': 49, 'num_leaves': 458, 'reg_alpha': 0.010779347821900253, 'reg_lambda': 0.6817086492892097, 'subsample': 0.4347818054646702, 'min_child_weight': 0.038119003669590215, 'n_jobs': 4, 'learning_rate': 0.12594153576748252}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 27/3257 [00:00<00:12, 266.30it/s]  2%|‚ñè         | 54/3257 [00:00<00:12, 264.01it/s]  3%|‚ñé         | 85/3257 [00:00<00:11, 283.52it/s]  4%|‚ñé         | 114/3257 [00:00<00:11, 270.50it/s]  4%|‚ñç         | 146/3257 [00:00<00:10, 284.55it/s]  5%|‚ñå         | 175/3257 [00:00<00:11, 273.66it/s]  6%|‚ñã         | 205/3257 [00:00<00:10, 280.54it/s]  7%|‚ñã         | 238/3257 [00:00<00:10, 295.58it/s]  8%|‚ñä         | 268/3257 [00:00<00:10, 290.25it/s]  9%|‚ñâ         | 301/3257 [00:01<00:09, 301.28it/s] 10%|‚ñà         | 332/3257 [00:01<00:09, 301.44it/s] 11%|‚ñà         | 363/3257 [00:01<00:09, 296.71it/s] 12%|‚ñà‚ñè        | 393/3257 [00:01<00:10, 278.34it/s] 13%|‚ñà‚ñé        | 422/3257 [00:01<00:10, 279.49it/s] 14%|‚ñà‚ñç        | 451/3257 [00:01<00:10, 255.33it/s] 15%|‚ñà‚ñç        | 479/3257 [00:01<00:10, 261.10it/s] 16%|‚ñà‚ñå        | 512/3257 [00:01<00:09, 278.28it/s] 17%|‚ñà‚ñã        | 541/3257 [00:01<00:09, 281.25it/s] 18%|‚ñà‚ñä        | 570/3257 [00:02<00:10, 266.39it/s] 18%|‚ñà‚ñä        | 597/3257 [00:02<00:10, 264.43it/s] 19%|‚ñà‚ñâ        | 624/3257 [00:02<00:13, 192.72it/s] 20%|‚ñà‚ñâ        | 651/3257 [00:02<00:12, 209.21it/s] 21%|‚ñà‚ñà        | 677/3257 [00:02<00:11, 220.85it/s] 22%|‚ñà‚ñà‚ñè       | 703/3257 [00:02<00:11, 228.67it/s] 22%|‚ñà‚ñà‚ñè       | 729/3257 [00:02<00:10, 235.29it/s] 23%|‚ñà‚ñà‚ñé       | 754/3257 [00:02<00:10, 239.19it/s] 24%|‚ñà‚ñà‚ñç       | 780/3257 [00:02<00:10, 244.69it/s] 25%|‚ñà‚ñà‚ñç       | 807/3257 [00:03<00:09, 251.00it/s] 26%|‚ñà‚ñà‚ñå       | 833/3257 [00:03<00:09, 250.74it/s] 26%|‚ñà‚ñà‚ñã       | 859/3257 [00:03<00:09, 244.06it/s] 27%|‚ñà‚ñà‚ñã       | 884/3257 [00:03<00:09, 240.03it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:03<00:09, 253.15it/s] 29%|‚ñà‚ñà‚ñâ       | 942/3257 [00:03<00:08, 260.44it/s] 30%|‚ñà‚ñà‚ñâ       | 970/3257 [00:03<00:08, 264.75it/s] 31%|‚ñà‚ñà‚ñà       | 997/3257 [00:03<00:08, 258.05it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1025/3257 [00:03<00:08, 258.87it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1051/3257 [00:04<00:08, 245.97it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:04<00:08, 252.33it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1108/3257 [00:04<00:08, 258.76it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1134/3257 [00:04<00:08, 256.98it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1162/3257 [00:04<00:07, 262.01it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1189/3257 [00:04<00:08, 242.15it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1214/3257 [00:04<00:08, 243.65it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1245/3257 [00:04<00:07, 259.06it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1272/3257 [00:04<00:07, 262.10it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1299/3257 [00:05<00:07, 250.67it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:05<00:07, 255.64it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1354/3257 [00:05<00:07, 256.44it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1381/3257 [00:05<00:07, 255.03it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1412/3257 [00:05<00:06, 269.65it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1440/3257 [00:05<00:06, 271.38it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1474/3257 [00:05<00:06, 289.42it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1506/3257 [00:05<00:05, 298.19it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1536/3257 [00:05<00:06, 273.43it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1564/3257 [00:06<00:06, 258.64it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1591/3257 [00:06<00:06, 253.59it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:06<00:06, 256.54it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1645/3257 [00:06<00:06, 245.93it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1670/3257 [00:06<00:06, 233.87it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1694/3257 [00:06<00:06, 228.82it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1723/3257 [00:06<00:06, 242.42it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1748/3257 [00:06<00:06, 232.56it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1772/3257 [00:07<00:09, 158.96it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1796/3257 [00:07<00:08, 174.54it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1823/3257 [00:07<00:07, 195.06it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1849/3257 [00:07<00:06, 209.71it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1879/3257 [00:07<00:05, 232.59it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1909/3257 [00:07<00:05, 250.06it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1937/3257 [00:07<00:05, 257.92it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1973/3257 [00:07<00:04, 284.56it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2003/3257 [00:07<00:04, 278.69it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2033/3257 [00:08<00:04, 282.76it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2062/3257 [00:08<00:04, 261.49it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2093/3257 [00:08<00:04, 272.46it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2121/3257 [00:08<00:04, 256.85it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2148/3257 [00:08<00:04, 255.44it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:08<00:04, 265.22it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:08<00:03, 268.50it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2235/3257 [00:08<00:03, 272.55it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2263/3257 [00:08<00:03, 266.30it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2291/3257 [00:09<00:03, 265.60it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2324/3257 [00:09<00:03, 283.28it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:09<00:03, 296.71it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2393/3257 [00:09<00:02, 305.14it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2424/3257 [00:09<00:02, 295.12it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2454/3257 [00:09<00:02, 276.63it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2486/3257 [00:09<00:02, 288.17it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2520/3257 [00:09<00:02, 302.77it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:09<00:02, 301.63it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2582/3257 [00:09<00:02, 289.69it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2617/3257 [00:10<00:02, 305.11it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2648/3257 [00:10<00:02, 299.94it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2679/3257 [00:10<00:01, 294.66it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2709/3257 [00:10<00:02, 269.64it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2740/3257 [00:10<00:01, 279.84it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2769/3257 [00:10<00:01, 273.32it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2803/3257 [00:10<00:01, 287.94it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2833/3257 [00:10<00:01, 271.98it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2866/3257 [00:10<00:01, 285.45it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2895/3257 [00:11<00:01, 278.40it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2926/3257 [00:11<00:01, 285.84it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2955/3257 [00:11<00:01, 266.51it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2982/3257 [00:11<00:01, 261.37it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3014/3257 [00:11<00:00, 275.88it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3046/3257 [00:11<00:00, 286.02it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:11<00:00, 297.50it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3110/3257 [00:11<00:00, 298.05it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3140/3257 [00:11<00:00, 291.98it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3170/3257 [00:12<00:00, 283.70it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3199/3257 [00:12<00:00, 279.98it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3228/3257 [00:12<00:00, 274.60it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:12<00:00, 263.40it/s]
2023-02-07 13:30:26.080 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:30:26,081][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d79,n5,mc4,s0.441568,t4>', 'datetime': '2023-02-07T13:30:26.081870', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:30:26,082][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:30:26,083][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:30:26,400][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 13:30:26,401][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:30:26,424][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 8978 unique words (68.74% of original 13061, drops 4083)', 'datetime': '2023-02-07T13:30:26.424032', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:30:26,424][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 3632716 word corpus (99.82% of original 3639370, drops 6654)', 'datetime': '2023-02-07T13:30:26.424353', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:30:26,453][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 13:30:26,454][gensim.models.word2vec][INFO] - sample=0.441568 downsamples 0 most-common words
[2023-02-07 13:30:26,454][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3632716 word corpus (100.0%% of prior 3632716)', 'datetime': '2023-02-07T13:30:26.454327', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:30:26,504][gensim.models.word2vec][INFO] - estimated required memory for 8978 words and 79 dimensions: 11843708 bytes
[2023-02-07 13:30:26,504][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:30:26,508][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 8978 vocabulary and 79 features, using sg=1 hs=0 sample=0.4415681610365531 negative=5 window=7 shrink_windows=True', 'datetime': '2023-02-07T13:30:26.508709', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:30:27,510][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 100.00% examples, 3635411 words/s, in_qsize 0, out_qsize 1
[2023-02-07 13:30:27,510][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3635973 effective words) took 1.0s, 3634343 effective words/s
[2023-02-07 13:30:28,423][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3635973 effective words) took 0.9s, 3987160 effective words/s
[2023-02-07 13:30:29,388][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3635973 effective words) took 1.0s, 3775549 effective words/s
[2023-02-07 13:30:30,351][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3635973 effective words) took 1.0s, 3777595 effective words/s
[2023-02-07 13:30:31,286][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3635973 effective words) took 0.9s, 3894055 effective words/s
[2023-02-07 13:30:32,217][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3635973 effective words) took 0.9s, 3913731 effective words/s
[2023-02-07 13:30:33,129][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3635973 effective words) took 0.9s, 3991274 effective words/s
[2023-02-07 13:30:34,021][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3635973 effective words) took 0.9s, 4078755 effective words/s
[2023-02-07 13:30:34,919][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3635973 effective words) took 0.9s, 4054738 effective words/s
[2023-02-07 13:30:35,818][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3635973 effective words) took 0.9s, 4051739 effective words/s
[2023-02-07 13:30:36,712][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3635973 effective words) took 0.9s, 4073856 effective words/s
[2023-02-07 13:30:37,579][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3635973 effective words) took 0.9s, 4201484 effective words/s
[2023-02-07 13:30:38,449][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3635973 effective words) took 0.9s, 4188720 effective words/s
[2023-02-07 13:30:39,317][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3635973 effective words) took 0.9s, 4195431 effective words/s
[2023-02-07 13:30:40,187][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3635973 effective words) took 0.9s, 4185364 effective words/s
[2023-02-07 13:30:41,065][gensim.models.word2vec][INFO] - EPOCH 15: training on 3639370 raw words (3635973 effective words) took 0.9s, 4147988 effective words/s
[2023-02-07 13:30:41,926][gensim.models.word2vec][INFO] - EPOCH 16: training on 3639370 raw words (3635973 effective words) took 0.9s, 4229204 effective words/s
[2023-02-07 13:30:42,770][gensim.models.word2vec][INFO] - EPOCH 17: training on 3639370 raw words (3635973 effective words) took 0.8s, 4312669 effective words/s
[2023-02-07 13:30:43,621][gensim.models.word2vec][INFO] - EPOCH 18: training on 3639370 raw words (3635973 effective words) took 0.8s, 4279740 effective words/s
[2023-02-07 13:30:44,483][gensim.models.word2vec][INFO] - EPOCH 19: training on 3639370 raw words (3635973 effective words) took 0.9s, 4223326 effective words/s
[2023-02-07 13:30:45,342][gensim.models.word2vec][INFO] - EPOCH 20: training on 3639370 raw words (3635973 effective words) took 0.9s, 4239903 effective words/s
[2023-02-07 13:30:46,189][gensim.models.word2vec][INFO] - EPOCH 21: training on 3639370 raw words (3635973 effective words) took 0.8s, 4299226 effective words/s
[2023-02-07 13:30:47,039][gensim.models.word2vec][INFO] - EPOCH 22: training on 3639370 raw words (3635973 effective words) took 0.8s, 4283862 effective words/s
[2023-02-07 13:30:47,877][gensim.models.word2vec][INFO] - EPOCH 23: training on 3639370 raw words (3635973 effective words) took 0.8s, 4348183 effective words/s
[2023-02-07 13:30:48,720][gensim.models.word2vec][INFO] - EPOCH 24: training on 3639370 raw words (3635973 effective words) took 0.8s, 4316083 effective words/s
[2023-02-07 13:30:49,563][gensim.models.word2vec][INFO] - EPOCH 25: training on 3639370 raw words (3635973 effective words) took 0.8s, 4318531 effective words/s
[2023-02-07 13:30:50,422][gensim.models.word2vec][INFO] - EPOCH 26: training on 3639370 raw words (3635973 effective words) took 0.9s, 4237821 effective words/s
[2023-02-07 13:30:51,278][gensim.models.word2vec][INFO] - EPOCH 27: training on 3639370 raw words (3635973 effective words) took 0.9s, 4257275 effective words/s
[2023-02-07 13:30:52,135][gensim.models.word2vec][INFO] - EPOCH 28: training on 3639370 raw words (3635973 effective words) took 0.9s, 4256065 effective words/s
[2023-02-07 13:30:52,979][gensim.models.word2vec][INFO] - EPOCH 29: training on 3639370 raw words (3635973 effective words) took 0.8s, 4314171 effective words/s
[2023-02-07 13:30:53,827][gensim.models.word2vec][INFO] - EPOCH 30: training on 3639370 raw words (3635973 effective words) took 0.8s, 4293247 effective words/s
[2023-02-07 13:30:54,685][gensim.models.word2vec][INFO] - EPOCH 31: training on 3639370 raw words (3635973 effective words) took 0.9s, 4247990 effective words/s
[2023-02-07 13:30:55,525][gensim.models.word2vec][INFO] - EPOCH 32: training on 3639370 raw words (3635973 effective words) took 0.8s, 4334301 effective words/s
[2023-02-07 13:30:56,374][gensim.models.word2vec][INFO] - EPOCH 33: training on 3639370 raw words (3635973 effective words) took 0.8s, 4285470 effective words/s
[2023-02-07 13:30:57,221][gensim.models.word2vec][INFO] - EPOCH 34: training on 3639370 raw words (3635973 effective words) took 0.8s, 4301831 effective words/s
[2023-02-07 13:30:58,061][gensim.models.word2vec][INFO] - EPOCH 35: training on 3639370 raw words (3635973 effective words) took 0.8s, 4331331 effective words/s
[2023-02-07 13:30:58,911][gensim.models.word2vec][INFO] - EPOCH 36: training on 3639370 raw words (3635973 effective words) took 0.8s, 4291648 effective words/s
[2023-02-07 13:30:59,764][gensim.models.word2vec][INFO] - EPOCH 37: training on 3639370 raw words (3635973 effective words) took 0.9s, 4266648 effective words/s
[2023-02-07 13:31:00,624][gensim.models.word2vec][INFO] - EPOCH 38: training on 3639370 raw words (3635973 effective words) took 0.9s, 4234629 effective words/s
[2023-02-07 13:31:01,473][gensim.models.word2vec][INFO] - EPOCH 39: training on 3639370 raw words (3635973 effective words) took 0.8s, 4289649 effective words/s
[2023-02-07 13:31:02,318][gensim.models.word2vec][INFO] - EPOCH 40: training on 3639370 raw words (3635973 effective words) took 0.8s, 4308601 effective words/s
[2023-02-07 13:31:03,154][gensim.models.word2vec][INFO] - EPOCH 41: training on 3639370 raw words (3635973 effective words) took 0.8s, 4357504 effective words/s
[2023-02-07 13:31:03,977][gensim.models.word2vec][INFO] - EPOCH 42: training on 3639370 raw words (3635973 effective words) took 0.8s, 4423220 effective words/s
[2023-02-07 13:31:04,857][gensim.models.word2vec][INFO] - EPOCH 43: training on 3639370 raw words (3635973 effective words) took 0.9s, 4140648 effective words/s
[2023-02-07 13:31:05,741][gensim.models.word2vec][INFO] - EPOCH 44: training on 3639370 raw words (3635973 effective words) took 0.9s, 4114056 effective words/s
[2023-02-07 13:31:06,629][gensim.models.word2vec][INFO] - EPOCH 45: training on 3639370 raw words (3635973 effective words) took 0.9s, 4104354 effective words/s
[2023-02-07 13:31:07,514][gensim.models.word2vec][INFO] - EPOCH 46: training on 3639370 raw words (3635973 effective words) took 0.9s, 4111626 effective words/s
[2023-02-07 13:31:08,395][gensim.models.word2vec][INFO] - EPOCH 47: training on 3639370 raw words (3635973 effective words) took 0.9s, 4137853 effective words/s
[2023-02-07 13:31:09,267][gensim.models.word2vec][INFO] - EPOCH 48: training on 3639370 raw words (3635973 effective words) took 0.9s, 4175999 effective words/s
[2023-02-07 13:31:10,148][gensim.models.word2vec][INFO] - EPOCH 49: training on 3639370 raw words (3635973 effective words) took 0.9s, 4133561 effective words/s
[2023-02-07 13:31:11,029][gensim.models.word2vec][INFO] - EPOCH 50: training on 3639370 raw words (3635973 effective words) took 0.9s, 4134253 effective words/s
[2023-02-07 13:31:11,916][gensim.models.word2vec][INFO] - EPOCH 51: training on 3639370 raw words (3635973 effective words) took 0.9s, 4109652 effective words/s
[2023-02-07 13:31:12,822][gensim.models.word2vec][INFO] - EPOCH 52: training on 3639370 raw words (3635973 effective words) took 0.9s, 4019939 effective words/s
[2023-02-07 13:31:13,716][gensim.models.word2vec][INFO] - EPOCH 53: training on 3639370 raw words (3635973 effective words) took 0.9s, 4080433 effective words/s
[2023-02-07 13:31:14,615][gensim.models.word2vec][INFO] - EPOCH 54: training on 3639370 raw words (3635973 effective words) took 0.9s, 4051788 effective words/s
[2023-02-07 13:31:15,518][gensim.models.word2vec][INFO] - EPOCH 55: training on 3639370 raw words (3635973 effective words) took 0.9s, 4031939 effective words/s
[2023-02-07 13:31:16,409][gensim.models.word2vec][INFO] - EPOCH 56: training on 3639370 raw words (3635973 effective words) took 0.9s, 4087245 effective words/s
[2023-02-07 13:31:17,314][gensim.models.word2vec][INFO] - EPOCH 57: training on 3639370 raw words (3635973 effective words) took 0.9s, 4026593 effective words/s
[2023-02-07 13:31:18,153][gensim.models.word2vec][INFO] - EPOCH 58: training on 3639370 raw words (3635973 effective words) took 0.8s, 4341369 effective words/s
[2023-02-07 13:31:18,990][gensim.models.word2vec][INFO] - EPOCH 59: training on 3639370 raw words (3635973 effective words) took 0.8s, 4349107 effective words/s
[2023-02-07 13:31:19,819][gensim.models.word2vec][INFO] - EPOCH 60: training on 3639370 raw words (3635973 effective words) took 0.8s, 4392646 effective words/s
[2023-02-07 13:31:20,651][gensim.models.word2vec][INFO] - EPOCH 61: training on 3639370 raw words (3635973 effective words) took 0.8s, 4379615 effective words/s
[2023-02-07 13:31:21,473][gensim.models.word2vec][INFO] - EPOCH 62: training on 3639370 raw words (3635973 effective words) took 0.8s, 4433037 effective words/s
[2023-02-07 13:31:22,281][gensim.models.word2vec][INFO] - EPOCH 63: training on 3639370 raw words (3635973 effective words) took 0.8s, 4520500 effective words/s
[2023-02-07 13:31:23,050][gensim.models.word2vec][INFO] - EPOCH 64: training on 3639370 raw words (3635973 effective words) took 0.8s, 4737455 effective words/s
[2023-02-07 13:31:23,820][gensim.models.word2vec][INFO] - EPOCH 65: training on 3639370 raw words (3635973 effective words) took 0.8s, 4728681 effective words/s
[2023-02-07 13:31:24,628][gensim.models.word2vec][INFO] - EPOCH 66: training on 3639370 raw words (3635973 effective words) took 0.8s, 4508244 effective words/s
[2023-02-07 13:31:25,508][gensim.models.word2vec][INFO] - EPOCH 67: training on 3639370 raw words (3635973 effective words) took 0.9s, 4133871 effective words/s
[2023-02-07 13:31:26,407][gensim.models.word2vec][INFO] - EPOCH 68: training on 3639370 raw words (3635973 effective words) took 0.9s, 4053291 effective words/s
[2023-02-07 13:31:27,314][gensim.models.word2vec][INFO] - EPOCH 69: training on 3639370 raw words (3635973 effective words) took 0.9s, 4012910 effective words/s
[2023-02-07 13:31:28,212][gensim.models.word2vec][INFO] - EPOCH 70: training on 3639370 raw words (3635973 effective words) took 0.9s, 4058176 effective words/s
[2023-02-07 13:31:29,114][gensim.models.word2vec][INFO] - EPOCH 71: training on 3639370 raw words (3635973 effective words) took 0.9s, 4033459 effective words/s
[2023-02-07 13:31:29,985][gensim.models.word2vec][INFO] - EPOCH 72: training on 3639370 raw words (3635973 effective words) took 0.9s, 4179958 effective words/s
[2023-02-07 13:31:30,807][gensim.models.word2vec][INFO] - EPOCH 73: training on 3639370 raw words (3635973 effective words) took 0.8s, 4429471 effective words/s
[2023-02-07 13:31:31,602][gensim.models.word2vec][INFO] - EPOCH 74: training on 3639370 raw words (3635973 effective words) took 0.8s, 4578251 effective words/s
[2023-02-07 13:31:32,408][gensim.models.word2vec][INFO] - EPOCH 75: training on 3639370 raw words (3635973 effective words) took 0.8s, 4521591 effective words/s
[2023-02-07 13:31:33,215][gensim.models.word2vec][INFO] - EPOCH 76: training on 3639370 raw words (3635973 effective words) took 0.8s, 4508558 effective words/s
[2023-02-07 13:31:34,026][gensim.models.word2vec][INFO] - EPOCH 77: training on 3639370 raw words (3635973 effective words) took 0.8s, 4493859 effective words/s
[2023-02-07 13:31:34,822][gensim.models.word2vec][INFO] - EPOCH 78: training on 3639370 raw words (3635973 effective words) took 0.8s, 4570586 effective words/s
[2023-02-07 13:31:35,629][gensim.models.word2vec][INFO] - EPOCH 79: training on 3639370 raw words (3635973 effective words) took 0.8s, 4517057 effective words/s
[2023-02-07 13:31:36,441][gensim.models.word2vec][INFO] - EPOCH 80: training on 3639370 raw words (3635973 effective words) took 0.8s, 4484085 effective words/s
[2023-02-07 13:31:37,251][gensim.models.word2vec][INFO] - EPOCH 81: training on 3639370 raw words (3635973 effective words) took 0.8s, 4494803 effective words/s
[2023-02-07 13:31:38,066][gensim.models.word2vec][INFO] - EPOCH 82: training on 3639370 raw words (3635973 effective words) took 0.8s, 4468713 effective words/s
[2023-02-07 13:31:38,886][gensim.models.word2vec][INFO] - EPOCH 83: training on 3639370 raw words (3635973 effective words) took 0.8s, 4437859 effective words/s
[2023-02-07 13:31:39,695][gensim.models.word2vec][INFO] - EPOCH 84: training on 3639370 raw words (3635973 effective words) took 0.8s, 4505487 effective words/s
[2023-02-07 13:31:40,499][gensim.models.word2vec][INFO] - EPOCH 85: training on 3639370 raw words (3635973 effective words) took 0.8s, 4526257 effective words/s
[2023-02-07 13:31:41,319][gensim.models.word2vec][INFO] - EPOCH 86: training on 3639370 raw words (3635973 effective words) took 0.8s, 4440475 effective words/s
[2023-02-07 13:31:42,151][gensim.models.word2vec][INFO] - EPOCH 87: training on 3639370 raw words (3635973 effective words) took 0.8s, 4379049 effective words/s
[2023-02-07 13:31:42,976][gensim.models.word2vec][INFO] - EPOCH 88: training on 3639370 raw words (3635973 effective words) took 0.8s, 4414335 effective words/s
[2023-02-07 13:31:43,801][gensim.models.word2vec][INFO] - EPOCH 89: training on 3639370 raw words (3635973 effective words) took 0.8s, 4412903 effective words/s
[2023-02-07 13:31:43,802][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 327543300 raw words (327237570 effective words) took 77.3s, 4233728 effective words/s', 'datetime': '2023-02-07T13:31:43.802031', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:31:43.802 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:31:49,482][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133006-yn8cluu7/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:31:49.482733', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:31:49,483][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:31:49,506][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133006-yn8cluu7/files/../tmp/embedding_model.pt
2023-02-07 13:31:49.507 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:31:50.477 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:31:50.848 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:31:51.362 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8502029553808967, 'test_mae': 1.0621857139813298, 'test_r2': 0.12474337866974161}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.038 MB uploaded (0.000 MB deduped)wandb: | 0.035 MB of 0.038 MB uploaded (0.000 MB deduped)wandb: / 0.035 MB of 0.038 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.88
wandb: percentage 0.31261
wandb:   test_mae 1.06219
wandb:   test_mse 1.8502
wandb:    test_r2 0.12474
wandb: 
wandb: üöÄ View run fresh-sweep-45 at: https://wandb.ai/xiaoqiz/mof2vec/runs/yn8cluu7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_133006-yn8cluu7/logs
wandb: Agent Starting Run: ov4vkcy6 with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 977
wandb: 	model.gensim.alpha: 0.011242935631649149
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 33
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.7176839169166784
wandb: 	model.gensim.vector_size: 60
wandb: 	model.gensim.window: 10
wandb: 	model.sklearn.learning_rate: 0.07476178203009652
wandb: 	model.sklearn.max_depth: 26
wandb: 	model.sklearn.min_child_weight: 0.002549542121615341
wandb: 	model.sklearn.n_estimators: 3054
wandb: 	model.sklearn.num_leaves: 406
wandb: 	model.sklearn.reg_alpha: 0.06309098762511975
wandb: 	model.sklearn.reg_lambda: 0.15732686704464813
wandb: 	model.sklearn.subsample: 0.22733636810977187
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133201-ov4vkcy6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-sweep-46
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/ov4vkcy6
2023-02-07 13:32:09.382 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 13:32:09.383 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 977 for sweep.
2023-02-07 13:32:09.383 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.011242935631649149 for sweep.
2023-02-07 13:32:09.384 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:32:09.384 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 33 for sweep.
2023-02-07 13:32:09.384 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 13:32:09.384 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7176839169166784 for sweep.
2023-02-07 13:32:09.384 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 60 for sweep.
2023-02-07 13:32:09.385 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 10 for sweep.
2023-02-07 13:32:09.385 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.07476178203009652 for sweep.
2023-02-07 13:32:09.385 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 26 for sweep.
2023-02-07 13:32:09.385 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.002549542121615341 for sweep.
2023-02-07 13:32:09.385 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3054 for sweep.
2023-02-07 13:32:09.386 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 406 for sweep.
2023-02-07 13:32:09.386 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.06309098762511975 for sweep.
2023-02-07 13:32:09.386 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.15732686704464813 for sweep.
2023-02-07 13:32:09.386 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.22733636810977187 for sweep.
2023-02-07 13:32:09.386 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:32:09.393 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133201-ov4vkcy6/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 977, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 60, 'window': 10, 'min_count': 1, 'dm': 0, 'sample': 0.7176839169166784, 'workers': 4, 'alpha': 0.011242935631649149, 'epochs': 33}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3054, 'max_depth': 26, 'num_leaves': 406, 'reg_alpha': 0.06309098762511975, 'reg_lambda': 0.15732686704464813, 'subsample': 0.22733636810977187, 'min_child_weight': 0.002549542121615341, 'n_jobs': 4, 'learning_rate': 0.07476178203009652}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 20/3257 [00:00<00:27, 117.44it/s]  2%|‚ñè         | 56/3257 [00:00<00:14, 227.42it/s]  3%|‚ñé         | 98/3257 [00:00<00:10, 300.74it/s]  4%|‚ñç         | 137/3257 [00:00<00:09, 331.49it/s]  5%|‚ñå         | 176/3257 [00:00<00:08, 349.56it/s]  7%|‚ñã         | 220/3257 [00:00<00:08, 372.31it/s]  8%|‚ñä         | 262/3257 [00:00<00:07, 384.83it/s]  9%|‚ñâ         | 307/3257 [00:00<00:07, 403.01it/s] 11%|‚ñà         | 348/3257 [00:00<00:07, 402.56it/s] 12%|‚ñà‚ñè        | 389/3257 [00:01<00:07, 398.29it/s] 13%|‚ñà‚ñé        | 430/3257 [00:01<00:07, 381.26it/s] 14%|‚ñà‚ñç        | 471/3257 [00:01<00:07, 387.01it/s] 16%|‚ñà‚ñå        | 512/3257 [00:01<00:07, 391.71it/s] 17%|‚ñà‚ñã        | 552/3257 [00:01<00:06, 391.59it/s] 18%|‚ñà‚ñä        | 592/3257 [00:01<00:07, 375.51it/s] 20%|‚ñà‚ñâ        | 636/3257 [00:01<00:06, 390.99it/s] 21%|‚ñà‚ñà        | 676/3257 [00:01<00:07, 367.85it/s] 22%|‚ñà‚ñà‚ñè       | 717/3257 [00:01<00:06, 375.71it/s] 23%|‚ñà‚ñà‚ñé       | 755/3257 [00:02<00:06, 374.91it/s] 24%|‚ñà‚ñà‚ñç       | 796/3257 [00:02<00:06, 383.38it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:02<00:06, 378.70it/s] 27%|‚ñà‚ñà‚ñã       | 873/3257 [00:02<00:06, 367.46it/s] 28%|‚ñà‚ñà‚ñä       | 912/3257 [00:02<00:06, 373.06it/s] 29%|‚ñà‚ñà‚ñâ       | 950/3257 [00:02<00:06, 372.01it/s] 30%|‚ñà‚ñà‚ñà       | 988/3257 [00:02<00:06, 364.25it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1025/3257 [00:02<00:06, 363.87it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1062/3257 [00:02<00:06, 355.53it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1099/3257 [00:03<00:06, 358.46it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1137/3257 [00:03<00:05, 362.63it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1174/3257 [00:03<00:05, 360.51it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1211/3257 [00:03<00:06, 338.77it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:03<00:05, 346.34it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1284/3257 [00:03<00:05, 343.16it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1319/3257 [00:03<00:07, 256.10it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1355/3257 [00:03<00:06, 279.37it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1390/3257 [00:03<00:06, 296.14it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:04<00:05, 327.46it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1474/3257 [00:04<00:05, 351.12it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1514/3257 [00:04<00:04, 362.35it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1552/3257 [00:04<00:04, 349.99it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1589/3257 [00:04<00:04, 354.26it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1630/3257 [00:04<00:04, 367.62it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1668/3257 [00:04<00:04, 351.29it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1705/3257 [00:04<00:04, 354.62it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1741/3257 [00:04<00:04, 343.35it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1783/3257 [00:05<00:04, 362.81it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1820/3257 [00:05<00:03, 363.79it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1857/3257 [00:05<00:03, 360.24it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1894/3257 [00:05<00:03, 360.60it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1931/3257 [00:05<00:03, 362.26it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1974/3257 [00:05<00:03, 381.80it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2013/3257 [00:05<00:03, 379.27it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2051/3257 [00:05<00:03, 367.56it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2088/3257 [00:05<00:03, 362.82it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2125/3257 [00:05<00:03, 351.88it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2161/3257 [00:06<00:03, 349.63it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2199/3257 [00:06<00:02, 357.65it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2235/3257 [00:06<00:02, 356.24it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2271/3257 [00:06<00:02, 344.14it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2310/3257 [00:06<00:02, 355.35it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2355/3257 [00:06<00:02, 382.32it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2395/3257 [00:06<00:02, 386.59it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2434/3257 [00:06<00:02, 366.16it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2474/3257 [00:06<00:02, 375.67it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2515/3257 [00:07<00:01, 385.45it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2554/3257 [00:07<00:01, 383.04it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2593/3257 [00:07<00:01, 368.69it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2639/3257 [00:07<00:01, 393.46it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2679/3257 [00:07<00:01, 380.20it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2718/3257 [00:07<00:01, 360.04it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:07<00:01, 373.47it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2800/3257 [00:07<00:01, 377.90it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2839/3257 [00:08<00:01, 252.22it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:08<00:01, 293.87it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2923/3257 [00:08<00:01, 311.35it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2959/3257 [00:08<00:00, 315.69it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2994/3257 [00:08<00:00, 324.49it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3035/3257 [00:08<00:00, 345.74it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3076/3257 [00:08<00:00, 363.35it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3116/3257 [00:08<00:00, 372.42it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3155/3257 [00:08<00:00, 366.28it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3193/3257 [00:09<00:00, 365.04it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3231/3257 [00:09<00:00, 365.01it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 355.06it/s]
2023-02-07 13:32:18.819 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:32:18,821][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d60,n5,s0.717684,t4>', 'datetime': '2023-02-07T13:32:18.821008', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:32:18,821][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:32:18,821][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:32:19,010][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 13:32:19,010][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:32:19,017][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 2819 unique words (100.00% of original 2819, drops 0)', 'datetime': '2023-02-07T13:32:19.017762', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:32:19,019][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 2183622 word corpus (100.00% of original 2183622, drops 0)', 'datetime': '2023-02-07T13:32:19.019340', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:32:19,028][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 13:32:19,028][gensim.models.word2vec][INFO] - sample=0.717684 downsamples 0 most-common words
[2023-02-07 13:32:19,029][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2183622 word corpus (100.0%% of prior 2183622)', 'datetime': '2023-02-07T13:32:19.029056', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:32:19,044][gensim.models.word2vec][INFO] - estimated required memory for 2819 words and 60 dimensions: 4195700 bytes
[2023-02-07 13:32:19,044][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:32:19,046][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2819 vocabulary and 60 features, using sg=1 hs=0 sample=0.7176839169166784 negative=5 window=10 shrink_windows=True', 'datetime': '2023-02-07T13:32:19.046692', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:32:19,663][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2186879 effective words) took 0.6s, 3559181 effective words/s
[2023-02-07 13:32:20,200][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2186879 effective words) took 0.5s, 4078803 effective words/s
[2023-02-07 13:32:20,748][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2186879 effective words) took 0.5s, 3996075 effective words/s
[2023-02-07 13:32:21,292][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2186879 effective words) took 0.5s, 4034187 effective words/s
[2023-02-07 13:32:21,837][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2186879 effective words) took 0.5s, 4014136 effective words/s
[2023-02-07 13:32:22,384][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2186879 effective words) took 0.5s, 4010755 effective words/s
[2023-02-07 13:32:22,921][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2186879 effective words) took 0.5s, 4078807 effective words/s
[2023-02-07 13:32:23,451][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2186879 effective words) took 0.5s, 4135097 effective words/s
[2023-02-07 13:32:23,983][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2186879 effective words) took 0.5s, 4120468 effective words/s
[2023-02-07 13:32:24,517][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2186879 effective words) took 0.5s, 4103513 effective words/s
[2023-02-07 13:32:25,045][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2186879 effective words) took 0.5s, 4148366 effective words/s
[2023-02-07 13:32:25,570][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2186879 effective words) took 0.5s, 4172867 effective words/s
[2023-02-07 13:32:26,092][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2186879 effective words) took 0.5s, 4196364 effective words/s
[2023-02-07 13:32:26,618][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2186879 effective words) took 0.5s, 4169816 effective words/s
[2023-02-07 13:32:27,144][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2186879 effective words) took 0.5s, 4172893 effective words/s
[2023-02-07 13:32:27,666][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2186879 effective words) took 0.5s, 4202925 effective words/s
[2023-02-07 13:32:28,186][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2186879 effective words) took 0.5s, 4215416 effective words/s
[2023-02-07 13:32:28,705][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2186879 effective words) took 0.5s, 4222718 effective words/s
[2023-02-07 13:32:29,223][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2186879 effective words) took 0.5s, 4224498 effective words/s
[2023-02-07 13:32:29,740][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2186879 effective words) took 0.5s, 4241093 effective words/s
[2023-02-07 13:32:30,258][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2186879 effective words) took 0.5s, 4227179 effective words/s
[2023-02-07 13:32:30,781][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2186879 effective words) took 0.5s, 4195837 effective words/s
[2023-02-07 13:32:31,298][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2186879 effective words) took 0.5s, 4236300 effective words/s
[2023-02-07 13:32:31,816][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2186879 effective words) took 0.5s, 4230251 effective words/s
[2023-02-07 13:32:32,330][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2186879 effective words) took 0.5s, 4261445 effective words/s
[2023-02-07 13:32:32,843][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2186879 effective words) took 0.5s, 4276112 effective words/s
[2023-02-07 13:32:33,357][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2186879 effective words) took 0.5s, 4265171 effective words/s
[2023-02-07 13:32:33,870][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2186879 effective words) took 0.5s, 4270898 effective words/s
[2023-02-07 13:32:34,382][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2186879 effective words) took 0.5s, 4277655 effective words/s
[2023-02-07 13:32:34,898][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2186879 effective words) took 0.5s, 4246401 effective words/s
[2023-02-07 13:32:35,415][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2186879 effective words) took 0.5s, 4239917 effective words/s
[2023-02-07 13:32:35,932][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2186879 effective words) took 0.5s, 4239782 effective words/s
[2023-02-07 13:32:36,446][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2186879 effective words) took 0.5s, 4267282 effective words/s
[2023-02-07 13:32:36,446][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 72059526 raw words (72167007 effective words) took 17.4s, 4147586 effective words/s', 'datetime': '2023-02-07T13:32:36.446644', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:32:36.446 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:32:37,971][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133201-ov4vkcy6/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:32:37.971031', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:32:37,972][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:32:37,979][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133201-ov4vkcy6/files/../tmp/embedding_model.pt
2023-02-07 13:32:37.980 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:32:38.892 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:32:39.266 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:32:39.720 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9029856272537784, 'test_mae': 1.065123770100299, 'test_r2': 0.09977401900361171}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.92
wandb: percentage 0.0
wandb:   test_mae 1.06512
wandb:   test_mse 1.90299
wandb:    test_r2 0.09977
wandb: 
wandb: üöÄ View run peach-sweep-46 at: https://wandb.ai/xiaoqiz/mof2vec/runs/ov4vkcy6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_133201-ov4vkcy6/logs
wandb: Agent Starting Run: 83ms40in with config:
wandb: 	data.data.wl_step: 8
wandb: 	data.nn.batch_size: 727
wandb: 	model.gensim.alpha: 0.0004708142870088897
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 49
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.4931800483466382
wandb: 	model.gensim.vector_size: 13
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.49363062862079704
wandb: 	model.sklearn.max_depth: 38
wandb: 	model.sklearn.min_child_weight: 0.001326461477154892
wandb: 	model.sklearn.n_estimators: 3823
wandb: 	model.sklearn.num_leaves: 493
wandb: 	model.sklearn.reg_alpha: 0.011588126662095912
wandb: 	model.sklearn.reg_lambda: 0.11590799450261538
wandb: 	model.sklearn.subsample: 0.3894618260083054
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133249-83ms40in
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-47
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/83ms40in
2023-02-07 13:32:57.096 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 8 for sweep.
2023-02-07 13:32:57.097 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 727 for sweep.
2023-02-07 13:32:57.097 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0004708142870088897 for sweep.
2023-02-07 13:32:57.098 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:32:57.098 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 49 for sweep.
2023-02-07 13:32:57.098 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 13:32:57.098 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4931800483466382 for sweep.
2023-02-07 13:32:57.098 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 13 for sweep.
2023-02-07 13:32:57.099 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 13:32:57.099 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.49363062862079704 for sweep.
2023-02-07 13:32:57.099 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 38 for sweep.
2023-02-07 13:32:57.100 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.001326461477154892 for sweep.
2023-02-07 13:32:57.100 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3823 for sweep.
2023-02-07 13:32:57.100 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 493 for sweep.
2023-02-07 13:32:57.100 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.011588126662095912 for sweep.
2023-02-07 13:32:57.100 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.11590799450261538 for sweep.
2023-02-07 13:32:57.101 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3894618260083054 for sweep.
2023-02-07 13:32:57.101 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:32:57.106 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 8}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133249-83ms40in/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 727, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 13, 'window': 3, 'min_count': 6, 'dm': 0, 'sample': 0.4931800483466382, 'workers': 4, 'alpha': 0.0004708142870088897, 'epochs': 49}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3823, 'max_depth': 38, 'num_leaves': 493, 'reg_alpha': 0.011588126662095912, 'reg_lambda': 0.11590799450261538, 'subsample': 0.3894618260083054, 'min_child_weight': 0.001326461477154892, 'n_jobs': 4, 'learning_rate': 0.49363062862079704}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:18, 172.99it/s]  1%|          | 39/3257 [00:00<00:16, 193.47it/s]  2%|‚ñè         | 59/3257 [00:00<00:17, 183.32it/s]  2%|‚ñè         | 81/3257 [00:00<00:16, 196.60it/s]  3%|‚ñé         | 101/3257 [00:00<00:16, 194.94it/s]  4%|‚ñé         | 121/3257 [00:00<00:17, 182.59it/s]  4%|‚ñç         | 145/3257 [00:00<00:15, 197.71it/s]  5%|‚ñå         | 165/3257 [00:00<00:16, 188.32it/s]  6%|‚ñå         | 186/3257 [00:00<00:15, 193.16it/s]  6%|‚ñã         | 206/3257 [00:01<00:15, 193.67it/s]  7%|‚ñã         | 231/3257 [00:01<00:14, 208.94it/s]  8%|‚ñä         | 253/3257 [00:01<00:14, 205.50it/s]  8%|‚ñä         | 275/3257 [00:01<00:14, 206.94it/s]  9%|‚ñâ         | 298/3257 [00:01<00:13, 213.39it/s] 10%|‚ñâ         | 320/3257 [00:01<00:13, 210.16it/s] 11%|‚ñà         | 342/3257 [00:01<00:14, 204.18it/s] 11%|‚ñà         | 364/3257 [00:01<00:14, 206.08it/s] 12%|‚ñà‚ñè        | 385/3257 [00:01<00:14, 193.61it/s] 12%|‚ñà‚ñè        | 405/3257 [00:02<00:14, 192.12it/s] 13%|‚ñà‚ñé        | 425/3257 [00:02<00:15, 178.44it/s] 14%|‚ñà‚ñé        | 444/3257 [00:02<00:16, 171.22it/s] 14%|‚ñà‚ñç        | 465/3257 [00:02<00:15, 180.96it/s] 15%|‚ñà‚ñç        | 484/3257 [00:02<00:15, 180.08it/s] 16%|‚ñà‚ñå        | 508/3257 [00:02<00:14, 196.02it/s] 16%|‚ñà‚ñå        | 528/3257 [00:02<00:14, 187.43it/s] 17%|‚ñà‚ñã        | 548/3257 [00:02<00:14, 189.06it/s] 17%|‚ñà‚ñã        | 568/3257 [00:02<00:15, 176.67it/s] 18%|‚ñà‚ñä        | 586/3257 [00:03<00:15, 167.69it/s] 19%|‚ñà‚ñä        | 605/3257 [00:03<00:15, 172.97it/s] 19%|‚ñà‚ñâ        | 623/3257 [00:03<00:15, 170.75it/s] 20%|‚ñà‚ñâ        | 644/3257 [00:03<00:14, 179.36it/s] 20%|‚ñà‚ñà        | 663/3257 [00:03<00:15, 167.23it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:15, 170.78it/s] 22%|‚ñà‚ñà‚ñè       | 703/3257 [00:03<00:14, 176.87it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:03<00:14, 177.92it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:03<00:14, 171.62it/s] 23%|‚ñà‚ñà‚ñé       | 762/3257 [00:04<00:13, 184.80it/s] 24%|‚ñà‚ñà‚ñç       | 781/3257 [00:04<00:13, 179.14it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:04<00:13, 186.32it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:04<00:13, 177.76it/s] 26%|‚ñà‚ñà‚ñå       | 841/3257 [00:04<00:14, 170.24it/s] 26%|‚ñà‚ñà‚ñã       | 861/3257 [00:04<00:13, 177.35it/s] 27%|‚ñà‚ñà‚ñã       | 879/3257 [00:04<00:13, 173.10it/s] 28%|‚ñà‚ñà‚ñä       | 901/3257 [00:04<00:12, 185.28it/s] 28%|‚ñà‚ñà‚ñä       | 920/3257 [00:04<00:12, 182.99it/s] 29%|‚ñà‚ñà‚ñâ       | 939/3257 [00:05<00:12, 182.96it/s] 29%|‚ñà‚ñà‚ñâ       | 960/3257 [00:05<00:12, 189.85it/s] 30%|‚ñà‚ñà‚ñà       | 980/3257 [00:05<00:12, 180.21it/s] 31%|‚ñà‚ñà‚ñà       | 999/3257 [00:05<00:19, 116.89it/s] 31%|‚ñà‚ñà‚ñà       | 1016/3257 [00:05<00:17, 126.77it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1032/3257 [00:05<00:16, 131.96it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1048/3257 [00:05<00:16, 135.93it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1066/3257 [00:06<00:14, 146.62it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1082/3257 [00:06<00:14, 147.14it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1099/3257 [00:06<00:14, 152.18it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:06<00:13, 156.10it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1134/3257 [00:06<00:13, 154.74it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1150/3257 [00:06<00:13, 151.76it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:06<00:12, 164.14it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1188/3257 [00:06<00:14, 147.02it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1204/3257 [00:06<00:13, 147.75it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1220/3257 [00:07<00:13, 150.57it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1242/3257 [00:07<00:12, 166.93it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1259/3257 [00:07<00:12, 164.17it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1276/3257 [00:07<00:12, 159.47it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1293/3257 [00:07<00:12, 154.57it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1310/3257 [00:07<00:12, 158.03it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:07<00:11, 163.86it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1347/3257 [00:07<00:11, 170.24it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1365/3257 [00:07<00:11, 162.78it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1382/3257 [00:08<00:11, 158.53it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1401/3257 [00:08<00:11, 167.11it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:08<00:10, 175.60it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1441/3257 [00:08<00:10, 174.79it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1463/3257 [00:08<00:09, 185.36it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1482/3257 [00:08<00:09, 186.52it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1504/3257 [00:08<00:09, 194.70it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1524/3257 [00:08<00:09, 179.24it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1543/3257 [00:08<00:09, 173.14it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1561/3257 [00:08<00:10, 166.81it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1578/3257 [00:09<00:10, 165.41it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1597/3257 [00:09<00:09, 171.60it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1617/3257 [00:09<00:09, 177.05it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1635/3257 [00:09<00:09, 170.02it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1653/3257 [00:09<00:09, 166.24it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1670/3257 [00:09<00:09, 164.58it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1687/3257 [00:09<00:09, 161.25it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1706/3257 [00:09<00:09, 167.32it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1724/3257 [00:09<00:09, 169.83it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1742/3257 [00:10<00:10, 149.87it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:10<00:09, 163.35it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1783/3257 [00:10<00:08, 172.75it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1801/3257 [00:10<00:08, 170.66it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1819/3257 [00:10<00:08, 167.16it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1836/3257 [00:10<00:08, 162.11it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1853/3257 [00:10<00:08, 163.73it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1873/3257 [00:10<00:08, 172.79it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1891/3257 [00:10<00:08, 167.51it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1911/3257 [00:11<00:07, 176.19it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1929/3257 [00:11<00:07, 169.44it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1957/3257 [00:11<00:06, 196.54it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:11<00:06, 189.23it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1997/3257 [00:11<00:06, 189.05it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2016/3257 [00:11<00:06, 184.65it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2036/3257 [00:11<00:06, 188.35it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2055/3257 [00:11<00:07, 169.67it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2073/3257 [00:11<00:07, 167.66it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2091/3257 [00:12<00:06, 169.96it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2109/3257 [00:12<00:07, 163.29it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2126/3257 [00:12<00:07, 155.66it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2142/3257 [00:12<00:07, 155.80it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2160/3257 [00:12<00:06, 159.30it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2179/3257 [00:12<00:06, 166.56it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2197/3257 [00:12<00:06, 169.92it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2215/3257 [00:12<00:06, 162.60it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2234/3257 [00:12<00:06, 169.59it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2252/3257 [00:13<00:06, 161.71it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2269/3257 [00:13<00:10, 97.14it/s]  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2287/3257 [00:13<00:08, 112.64it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2304/3257 [00:13<00:07, 124.17it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2328/3257 [00:13<00:06, 149.50it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2352/3257 [00:13<00:05, 171.34it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2372/3257 [00:13<00:05, 171.23it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2395/3257 [00:14<00:04, 185.22it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2415/3257 [00:14<00:04, 174.93it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2434/3257 [00:14<00:04, 167.76it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2452/3257 [00:14<00:04, 166.56it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2475/3257 [00:14<00:04, 181.34it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2495/3257 [00:14<00:04, 185.95it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2517/3257 [00:14<00:03, 193.88it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2539/3257 [00:14<00:03, 199.52it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2560/3257 [00:14<00:03, 183.38it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2579/3257 [00:15<00:03, 170.69it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2597/3257 [00:15<00:03, 171.53it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2621/3257 [00:15<00:03, 189.99it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2641/3257 [00:15<00:03, 188.30it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2661/3257 [00:15<00:03, 174.57it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2681/3257 [00:15<00:03, 181.28it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2700/3257 [00:15<00:03, 159.36it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2717/3257 [00:15<00:03, 157.65it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2740/3257 [00:16<00:02, 176.10it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2759/3257 [00:16<00:02, 178.34it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2778/3257 [00:16<00:02, 170.67it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2801/3257 [00:16<00:02, 185.60it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2820/3257 [00:16<00:02, 177.24it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2839/3257 [00:16<00:02, 168.65it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2864/3257 [00:16<00:02, 189.29it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:16<00:01, 194.25it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2906/3257 [00:16<00:01, 178.40it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:17<00:01, 182.73it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2947/3257 [00:17<00:01, 175.97it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2966/3257 [00:17<00:01, 176.77it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2984/3257 [00:17<00:01, 165.43it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3007/3257 [00:17<00:01, 181.04it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3026/3257 [00:17<00:01, 174.39it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3048/3257 [00:17<00:01, 184.74it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3071/3257 [00:17<00:00, 195.34it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3091/3257 [00:17<00:00, 190.27it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3116/3257 [00:18<00:00, 204.42it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3137/3257 [00:18<00:00, 194.46it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3157/3257 [00:18<00:00, 185.32it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3176/3257 [00:18<00:00, 182.90it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3196/3257 [00:18<00:00, 187.02it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3215/3257 [00:18<00:00, 172.25it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3238/3257 [00:18<00:00, 186.78it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 187.62it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:18<00:00, 172.88it/s]
2023-02-07 13:33:16.759 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:33:16,760][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d13,n5,mc6,s0.49318,t4>', 'datetime': '2023-02-07T13:33:16.760503', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:33:16,760][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:33:16,761][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:33:17,329][gensim.models.doc2vec][INFO] - collected 54054 word types and 3257 unique tags from a corpus of 3257 examples and 6550866 words
[2023-02-07 13:33:17,329][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:33:17,399][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 25857 unique words (47.84% of original 54054, drops 28197)', 'datetime': '2023-02-07T13:33:17.399356', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:33:17,399][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 6477431 word corpus (98.88% of original 6550866, drops 73435)', 'datetime': '2023-02-07T13:33:17.399694', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:33:17,485][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 54054 items
[2023-02-07 13:33:17,486][gensim.models.word2vec][INFO] - sample=0.49318 downsamples 0 most-common words
[2023-02-07 13:33:17,486][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 6477431 word corpus (100.0%% of prior 6477431)', 'datetime': '2023-02-07T13:33:17.486881', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:33:17,631][gensim.models.word2vec][INFO] - estimated required memory for 25857 words and 13 dimensions: 16438392 bytes
[2023-02-07 13:33:17,631][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:33:17,633][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 25857 vocabulary and 13 features, using sg=1 hs=0 sample=0.4931800483466382 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T13:33:17.633614', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:33:18,636][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 58.80% examples, 3847286 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:19,301][gensim.models.word2vec][INFO] - EPOCH 0: training on 6550866 raw words (6423064 effective words) took 1.7s, 3856709 effective words/s
[2023-02-07 13:33:20,302][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 59.26% examples, 3887162 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:20,994][gensim.models.word2vec][INFO] - EPOCH 1: training on 6550866 raw words (6423064 effective words) took 1.7s, 3796687 effective words/s
[2023-02-07 13:33:21,997][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 55.70% examples, 3650677 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:22,750][gensim.models.word2vec][INFO] - EPOCH 2: training on 6550866 raw words (6423064 effective words) took 1.8s, 3659746 effective words/s
[2023-02-07 13:33:23,753][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 55.88% examples, 3664096 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:24,498][gensim.models.word2vec][INFO] - EPOCH 3: training on 6550866 raw words (6423064 effective words) took 1.7s, 3678503 effective words/s
[2023-02-07 13:33:25,506][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 55.48% examples, 3630335 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:26,247][gensim.models.word2vec][INFO] - EPOCH 4: training on 6550866 raw words (6423064 effective words) took 1.7s, 3676830 effective words/s
[2023-02-07 13:33:27,250][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 56.65% examples, 3716276 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:27,973][gensim.models.word2vec][INFO] - EPOCH 5: training on 6550866 raw words (6423064 effective words) took 1.7s, 3724759 effective words/s
[2023-02-07 13:33:28,977][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 56.09% examples, 3678553 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:29,727][gensim.models.word2vec][INFO] - EPOCH 6: training on 6550866 raw words (6423064 effective words) took 1.8s, 3665315 effective words/s
[2023-02-07 13:33:30,731][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 55.66% examples, 3648802 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:31,486][gensim.models.word2vec][INFO] - EPOCH 7: training on 6550866 raw words (6423064 effective words) took 1.8s, 3654481 effective words/s
[2023-02-07 13:33:32,489][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 55.70% examples, 3653394 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:33,227][gensim.models.word2vec][INFO] - EPOCH 8: training on 6550866 raw words (6423064 effective words) took 1.7s, 3690779 effective words/s
[2023-02-07 13:33:34,230][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 56.03% examples, 3680318 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:34,964][gensim.models.word2vec][INFO] - EPOCH 9: training on 6550866 raw words (6423064 effective words) took 1.7s, 3703347 effective words/s
[2023-02-07 13:33:35,969][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 56.65% examples, 3708503 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:36,690][gensim.models.word2vec][INFO] - EPOCH 10: training on 6550866 raw words (6423064 effective words) took 1.7s, 3725229 effective words/s
[2023-02-07 13:33:37,692][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 56.74% examples, 3725257 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:38,392][gensim.models.word2vec][INFO] - EPOCH 11: training on 6550866 raw words (6423064 effective words) took 1.7s, 3776892 effective words/s
[2023-02-07 13:33:39,396][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 61.74% examples, 4009736 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:39,990][gensim.models.word2vec][INFO] - EPOCH 12: training on 6550866 raw words (6423064 effective words) took 1.6s, 4020846 effective words/s
[2023-02-07 13:33:40,993][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 60.58% examples, 3938853 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:41,625][gensim.models.word2vec][INFO] - EPOCH 13: training on 6550866 raw words (6423064 effective words) took 1.6s, 3931397 effective words/s
[2023-02-07 13:33:42,628][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 56.74% examples, 3723700 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:43,330][gensim.models.word2vec][INFO] - EPOCH 14: training on 6550866 raw words (6423064 effective words) took 1.7s, 3769284 effective words/s
[2023-02-07 13:33:44,332][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 56.83% examples, 3733709 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:45,033][gensim.models.word2vec][INFO] - EPOCH 15: training on 6550866 raw words (6423064 effective words) took 1.7s, 3775629 effective words/s
[2023-02-07 13:33:46,036][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 58.37% examples, 3824730 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:46,714][gensim.models.word2vec][INFO] - EPOCH 16: training on 6550866 raw words (6423064 effective words) took 1.7s, 3824035 effective words/s
[2023-02-07 13:33:47,716][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 57.14% examples, 3746509 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:48,410][gensim.models.word2vec][INFO] - EPOCH 17: training on 6550866 raw words (6423064 effective words) took 1.7s, 3790004 effective words/s
[2023-02-07 13:33:49,414][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 58.27% examples, 3810053 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:50,094][gensim.models.word2vec][INFO] - EPOCH 18: training on 6550866 raw words (6423064 effective words) took 1.7s, 3815705 effective words/s
[2023-02-07 13:33:51,098][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 58.37% examples, 3821428 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:51,760][gensim.models.word2vec][INFO] - EPOCH 19: training on 6550866 raw words (6423064 effective words) took 1.7s, 3858767 effective words/s
[2023-02-07 13:33:52,763][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 58.80% examples, 3845577 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:53,421][gensim.models.word2vec][INFO] - EPOCH 20: training on 6550866 raw words (6423064 effective words) took 1.7s, 3871300 effective words/s
[2023-02-07 13:33:54,423][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 59.53% examples, 3893040 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:55,082][gensim.models.word2vec][INFO] - EPOCH 21: training on 6550866 raw words (6423064 effective words) took 1.7s, 3869029 effective words/s
[2023-02-07 13:33:56,086][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 58.80% examples, 3841201 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:56,747][gensim.models.word2vec][INFO] - EPOCH 22: training on 6550866 raw words (6423064 effective words) took 1.7s, 3860489 effective words/s
[2023-02-07 13:33:57,752][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 58.83% examples, 3850338 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:33:58,401][gensim.models.word2vec][INFO] - EPOCH 23: training on 6550866 raw words (6423064 effective words) took 1.7s, 3889044 effective words/s
[2023-02-07 13:33:59,403][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 59.10% examples, 3874354 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:00,049][gensim.models.word2vec][INFO] - EPOCH 24: training on 6550866 raw words (6423064 effective words) took 1.6s, 3900332 effective words/s
[2023-02-07 13:34:01,053][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 58.89% examples, 3848296 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:01,706][gensim.models.word2vec][INFO] - EPOCH 25: training on 6550866 raw words (6423064 effective words) took 1.7s, 3880914 effective words/s
[2023-02-07 13:34:02,709][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 60.09% examples, 3911078 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:03,375][gensim.models.word2vec][INFO] - EPOCH 26: training on 6550866 raw words (6423064 effective words) took 1.7s, 3850777 effective words/s
[2023-02-07 13:34:04,382][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 56.65% examples, 3702943 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:05,106][gensim.models.word2vec][INFO] - EPOCH 27: training on 6550866 raw words (6423064 effective words) took 1.7s, 3713584 effective words/s
[2023-02-07 13:34:06,109][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 54.99% examples, 3603802 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:06,885][gensim.models.word2vec][INFO] - EPOCH 28: training on 6550866 raw words (6423064 effective words) took 1.8s, 3615158 effective words/s
[2023-02-07 13:34:07,887][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 56.31% examples, 3695438 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:08,608][gensim.models.word2vec][INFO] - EPOCH 29: training on 6550866 raw words (6423064 effective words) took 1.7s, 3731636 effective words/s
[2023-02-07 13:34:09,612][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 57.35% examples, 3750818 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:10,314][gensim.models.word2vec][INFO] - EPOCH 30: training on 6550866 raw words (6423064 effective words) took 1.7s, 3768200 effective words/s
[2023-02-07 13:34:11,316][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 56.74% examples, 3723617 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:12,048][gensim.models.word2vec][INFO] - EPOCH 31: training on 6550866 raw words (6423064 effective words) took 1.7s, 3705471 effective words/s
[2023-02-07 13:34:13,052][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 56.74% examples, 3717273 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:13,773][gensim.models.word2vec][INFO] - EPOCH 32: training on 6550866 raw words (6423064 effective words) took 1.7s, 3725668 effective words/s
[2023-02-07 13:34:14,777][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 56.31% examples, 3688369 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:15,500][gensim.models.word2vec][INFO] - EPOCH 33: training on 6550866 raw words (6423064 effective words) took 1.7s, 3724433 effective words/s
[2023-02-07 13:34:16,502][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 56.40% examples, 3702167 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:34:17,223][gensim.models.word2vec][INFO] - EPOCH 34: training on 6550866 raw words (6423064 effective words) took 1.7s, 3730898 effective words/s
[2023-02-07 13:34:18,226][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 57.14% examples, 3746358 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:18,933][gensim.models.word2vec][INFO] - EPOCH 35: training on 6550866 raw words (6423064 effective words) took 1.7s, 3759565 effective words/s
[2023-02-07 13:34:19,937][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 57.45% examples, 3761667 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:20,650][gensim.models.word2vec][INFO] - EPOCH 36: training on 6550866 raw words (6423064 effective words) took 1.7s, 3743286 effective words/s
[2023-02-07 13:34:21,653][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 62.08% examples, 4038688 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:22,244][gensim.models.word2vec][INFO] - EPOCH 37: training on 6550866 raw words (6423064 effective words) took 1.6s, 4035262 effective words/s
[2023-02-07 13:34:23,246][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 61.50% examples, 4003694 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:23,857][gensim.models.word2vec][INFO] - EPOCH 38: training on 6550866 raw words (6423064 effective words) took 1.6s, 3982878 effective words/s
[2023-02-07 13:34:24,860][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 58.21% examples, 3816232 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:25,544][gensim.models.word2vec][INFO] - EPOCH 39: training on 6550866 raw words (6423064 effective words) took 1.7s, 3810148 effective words/s
[2023-02-07 13:34:26,547][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 58.06% examples, 3808387 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:27,225][gensim.models.word2vec][INFO] - EPOCH 40: training on 6550866 raw words (6423064 effective words) took 1.7s, 3825587 effective words/s
[2023-02-07 13:34:28,227][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 58.89% examples, 3855071 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:28,888][gensim.models.word2vec][INFO] - EPOCH 41: training on 6550866 raw words (6423064 effective words) took 1.7s, 3864466 effective words/s
[2023-02-07 13:34:29,891][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 58.06% examples, 3804721 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:30,573][gensim.models.word2vec][INFO] - EPOCH 42: training on 6550866 raw words (6423064 effective words) took 1.7s, 3814523 effective words/s
[2023-02-07 13:34:31,575][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 57.97% examples, 3802205 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:32,254][gensim.models.word2vec][INFO] - EPOCH 43: training on 6550866 raw words (6423064 effective words) took 1.7s, 3823150 effective words/s
[2023-02-07 13:34:33,257][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 59.10% examples, 3873199 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:33,905][gensim.models.word2vec][INFO] - EPOCH 44: training on 6550866 raw words (6423064 effective words) took 1.6s, 3895176 effective words/s
[2023-02-07 13:34:34,910][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 59.26% examples, 3879693 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:35,554][gensim.models.word2vec][INFO] - EPOCH 45: training on 6550866 raw words (6423064 effective words) took 1.6s, 3901832 effective words/s
[2023-02-07 13:34:36,556][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 60.09% examples, 3916412 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:37,201][gensim.models.word2vec][INFO] - EPOCH 46: training on 6550866 raw words (6423064 effective words) took 1.6s, 3902980 effective words/s
[2023-02-07 13:34:38,205][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 59.07% examples, 3864987 words/s, in_qsize 6, out_qsize 1
[2023-02-07 13:34:38,853][gensim.models.word2vec][INFO] - EPOCH 47: training on 6550866 raw words (6423064 effective words) took 1.7s, 3889111 effective words/s
[2023-02-07 13:34:39,855][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 59.10% examples, 3875049 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:34:40,500][gensim.models.word2vec][INFO] - EPOCH 48: training on 6550866 raw words (6423064 effective words) took 1.6s, 3903336 effective words/s
[2023-02-07 13:34:40,500][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 320992434 raw words (314730136 effective words) took 82.9s, 3798023 effective words/s', 'datetime': '2023-02-07T13:34:40.500752', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:34:40.501 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:34:46,893][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133249-83ms40in/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:34:46.893681', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:34:46,894][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:34:46,931][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133249-83ms40in/files/../tmp/embedding_model.pt
2023-02-07 13:34:46.931 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:34:47.926 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:34:48.266 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:34:48.429 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.185737063404106, 'test_mae': 1.107732402595, 'test_r2': -0.03398431597335616}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.77
wandb: percentage 0.52165
wandb:   test_mae 1.10773
wandb:   test_mse 2.18574
wandb:    test_r2 -0.03398
wandb: 
wandb: üöÄ View run polar-sweep-47 at: https://wandb.ai/xiaoqiz/mof2vec/runs/83ms40in
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_133249-83ms40in/logs
wandb: Agent Starting Run: rvslblor with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 244
wandb: 	model.gensim.alpha: 0.0003879374973183632
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 93
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.35125811111440863
wandb: 	model.gensim.vector_size: 52
wandb: 	model.gensim.window: 1
wandb: 	model.sklearn.learning_rate: 0.9032386039670928
wandb: 	model.sklearn.max_depth: 34
wandb: 	model.sklearn.min_child_weight: 0.032478984320930944
wandb: 	model.sklearn.n_estimators: 1729
wandb: 	model.sklearn.num_leaves: 310
wandb: 	model.sklearn.reg_alpha: 0.002497239773058854
wandb: 	model.sklearn.reg_lambda: 0.9100149570046784
wandb: 	model.sklearn.subsample: 0.25488456329212306
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133501-rvslblor
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-sweep-48
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/rvslblor
2023-02-07 13:35:09.286 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 13:35:09.287 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 244 for sweep.
2023-02-07 13:35:09.287 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0003879374973183632 for sweep.
2023-02-07 13:35:09.287 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:35:09.287 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 93 for sweep.
2023-02-07 13:35:09.288 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 13:35:09.288 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.35125811111440863 for sweep.
2023-02-07 13:35:09.288 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 52 for sweep.
2023-02-07 13:35:09.289 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 1 for sweep.
2023-02-07 13:35:09.289 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.9032386039670928 for sweep.
2023-02-07 13:35:09.289 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 34 for sweep.
2023-02-07 13:35:09.289 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.032478984320930944 for sweep.
2023-02-07 13:35:09.290 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1729 for sweep.
2023-02-07 13:35:09.290 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 310 for sweep.
2023-02-07 13:35:09.290 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.002497239773058854 for sweep.
2023-02-07 13:35:09.290 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.9100149570046784 for sweep.
2023-02-07 13:35:09.290 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.25488456329212306 for sweep.
2023-02-07 13:35:09.291 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:35:09.296 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133501-rvslblor/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 244, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 52, 'window': 1, 'min_count': 1, 'dm': 0, 'sample': 0.35125811111440863, 'workers': 4, 'alpha': 0.0003879374973183632, 'epochs': 93}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1729, 'max_depth': 34, 'num_leaves': 310, 'reg_alpha': 0.002497239773058854, 'reg_lambda': 0.9100149570046784, 'subsample': 0.25488456329212306, 'min_child_weight': 0.032478984320930944, 'n_jobs': 4, 'learning_rate': 0.9032386039670928}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 34/3257 [00:00<00:09, 329.73it/s]  2%|‚ñè         | 71/3257 [00:00<00:09, 352.18it/s]  3%|‚ñé         | 107/3257 [00:00<00:08, 351.64it/s]  5%|‚ñç         | 148/3257 [00:00<00:08, 370.83it/s]  6%|‚ñå         | 186/3257 [00:00<00:08, 362.71it/s]  7%|‚ñã         | 229/3257 [00:00<00:07, 383.96it/s]  8%|‚ñä         | 268/3257 [00:00<00:08, 371.49it/s] 10%|‚ñâ         | 311/3257 [00:00<00:07, 388.78it/s] 11%|‚ñà         | 351/3257 [00:00<00:07, 381.65it/s] 12%|‚ñà‚ñè        | 390/3257 [00:01<00:07, 366.54it/s] 13%|‚ñà‚ñé        | 427/3257 [00:01<00:08, 348.36it/s] 14%|‚ñà‚ñç        | 463/3257 [00:01<00:07, 350.49it/s] 15%|‚ñà‚ñå        | 501/3257 [00:01<00:07, 355.15it/s] 17%|‚ñà‚ñã        | 538/3257 [00:01<00:07, 358.99it/s] 18%|‚ñà‚ñä        | 575/3257 [00:01<00:10, 252.67it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:01<00:09, 284.58it/s] 20%|‚ñà‚ñâ        | 649/3257 [00:01<00:08, 295.50it/s] 21%|‚ñà‚ñà        | 683/3257 [00:02<00:08, 302.99it/s] 22%|‚ñà‚ñà‚ñè       | 720/3257 [00:02<00:07, 320.74it/s] 23%|‚ñà‚ñà‚ñé       | 754/3257 [00:02<00:07, 323.30it/s] 24%|‚ñà‚ñà‚ñç       | 790/3257 [00:02<00:07, 332.42it/s] 25%|‚ñà‚ñà‚ñå       | 825/3257 [00:02<00:07, 331.63it/s] 26%|‚ñà‚ñà‚ñã       | 859/3257 [00:02<00:07, 333.09it/s] 27%|‚ñà‚ñà‚ñã       | 894/3257 [00:02<00:07, 333.88it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:02<00:06, 343.46it/s] 30%|‚ñà‚ñà‚ñâ       | 968/3257 [00:02<00:06, 350.75it/s] 31%|‚ñà‚ñà‚ñà       | 1004/3257 [00:02<00:06, 348.52it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1039/3257 [00:03<00:06, 332.10it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1077/3257 [00:03<00:06, 345.07it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1112/3257 [00:03<00:06, 341.26it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:03<00:06, 333.44it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1181/3257 [00:03<00:06, 333.54it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1215/3257 [00:03<00:06, 321.60it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1253/3257 [00:03<00:05, 337.02it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1287/3257 [00:03<00:06, 327.54it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1323/3257 [00:03<00:05, 335.85it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1359/3257 [00:04<00:05, 341.81it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1394/3257 [00:04<00:05, 337.43it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1434/3257 [00:04<00:05, 353.09it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1474/3257 [00:04<00:04, 366.25it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:04<00:04, 371.62it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1551/3257 [00:04<00:04, 354.31it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1587/3257 [00:04<00:04, 351.77it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1624/3257 [00:04<00:04, 356.37it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1660/3257 [00:04<00:04, 349.62it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1696/3257 [00:04<00:04, 340.78it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:05<00:04, 335.33it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1766/3257 [00:05<00:04, 339.26it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1804/3257 [00:05<00:04, 348.53it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1839/3257 [00:05<00:04, 346.80it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1879/3257 [00:05<00:03, 356.82it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:05<00:03, 357.13it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1960/3257 [00:05<00:03, 379.70it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1999/3257 [00:05<00:03, 370.64it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2037/3257 [00:06<00:04, 258.85it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2068/3257 [00:06<00:04, 267.78it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2104/3257 [00:06<00:03, 289.69it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2136/3257 [00:06<00:03, 297.00it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2172/3257 [00:06<00:03, 313.56it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2208/3257 [00:06<00:03, 325.72it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2242/3257 [00:06<00:03, 329.50it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2276/3257 [00:06<00:02, 329.81it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2315/3257 [00:06<00:02, 346.41it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:06<00:02, 372.72it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2401/3257 [00:07<00:02, 383.30it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2440/3257 [00:07<00:02, 365.03it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2478/3257 [00:07<00:02, 367.99it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2520/3257 [00:07<00:01, 381.31it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2559/3257 [00:07<00:01, 373.92it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2597/3257 [00:07<00:01, 366.58it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2641/3257 [00:07<00:01, 386.21it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2680/3257 [00:07<00:01, 377.60it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2718/3257 [00:07<00:01, 354.60it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2760/3257 [00:08<00:01, 372.44it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2798/3257 [00:08<00:01, 374.19it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2836/3257 [00:08<00:01, 359.92it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:08<00:00, 384.83it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2920/3257 [00:08<00:00, 375.12it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2958/3257 [00:08<00:00, 362.33it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2995/3257 [00:08<00:00, 363.09it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3036/3257 [00:08<00:00, 376.25it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:08<00:00, 390.61it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3120/3257 [00:09<00:00, 394.93it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3160/3257 [00:09<00:00, 377.32it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3198/3257 [00:09<00:00, 374.29it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3237/3257 [00:09<00:00, 377.80it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 347.55it/s]
2023-02-07 13:35:18.938 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:35:18,939][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d52,n5,s0.351258,t4>', 'datetime': '2023-02-07T13:35:18.939689', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:35:18,940][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:35:18,940][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:35:19,127][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 13:35:19,127][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:35:19,134][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 2819 unique words (100.00% of original 2819, drops 0)', 'datetime': '2023-02-07T13:35:19.134477', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:35:19,134][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 2183622 word corpus (100.00% of original 2183622, drops 0)', 'datetime': '2023-02-07T13:35:19.134732', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:35:19,143][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 13:35:19,145][gensim.models.word2vec][INFO] - sample=0.351258 downsamples 0 most-common words
[2023-02-07 13:35:19,146][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2183622 word corpus (100.0%% of prior 2183622)', 'datetime': '2023-02-07T13:35:19.146204', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:35:19,162][gensim.models.word2vec][INFO] - estimated required memory for 2819 words and 52 dimensions: 3911060 bytes
[2023-02-07 13:35:19,162][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:35:19,163][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2819 vocabulary and 52 features, using sg=1 hs=0 sample=0.35125811111440863 negative=5 window=1 shrink_windows=True', 'datetime': '2023-02-07T13:35:19.163861', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:35:19,839][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2186879 effective words) took 0.7s, 3244086 effective words/s
[2023-02-07 13:35:20,511][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2186879 effective words) took 0.7s, 3265624 effective words/s
[2023-02-07 13:35:21,185][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2186879 effective words) took 0.7s, 3249179 effective words/s
[2023-02-07 13:35:21,853][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2186879 effective words) took 0.7s, 3282189 effective words/s
[2023-02-07 13:35:22,519][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2186879 effective words) took 0.7s, 3287260 effective words/s
[2023-02-07 13:35:23,192][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2186879 effective words) took 0.7s, 3258542 effective words/s
[2023-02-07 13:35:23,864][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2186879 effective words) took 0.7s, 3263452 effective words/s
[2023-02-07 13:35:24,537][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2186879 effective words) took 0.7s, 3255075 effective words/s
[2023-02-07 13:35:25,201][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2186879 effective words) took 0.7s, 3296888 effective words/s
[2023-02-07 13:35:25,880][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2186879 effective words) took 0.7s, 3225667 effective words/s
[2023-02-07 13:35:26,573][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2186879 effective words) took 0.7s, 3165644 effective words/s
[2023-02-07 13:35:27,274][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2186879 effective words) took 0.7s, 3124496 effective words/s
[2023-02-07 13:35:27,969][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2186879 effective words) took 0.7s, 3148763 effective words/s
[2023-02-07 13:35:28,662][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2186879 effective words) took 0.7s, 3164251 effective words/s
[2023-02-07 13:35:29,357][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2186879 effective words) took 0.7s, 3152959 effective words/s
[2023-02-07 13:35:30,056][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2186879 effective words) took 0.7s, 3139242 effective words/s
[2023-02-07 13:35:30,761][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2186879 effective words) took 0.7s, 3104455 effective words/s
[2023-02-07 13:35:31,456][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2186879 effective words) took 0.7s, 3154144 effective words/s
[2023-02-07 13:35:32,144][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2186879 effective words) took 0.7s, 3187139 effective words/s
[2023-02-07 13:35:32,836][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2186879 effective words) took 0.7s, 3168427 effective words/s
[2023-02-07 13:35:33,527][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2186879 effective words) took 0.7s, 3176246 effective words/s
[2023-02-07 13:35:34,220][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2186879 effective words) took 0.7s, 3161460 effective words/s
[2023-02-07 13:35:34,908][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2186879 effective words) took 0.7s, 3185564 effective words/s
[2023-02-07 13:35:35,605][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2186879 effective words) took 0.7s, 3142054 effective words/s
[2023-02-07 13:35:36,297][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2186879 effective words) took 0.7s, 3167331 effective words/s
[2023-02-07 13:35:36,988][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2186879 effective words) took 0.7s, 3176407 effective words/s
[2023-02-07 13:35:37,673][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2186879 effective words) took 0.7s, 3193921 effective words/s
[2023-02-07 13:35:38,358][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2186879 effective words) took 0.7s, 3199547 effective words/s
[2023-02-07 13:35:39,040][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2186879 effective words) took 0.7s, 3218247 effective words/s
[2023-02-07 13:35:39,722][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2186879 effective words) took 0.7s, 3210782 effective words/s
[2023-02-07 13:35:40,405][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2186879 effective words) took 0.7s, 3206646 effective words/s
[2023-02-07 13:35:41,089][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2186879 effective words) took 0.7s, 3206971 effective words/s
[2023-02-07 13:35:41,773][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2186879 effective words) took 0.7s, 3204438 effective words/s
[2023-02-07 13:35:42,459][gensim.models.word2vec][INFO] - EPOCH 33: training on 2183622 raw words (2186879 effective words) took 0.7s, 3190497 effective words/s
[2023-02-07 13:35:43,146][gensim.models.word2vec][INFO] - EPOCH 34: training on 2183622 raw words (2186879 effective words) took 0.7s, 3192403 effective words/s
[2023-02-07 13:35:43,831][gensim.models.word2vec][INFO] - EPOCH 35: training on 2183622 raw words (2186879 effective words) took 0.7s, 3196105 effective words/s
[2023-02-07 13:35:44,510][gensim.models.word2vec][INFO] - EPOCH 36: training on 2183622 raw words (2186879 effective words) took 0.7s, 3227187 effective words/s
[2023-02-07 13:35:45,155][gensim.models.word2vec][INFO] - EPOCH 37: training on 2183622 raw words (2186879 effective words) took 0.6s, 3397477 effective words/s
[2023-02-07 13:35:45,788][gensim.models.word2vec][INFO] - EPOCH 38: training on 2183622 raw words (2186879 effective words) took 0.6s, 3466091 effective words/s
[2023-02-07 13:35:46,421][gensim.models.word2vec][INFO] - EPOCH 39: training on 2183622 raw words (2186879 effective words) took 0.6s, 3460295 effective words/s
[2023-02-07 13:35:47,055][gensim.models.word2vec][INFO] - EPOCH 40: training on 2183622 raw words (2186879 effective words) took 0.6s, 3450594 effective words/s
[2023-02-07 13:35:47,689][gensim.models.word2vec][INFO] - EPOCH 41: training on 2183622 raw words (2186879 effective words) took 0.6s, 3460274 effective words/s
[2023-02-07 13:35:48,321][gensim.models.word2vec][INFO] - EPOCH 42: training on 2183622 raw words (2186879 effective words) took 0.6s, 3463897 effective words/s
[2023-02-07 13:35:48,953][gensim.models.word2vec][INFO] - EPOCH 43: training on 2183622 raw words (2186879 effective words) took 0.6s, 3467656 effective words/s
[2023-02-07 13:35:49,588][gensim.models.word2vec][INFO] - EPOCH 44: training on 2183622 raw words (2186879 effective words) took 0.6s, 3460797 effective words/s
[2023-02-07 13:35:50,215][gensim.models.word2vec][INFO] - EPOCH 45: training on 2183622 raw words (2186879 effective words) took 0.6s, 3492406 effective words/s
[2023-02-07 13:35:50,845][gensim.models.word2vec][INFO] - EPOCH 46: training on 2183622 raw words (2186879 effective words) took 0.6s, 3473996 effective words/s
[2023-02-07 13:35:51,477][gensim.models.word2vec][INFO] - EPOCH 47: training on 2183622 raw words (2186879 effective words) took 0.6s, 3467164 effective words/s
[2023-02-07 13:35:52,104][gensim.models.word2vec][INFO] - EPOCH 48: training on 2183622 raw words (2186879 effective words) took 0.6s, 3495315 effective words/s
[2023-02-07 13:35:52,732][gensim.models.word2vec][INFO] - EPOCH 49: training on 2183622 raw words (2186879 effective words) took 0.6s, 3489205 effective words/s
[2023-02-07 13:35:53,361][gensim.models.word2vec][INFO] - EPOCH 50: training on 2183622 raw words (2186879 effective words) took 0.6s, 3482536 effective words/s
[2023-02-07 13:35:53,989][gensim.models.word2vec][INFO] - EPOCH 51: training on 2183622 raw words (2186879 effective words) took 0.6s, 3490693 effective words/s
[2023-02-07 13:35:54,616][gensim.models.word2vec][INFO] - EPOCH 52: training on 2183622 raw words (2186879 effective words) took 0.6s, 3492566 effective words/s
[2023-02-07 13:35:55,241][gensim.models.word2vec][INFO] - EPOCH 53: training on 2183622 raw words (2186879 effective words) took 0.6s, 3506815 effective words/s
[2023-02-07 13:35:55,867][gensim.models.word2vec][INFO] - EPOCH 54: training on 2183622 raw words (2186879 effective words) took 0.6s, 3497429 effective words/s
[2023-02-07 13:35:56,494][gensim.models.word2vec][INFO] - EPOCH 55: training on 2183622 raw words (2186879 effective words) took 0.6s, 3493541 effective words/s
[2023-02-07 13:35:57,123][gensim.models.word2vec][INFO] - EPOCH 56: training on 2183622 raw words (2186879 effective words) took 0.6s, 3484761 effective words/s
[2023-02-07 13:35:57,756][gensim.models.word2vec][INFO] - EPOCH 57: training on 2183622 raw words (2186879 effective words) took 0.6s, 3461405 effective words/s
[2023-02-07 13:35:58,387][gensim.models.word2vec][INFO] - EPOCH 58: training on 2183622 raw words (2186879 effective words) took 0.6s, 3474448 effective words/s
[2023-02-07 13:35:59,013][gensim.models.word2vec][INFO] - EPOCH 59: training on 2183622 raw words (2186879 effective words) took 0.6s, 3496066 effective words/s
[2023-02-07 13:35:59,649][gensim.models.word2vec][INFO] - EPOCH 60: training on 2183622 raw words (2186879 effective words) took 0.6s, 3448379 effective words/s
[2023-02-07 13:36:00,281][gensim.models.word2vec][INFO] - EPOCH 61: training on 2183622 raw words (2186879 effective words) took 0.6s, 3462897 effective words/s
[2023-02-07 13:36:00,910][gensim.models.word2vec][INFO] - EPOCH 62: training on 2183622 raw words (2186879 effective words) took 0.6s, 3485767 effective words/s
[2023-02-07 13:36:01,532][gensim.models.word2vec][INFO] - EPOCH 63: training on 2183622 raw words (2186879 effective words) took 0.6s, 3523052 effective words/s
[2023-02-07 13:36:02,196][gensim.models.word2vec][INFO] - EPOCH 64: training on 2183622 raw words (2186879 effective words) took 0.7s, 3301265 effective words/s
[2023-02-07 13:36:02,858][gensim.models.word2vec][INFO] - EPOCH 65: training on 2183622 raw words (2186879 effective words) took 0.7s, 3309836 effective words/s
[2023-02-07 13:36:03,519][gensim.models.word2vec][INFO] - EPOCH 66: training on 2183622 raw words (2186879 effective words) took 0.7s, 3313346 effective words/s
[2023-02-07 13:36:04,206][gensim.models.word2vec][INFO] - EPOCH 67: training on 2183622 raw words (2186879 effective words) took 0.7s, 3190286 effective words/s
[2023-02-07 13:36:04,893][gensim.models.word2vec][INFO] - EPOCH 68: training on 2183622 raw words (2186879 effective words) took 0.7s, 3191798 effective words/s
[2023-02-07 13:36:05,582][gensim.models.word2vec][INFO] - EPOCH 69: training on 2183622 raw words (2186879 effective words) took 0.7s, 3178960 effective words/s
[2023-02-07 13:36:06,271][gensim.models.word2vec][INFO] - EPOCH 70: training on 2183622 raw words (2186879 effective words) took 0.7s, 3184285 effective words/s
[2023-02-07 13:36:06,958][gensim.models.word2vec][INFO] - EPOCH 71: training on 2183622 raw words (2186879 effective words) took 0.7s, 3188890 effective words/s
[2023-02-07 13:36:07,641][gensim.models.word2vec][INFO] - EPOCH 72: training on 2183622 raw words (2186879 effective words) took 0.7s, 3207210 effective words/s
[2023-02-07 13:36:08,330][gensim.models.word2vec][INFO] - EPOCH 73: training on 2183622 raw words (2186879 effective words) took 0.7s, 3180151 effective words/s
[2023-02-07 13:36:09,017][gensim.models.word2vec][INFO] - EPOCH 74: training on 2183622 raw words (2186879 effective words) took 0.7s, 3189335 effective words/s
[2023-02-07 13:36:09,702][gensim.models.word2vec][INFO] - EPOCH 75: training on 2183622 raw words (2186879 effective words) took 0.7s, 3197583 effective words/s
[2023-02-07 13:36:10,390][gensim.models.word2vec][INFO] - EPOCH 76: training on 2183622 raw words (2186879 effective words) took 0.7s, 3188965 effective words/s
[2023-02-07 13:36:11,075][gensim.models.word2vec][INFO] - EPOCH 77: training on 2183622 raw words (2186879 effective words) took 0.7s, 3199801 effective words/s
[2023-02-07 13:36:11,765][gensim.models.word2vec][INFO] - EPOCH 78: training on 2183622 raw words (2186879 effective words) took 0.7s, 3175881 effective words/s
[2023-02-07 13:36:12,452][gensim.models.word2vec][INFO] - EPOCH 79: training on 2183622 raw words (2186879 effective words) took 0.7s, 3186226 effective words/s
[2023-02-07 13:36:13,146][gensim.models.word2vec][INFO] - EPOCH 80: training on 2183622 raw words (2186879 effective words) took 0.7s, 3157479 effective words/s
[2023-02-07 13:36:13,830][gensim.models.word2vec][INFO] - EPOCH 81: training on 2183622 raw words (2186879 effective words) took 0.7s, 3203926 effective words/s
[2023-02-07 13:36:14,534][gensim.models.word2vec][INFO] - EPOCH 82: training on 2183622 raw words (2186879 effective words) took 0.7s, 3109613 effective words/s
[2023-02-07 13:36:15,234][gensim.models.word2vec][INFO] - EPOCH 83: training on 2183622 raw words (2186879 effective words) took 0.7s, 3132335 effective words/s
[2023-02-07 13:36:15,926][gensim.models.word2vec][INFO] - EPOCH 84: training on 2183622 raw words (2186879 effective words) took 0.7s, 3162355 effective words/s
[2023-02-07 13:36:16,615][gensim.models.word2vec][INFO] - EPOCH 85: training on 2183622 raw words (2186879 effective words) took 0.7s, 3180494 effective words/s
[2023-02-07 13:36:17,307][gensim.models.word2vec][INFO] - EPOCH 86: training on 2183622 raw words (2186879 effective words) took 0.7s, 3171395 effective words/s
[2023-02-07 13:36:17,999][gensim.models.word2vec][INFO] - EPOCH 87: training on 2183622 raw words (2186879 effective words) took 0.7s, 3164347 effective words/s
[2023-02-07 13:36:18,693][gensim.models.word2vec][INFO] - EPOCH 88: training on 2183622 raw words (2186879 effective words) took 0.7s, 3157612 effective words/s
[2023-02-07 13:36:19,386][gensim.models.word2vec][INFO] - EPOCH 89: training on 2183622 raw words (2186879 effective words) took 0.7s, 3160161 effective words/s
[2023-02-07 13:36:20,074][gensim.models.word2vec][INFO] - EPOCH 90: training on 2183622 raw words (2186879 effective words) took 0.7s, 3186443 effective words/s
[2023-02-07 13:36:20,757][gensim.models.word2vec][INFO] - EPOCH 91: training on 2183622 raw words (2186879 effective words) took 0.7s, 3206093 effective words/s
[2023-02-07 13:36:21,448][gensim.models.word2vec][INFO] - EPOCH 92: training on 2183622 raw words (2186879 effective words) took 0.7s, 3169546 effective words/s
[2023-02-07 13:36:21,449][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 203076846 raw words (203379747 effective words) took 62.3s, 3265297 effective words/s', 'datetime': '2023-02-07T13:36:21.449325', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:36:21.450 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:36:25,343][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133501-rvslblor/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:36:25.343102', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:36:25,343][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:36:25,352][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133501-rvslblor/files/../tmp/embedding_model.pt
2023-02-07 13:36:25.352 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:36:26.291 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:36:26.659 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:36:27.078 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8960902695654218, 'test_mae': 1.0447664980439504, 'test_r2': 0.10303593546289669}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.85
wandb: percentage 0.0
wandb:   test_mae 1.04477
wandb:   test_mse 1.89609
wandb:    test_r2 0.10304
wandb: 
wandb: üöÄ View run clean-sweep-48 at: https://wandb.ai/xiaoqiz/mof2vec/runs/rvslblor
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_133501-rvslblor/logs
wandb: Agent Starting Run: 7fnva01t with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 813
wandb: 	model.gensim.alpha: 0.0006284201558225726
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 45
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.3132564801212039
wandb: 	model.gensim.vector_size: 213
wandb: 	model.gensim.window: 2
wandb: 	model.sklearn.learning_rate: 0.03401770560809286
wandb: 	model.sklearn.max_depth: 14
wandb: 	model.sklearn.min_child_weight: 0.0315946836486475
wandb: 	model.sklearn.n_estimators: 4827
wandb: 	model.sklearn.num_leaves: 399
wandb: 	model.sklearn.reg_alpha: 0.09832395956246248
wandb: 	model.sklearn.reg_lambda: 0.35795980146883877
wandb: 	model.sklearn.subsample: 0.423651630294461
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133636-7fnva01t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-49
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/7fnva01t
2023-02-07 13:36:44.281 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 13:36:44.281 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 813 for sweep.
2023-02-07 13:36:44.282 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0006284201558225726 for sweep.
2023-02-07 13:36:44.282 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:36:44.282 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 45 for sweep.
2023-02-07 13:36:44.283 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 13:36:44.283 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3132564801212039 for sweep.
2023-02-07 13:36:44.283 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 213 for sweep.
2023-02-07 13:36:44.283 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 2 for sweep.
2023-02-07 13:36:44.283 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.03401770560809286 for sweep.
2023-02-07 13:36:44.284 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 14 for sweep.
2023-02-07 13:36:44.284 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0315946836486475 for sweep.
2023-02-07 13:36:44.284 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4827 for sweep.
2023-02-07 13:36:44.284 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 399 for sweep.
2023-02-07 13:36:44.284 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.09832395956246248 for sweep.
2023-02-07 13:36:44.285 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.35795980146883877 for sweep.
2023-02-07 13:36:44.285 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.423651630294461 for sweep.
2023-02-07 13:36:44.285 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:36:44.289 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133636-7fnva01t/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 813, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 213, 'window': 2, 'min_count': 5, 'dm': 0, 'sample': 0.3132564801212039, 'workers': 4, 'alpha': 0.0006284201558225726, 'epochs': 45}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4827, 'max_depth': 14, 'num_leaves': 399, 'reg_alpha': 0.09832395956246248, 'reg_lambda': 0.35795980146883877, 'subsample': 0.423651630294461, 'min_child_weight': 0.0315946836486475, 'n_jobs': 4, 'learning_rate': 0.03401770560809286}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 30/3257 [00:00<00:10, 299.87it/s]  2%|‚ñè         | 61/3257 [00:00<00:10, 303.00it/s]  3%|‚ñé         | 95/3257 [00:00<00:09, 316.52it/s]  4%|‚ñç         | 127/3257 [00:00<00:14, 212.18it/s]  5%|‚ñç         | 161/3257 [00:00<00:12, 246.69it/s]  6%|‚ñå         | 193/3257 [00:00<00:11, 264.90it/s]  7%|‚ñã         | 230/3257 [00:00<00:10, 292.75it/s]  8%|‚ñä         | 262/3257 [00:00<00:10, 297.57it/s]  9%|‚ñâ         | 299/3257 [00:01<00:09, 318.28it/s] 10%|‚ñà         | 333/3257 [00:01<00:09, 321.84it/s] 11%|‚ñà‚ñè        | 367/3257 [00:01<00:09, 312.44it/s] 12%|‚ñà‚ñè        | 399/3257 [00:01<00:09, 302.46it/s] 13%|‚ñà‚ñé        | 430/3257 [00:01<00:09, 290.01it/s] 14%|‚ñà‚ñç        | 462/3257 [00:01<00:09, 297.70it/s] 15%|‚ñà‚ñå        | 494/3257 [00:01<00:09, 302.73it/s] 16%|‚ñà‚ñå        | 527/3257 [00:01<00:08, 308.82it/s] 17%|‚ñà‚ñã        | 559/3257 [00:01<00:08, 303.44it/s] 18%|‚ñà‚ñä        | 590/3257 [00:02<00:08, 299.75it/s] 19%|‚ñà‚ñâ        | 622/3257 [00:02<00:08, 303.51it/s] 20%|‚ñà‚ñà        | 654/3257 [00:02<00:08, 305.49it/s] 21%|‚ñà‚ñà        | 685/3257 [00:02<00:08, 293.38it/s] 22%|‚ñà‚ñà‚ñè       | 719/3257 [00:02<00:08, 303.66it/s] 23%|‚ñà‚ñà‚ñé       | 750/3257 [00:02<00:08, 291.41it/s] 24%|‚ñà‚ñà‚ñç       | 782/3257 [00:02<00:08, 297.87it/s] 25%|‚ñà‚ñà‚ñç       | 814/3257 [00:02<00:08, 303.61it/s] 26%|‚ñà‚ñà‚ñå       | 845/3257 [00:02<00:08, 285.91it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:02<00:08, 289.39it/s] 28%|‚ñà‚ñà‚ñä       | 907/3257 [00:03<00:07, 297.95it/s] 29%|‚ñà‚ñà‚ñâ       | 937/3257 [00:03<00:07, 297.41it/s] 30%|‚ñà‚ñà‚ñâ       | 970/3257 [00:03<00:07, 303.80it/s] 31%|‚ñà‚ñà‚ñà       | 1001/3257 [00:03<00:07, 300.61it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1032/3257 [00:03<00:07, 291.09it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1062/3257 [00:03<00:07, 286.81it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:03<00:07, 285.21it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1123/3257 [00:03<00:07, 294.06it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1153/3257 [00:03<00:07, 291.20it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1183/3257 [00:04<00:07, 290.14it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1213/3257 [00:04<00:07, 274.37it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1247/3257 [00:04<00:06, 291.64it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1277/3257 [00:04<00:06, 287.68it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1306/3257 [00:04<00:06, 286.95it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:04<00:06, 294.98it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1369/3257 [00:04<00:06, 294.44it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1399/3257 [00:04<00:06, 293.79it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1429/3257 [00:05<00:08, 214.85it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1463/3257 [00:05<00:07, 242.97it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1498/3257 [00:05<00:06, 269.54it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1528/3257 [00:05<00:06, 272.35it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1558/3257 [00:05<00:06, 273.48it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1590/3257 [00:05<00:05, 285.18it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1622/3257 [00:05<00:05, 293.10it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1653/3257 [00:05<00:05, 288.57it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1683/3257 [00:05<00:05, 285.78it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:05<00:05, 291.47it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1745/3257 [00:06<00:05, 282.47it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1779/3257 [00:06<00:05, 295.27it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1809/3257 [00:06<00:04, 294.61it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1842/3257 [00:06<00:04, 304.24it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1876/3257 [00:06<00:04, 311.12it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1908/3257 [00:06<00:04, 313.02it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1942/3257 [00:06<00:04, 320.92it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:06<00:03, 328.44it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2010/3257 [00:06<00:03, 325.86it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2043/3257 [00:06<00:03, 324.74it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2076/3257 [00:07<00:03, 309.93it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2108/3257 [00:07<00:03, 303.67it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2139/3257 [00:07<00:03, 294.29it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:07<00:03, 293.34it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2201/3257 [00:07<00:03, 300.38it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2232/3257 [00:07<00:03, 297.70it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2262/3257 [00:07<00:03, 292.45it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2294/3257 [00:07<00:03, 299.85it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2330/3257 [00:07<00:02, 314.58it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2367/3257 [00:08<00:02, 326.07it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2403/3257 [00:08<00:02, 335.76it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2437/3257 [00:08<00:02, 313.61it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2471/3257 [00:08<00:02, 320.97it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2507/3257 [00:08<00:02, 327.23it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2541/3257 [00:08<00:02, 328.83it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2575/3257 [00:08<00:02, 315.64it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2608/3257 [00:08<00:02, 318.91it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2643/3257 [00:08<00:01, 327.32it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2676/3257 [00:09<00:01, 318.90it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2709/3257 [00:09<00:01, 296.26it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2744/3257 [00:09<00:01, 309.68it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2776/3257 [00:09<00:01, 305.89it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2810/3257 [00:09<00:01, 314.84it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2842/3257 [00:09<00:01, 302.20it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2873/3257 [00:09<00:01, 203.90it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2901/3257 [00:09<00:01, 217.52it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2934/3257 [00:10<00:01, 242.12it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2962/3257 [00:10<00:01, 245.03it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2989/3257 [00:10<00:01, 248.73it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3021/3257 [00:10<00:00, 266.34it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3053/3257 [00:10<00:00, 278.13it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3087/3257 [00:10<00:00, 290.89it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3123/3257 [00:10<00:00, 309.22it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3155/3257 [00:10<00:00, 292.89it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3185/3257 [00:10<00:00, 285.15it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3215/3257 [00:11<00:00, 287.07it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3252/3257 [00:11<00:00, 308.16it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 292.28it/s]
2023-02-07 13:36:55.790 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:36:55,791][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d213,n5,mc5,s0.313256,t4>', 'datetime': '2023-02-07T13:36:55.791514', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:36:55,791][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:36:55,791][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:36:56,047][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 13:36:56,048][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:36:56,057][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 3705 unique words (55.61% of original 6662, drops 2957)', 'datetime': '2023-02-07T13:36:56.057775', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:36:56,059][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 2904334 word corpus (99.75% of original 2911496, drops 7162)', 'datetime': '2023-02-07T13:36:56.059093', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:36:56,071][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 13:36:56,071][gensim.models.word2vec][INFO] - sample=0.313256 downsamples 0 most-common words
[2023-02-07 13:36:56,071][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2904334 word corpus (100.0%% of prior 2904334)', 'datetime': '2023-02-07T13:36:56.071926', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:36:56,092][gensim.models.word2vec][INFO] - estimated required memory for 3705 words and 213 dimensions: 11592184 bytes
[2023-02-07 13:36:56,093][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:36:56,098][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 3705 vocabulary and 213 features, using sg=1 hs=0 sample=0.3132564801212039 negative=5 window=2 shrink_windows=True', 'datetime': '2023-02-07T13:36:56.098670', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:36:57,103][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 77.86% examples, 2277926 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:36:57,368][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2907591 effective words) took 1.3s, 2294114 effective words/s
[2023-02-07 13:36:58,374][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 77.43% examples, 2265671 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:36:58,646][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2907591 effective words) took 1.3s, 2278224 effective words/s
[2023-02-07 13:36:59,649][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 77.86% examples, 2279596 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:36:59,918][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2907591 effective words) took 1.3s, 2288079 effective words/s
[2023-02-07 13:37:00,922][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 77.86% examples, 2277710 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:01,182][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2907591 effective words) took 1.3s, 2302550 effective words/s
[2023-02-07 13:37:02,194][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 78.72% examples, 2289033 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:37:02,447][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2907591 effective words) took 1.3s, 2301698 effective words/s
[2023-02-07 13:37:03,448][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 77.46% examples, 2275636 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:03,723][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2907591 effective words) took 1.3s, 2281998 effective words/s
[2023-02-07 13:37:04,727][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 76.17% examples, 2239077 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:05,011][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2907591 effective words) took 1.3s, 2258895 effective words/s
[2023-02-07 13:37:06,016][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 78.72% examples, 2306546 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:06,267][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2907591 effective words) took 1.3s, 2319015 effective words/s
[2023-02-07 13:37:07,269][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 78.94% examples, 2320842 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:07,516][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2907591 effective words) took 1.2s, 2330477 effective words/s
[2023-02-07 13:37:08,524][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 80.01% examples, 2337881 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:08,762][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2907591 effective words) took 1.2s, 2337173 effective words/s
[2023-02-07 13:37:09,768][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 78.17% examples, 2284654 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:10,025][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2907591 effective words) took 1.3s, 2306112 effective words/s
[2023-02-07 13:37:11,037][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 78.72% examples, 2290022 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:11,286][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2907591 effective words) took 1.3s, 2309803 effective words/s
[2023-02-07 13:37:12,288][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 80.01% examples, 2351445 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:12,524][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2907591 effective words) took 1.2s, 2352973 effective words/s
[2023-02-07 13:37:13,526][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 78.94% examples, 2321737 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:13,780][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2907591 effective words) took 1.3s, 2319301 effective words/s
[2023-02-07 13:37:14,786][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 78.94% examples, 2310951 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:37:15,033][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2907591 effective words) took 1.3s, 2322302 effective words/s
[2023-02-07 13:37:16,038][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 78.72% examples, 2306043 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:16,289][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2907591 effective words) took 1.3s, 2319531 effective words/s
[2023-02-07 13:37:17,291][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 78.72% examples, 2312399 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:17,540][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2907591 effective words) took 1.2s, 2326339 effective words/s
[2023-02-07 13:37:18,547][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 78.94% examples, 2309343 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:18,796][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2907591 effective words) took 1.3s, 2317551 effective words/s
[2023-02-07 13:37:19,810][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 78.94% examples, 2295322 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:37:20,052][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2907591 effective words) took 1.3s, 2317828 effective words/s
[2023-02-07 13:37:21,057][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 78.72% examples, 2307805 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:21,311][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2907591 effective words) took 1.3s, 2314321 effective words/s
[2023-02-07 13:37:22,322][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 80.01% examples, 2331638 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:22,553][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2907591 effective words) took 1.2s, 2344154 effective words/s
[2023-02-07 13:37:23,559][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 80.01% examples, 2343301 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:23,791][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2907591 effective words) took 1.2s, 2353047 effective words/s
[2023-02-07 13:37:24,796][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 78.63% examples, 2305602 words/s, in_qsize 6, out_qsize 1
[2023-02-07 13:37:25,041][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2907591 effective words) took 1.2s, 2327701 effective words/s
[2023-02-07 13:37:26,049][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 82.90% examples, 2423987 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:26,229][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2907591 effective words) took 1.2s, 2451612 effective words/s
[2023-02-07 13:37:27,232][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 84.74% examples, 2481965 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:27,398][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2907591 effective words) took 1.2s, 2491081 effective words/s
[2023-02-07 13:37:28,401][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 85.32% examples, 2498652 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:28,560][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2907591 effective words) took 1.2s, 2503719 effective words/s
[2023-02-07 13:37:29,566][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 84.28% examples, 2464008 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:29,732][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2907591 effective words) took 1.2s, 2484281 effective words/s
[2023-02-07 13:37:30,735][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 85.32% examples, 2500118 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:30,888][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2907591 effective words) took 1.2s, 2518120 effective words/s
[2023-02-07 13:37:31,891][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 84.99% examples, 2489591 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:32,053][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2907591 effective words) took 1.2s, 2498108 effective words/s
[2023-02-07 13:37:33,056][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 85.32% examples, 2499605 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:33,214][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2907591 effective words) took 1.2s, 2506704 effective words/s
[2023-02-07 13:37:34,217][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 83.94% examples, 2462766 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:34,389][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2907591 effective words) took 1.2s, 2477660 effective words/s
[2023-02-07 13:37:35,396][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 84.99% examples, 2479659 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:37:35,558][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2907591 effective words) took 1.2s, 2488926 effective words/s
[2023-02-07 13:37:36,562][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 85.32% examples, 2497209 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:36,719][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2907591 effective words) took 1.2s, 2507130 effective words/s
[2023-02-07 13:37:37,729][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 80.87% examples, 2351346 words/s, in_qsize 6, out_qsize 1
[2023-02-07 13:37:37,952][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2907591 effective words) took 1.2s, 2361724 effective words/s
[2023-02-07 13:37:38,955][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 82.74% examples, 2422908 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:39,144][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2907591 effective words) took 1.2s, 2442331 effective words/s
[2023-02-07 13:37:40,146][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 83.94% examples, 2463617 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:40,334][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2907591 effective words) took 1.2s, 2445515 effective words/s
[2023-02-07 13:37:41,339][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 81.82% examples, 2389900 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:41,538][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2907591 effective words) took 1.2s, 2416983 effective words/s
[2023-02-07 13:37:42,541][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 82.16% examples, 2403280 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:42,744][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2907591 effective words) took 1.2s, 2412876 effective words/s
[2023-02-07 13:37:43,754][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 85.32% examples, 2483326 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:43,908][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2907591 effective words) took 1.2s, 2502141 effective words/s
[2023-02-07 13:37:44,914][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 85.32% examples, 2493419 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:45,069][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2907591 effective words) took 1.2s, 2506782 effective words/s
[2023-02-07 13:37:46,073][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 86.15% examples, 2517654 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:46,220][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2907591 effective words) took 1.1s, 2528373 effective words/s
[2023-02-07 13:37:47,228][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 85.32% examples, 2489192 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:47,384][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2907591 effective words) took 1.2s, 2502093 effective words/s
[2023-02-07 13:37:48,391][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 85.78% examples, 2500319 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:48,540][gensim.models.word2vec][INFO] - EPOCH 42: training on 2911496 raw words (2907591 effective words) took 1.2s, 2518505 effective words/s
[2023-02-07 13:37:49,542][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 85.45% examples, 2502363 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:49,690][gensim.models.word2vec][INFO] - EPOCH 43: training on 2911496 raw words (2907591 effective words) took 1.1s, 2531719 effective words/s
[2023-02-07 13:37:50,692][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 80.93% examples, 2368992 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:37:50,919][gensim.models.word2vec][INFO] - EPOCH 44: training on 2911496 raw words (2907591 effective words) took 1.2s, 2368122 effective words/s
[2023-02-07 13:37:50,919][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 131017320 raw words (130841595 effective words) took 54.8s, 2386708 effective words/s', 'datetime': '2023-02-07T13:37:50.919895', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:37:50.920 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:37:54,231][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133636-7fnva01t/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:37:54.230980', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:37:54,232][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:37:54,261][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133636-7fnva01t/files/../tmp/embedding_model.pt
2023-02-07 13:37:54.262 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:37:55.713 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:37:56.264 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:37:57.685 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.034620744078377, 'test_mae': 1.0586657490398437, 'test_r2': 0.037502738296143034}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.79
wandb: percentage 0.44386
wandb:   test_mae 1.05867
wandb:   test_mse 2.03462
wandb:    test_r2 0.0375
wandb: 
wandb: üöÄ View run icy-sweep-49 at: https://wandb.ai/xiaoqiz/mof2vec/runs/7fnva01t
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_133636-7fnva01t/logs
wandb: Agent Starting Run: mry9n0ct with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 731
wandb: 	model.gensim.alpha: 0.0005870844606001192
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 32
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.2485177593708202
wandb: 	model.gensim.vector_size: 130
wandb: 	model.gensim.window: 4
wandb: 	model.sklearn.learning_rate: 0.785612522856308
wandb: 	model.sklearn.max_depth: 30
wandb: 	model.sklearn.min_child_weight: 0.020875371344770396
wandb: 	model.sklearn.n_estimators: 474
wandb: 	model.sklearn.num_leaves: 388
wandb: 	model.sklearn.reg_alpha: 0.0620438086273427
wandb: 	model.sklearn.reg_lambda: 0.8261513436741057
wandb: 	model.sklearn.subsample: 0.4885153175405004
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133810-mry9n0ct
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-sweep-50
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/mry9n0ct
2023-02-07 13:38:18.641 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 13:38:18.642 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 731 for sweep.
2023-02-07 13:38:18.642 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0005870844606001192 for sweep.
2023-02-07 13:38:18.643 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:38:18.643 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 32 for sweep.
2023-02-07 13:38:18.643 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 13:38:18.643 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2485177593708202 for sweep.
2023-02-07 13:38:18.643 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 130 for sweep.
2023-02-07 13:38:18.644 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 4 for sweep.
2023-02-07 13:38:18.644 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.785612522856308 for sweep.
2023-02-07 13:38:18.644 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 30 for sweep.
2023-02-07 13:38:18.645 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.020875371344770396 for sweep.
2023-02-07 13:38:18.645 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 474 for sweep.
2023-02-07 13:38:18.645 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 388 for sweep.
2023-02-07 13:38:18.645 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.0620438086273427 for sweep.
2023-02-07 13:38:18.646 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.8261513436741057 for sweep.
2023-02-07 13:38:18.646 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.4885153175405004 for sweep.
2023-02-07 13:38:18.646 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:38:18.652 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133810-mry9n0ct/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 731, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 130, 'window': 4, 'min_count': 4, 'dm': 0, 'sample': 0.2485177593708202, 'workers': 4, 'alpha': 0.0005870844606001192, 'epochs': 32}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 474, 'max_depth': 30, 'num_leaves': 388, 'reg_alpha': 0.0620438086273427, 'reg_lambda': 0.8261513436741057, 'subsample': 0.4885153175405004, 'min_child_weight': 0.020875371344770396, 'n_jobs': 4, 'learning_rate': 0.785612522856308}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 32/3257 [00:00<00:10, 313.92it/s]  2%|‚ñè         | 64/3257 [00:00<00:10, 312.34it/s]  3%|‚ñé         | 96/3257 [00:00<00:10, 311.41it/s]  4%|‚ñç         | 128/3257 [00:00<00:10, 310.19it/s]  5%|‚ñå         | 163/3257 [00:00<00:09, 317.75it/s]  6%|‚ñå         | 198/3257 [00:00<00:09, 326.80it/s]  7%|‚ñã         | 236/3257 [00:00<00:08, 341.40it/s]  8%|‚ñä         | 271/3257 [00:00<00:08, 337.17it/s] 10%|‚ñâ         | 310/3257 [00:00<00:08, 352.38it/s] 11%|‚ñà         | 346/3257 [00:01<00:08, 346.91it/s] 12%|‚ñà‚ñè        | 381/3257 [00:01<00:08, 339.32it/s] 13%|‚ñà‚ñé        | 416/3257 [00:01<00:08, 340.16it/s] 14%|‚ñà‚ñç        | 451/3257 [00:01<00:09, 310.40it/s] 15%|‚ñà‚ñç        | 484/3257 [00:01<00:08, 314.66it/s] 16%|‚ñà‚ñå        | 521/3257 [00:01<00:08, 328.86it/s] 17%|‚ñà‚ñã        | 555/3257 [00:01<00:08, 324.85it/s] 18%|‚ñà‚ñä        | 588/3257 [00:01<00:08, 297.06it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:01<00:08, 299.80it/s] 20%|‚ñà‚ñà        | 653/3257 [00:02<00:08, 307.25it/s] 21%|‚ñà‚ñà        | 685/3257 [00:02<00:08, 298.26it/s] 22%|‚ñà‚ñà‚ñè       | 720/3257 [00:02<00:08, 307.43it/s] 23%|‚ñà‚ñà‚ñé       | 751/3257 [00:02<00:08, 299.83it/s] 24%|‚ñà‚ñà‚ñç       | 783/3257 [00:02<00:08, 301.88it/s] 25%|‚ñà‚ñà‚ñå       | 817/3257 [00:02<00:07, 312.15it/s] 26%|‚ñà‚ñà‚ñå       | 849/3257 [00:02<00:08, 296.90it/s] 27%|‚ñà‚ñà‚ñã       | 879/3257 [00:02<00:08, 296.72it/s] 28%|‚ñà‚ñà‚ñä       | 913/3257 [00:02<00:07, 308.20it/s] 29%|‚ñà‚ñà‚ñâ       | 944/3257 [00:03<00:10, 223.03it/s] 30%|‚ñà‚ñà‚ñà       | 978/3257 [00:03<00:09, 247.94it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:03<00:08, 254.34it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1037/3257 [00:03<00:08, 265.88it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:03<00:08, 273.65it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1097/3257 [00:03<00:07, 280.50it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1127/3257 [00:03<00:07, 285.34it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1159/3257 [00:03<00:07, 294.19it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1189/3257 [00:03<00:07, 277.71it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1218/3257 [00:04<00:07, 280.43it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1254/3257 [00:04<00:06, 302.24it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1285/3257 [00:04<00:06, 288.76it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1316/3257 [00:04<00:06, 293.65it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1347/3257 [00:04<00:06, 297.73it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1377/3257 [00:04<00:06, 293.28it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1407/3257 [00:04<00:06, 294.08it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1440/3257 [00:04<00:06, 302.62it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1477/3257 [00:04<00:05, 318.24it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:04<00:05, 324.55it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1544/3257 [00:05<00:05, 296.06it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1575/3257 [00:05<00:05, 295.12it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1612/3257 [00:05<00:05, 314.55it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1644/3257 [00:05<00:05, 307.18it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1675/3257 [00:05<00:05, 302.08it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1707/3257 [00:05<00:05, 305.11it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1738/3257 [00:05<00:05, 292.17it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1773/3257 [00:05<00:04, 307.75it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:05<00:04, 307.27it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1836/3257 [00:06<00:04, 306.26it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1870/3257 [00:06<00:04, 315.39it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1904/3257 [00:06<00:04, 322.57it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1939/3257 [00:06<00:03, 329.58it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:06<00:03, 340.23it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2012/3257 [00:06<00:03, 336.94it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2047/3257 [00:06<00:03, 331.00it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2081/3257 [00:06<00:03, 327.45it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2114/3257 [00:06<00:03, 326.09it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2147/3257 [00:07<00:03, 302.21it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2182/3257 [00:07<00:03, 310.96it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2217/3257 [00:07<00:03, 319.29it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2250/3257 [00:07<00:04, 209.73it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2276/3257 [00:07<00:04, 219.83it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2309/3257 [00:07<00:03, 243.77it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2351/3257 [00:07<00:03, 286.80it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2387/3257 [00:07<00:02, 305.76it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2421/3257 [00:08<00:02, 311.04it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2454/3257 [00:08<00:02, 306.08it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2490/3257 [00:08<00:02, 320.76it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2529/3257 [00:08<00:02, 339.45it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2564/3257 [00:08<00:02, 332.96it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2598/3257 [00:08<00:02, 324.23it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2638/3257 [00:08<00:01, 345.51it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2673/3257 [00:08<00:01, 336.47it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2707/3257 [00:08<00:01, 320.32it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2742/3257 [00:09<00:01, 328.26it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2776/3257 [00:09<00:01, 326.16it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2813/3257 [00:09<00:01, 336.65it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2847/3257 [00:09<00:01, 327.49it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2888/3257 [00:09<00:01, 347.30it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2923/3257 [00:09<00:00, 346.03it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2958/3257 [00:09<00:00, 326.67it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2991/3257 [00:09<00:00, 327.07it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3028/3257 [00:09<00:00, 337.22it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3068/3257 [00:09<00:00, 355.28it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3105/3257 [00:10<00:00, 358.38it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3141/3257 [00:10<00:00, 354.28it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3177/3257 [00:10<00:00, 328.78it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3211/3257 [00:10<00:00, 322.63it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:10<00:00, 323.88it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:10<00:00, 308.72it/s]
2023-02-07 13:38:29.546 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:38:29,547][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d130,n5,mc4,s0.248518,t4>', 'datetime': '2023-02-07T13:38:29.547361', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:38:29,547][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:38:29,547][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:38:29,799][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 13:38:29,800][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:38:29,812][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 4674 unique words (70.16% of original 6662, drops 1988)', 'datetime': '2023-02-07T13:38:29.812333', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:38:29,812][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 2908210 word corpus (99.89% of original 2911496, drops 3286)', 'datetime': '2023-02-07T13:38:29.812701', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:38:29,828][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 13:38:29,828][gensim.models.word2vec][INFO] - sample=0.248518 downsamples 0 most-common words
[2023-02-07 13:38:29,829][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2908210 word corpus (100.0%% of prior 2908210)', 'datetime': '2023-02-07T13:38:29.829914', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:38:29,857][gensim.models.word2vec][INFO] - estimated required memory for 4674 words and 130 dimensions: 9543000 bytes
[2023-02-07 13:38:29,858][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:38:29,862][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 4674 vocabulary and 130 features, using sg=1 hs=0 sample=0.2485177593708202 negative=5 window=4 shrink_windows=True', 'datetime': '2023-02-07T13:38:29.862294', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:38:30,803][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2911467 effective words) took 0.9s, 3103051 effective words/s
[2023-02-07 13:38:31,724][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2911467 effective words) took 0.9s, 3165203 effective words/s
[2023-02-07 13:38:32,656][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2911467 effective words) took 0.9s, 3125687 effective words/s
[2023-02-07 13:38:33,576][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2911467 effective words) took 0.9s, 3172546 effective words/s
[2023-02-07 13:38:34,501][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2911467 effective words) took 0.9s, 3149046 effective words/s
[2023-02-07 13:38:35,426][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2911467 effective words) took 0.9s, 3154316 effective words/s
[2023-02-07 13:38:36,344][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2911467 effective words) took 0.9s, 3172977 effective words/s
[2023-02-07 13:38:37,265][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2911467 effective words) took 0.9s, 3168645 effective words/s
[2023-02-07 13:38:38,179][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2911467 effective words) took 0.9s, 3188889 effective words/s
[2023-02-07 13:38:39,091][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2911467 effective words) took 0.9s, 3198058 effective words/s
[2023-02-07 13:38:40,007][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2911467 effective words) took 0.9s, 3181617 effective words/s
[2023-02-07 13:38:40,926][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2911467 effective words) took 0.9s, 3175326 effective words/s
[2023-02-07 13:38:41,834][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2911467 effective words) took 0.9s, 3209085 effective words/s
[2023-02-07 13:38:42,766][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2911467 effective words) took 0.9s, 3133269 effective words/s
[2023-02-07 13:38:43,687][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2911467 effective words) took 0.9s, 3164770 effective words/s
[2023-02-07 13:38:44,611][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2911467 effective words) took 0.9s, 3156178 effective words/s
[2023-02-07 13:38:45,533][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2911467 effective words) took 0.9s, 3164654 effective words/s
[2023-02-07 13:38:46,458][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2911467 effective words) took 0.9s, 3148320 effective words/s
[2023-02-07 13:38:47,349][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2911467 effective words) took 0.9s, 3276210 effective words/s
[2023-02-07 13:38:48,213][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2911467 effective words) took 0.9s, 3374987 effective words/s
[2023-02-07 13:38:49,080][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2911467 effective words) took 0.9s, 3367418 effective words/s
[2023-02-07 13:38:49,937][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2911467 effective words) took 0.9s, 3402213 effective words/s
[2023-02-07 13:38:50,817][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2911467 effective words) took 0.9s, 3312846 effective words/s
[2023-02-07 13:38:51,700][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2911467 effective words) took 0.9s, 3299673 effective words/s
[2023-02-07 13:38:52,585][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2911467 effective words) took 0.9s, 3296229 effective words/s
[2023-02-07 13:38:53,496][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2911467 effective words) took 0.9s, 3202705 effective words/s
[2023-02-07 13:38:54,412][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2911467 effective words) took 0.9s, 3182214 effective words/s
[2023-02-07 13:38:55,313][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2911467 effective words) took 0.9s, 3237088 effective words/s
[2023-02-07 13:38:56,223][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2911467 effective words) took 0.9s, 3201605 effective words/s
[2023-02-07 13:38:57,109][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2911467 effective words) took 0.9s, 3288879 effective words/s
[2023-02-07 13:38:58,010][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2911467 effective words) took 0.9s, 3236081 effective words/s
[2023-02-07 13:38:58,921][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2911467 effective words) took 0.9s, 3200307 effective words/s
[2023-02-07 13:38:58,921][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 93167872 raw words (93166944 effective words) took 29.1s, 3206110 effective words/s', 'datetime': '2023-02-07T13:38:58.921806', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:38:58.921 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:39:00,674][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133810-mry9n0ct/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:39:00.674794', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:39:00,675][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:39:00,693][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133810-mry9n0ct/files/../tmp/embedding_model.pt
2023-02-07 13:39:00.693 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:39:01.795 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:39:02.222 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:39:03.073 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.7868466976112758, 'test_mae': 1.0298135617370803, 'test_r2': 0.15471467665858918}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.67
wandb: percentage 0.29841
wandb:   test_mae 1.02981
wandb:   test_mse 1.78685
wandb:    test_r2 0.15471
wandb: 
wandb: üöÄ View run sunny-sweep-50 at: https://wandb.ai/xiaoqiz/mof2vec/runs/mry9n0ct
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_133810-mry9n0ct/logs
wandb: Agent Starting Run: ncewides with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 242
wandb: 	model.gensim.alpha: 0.002850066040289859
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 98
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.6001186619967673
wandb: 	model.gensim.vector_size: 72
wandb: 	model.gensim.window: 2
wandb: 	model.sklearn.learning_rate: 0.14832058779696167
wandb: 	model.sklearn.max_depth: 37
wandb: 	model.sklearn.min_child_weight: 0.05169229235266506
wandb: 	model.sklearn.n_estimators: 2265
wandb: 	model.sklearn.num_leaves: 478
wandb: 	model.sklearn.reg_alpha: 0.00440522994054103
wandb: 	model.sklearn.reg_lambda: 0.5250365204075885
wandb: 	model.sklearn.subsample: 0.384285229861119
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133915-ncewides
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-sweep-51
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/ncewides
2023-02-07 13:39:23.527 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 13:39:23.528 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 242 for sweep.
2023-02-07 13:39:23.528 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.002850066040289859 for sweep.
2023-02-07 13:39:23.528 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:39:23.528 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 98 for sweep.
2023-02-07 13:39:23.529 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 13:39:23.529 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6001186619967673 for sweep.
2023-02-07 13:39:23.529 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 72 for sweep.
2023-02-07 13:39:23.529 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 2 for sweep.
2023-02-07 13:39:23.529 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.14832058779696167 for sweep.
2023-02-07 13:39:23.530 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 37 for sweep.
2023-02-07 13:39:23.530 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05169229235266506 for sweep.
2023-02-07 13:39:23.530 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2265 for sweep.
2023-02-07 13:39:23.530 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 478 for sweep.
2023-02-07 13:39:23.530 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.00440522994054103 for sweep.
2023-02-07 13:39:23.531 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.5250365204075885 for sweep.
2023-02-07 13:39:23.532 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.384285229861119 for sweep.
2023-02-07 13:39:23.532 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:39:23.538 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133915-ncewides/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 242, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 72, 'window': 2, 'min_count': 1, 'dm': 0, 'sample': 0.6001186619967673, 'workers': 4, 'alpha': 0.002850066040289859, 'epochs': 98}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2265, 'max_depth': 37, 'num_leaves': 478, 'reg_alpha': 0.00440522994054103, 'reg_lambda': 0.5250365204075885, 'subsample': 0.384285229861119, 'min_child_weight': 0.05169229235266506, 'n_jobs': 4, 'learning_rate': 0.14832058779696167}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 34/3257 [00:00<00:09, 325.73it/s]  2%|‚ñè         | 70/3257 [00:00<00:09, 344.75it/s]  3%|‚ñé         | 106/3257 [00:00<00:09, 341.75it/s]  4%|‚ñç         | 145/3257 [00:00<00:08, 359.70it/s]  6%|‚ñå         | 182/3257 [00:00<00:08, 349.51it/s]  7%|‚ñã         | 221/3257 [00:00<00:08, 360.82it/s]  8%|‚ñä         | 259/3257 [00:00<00:08, 362.98it/s]  9%|‚ñâ         | 299/3257 [00:00<00:07, 372.08it/s] 10%|‚ñà         | 337/3257 [00:00<00:07, 370.32it/s] 12%|‚ñà‚ñè        | 375/3257 [00:01<00:07, 367.39it/s] 13%|‚ñà‚ñé        | 412/3257 [00:01<00:07, 364.97it/s] 14%|‚ñà‚ñç        | 449/3257 [00:01<00:11, 247.33it/s] 15%|‚ñà‚ñç        | 486/3257 [00:01<00:10, 273.13it/s] 16%|‚ñà‚ñå        | 524/3257 [00:01<00:09, 298.66it/s] 17%|‚ñà‚ñã        | 558/3257 [00:01<00:08, 306.18it/s] 18%|‚ñà‚ñä        | 592/3257 [00:01<00:08, 313.10it/s] 19%|‚ñà‚ñâ        | 630/3257 [00:01<00:07, 329.55it/s] 20%|‚ñà‚ñà        | 665/3257 [00:02<00:07, 325.17it/s] 21%|‚ñà‚ñà‚ñè       | 700/3257 [00:02<00:07, 329.69it/s] 23%|‚ñà‚ñà‚ñé       | 736/3257 [00:02<00:07, 331.65it/s] 24%|‚ñà‚ñà‚ñé       | 773/3257 [00:02<00:07, 337.40it/s] 25%|‚ñà‚ñà‚ñç       | 808/3257 [00:02<00:07, 340.69it/s] 26%|‚ñà‚ñà‚ñå       | 843/3257 [00:02<00:07, 331.68it/s] 27%|‚ñà‚ñà‚ñã       | 877/3257 [00:02<00:07, 328.35it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:02<00:06, 336.29it/s] 29%|‚ñà‚ñà‚ñâ       | 951/3257 [00:02<00:06, 345.85it/s] 30%|‚ñà‚ñà‚ñà       | 986/3257 [00:02<00:06, 340.42it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1022/3257 [00:03<00:06, 344.78it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1057/3257 [00:03<00:06, 330.34it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:03<00:06, 331.78it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1126/3257 [00:03<00:06, 336.45it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1161/3257 [00:03<00:06, 339.12it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1195/3257 [00:03<00:06, 324.80it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1232/3257 [00:03<00:06, 332.49it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1268/3257 [00:03<00:05, 338.60it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1302/3257 [00:03<00:06, 323.91it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1340/3257 [00:04<00:05, 338.94it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1375/3257 [00:04<00:05, 334.73it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1412/3257 [00:04<00:05, 344.76it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1452/3257 [00:04<00:05, 356.88it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1491/3257 [00:04<00:04, 365.85it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1528/3257 [00:04<00:04, 359.05it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1564/3257 [00:04<00:04, 348.41it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1600/3257 [00:04<00:04, 350.36it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1636/3257 [00:04<00:04, 346.71it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1671/3257 [00:04<00:04, 341.93it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1707/3257 [00:05<00:04, 346.27it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1742/3257 [00:05<00:04, 332.04it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1776/3257 [00:05<00:05, 251.33it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1811/3257 [00:05<00:05, 273.05it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1846/3257 [00:05<00:04, 291.60it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1883/3257 [00:05<00:04, 311.21it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1918/3257 [00:05<00:04, 318.09it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1963/3257 [00:05<00:03, 353.00it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2000/3257 [00:06<00:03, 355.70it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2037/3257 [00:06<00:03, 356.79it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2074/3257 [00:06<00:03, 343.32it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2109/3257 [00:06<00:03, 340.91it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2144/3257 [00:06<00:03, 326.22it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:06<00:03, 334.65it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2216/3257 [00:06<00:03, 337.76it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2250/3257 [00:06<00:02, 336.73it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2285/3257 [00:06<00:02, 339.95it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2322/3257 [00:06<00:02, 347.41it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2362/3257 [00:07<00:02, 362.61it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2402/3257 [00:07<00:02, 371.03it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2440/3257 [00:07<00:02, 353.51it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2478/3257 [00:07<00:02, 357.74it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2519/3257 [00:07<00:01, 370.01it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2557/3257 [00:07<00:01, 364.14it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2594/3257 [00:07<00:01, 351.35it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2638/3257 [00:07<00:01, 373.85it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2676/3257 [00:07<00:01, 363.18it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2713/3257 [00:08<00:01, 345.28it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2752/3257 [00:08<00:01, 356.29it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2789/3257 [00:08<00:01, 358.07it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2825/3257 [00:08<00:01, 345.22it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2864/3257 [00:08<00:01, 356.72it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2900/3257 [00:08<00:01, 355.22it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2936/3257 [00:08<00:00, 356.28it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2972/3257 [00:08<00:00, 350.83it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3010/3257 [00:08<00:00, 358.31it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3047/3257 [00:08<00:00, 359.87it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3086/3257 [00:09<00:00, 367.26it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3127/3257 [00:09<00:00, 376.39it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3165/3257 [00:09<00:00, 361.81it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3202/3257 [00:09<00:00, 362.35it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3239/3257 [00:09<00:00, 362.07it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 340.63it/s]
2023-02-07 13:39:33.380 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:39:33,381][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d72,n5,s0.600119,t4>', 'datetime': '2023-02-07T13:39:33.381265', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:39:33,384][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:39:33,384][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:39:33,574][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 13:39:33,574][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:39:33,581][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 2819 unique words (100.00% of original 2819, drops 0)', 'datetime': '2023-02-07T13:39:33.581132', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:39:33,582][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 2183622 word corpus (100.00% of original 2183622, drops 0)', 'datetime': '2023-02-07T13:39:33.582120', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:39:33,591][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 13:39:33,591][gensim.models.word2vec][INFO] - sample=0.600119 downsamples 0 most-common words
[2023-02-07 13:39:33,591][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2183622 word corpus (100.0%% of prior 2183622)', 'datetime': '2023-02-07T13:39:33.591898', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:39:33,606][gensim.models.word2vec][INFO] - estimated required memory for 2819 words and 72 dimensions: 4622660 bytes
[2023-02-07 13:39:33,607][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:39:33,609][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2819 vocabulary and 72 features, using sg=1 hs=0 sample=0.6001186619967673 negative=5 window=2 shrink_windows=True', 'datetime': '2023-02-07T13:39:33.609103', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:39:34,209][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2186879 effective words) took 0.6s, 3656548 effective words/s
[2023-02-07 13:39:34,727][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2186879 effective words) took 0.5s, 4229953 effective words/s
[2023-02-07 13:39:35,234][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2186879 effective words) took 0.5s, 4325836 effective words/s
[2023-02-07 13:39:35,741][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2186879 effective words) took 0.5s, 4325766 effective words/s
[2023-02-07 13:39:36,251][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2186879 effective words) took 0.5s, 4289974 effective words/s
[2023-02-07 13:39:36,783][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2186879 effective words) took 0.5s, 4125849 effective words/s
[2023-02-07 13:39:37,301][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2186879 effective words) took 0.5s, 4234106 effective words/s
[2023-02-07 13:39:37,812][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2186879 effective words) took 0.5s, 4289548 effective words/s
[2023-02-07 13:39:38,339][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2186879 effective words) took 0.5s, 4161996 effective words/s
[2023-02-07 13:39:38,871][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2186879 effective words) took 0.5s, 4116723 effective words/s
[2023-02-07 13:39:39,402][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2186879 effective words) took 0.5s, 4130102 effective words/s
[2023-02-07 13:39:39,925][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2186879 effective words) took 0.5s, 4187169 effective words/s
[2023-02-07 13:39:40,457][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2186879 effective words) took 0.5s, 4118522 effective words/s
[2023-02-07 13:39:40,983][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2186879 effective words) took 0.5s, 4167834 effective words/s
[2023-02-07 13:39:41,531][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2186879 effective words) took 0.5s, 4006264 effective words/s
[2023-02-07 13:39:42,071][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2186879 effective words) took 0.5s, 4057133 effective words/s
[2023-02-07 13:39:42,607][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2186879 effective words) took 0.5s, 4089819 effective words/s
[2023-02-07 13:39:43,146][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2186879 effective words) took 0.5s, 4067491 effective words/s
[2023-02-07 13:39:43,689][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2186879 effective words) took 0.5s, 4037449 effective words/s
[2023-02-07 13:39:44,241][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2186879 effective words) took 0.6s, 3966417 effective words/s
[2023-02-07 13:39:44,767][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2186879 effective words) took 0.5s, 4171520 effective words/s
[2023-02-07 13:39:45,299][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2186879 effective words) took 0.5s, 4121501 effective words/s
[2023-02-07 13:39:45,838][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2186879 effective words) took 0.5s, 4064600 effective words/s
[2023-02-07 13:39:46,378][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2186879 effective words) took 0.5s, 4061464 effective words/s
[2023-02-07 13:39:46,918][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2186879 effective words) took 0.5s, 4059273 effective words/s
[2023-02-07 13:39:47,459][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2186879 effective words) took 0.5s, 4051272 effective words/s
[2023-02-07 13:39:47,988][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2186879 effective words) took 0.5s, 4143192 effective words/s
[2023-02-07 13:39:48,511][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2186879 effective words) took 0.5s, 4190790 effective words/s
[2023-02-07 13:39:49,039][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2186879 effective words) took 0.5s, 4152600 effective words/s
[2023-02-07 13:39:49,569][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2186879 effective words) took 0.5s, 4139824 effective words/s
[2023-02-07 13:39:50,091][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2186879 effective words) took 0.5s, 4199391 effective words/s
[2023-02-07 13:39:50,615][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2186879 effective words) took 0.5s, 4182819 effective words/s
[2023-02-07 13:39:51,139][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2186879 effective words) took 0.5s, 4185427 effective words/s
[2023-02-07 13:39:51,659][gensim.models.word2vec][INFO] - EPOCH 33: training on 2183622 raw words (2186879 effective words) took 0.5s, 4213434 effective words/s
[2023-02-07 13:39:52,186][gensim.models.word2vec][INFO] - EPOCH 34: training on 2183622 raw words (2186879 effective words) took 0.5s, 4165615 effective words/s
[2023-02-07 13:39:52,716][gensim.models.word2vec][INFO] - EPOCH 35: training on 2183622 raw words (2186879 effective words) took 0.5s, 4131698 effective words/s
[2023-02-07 13:39:53,241][gensim.models.word2vec][INFO] - EPOCH 36: training on 2183622 raw words (2186879 effective words) took 0.5s, 4176566 effective words/s
[2023-02-07 13:39:53,768][gensim.models.word2vec][INFO] - EPOCH 37: training on 2183622 raw words (2186879 effective words) took 0.5s, 4157629 effective words/s
[2023-02-07 13:39:54,290][gensim.models.word2vec][INFO] - EPOCH 38: training on 2183622 raw words (2186879 effective words) took 0.5s, 4202939 effective words/s
[2023-02-07 13:39:54,817][gensim.models.word2vec][INFO] - EPOCH 39: training on 2183622 raw words (2186879 effective words) took 0.5s, 4155791 effective words/s
[2023-02-07 13:39:55,348][gensim.models.word2vec][INFO] - EPOCH 40: training on 2183622 raw words (2186879 effective words) took 0.5s, 4138726 effective words/s
[2023-02-07 13:39:55,861][gensim.models.word2vec][INFO] - EPOCH 41: training on 2183622 raw words (2186879 effective words) took 0.5s, 4269844 effective words/s
[2023-02-07 13:39:56,385][gensim.models.word2vec][INFO] - EPOCH 42: training on 2183622 raw words (2186879 effective words) took 0.5s, 4185972 effective words/s
[2023-02-07 13:39:56,905][gensim.models.word2vec][INFO] - EPOCH 43: training on 2183622 raw words (2186879 effective words) took 0.5s, 4212410 effective words/s
[2023-02-07 13:39:57,423][gensim.models.word2vec][INFO] - EPOCH 44: training on 2183622 raw words (2186879 effective words) took 0.5s, 4235391 effective words/s
[2023-02-07 13:39:57,941][gensim.models.word2vec][INFO] - EPOCH 45: training on 2183622 raw words (2186879 effective words) took 0.5s, 4228482 effective words/s
[2023-02-07 13:39:58,460][gensim.models.word2vec][INFO] - EPOCH 46: training on 2183622 raw words (2186879 effective words) took 0.5s, 4221729 effective words/s
[2023-02-07 13:39:58,983][gensim.models.word2vec][INFO] - EPOCH 47: training on 2183622 raw words (2186879 effective words) took 0.5s, 4194273 effective words/s
[2023-02-07 13:39:59,501][gensim.models.word2vec][INFO] - EPOCH 48: training on 2183622 raw words (2186879 effective words) took 0.5s, 4237741 effective words/s
[2023-02-07 13:40:00,022][gensim.models.word2vec][INFO] - EPOCH 49: training on 2183622 raw words (2186879 effective words) took 0.5s, 4201352 effective words/s
[2023-02-07 13:40:00,544][gensim.models.word2vec][INFO] - EPOCH 50: training on 2183622 raw words (2186879 effective words) took 0.5s, 4197993 effective words/s
[2023-02-07 13:40:01,060][gensim.models.word2vec][INFO] - EPOCH 51: training on 2183622 raw words (2186879 effective words) took 0.5s, 4255459 effective words/s
[2023-02-07 13:40:01,595][gensim.models.word2vec][INFO] - EPOCH 52: training on 2183622 raw words (2186879 effective words) took 0.5s, 4096636 effective words/s
[2023-02-07 13:40:02,128][gensim.models.word2vec][INFO] - EPOCH 53: training on 2183622 raw words (2186879 effective words) took 0.5s, 4109920 effective words/s
[2023-02-07 13:40:02,664][gensim.models.word2vec][INFO] - EPOCH 54: training on 2183622 raw words (2186879 effective words) took 0.5s, 4101002 effective words/s
[2023-02-07 13:40:03,200][gensim.models.word2vec][INFO] - EPOCH 55: training on 2183622 raw words (2186879 effective words) took 0.5s, 4090257 effective words/s
[2023-02-07 13:40:03,734][gensim.models.word2vec][INFO] - EPOCH 56: training on 2183622 raw words (2186879 effective words) took 0.5s, 4103773 effective words/s
[2023-02-07 13:40:04,272][gensim.models.word2vec][INFO] - EPOCH 57: training on 2183622 raw words (2186879 effective words) took 0.5s, 4078159 effective words/s
[2023-02-07 13:40:04,806][gensim.models.word2vec][INFO] - EPOCH 58: training on 2183622 raw words (2186879 effective words) took 0.5s, 4107070 effective words/s
[2023-02-07 13:40:05,342][gensim.models.word2vec][INFO] - EPOCH 59: training on 2183622 raw words (2186879 effective words) took 0.5s, 4095053 effective words/s
[2023-02-07 13:40:05,875][gensim.models.word2vec][INFO] - EPOCH 60: training on 2183622 raw words (2186879 effective words) took 0.5s, 4112657 effective words/s
[2023-02-07 13:40:06,411][gensim.models.word2vec][INFO] - EPOCH 61: training on 2183622 raw words (2186879 effective words) took 0.5s, 4090960 effective words/s
[2023-02-07 13:40:06,942][gensim.models.word2vec][INFO] - EPOCH 62: training on 2183622 raw words (2186879 effective words) took 0.5s, 4127689 effective words/s
[2023-02-07 13:40:07,482][gensim.models.word2vec][INFO] - EPOCH 63: training on 2183622 raw words (2186879 effective words) took 0.5s, 4056937 effective words/s
[2023-02-07 13:40:08,015][gensim.models.word2vec][INFO] - EPOCH 64: training on 2183622 raw words (2186879 effective words) took 0.5s, 4117780 effective words/s
[2023-02-07 13:40:08,543][gensim.models.word2vec][INFO] - EPOCH 65: training on 2183622 raw words (2186879 effective words) took 0.5s, 4161779 effective words/s
[2023-02-07 13:40:09,073][gensim.models.word2vec][INFO] - EPOCH 66: training on 2183622 raw words (2186879 effective words) took 0.5s, 4141188 effective words/s
[2023-02-07 13:40:09,607][gensim.models.word2vec][INFO] - EPOCH 67: training on 2183622 raw words (2186879 effective words) took 0.5s, 4102202 effective words/s
[2023-02-07 13:40:10,141][gensim.models.word2vec][INFO] - EPOCH 68: training on 2183622 raw words (2186879 effective words) took 0.5s, 4107334 effective words/s
[2023-02-07 13:40:10,670][gensim.models.word2vec][INFO] - EPOCH 69: training on 2183622 raw words (2186879 effective words) took 0.5s, 4143368 effective words/s
[2023-02-07 13:40:11,206][gensim.models.word2vec][INFO] - EPOCH 70: training on 2183622 raw words (2186879 effective words) took 0.5s, 4097145 effective words/s
[2023-02-07 13:40:11,734][gensim.models.word2vec][INFO] - EPOCH 71: training on 2183622 raw words (2186879 effective words) took 0.5s, 4149265 effective words/s
[2023-02-07 13:40:12,267][gensim.models.word2vec][INFO] - EPOCH 72: training on 2183622 raw words (2186879 effective words) took 0.5s, 4119949 effective words/s
[2023-02-07 13:40:12,793][gensim.models.word2vec][INFO] - EPOCH 73: training on 2183622 raw words (2186879 effective words) took 0.5s, 4169762 effective words/s
[2023-02-07 13:40:13,324][gensim.models.word2vec][INFO] - EPOCH 74: training on 2183622 raw words (2186879 effective words) took 0.5s, 4132444 effective words/s
[2023-02-07 13:40:13,851][gensim.models.word2vec][INFO] - EPOCH 75: training on 2183622 raw words (2186879 effective words) took 0.5s, 4153206 effective words/s
[2023-02-07 13:40:14,377][gensim.models.word2vec][INFO] - EPOCH 76: training on 2183622 raw words (2186879 effective words) took 0.5s, 4169674 effective words/s
[2023-02-07 13:40:14,904][gensim.models.word2vec][INFO] - EPOCH 77: training on 2183622 raw words (2186879 effective words) took 0.5s, 4161915 effective words/s
[2023-02-07 13:40:15,430][gensim.models.word2vec][INFO] - EPOCH 78: training on 2183622 raw words (2186879 effective words) took 0.5s, 4164914 effective words/s
[2023-02-07 13:40:15,954][gensim.models.word2vec][INFO] - EPOCH 79: training on 2183622 raw words (2186879 effective words) took 0.5s, 4192634 effective words/s
[2023-02-07 13:40:16,486][gensim.models.word2vec][INFO] - EPOCH 80: training on 2183622 raw words (2186879 effective words) took 0.5s, 4122218 effective words/s
[2023-02-07 13:40:17,016][gensim.models.word2vec][INFO] - EPOCH 81: training on 2183622 raw words (2186879 effective words) took 0.5s, 4140131 effective words/s
[2023-02-07 13:40:17,546][gensim.models.word2vec][INFO] - EPOCH 82: training on 2183622 raw words (2186879 effective words) took 0.5s, 4140851 effective words/s
[2023-02-07 13:40:18,070][gensim.models.word2vec][INFO] - EPOCH 83: training on 2183622 raw words (2186879 effective words) took 0.5s, 4184490 effective words/s
[2023-02-07 13:40:18,597][gensim.models.word2vec][INFO] - EPOCH 84: training on 2183622 raw words (2186879 effective words) took 0.5s, 4157197 effective words/s
[2023-02-07 13:40:19,098][gensim.models.word2vec][INFO] - EPOCH 85: training on 2183622 raw words (2186879 effective words) took 0.5s, 4380740 effective words/s
[2023-02-07 13:40:19,583][gensim.models.word2vec][INFO] - EPOCH 86: training on 2183622 raw words (2186879 effective words) took 0.5s, 4522887 effective words/s
[2023-02-07 13:40:20,069][gensim.models.word2vec][INFO] - EPOCH 87: training on 2183622 raw words (2186879 effective words) took 0.5s, 4510115 effective words/s
[2023-02-07 13:40:20,556][gensim.models.word2vec][INFO] - EPOCH 88: training on 2183622 raw words (2186879 effective words) took 0.5s, 4504216 effective words/s
[2023-02-07 13:40:21,045][gensim.models.word2vec][INFO] - EPOCH 89: training on 2183622 raw words (2186879 effective words) took 0.5s, 4477002 effective words/s
[2023-02-07 13:40:21,535][gensim.models.word2vec][INFO] - EPOCH 90: training on 2183622 raw words (2186879 effective words) took 0.5s, 4480321 effective words/s
[2023-02-07 13:40:22,042][gensim.models.word2vec][INFO] - EPOCH 91: training on 2183622 raw words (2186879 effective words) took 0.5s, 4321154 effective words/s
[2023-02-07 13:40:22,566][gensim.models.word2vec][INFO] - EPOCH 92: training on 2183622 raw words (2186879 effective words) took 0.5s, 4183301 effective words/s
[2023-02-07 13:40:23,092][gensim.models.word2vec][INFO] - EPOCH 93: training on 2183622 raw words (2186879 effective words) took 0.5s, 4171017 effective words/s
[2023-02-07 13:40:23,623][gensim.models.word2vec][INFO] - EPOCH 94: training on 2183622 raw words (2186879 effective words) took 0.5s, 4124903 effective words/s
[2023-02-07 13:40:24,141][gensim.models.word2vec][INFO] - EPOCH 95: training on 2183622 raw words (2186879 effective words) took 0.5s, 4233839 effective words/s
[2023-02-07 13:40:24,665][gensim.models.word2vec][INFO] - EPOCH 96: training on 2183622 raw words (2186879 effective words) took 0.5s, 4180674 effective words/s
[2023-02-07 13:40:25,192][gensim.models.word2vec][INFO] - EPOCH 97: training on 2183622 raw words (2186879 effective words) took 0.5s, 4163324 effective words/s
[2023-02-07 13:40:25,192][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 213994956 raw words (214314142 effective words) took 51.6s, 4154723 effective words/s', 'datetime': '2023-02-07T13:40:25.192603', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:40:25.192 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:40:28,540][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133915-ncewides/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:40:28.540862', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:40:28,541][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:40:28,554][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_133915-ncewides/files/../tmp/embedding_model.pt
2023-02-07 13:40:28.554 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:40:29.540 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:40:29.920 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:40:30.472 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.734119997254075, 'test_mae': 0.9902281558792467, 'test_r2': 0.17965755845127107}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.95
wandb: percentage 0.0
wandb:   test_mae 0.99023
wandb:   test_mse 1.73412
wandb:    test_r2 0.17966
wandb: 
wandb: üöÄ View run major-sweep-51 at: https://wandb.ai/xiaoqiz/mof2vec/runs/ncewides
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_133915-ncewides/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 2bac1157 with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 596
wandb: 	model.gensim.alpha: 0.0005489858464833436
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 86
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.8674833438843939
wandb: 	model.gensim.vector_size: 123
wandb: 	model.gensim.window: 9
wandb: 	model.sklearn.learning_rate: 0.7112462971862595
wandb: 	model.sklearn.max_depth: 7
wandb: 	model.sklearn.min_child_weight: 0.01903927865743756
wandb: 	model.sklearn.n_estimators: 66
wandb: 	model.sklearn.num_leaves: 471
wandb: 	model.sklearn.reg_alpha: 0.2586633108632161
wandb: 	model.sklearn.reg_lambda: 0.9913641530078944
wandb: 	model.sklearn.subsample: 0.841479562333239
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134049-2bac1157
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-sweep-52
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/2bac1157
2023-02-07 13:40:57.697 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 13:40:57.698 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 596 for sweep.
2023-02-07 13:40:57.698 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0005489858464833436 for sweep.
2023-02-07 13:40:57.698 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:40:57.699 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 86 for sweep.
2023-02-07 13:40:57.699 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 13:40:57.699 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.8674833438843939 for sweep.
2023-02-07 13:40:57.699 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 123 for sweep.
2023-02-07 13:40:57.700 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 9 for sweep.
2023-02-07 13:40:57.700 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.7112462971862595 for sweep.
2023-02-07 13:40:57.700 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 7 for sweep.
2023-02-07 13:40:57.701 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.01903927865743756 for sweep.
2023-02-07 13:40:57.701 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 66 for sweep.
2023-02-07 13:40:57.701 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 471 for sweep.
2023-02-07 13:40:57.701 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.2586633108632161 for sweep.
2023-02-07 13:40:57.701 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.9913641530078944 for sweep.
2023-02-07 13:40:57.702 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.841479562333239 for sweep.
2023-02-07 13:40:57.702 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:40:57.708 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134049-2bac1157/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 596, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 123, 'window': 9, 'min_count': 3, 'dm': 0, 'sample': 0.8674833438843939, 'workers': 4, 'alpha': 0.0005489858464833436, 'epochs': 86}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 66, 'max_depth': 7, 'num_leaves': 471, 'reg_alpha': 0.2586633108632161, 'reg_lambda': 0.9913641530078944, 'subsample': 0.841479562333239, 'min_child_weight': 0.01903927865743756, 'n_jobs': 4, 'learning_rate': 0.7112462971862595}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 33/3257 [00:00<00:10, 320.54it/s]  2%|‚ñè         | 67/3257 [00:00<00:10, 317.98it/s]  3%|‚ñé         | 103/3257 [00:00<00:09, 335.78it/s]  4%|‚ñç         | 137/3257 [00:00<00:09, 325.30it/s]  5%|‚ñå         | 170/3257 [00:00<00:09, 326.55it/s]  6%|‚ñå         | 203/3257 [00:00<00:09, 324.59it/s]  7%|‚ñã         | 243/3257 [00:00<00:08, 347.96it/s]  9%|‚ñä         | 278/3257 [00:00<00:08, 347.90it/s] 10%|‚ñâ         | 313/3257 [00:00<00:08, 343.59it/s] 11%|‚ñà         | 348/3257 [00:01<00:08, 340.40it/s] 12%|‚ñà‚ñè        | 383/3257 [00:01<00:08, 332.25it/s] 13%|‚ñà‚ñé        | 417/3257 [00:01<00:08, 330.22it/s] 14%|‚ñà‚ñç        | 451/3257 [00:01<00:09, 306.39it/s] 15%|‚ñà‚ñç        | 486/3257 [00:01<00:08, 317.73it/s] 16%|‚ñà‚ñå        | 523/3257 [00:01<00:08, 332.20it/s] 17%|‚ñà‚ñã        | 558/3257 [00:01<00:08, 326.55it/s] 18%|‚ñà‚ñä        | 591/3257 [00:01<00:08, 319.16it/s] 19%|‚ñà‚ñâ        | 625/3257 [00:01<00:08, 324.29it/s] 20%|‚ñà‚ñà        | 658/3257 [00:02<00:08, 319.45it/s] 21%|‚ñà‚ñà        | 691/3257 [00:02<00:07, 321.92it/s] 22%|‚ñà‚ñà‚ñè       | 724/3257 [00:02<00:07, 322.30it/s] 23%|‚ñà‚ñà‚ñé       | 757/3257 [00:02<00:07, 321.73it/s] 24%|‚ñà‚ñà‚ñç       | 790/3257 [00:02<00:07, 323.78it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:02<00:07, 324.16it/s] 26%|‚ñà‚ñà‚ñã       | 856/3257 [00:02<00:07, 319.65it/s] 27%|‚ñà‚ñà‚ñã       | 889/3257 [00:02<00:07, 321.57it/s] 28%|‚ñà‚ñà‚ñä       | 925/3257 [00:02<00:07, 332.72it/s] 29%|‚ñà‚ñà‚ñâ       | 959/3257 [00:02<00:06, 332.18it/s] 30%|‚ñà‚ñà‚ñà       | 993/3257 [00:03<00:06, 326.95it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1026/3257 [00:03<00:06, 324.99it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1059/3257 [00:03<00:06, 314.05it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:03<00:06, 311.30it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1125/3257 [00:03<00:06, 319.45it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1158/3257 [00:03<00:06, 322.30it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:03<00:06, 309.96it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1223/3257 [00:03<00:06, 312.18it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1259/3257 [00:03<00:06, 325.20it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1292/3257 [00:03<00:06, 314.21it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:04<00:06, 321.31it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1361/3257 [00:04<00:08, 232.48it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1394/3257 [00:04<00:07, 254.41it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1435/3257 [00:04<00:06, 291.54it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1477/3257 [00:04<00:05, 321.62it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:04<00:05, 340.55it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1555/3257 [00:04<00:05, 338.05it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1593/3257 [00:04<00:04, 349.31it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1633/3257 [00:05<00:04, 362.27it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1670/3257 [00:05<00:04, 342.65it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1706/3257 [00:05<00:04, 345.74it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1742/3257 [00:05<00:04, 332.46it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1783/3257 [00:05<00:04, 353.41it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1819/3257 [00:05<00:04, 354.87it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1855/3257 [00:05<00:03, 352.34it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1892/3257 [00:05<00:03, 354.70it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1929/3257 [00:05<00:03, 355.58it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1974/3257 [00:06<00:03, 378.65it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2012/3257 [00:06<00:03, 372.45it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2050/3257 [00:06<00:03, 364.55it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2087/3257 [00:06<00:03, 359.87it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2124/3257 [00:06<00:03, 346.81it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2160/3257 [00:06<00:03, 348.02it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2201/3257 [00:06<00:02, 363.35it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2238/3257 [00:06<00:02, 354.53it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2274/3257 [00:06<00:02, 342.62it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2312/3257 [00:06<00:02, 352.58it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2357/3257 [00:07<00:02, 379.76it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2396/3257 [00:07<00:02, 380.59it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2435/3257 [00:07<00:02, 358.43it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2474/3257 [00:07<00:02, 364.49it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2513/3257 [00:07<00:02, 371.72it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:07<00:01, 369.54it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2589/3257 [00:07<00:01, 347.00it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2632/3257 [00:07<00:01, 369.19it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:07<00:01, 361.55it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2707/3257 [00:08<00:01, 346.28it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2745/3257 [00:08<00:01, 355.30it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2781/3257 [00:08<00:01, 352.44it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2817/3257 [00:08<00:01, 351.29it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2853/3257 [00:08<00:01, 351.13it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2889/3257 [00:08<00:01, 243.28it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2926/3257 [00:08<00:01, 270.14it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2958/3257 [00:08<00:01, 275.37it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2994/3257 [00:09<00:00, 295.95it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3033/3257 [00:09<00:00, 319.40it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3075/3257 [00:09<00:00, 344.79it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3114/3257 [00:09<00:00, 356.50it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3151/3257 [00:09<00:00, 342.58it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3187/3257 [00:09<00:00, 338.32it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3222/3257 [00:09<00:00, 340.75it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 333.31it/s]
2023-02-07 13:41:07.743 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:41:07,744][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d123,n5,mc3,s0.867483,t4>', 'datetime': '2023-02-07T13:41:07.744385', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:41:07,744][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:41:07,744][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:41:07,945][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 13:41:07,945][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:41:07,951][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 2159 unique words (76.59% of original 2819, drops 660)', 'datetime': '2023-02-07T13:41:07.951423', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:41:07,951][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 2182579 word corpus (99.95% of original 2183622, drops 1043)', 'datetime': '2023-02-07T13:41:07.951660', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:41:07,959][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 13:41:07,960][gensim.models.word2vec][INFO] - sample=0.867483 downsamples 0 most-common words
[2023-02-07 13:41:07,960][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2182579 word corpus (100.0%% of prior 2182579)', 'datetime': '2023-02-07T13:41:07.960496', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:41:07,973][gensim.models.word2vec][INFO] - estimated required memory for 2159 words and 123 dimensions: 5457800 bytes
[2023-02-07 13:41:07,974][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:41:07,977][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2159 vocabulary and 123 features, using sg=1 hs=0 sample=0.8674833438843939 negative=5 window=9 shrink_windows=True', 'datetime': '2023-02-07T13:41:07.977486', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:41:08,858][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2185836 effective words) took 0.9s, 2485913 effective words/s
[2023-02-07 13:41:09,739][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2185836 effective words) took 0.9s, 2487118 effective words/s
[2023-02-07 13:41:10,599][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2185836 effective words) took 0.9s, 2543862 effective words/s
[2023-02-07 13:41:11,481][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2185836 effective words) took 0.9s, 2483703 effective words/s
[2023-02-07 13:41:12,345][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2185836 effective words) took 0.9s, 2534302 effective words/s
[2023-02-07 13:41:13,214][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2185836 effective words) took 0.9s, 2517870 effective words/s
[2023-02-07 13:41:14,091][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2185836 effective words) took 0.9s, 2498366 effective words/s
[2023-02-07 13:41:14,959][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2185836 effective words) took 0.9s, 2521846 effective words/s
[2023-02-07 13:41:15,823][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2185836 effective words) took 0.9s, 2534032 effective words/s
[2023-02-07 13:41:16,700][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2185836 effective words) took 0.9s, 2496613 effective words/s
[2023-02-07 13:41:17,574][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2185836 effective words) took 0.9s, 2504845 effective words/s
[2023-02-07 13:41:18,436][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2185836 effective words) took 0.9s, 2540595 effective words/s
[2023-02-07 13:41:19,302][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2185836 effective words) took 0.9s, 2527336 effective words/s
[2023-02-07 13:41:20,168][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2185836 effective words) took 0.9s, 2525835 effective words/s
[2023-02-07 13:41:21,036][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2185836 effective words) took 0.9s, 2523360 effective words/s
[2023-02-07 13:41:21,894][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2185836 effective words) took 0.9s, 2550296 effective words/s
[2023-02-07 13:41:22,755][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2185836 effective words) took 0.9s, 2544747 effective words/s
[2023-02-07 13:41:23,609][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2185836 effective words) took 0.9s, 2563642 effective words/s
[2023-02-07 13:41:24,471][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2185836 effective words) took 0.9s, 2541759 effective words/s
[2023-02-07 13:41:25,329][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2185836 effective words) took 0.9s, 2551672 effective words/s
[2023-02-07 13:41:26,188][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2185836 effective words) took 0.9s, 2549633 effective words/s
[2023-02-07 13:41:27,068][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2185836 effective words) took 0.9s, 2484951 effective words/s
[2023-02-07 13:41:27,966][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2185836 effective words) took 0.9s, 2440449 effective words/s
[2023-02-07 13:41:28,862][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2185836 effective words) took 0.9s, 2442885 effective words/s
[2023-02-07 13:41:29,752][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2185836 effective words) took 0.9s, 2463106 effective words/s
[2023-02-07 13:41:30,647][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2185836 effective words) took 0.9s, 2447367 effective words/s
[2023-02-07 13:41:31,538][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2185836 effective words) took 0.9s, 2456096 effective words/s
[2023-02-07 13:41:32,424][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2185836 effective words) took 0.9s, 2472431 effective words/s
[2023-02-07 13:41:33,311][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2185836 effective words) took 0.9s, 2468502 effective words/s
[2023-02-07 13:41:34,202][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2185836 effective words) took 0.9s, 2459637 effective words/s
[2023-02-07 13:41:35,094][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2185836 effective words) took 0.9s, 2453358 effective words/s
[2023-02-07 13:41:35,984][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2185836 effective words) took 0.9s, 2460946 effective words/s
[2023-02-07 13:41:36,866][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2185836 effective words) took 0.9s, 2481175 effective words/s
[2023-02-07 13:41:37,747][gensim.models.word2vec][INFO] - EPOCH 33: training on 2183622 raw words (2185836 effective words) took 0.9s, 2487246 effective words/s
[2023-02-07 13:41:38,628][gensim.models.word2vec][INFO] - EPOCH 34: training on 2183622 raw words (2185836 effective words) took 0.9s, 2484731 effective words/s
[2023-02-07 13:41:39,510][gensim.models.word2vec][INFO] - EPOCH 35: training on 2183622 raw words (2185836 effective words) took 0.9s, 2483823 effective words/s
[2023-02-07 13:41:40,395][gensim.models.word2vec][INFO] - EPOCH 36: training on 2183622 raw words (2185836 effective words) took 0.9s, 2473695 effective words/s
[2023-02-07 13:41:41,273][gensim.models.word2vec][INFO] - EPOCH 37: training on 2183622 raw words (2185836 effective words) took 0.9s, 2496394 effective words/s
[2023-02-07 13:41:42,151][gensim.models.word2vec][INFO] - EPOCH 38: training on 2183622 raw words (2185836 effective words) took 0.9s, 2492811 effective words/s
[2023-02-07 13:41:43,027][gensim.models.word2vec][INFO] - EPOCH 39: training on 2183622 raw words (2185836 effective words) took 0.9s, 2499459 effective words/s
[2023-02-07 13:41:43,905][gensim.models.word2vec][INFO] - EPOCH 40: training on 2183622 raw words (2185836 effective words) took 0.9s, 2494457 effective words/s
[2023-02-07 13:41:44,720][gensim.models.word2vec][INFO] - EPOCH 41: training on 2183622 raw words (2185836 effective words) took 0.8s, 2687096 effective words/s
[2023-02-07 13:41:45,529][gensim.models.word2vec][INFO] - EPOCH 42: training on 2183622 raw words (2185836 effective words) took 0.8s, 2706225 effective words/s
[2023-02-07 13:41:46,327][gensim.models.word2vec][INFO] - EPOCH 43: training on 2183622 raw words (2185836 effective words) took 0.8s, 2742627 effective words/s
[2023-02-07 13:41:47,183][gensim.models.word2vec][INFO] - EPOCH 44: training on 2183622 raw words (2185836 effective words) took 0.9s, 2559504 effective words/s
[2023-02-07 13:41:48,050][gensim.models.word2vec][INFO] - EPOCH 45: training on 2183622 raw words (2185836 effective words) took 0.9s, 2524454 effective words/s
[2023-02-07 13:41:48,922][gensim.models.word2vec][INFO] - EPOCH 46: training on 2183622 raw words (2185836 effective words) took 0.9s, 2511856 effective words/s
[2023-02-07 13:41:49,792][gensim.models.word2vec][INFO] - EPOCH 47: training on 2183622 raw words (2185836 effective words) took 0.9s, 2515171 effective words/s
[2023-02-07 13:41:50,660][gensim.models.word2vec][INFO] - EPOCH 48: training on 2183622 raw words (2185836 effective words) took 0.9s, 2524908 effective words/s
[2023-02-07 13:41:51,522][gensim.models.word2vec][INFO] - EPOCH 49: training on 2183622 raw words (2185836 effective words) took 0.9s, 2539362 effective words/s
[2023-02-07 13:41:52,385][gensim.models.word2vec][INFO] - EPOCH 50: training on 2183622 raw words (2185836 effective words) took 0.9s, 2534429 effective words/s
[2023-02-07 13:41:53,246][gensim.models.word2vec][INFO] - EPOCH 51: training on 2183622 raw words (2185836 effective words) took 0.9s, 2543040 effective words/s
[2023-02-07 13:41:54,099][gensim.models.word2vec][INFO] - EPOCH 52: training on 2183622 raw words (2185836 effective words) took 0.9s, 2567669 effective words/s
[2023-02-07 13:41:54,958][gensim.models.word2vec][INFO] - EPOCH 53: training on 2183622 raw words (2185836 effective words) took 0.9s, 2548345 effective words/s
[2023-02-07 13:41:55,819][gensim.models.word2vec][INFO] - EPOCH 54: training on 2183622 raw words (2185836 effective words) took 0.9s, 2544125 effective words/s
[2023-02-07 13:41:56,664][gensim.models.word2vec][INFO] - EPOCH 55: training on 2183622 raw words (2185836 effective words) took 0.8s, 2590863 effective words/s
[2023-02-07 13:41:57,516][gensim.models.word2vec][INFO] - EPOCH 56: training on 2183622 raw words (2185836 effective words) took 0.9s, 2569675 effective words/s
[2023-02-07 13:41:58,357][gensim.models.word2vec][INFO] - EPOCH 57: training on 2183622 raw words (2185836 effective words) took 0.8s, 2602965 effective words/s
[2023-02-07 13:41:59,210][gensim.models.word2vec][INFO] - EPOCH 58: training on 2183622 raw words (2185836 effective words) took 0.9s, 2563445 effective words/s
[2023-02-07 13:42:00,059][gensim.models.word2vec][INFO] - EPOCH 59: training on 2183622 raw words (2185836 effective words) took 0.8s, 2579539 effective words/s
[2023-02-07 13:42:00,919][gensim.models.word2vec][INFO] - EPOCH 60: training on 2183622 raw words (2185836 effective words) took 0.9s, 2548019 effective words/s
[2023-02-07 13:42:01,761][gensim.models.word2vec][INFO] - EPOCH 61: training on 2183622 raw words (2185836 effective words) took 0.8s, 2597776 effective words/s
[2023-02-07 13:42:02,609][gensim.models.word2vec][INFO] - EPOCH 62: training on 2183622 raw words (2185836 effective words) took 0.8s, 2581765 effective words/s
[2023-02-07 13:42:03,449][gensim.models.word2vec][INFO] - EPOCH 63: training on 2183622 raw words (2185836 effective words) took 0.8s, 2606750 effective words/s
[2023-02-07 13:42:04,297][gensim.models.word2vec][INFO] - EPOCH 64: training on 2183622 raw words (2185836 effective words) took 0.8s, 2582313 effective words/s
[2023-02-07 13:42:05,148][gensim.models.word2vec][INFO] - EPOCH 65: training on 2183622 raw words (2185836 effective words) took 0.8s, 2573132 effective words/s
[2023-02-07 13:42:05,991][gensim.models.word2vec][INFO] - EPOCH 66: training on 2183622 raw words (2185836 effective words) took 0.8s, 2596645 effective words/s
[2023-02-07 13:42:06,830][gensim.models.word2vec][INFO] - EPOCH 67: training on 2183622 raw words (2185836 effective words) took 0.8s, 2610988 effective words/s
[2023-02-07 13:42:07,704][gensim.models.word2vec][INFO] - EPOCH 68: training on 2183622 raw words (2185836 effective words) took 0.9s, 2503945 effective words/s
[2023-02-07 13:42:08,577][gensim.models.word2vec][INFO] - EPOCH 69: training on 2183622 raw words (2185836 effective words) took 0.9s, 2511524 effective words/s
[2023-02-07 13:42:09,441][gensim.models.word2vec][INFO] - EPOCH 70: training on 2183622 raw words (2185836 effective words) took 0.9s, 2531764 effective words/s
[2023-02-07 13:42:10,308][gensim.models.word2vec][INFO] - EPOCH 71: training on 2183622 raw words (2185836 effective words) took 0.9s, 2525620 effective words/s
[2023-02-07 13:42:11,178][gensim.models.word2vec][INFO] - EPOCH 72: training on 2183622 raw words (2185836 effective words) took 0.9s, 2516998 effective words/s
[2023-02-07 13:42:12,050][gensim.models.word2vec][INFO] - EPOCH 73: training on 2183622 raw words (2185836 effective words) took 0.9s, 2513521 effective words/s
[2023-02-07 13:42:12,919][gensim.models.word2vec][INFO] - EPOCH 74: training on 2183622 raw words (2185836 effective words) took 0.9s, 2520090 effective words/s
[2023-02-07 13:42:13,791][gensim.models.word2vec][INFO] - EPOCH 75: training on 2183622 raw words (2185836 effective words) took 0.9s, 2509857 effective words/s
[2023-02-07 13:42:14,660][gensim.models.word2vec][INFO] - EPOCH 76: training on 2183622 raw words (2185836 effective words) took 0.9s, 2520002 effective words/s
[2023-02-07 13:42:15,527][gensim.models.word2vec][INFO] - EPOCH 77: training on 2183622 raw words (2185836 effective words) took 0.9s, 2525231 effective words/s
[2023-02-07 13:42:16,390][gensim.models.word2vec][INFO] - EPOCH 78: training on 2183622 raw words (2185836 effective words) took 0.9s, 2535352 effective words/s
[2023-02-07 13:42:17,273][gensim.models.word2vec][INFO] - EPOCH 79: training on 2183622 raw words (2185836 effective words) took 0.9s, 2486260 effective words/s
[2023-02-07 13:42:18,150][gensim.models.word2vec][INFO] - EPOCH 80: training on 2183622 raw words (2185836 effective words) took 0.9s, 2496930 effective words/s
[2023-02-07 13:42:19,018][gensim.models.word2vec][INFO] - EPOCH 81: training on 2183622 raw words (2185836 effective words) took 0.9s, 2525547 effective words/s
[2023-02-07 13:42:19,890][gensim.models.word2vec][INFO] - EPOCH 82: training on 2183622 raw words (2185836 effective words) took 0.9s, 2511749 effective words/s
[2023-02-07 13:42:20,758][gensim.models.word2vec][INFO] - EPOCH 83: training on 2183622 raw words (2185836 effective words) took 0.9s, 2520963 effective words/s
[2023-02-07 13:42:21,629][gensim.models.word2vec][INFO] - EPOCH 84: training on 2183622 raw words (2185836 effective words) took 0.9s, 2516914 effective words/s
[2023-02-07 13:42:22,506][gensim.models.word2vec][INFO] - EPOCH 85: training on 2183622 raw words (2185836 effective words) took 0.9s, 2497937 effective words/s
[2023-02-07 13:42:22,506][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 187791492 raw words (187981896 effective words) took 74.5s, 2522272 effective words/s', 'datetime': '2023-02-07T13:42:22.506484', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:42:22.506 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:42:26,945][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134049-2bac1157/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:42:26.945006', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:42:26,945][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:42:26,955][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134049-2bac1157/files/../tmp/embedding_model.pt
2023-02-07 13:42:26.955 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:42:28.060 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:42:28.489 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:42:29.263 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9357311303054088, 'test_mae': 1.0221674517139885, 'test_r2': 0.08428343821009576}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.87
wandb: percentage 0.23413
wandb:   test_mae 1.02217
wandb:   test_mse 1.93573
wandb:    test_r2 0.08428
wandb: 
wandb: üöÄ View run frosty-sweep-52 at: https://wandb.ai/xiaoqiz/mof2vec/runs/2bac1157
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_134049-2bac1157/logs
wandb: Agent Starting Run: 6sby02io with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 739
wandb: 	model.gensim.alpha: 0.000661770671272144
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 42
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.47029124115513654
wandb: 	model.gensim.vector_size: 80
wandb: 	model.gensim.window: 17
wandb: 	model.sklearn.learning_rate: 0.19349412562550777
wandb: 	model.sklearn.max_depth: 30
wandb: 	model.sklearn.min_child_weight: 0.04847278085625551
wandb: 	model.sklearn.n_estimators: 3196
wandb: 	model.sklearn.num_leaves: 497
wandb: 	model.sklearn.reg_alpha: 0.010719889722052777
wandb: 	model.sklearn.reg_lambda: 0.6667767174446774
wandb: 	model.sklearn.subsample: 0.4911987205787205
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134238-6sby02io
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-53
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/6sby02io
2023-02-07 13:42:46.117 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 13:42:46.118 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 739 for sweep.
2023-02-07 13:42:46.118 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.000661770671272144 for sweep.
2023-02-07 13:42:46.119 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:42:46.119 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 42 for sweep.
2023-02-07 13:42:46.119 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 13:42:46.119 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.47029124115513654 for sweep.
2023-02-07 13:42:46.119 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 80 for sweep.
2023-02-07 13:42:46.120 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 17 for sweep.
2023-02-07 13:42:46.120 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.19349412562550777 for sweep.
2023-02-07 13:42:46.120 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 30 for sweep.
2023-02-07 13:42:46.120 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.04847278085625551 for sweep.
2023-02-07 13:42:46.121 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3196 for sweep.
2023-02-07 13:42:46.121 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 497 for sweep.
2023-02-07 13:42:46.121 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.010719889722052777 for sweep.
2023-02-07 13:42:46.121 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.6667767174446774 for sweep.
2023-02-07 13:42:46.122 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.4911987205787205 for sweep.
2023-02-07 13:42:46.122 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:42:46.128 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134238-6sby02io/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 739, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 80, 'window': 17, 'min_count': 1, 'dm': 0, 'sample': 0.47029124115513654, 'workers': 4, 'alpha': 0.000661770671272144, 'epochs': 42}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3196, 'max_depth': 30, 'num_leaves': 497, 'reg_alpha': 0.010719889722052777, 'reg_lambda': 0.6667767174446774, 'subsample': 0.4911987205787205, 'min_child_weight': 0.04847278085625551, 'n_jobs': 4, 'learning_rate': 0.19349412562550777}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 29/3257 [00:00<00:11, 287.10it/s]  2%|‚ñè         | 58/3257 [00:00<00:11, 282.86it/s]  3%|‚ñé         | 90/3257 [00:00<00:10, 298.62it/s]  4%|‚ñé         | 120/3257 [00:00<00:10, 295.65it/s]  5%|‚ñç         | 154/3257 [00:00<00:10, 306.43it/s]  6%|‚ñå         | 185/3257 [00:00<00:10, 303.31it/s]  7%|‚ñã         | 219/3257 [00:00<00:09, 313.08it/s]  8%|‚ñä         | 253/3257 [00:00<00:09, 321.12it/s]  9%|‚ñâ         | 290/3257 [00:00<00:08, 333.51it/s] 10%|‚ñâ         | 325/3257 [00:01<00:08, 336.44it/s] 11%|‚ñà         | 359/3257 [00:01<00:08, 325.97it/s] 12%|‚ñà‚ñè        | 392/3257 [00:01<00:09, 309.00it/s] 13%|‚ñà‚ñé        | 425/3257 [00:01<00:09, 303.54it/s] 14%|‚ñà‚ñç        | 456/3257 [00:01<00:09, 298.13it/s] 15%|‚ñà‚ñç        | 487/3257 [00:01<00:09, 300.87it/s] 16%|‚ñà‚ñå        | 522/3257 [00:01<00:08, 310.52it/s] 17%|‚ñà‚ñã        | 554/3257 [00:01<00:08, 311.76it/s] 18%|‚ñà‚ñä        | 586/3257 [00:01<00:09, 292.63it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:02<00:08, 297.05it/s] 20%|‚ñà‚ñà        | 653/3257 [00:02<00:08, 305.48it/s] 21%|‚ñà‚ñà        | 684/3257 [00:02<00:08, 293.94it/s] 22%|‚ñà‚ñà‚ñè       | 718/3257 [00:02<00:08, 304.64it/s] 23%|‚ñà‚ñà‚ñé       | 749/3257 [00:02<00:08, 289.84it/s] 24%|‚ñà‚ñà‚ñç       | 780/3257 [00:02<00:08, 294.55it/s] 25%|‚ñà‚ñà‚ñç       | 812/3257 [00:02<00:08, 299.46it/s] 26%|‚ñà‚ñà‚ñå       | 843/3257 [00:02<00:08, 284.60it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:02<00:08, 289.83it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:02<00:08, 291.19it/s] 29%|‚ñà‚ñà‚ñä       | 935/3257 [00:03<00:11, 203.40it/s] 30%|‚ñà‚ñà‚ñâ       | 965/3257 [00:03<00:10, 224.06it/s] 30%|‚ñà‚ñà‚ñà       | 991/3257 [00:03<00:09, 228.78it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1019/3257 [00:03<00:09, 240.96it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1045/3257 [00:03<00:09, 242.42it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:03<00:08, 258.93it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1112/3257 [00:03<00:07, 273.50it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1141/3257 [00:03<00:07, 271.23it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:04<00:07, 282.65it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1202/3257 [00:04<00:07, 264.51it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1235/3257 [00:04<00:07, 281.01it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1266/3257 [00:04<00:06, 287.43it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1296/3257 [00:04<00:07, 272.23it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:04<00:06, 282.24it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1359/3257 [00:04<00:06, 287.74it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1389/3257 [00:04<00:06, 278.17it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1425/3257 [00:04<00:06, 300.82it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1459/3257 [00:05<00:05, 311.26it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1493/3257 [00:05<00:05, 317.45it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1525/3257 [00:05<00:05, 306.69it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1556/3257 [00:05<00:05, 296.94it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1587/3257 [00:05<00:05, 299.53it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1620/3257 [00:05<00:05, 306.91it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1651/3257 [00:05<00:05, 298.78it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1681/3257 [00:05<00:05, 288.89it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1714/3257 [00:05<00:05, 298.29it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1744/3257 [00:06<00:05, 281.53it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1777/3257 [00:06<00:05, 293.77it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1807/3257 [00:06<00:04, 291.52it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1839/3257 [00:06<00:04, 297.56it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1871/3257 [00:06<00:04, 303.46it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1902/3257 [00:06<00:04, 304.55it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1933/3257 [00:06<00:04, 305.03it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1972/3257 [00:06<00:03, 328.30it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2005/3257 [00:06<00:03, 315.21it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2038/3257 [00:06<00:03, 315.69it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2070/3257 [00:07<00:04, 288.23it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2100/3257 [00:07<00:05, 194.88it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2127/3257 [00:07<00:05, 209.41it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2154/3257 [00:07<00:04, 222.76it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2182/3257 [00:07<00:04, 234.37it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2210/3257 [00:07<00:04, 243.73it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2239/3257 [00:07<00:04, 250.91it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2268/3257 [00:08<00:03, 260.76it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:08<00:03, 268.94it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2335/3257 [00:08<00:03, 292.16it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2370/3257 [00:08<00:02, 306.61it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:08<00:02, 307.73it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2437/3257 [00:08<00:02, 300.78it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2471/3257 [00:08<00:02, 311.48it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2505/3257 [00:08<00:02, 319.03it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2539/3257 [00:08<00:02, 324.97it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2572/3257 [00:08<00:02, 302.90it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2604/3257 [00:09<00:02, 307.32it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2641/3257 [00:09<00:01, 322.57it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2674/3257 [00:09<00:01, 311.83it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2706/3257 [00:09<00:01, 301.70it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2737/3257 [00:09<00:01, 303.52it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2769/3257 [00:09<00:01, 305.08it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2805/3257 [00:09<00:01, 318.88it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2838/3257 [00:09<00:01, 300.33it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2878/3257 [00:09<00:01, 327.98it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2912/3257 [00:10<00:01, 313.87it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2944/3257 [00:10<00:01, 299.70it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:10<00:00, 298.92it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3010/3257 [00:10<00:00, 308.58it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3042/3257 [00:10<00:00, 308.81it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:10<00:00, 322.55it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3114/3257 [00:10<00:00, 328.96it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3148/3257 [00:10<00:00, 316.85it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3180/3257 [00:10<00:00, 308.13it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3213/3257 [00:11<00:00, 307.85it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3252/3257 [00:11<00:00, 330.44it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 291.59it/s]
2023-02-07 13:42:57.687 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:42:57,689][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d80,n5,s0.470291,t4>', 'datetime': '2023-02-07T13:42:57.689330', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:42:57,690][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:42:57,690][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:42:57,947][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 13:42:57,947][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:42:57,964][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 6662 unique words (100.00% of original 6662, drops 0)', 'datetime': '2023-02-07T13:42:57.964122', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:42:57,964][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 2911496 word corpus (100.00% of original 2911496, drops 0)', 'datetime': '2023-02-07T13:42:57.964400', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:42:57,986][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 13:42:57,987][gensim.models.word2vec][INFO] - sample=0.470291 downsamples 0 most-common words
[2023-02-07 13:42:57,987][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2911496 word corpus (100.0%% of prior 2911496)', 'datetime': '2023-02-07T13:42:57.987268', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:42:58,025][gensim.models.word2vec][INFO] - estimated required memory for 6662 words and 80 dimensions: 9288320 bytes
[2023-02-07 13:42:58,026][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:42:58,029][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 6662 vocabulary and 80 features, using sg=1 hs=0 sample=0.47029124115513654 negative=5 window=17 shrink_windows=True', 'datetime': '2023-02-07T13:42:58.029731', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:42:58,924][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2914753 effective words) took 0.9s, 3269666 effective words/s
[2023-02-07 13:42:59,816][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2914753 effective words) took 0.9s, 3270214 effective words/s
[2023-02-07 13:43:00,706][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2914753 effective words) took 0.9s, 3281583 effective words/s
[2023-02-07 13:43:01,590][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2914753 effective words) took 0.9s, 3298601 effective words/s
[2023-02-07 13:43:02,483][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2914753 effective words) took 0.9s, 3270038 effective words/s
[2023-02-07 13:43:03,377][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2914753 effective words) took 0.9s, 3263588 effective words/s
[2023-02-07 13:43:04,265][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2914753 effective words) took 0.9s, 3286437 effective words/s
[2023-02-07 13:43:05,165][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2914753 effective words) took 0.9s, 3245936 effective words/s
[2023-02-07 13:43:06,066][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2914753 effective words) took 0.9s, 3239917 effective words/s
[2023-02-07 13:43:06,956][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2914753 effective words) took 0.9s, 3278448 effective words/s
[2023-02-07 13:43:07,838][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2914753 effective words) took 0.9s, 3311812 effective words/s
[2023-02-07 13:43:08,723][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2914753 effective words) took 0.9s, 3296217 effective words/s
[2023-02-07 13:43:09,609][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2914753 effective words) took 0.9s, 3295932 effective words/s
[2023-02-07 13:43:10,506][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2914753 effective words) took 0.9s, 3255065 effective words/s
[2023-02-07 13:43:11,411][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2914753 effective words) took 0.9s, 3224840 effective words/s
[2023-02-07 13:43:12,311][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2914753 effective words) took 0.9s, 3243098 effective words/s
[2023-02-07 13:43:13,198][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2914753 effective words) took 0.9s, 3289626 effective words/s
[2023-02-07 13:43:14,089][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2914753 effective words) took 0.9s, 3277435 effective words/s
[2023-02-07 13:43:14,982][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2914753 effective words) took 0.9s, 3266094 effective words/s
[2023-02-07 13:43:15,862][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2914753 effective words) took 0.9s, 3316883 effective words/s
[2023-02-07 13:43:16,737][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2914753 effective words) took 0.9s, 3336415 effective words/s
[2023-02-07 13:43:17,633][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2914753 effective words) took 0.9s, 3256520 effective words/s
[2023-02-07 13:43:18,513][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2914753 effective words) took 0.9s, 3317912 effective words/s
[2023-02-07 13:43:19,413][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2914753 effective words) took 0.9s, 3246046 effective words/s
[2023-02-07 13:43:20,284][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2914753 effective words) took 0.9s, 3351847 effective words/s
[2023-02-07 13:43:21,160][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2914753 effective words) took 0.9s, 3333322 effective words/s
[2023-02-07 13:43:22,035][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2914753 effective words) took 0.9s, 3337079 effective words/s
[2023-02-07 13:43:22,903][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2914753 effective words) took 0.9s, 3362275 effective words/s
[2023-02-07 13:43:23,793][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2914753 effective words) took 0.9s, 3281405 effective words/s
[2023-02-07 13:43:24,681][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2914753 effective words) took 0.9s, 3286099 effective words/s
[2023-02-07 13:43:25,564][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2914753 effective words) took 0.9s, 3305668 effective words/s
[2023-02-07 13:43:26,441][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2914753 effective words) took 0.9s, 3329494 effective words/s
[2023-02-07 13:43:27,316][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2914753 effective words) took 0.9s, 3334881 effective words/s
[2023-02-07 13:43:28,186][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2914753 effective words) took 0.9s, 3353765 effective words/s
[2023-02-07 13:43:29,053][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2914753 effective words) took 0.9s, 3368278 effective words/s
[2023-02-07 13:43:29,924][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2914753 effective words) took 0.9s, 3353000 effective words/s
[2023-02-07 13:43:30,793][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2914753 effective words) took 0.9s, 3356997 effective words/s
[2023-02-07 13:43:31,648][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2914753 effective words) took 0.9s, 3414604 effective words/s
[2023-02-07 13:43:32,508][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2914753 effective words) took 0.9s, 3393646 effective words/s
[2023-02-07 13:43:33,362][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2914753 effective words) took 0.9s, 3419495 effective words/s
[2023-02-07 13:43:34,213][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2914753 effective words) took 0.8s, 3431420 effective words/s
[2023-02-07 13:43:35,068][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2914753 effective words) took 0.9s, 3414732 effective words/s
[2023-02-07 13:43:35,068][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 122282832 raw words (122419626 effective words) took 37.0s, 3305290 effective words/s', 'datetime': '2023-02-07T13:43:35.068691', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:43:35.068 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:43:37,607][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134238-6sby02io/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:43:37.607520', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:43:37,608][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:43:37,631][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134238-6sby02io/files/../tmp/embedding_model.pt
2023-02-07 13:43:37.631 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:43:38.649 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:43:39.042 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:43:39.616 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1512196516968345, 'test_mae': 1.090259912086685, 'test_r2': -0.017655516443494834}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.78
wandb: percentage 0.0
wandb:   test_mae 1.09026
wandb:   test_mse 2.15122
wandb:    test_r2 -0.01766
wandb: 
wandb: üöÄ View run glorious-sweep-53 at: https://wandb.ai/xiaoqiz/mof2vec/runs/6sby02io
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_134238-6sby02io/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: y3vjo6tu with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 887
wandb: 	model.gensim.alpha: 0.030383711859688425
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 99
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.39564342825259075
wandb: 	model.gensim.vector_size: 26
wandb: 	model.gensim.window: 7
wandb: 	model.sklearn.learning_rate: 0.007991151292834458
wandb: 	model.sklearn.max_depth: 17
wandb: 	model.sklearn.min_child_weight: 0.004608978991702026
wandb: 	model.sklearn.n_estimators: 2213
wandb: 	model.sklearn.num_leaves: 495
wandb: 	model.sklearn.reg_alpha: 0.0030369411934037327
wandb: 	model.sklearn.reg_lambda: 0.05566814713881605
wandb: 	model.sklearn.subsample: 0.5308546167425009
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134358-y3vjo6tu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-54
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/y3vjo6tu
2023-02-07 13:44:05.611 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:44:05.612 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 887 for sweep.
2023-02-07 13:44:05.612 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.030383711859688425 for sweep.
2023-02-07 13:44:05.613 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:44:05.613 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 99 for sweep.
2023-02-07 13:44:05.613 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 13:44:05.613 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.39564342825259075 for sweep.
2023-02-07 13:44:05.613 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 26 for sweep.
2023-02-07 13:44:05.614 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 7 for sweep.
2023-02-07 13:44:05.614 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.007991151292834458 for sweep.
2023-02-07 13:44:05.614 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 17 for sweep.
2023-02-07 13:44:05.615 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.004608978991702026 for sweep.
2023-02-07 13:44:05.615 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2213 for sweep.
2023-02-07 13:44:05.615 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 495 for sweep.
2023-02-07 13:44:05.615 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.0030369411934037327 for sweep.
2023-02-07 13:44:05.615 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.05566814713881605 for sweep.
2023-02-07 13:44:05.616 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5308546167425009 for sweep.
2023-02-07 13:44:05.616 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:44:05.621 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134358-y3vjo6tu/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 887, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 26, 'window': 7, 'min_count': 2, 'dm': 0, 'sample': 0.39564342825259075, 'workers': 4, 'alpha': 0.030383711859688425, 'epochs': 99}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2213, 'max_depth': 17, 'num_leaves': 495, 'reg_alpha': 0.0030369411934037327, 'reg_lambda': 0.05566814713881605, 'subsample': 0.5308546167425009, 'min_child_weight': 0.004608978991702026, 'n_jobs': 4, 'learning_rate': 0.007991151292834458}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|‚ñè         | 42/3257 [00:00<00:07, 413.10it/s]  3%|‚ñé         | 90/3257 [00:00<00:07, 438.56it/s]  4%|‚ñç         | 134/3257 [00:00<00:07, 433.59it/s]  5%|‚ñå         | 179/3257 [00:00<00:07, 435.65it/s]  7%|‚ñã         | 230/3257 [00:00<00:06, 457.08it/s]  8%|‚ñä         | 276/3257 [00:00<00:09, 329.45it/s] 10%|‚ñà         | 327/3257 [00:00<00:07, 374.24it/s] 11%|‚ñà‚ñè        | 372/3257 [00:00<00:07, 394.11it/s] 13%|‚ñà‚ñé        | 416/3257 [00:01<00:07, 405.30it/s] 14%|‚ñà‚ñç        | 459/3257 [00:01<00:06, 401.62it/s] 16%|‚ñà‚ñå        | 507/3257 [00:01<00:06, 422.36it/s] 17%|‚ñà‚ñã        | 552/3257 [00:01<00:06, 429.93it/s] 18%|‚ñà‚ñä        | 596/3257 [00:01<00:06, 421.45it/s] 20%|‚ñà‚ñâ        | 642/3257 [00:01<00:06, 431.45it/s] 21%|‚ñà‚ñà        | 686/3257 [00:01<00:06, 420.91it/s] 22%|‚ñà‚ñà‚ñè       | 730/3257 [00:01<00:05, 423.50it/s] 24%|‚ñà‚ñà‚ñç       | 775/3257 [00:01<00:05, 429.54it/s] 25%|‚ñà‚ñà‚ñå       | 820/3257 [00:01<00:05, 435.15it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:02<00:05, 423.30it/s] 28%|‚ñà‚ñà‚ñä       | 911/3257 [00:02<00:05, 434.96it/s] 29%|‚ñà‚ñà‚ñâ       | 956/3257 [00:02<00:05, 438.43it/s] 31%|‚ñà‚ñà‚ñà       | 1000/3257 [00:02<00:05, 436.95it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1044/3257 [00:02<00:05, 424.49it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1088/3257 [00:02<00:05, 426.42it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1131/3257 [00:02<00:05, 416.67it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1175/3257 [00:02<00:04, 422.74it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1218/3257 [00:02<00:04, 411.46it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1267/3257 [00:03<00:04, 433.62it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1311/3257 [00:03<00:04, 423.64it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1355/3257 [00:03<00:04, 427.05it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1401/3257 [00:03<00:04, 434.04it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1452/3257 [00:03<00:03, 452.22it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1504/3257 [00:03<00:03, 470.85it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1552/3257 [00:03<00:03, 452.01it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1598/3257 [00:03<00:04, 337.56it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1642/3257 [00:03<00:04, 360.42it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1684/3257 [00:04<00:04, 373.76it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:04<00:03, 388.42it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1774/3257 [00:04<00:03, 403.53it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1820/3257 [00:04<00:03, 418.07it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1865/3257 [00:04<00:03, 425.90it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1912/3257 [00:04<00:03, 437.57it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1964/3257 [00:04<00:02, 460.18it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2011/3257 [00:04<00:02, 454.56it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2057/3257 [00:04<00:02, 444.72it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2102/3257 [00:05<00:02, 440.02it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2147/3257 [00:05<00:02, 430.10it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2195/3257 [00:05<00:02, 443.16it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2240/3257 [00:05<00:02, 432.38it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2286/3257 [00:05<00:02, 437.96it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2337/3257 [00:05<00:02, 455.60it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2387/3257 [00:05<00:01, 466.88it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2434/3257 [00:05<00:01, 440.04it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2479/3257 [00:05<00:01, 439.13it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2531/3257 [00:05<00:01, 461.35it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2578/3257 [00:06<00:01, 443.82it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2623/3257 [00:06<00:01, 442.96it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2668/3257 [00:06<00:01, 427.42it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2711/3257 [00:06<00:01, 407.08it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2759/3257 [00:06<00:01, 427.30it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2805/3257 [00:06<00:01, 435.52it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2849/3257 [00:06<00:00, 430.51it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2894/3257 [00:06<00:00, 434.95it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2943/3257 [00:06<00:00, 438.88it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2987/3257 [00:07<00:00, 437.02it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3037/3257 [00:07<00:00, 453.65it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3087/3257 [00:07<00:00, 465.40it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3134/3257 [00:07<00:00, 338.22it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3177/3257 [00:07<00:00, 357.74it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3223/3257 [00:07<00:00, 381.72it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:07<00:00, 421.27it/s]
2023-02-07 13:44:13.510 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:44:13,511][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d26,n5,mc2,s0.395643,t4>', 'datetime': '2023-02-07T13:44:13.511774', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:44:13,512][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:44:13,512][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:44:13,642][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:44:13,642][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:44:13,644][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 881 unique words (95.35% of original 924, drops 43)', 'datetime': '2023-02-07T13:44:13.644784', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:44:13,644][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 1455705 word corpus (100.00% of original 1455748, drops 43)', 'datetime': '2023-02-07T13:44:13.644946', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:44:13,647][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:44:13,648][gensim.models.word2vec][INFO] - sample=0.395643 downsamples 0 most-common words
[2023-02-07 13:44:13,649][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455705 word corpus (100.0%% of prior 1455705)', 'datetime': '2023-02-07T13:44:13.649002', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:44:13,654][gensim.models.word2vec][INFO] - estimated required memory for 881 words and 26 dimensions: 1613876 bytes
[2023-02-07 13:44:13,654][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:44:13,655][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 881 vocabulary and 26 features, using sg=1 hs=0 sample=0.39564342825259075 negative=5 window=7 shrink_windows=True', 'datetime': '2023-02-07T13:44:13.655139', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:44:13,971][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458962 effective words) took 0.3s, 4626137 effective words/s
[2023-02-07 13:44:14,259][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458962 effective words) took 0.3s, 5091264 effective words/s
[2023-02-07 13:44:14,547][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458962 effective words) took 0.3s, 5097778 effective words/s
[2023-02-07 13:44:14,833][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458962 effective words) took 0.3s, 5118891 effective words/s
[2023-02-07 13:44:15,115][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458962 effective words) took 0.3s, 5197652 effective words/s
[2023-02-07 13:44:15,403][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458962 effective words) took 0.3s, 5095256 effective words/s
[2023-02-07 13:44:15,679][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458962 effective words) took 0.3s, 5312331 effective words/s
[2023-02-07 13:44:15,956][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458962 effective words) took 0.3s, 5285553 effective words/s
[2023-02-07 13:44:16,248][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458962 effective words) took 0.3s, 5030948 effective words/s
[2023-02-07 13:44:16,537][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458962 effective words) took 0.3s, 5067878 effective words/s
[2023-02-07 13:44:16,833][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458962 effective words) took 0.3s, 4939606 effective words/s
[2023-02-07 13:44:17,128][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458962 effective words) took 0.3s, 4976733 effective words/s
[2023-02-07 13:44:17,421][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458962 effective words) took 0.3s, 4993775 effective words/s
[2023-02-07 13:44:17,716][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458962 effective words) took 0.3s, 4976246 effective words/s
[2023-02-07 13:44:18,007][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458962 effective words) took 0.3s, 5032241 effective words/s
[2023-02-07 13:44:18,298][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458962 effective words) took 0.3s, 5024873 effective words/s
[2023-02-07 13:44:18,589][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458962 effective words) took 0.3s, 5039677 effective words/s
[2023-02-07 13:44:18,880][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458962 effective words) took 0.3s, 5027566 effective words/s
[2023-02-07 13:44:19,172][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458962 effective words) took 0.3s, 5026620 effective words/s
[2023-02-07 13:44:19,468][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458962 effective words) took 0.3s, 4949345 effective words/s
[2023-02-07 13:44:19,767][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458962 effective words) took 0.3s, 4909389 effective words/s
[2023-02-07 13:44:20,056][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458962 effective words) took 0.3s, 5069254 effective words/s
[2023-02-07 13:44:20,344][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458962 effective words) took 0.3s, 5076288 effective words/s
[2023-02-07 13:44:20,633][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458962 effective words) took 0.3s, 5067548 effective words/s
[2023-02-07 13:44:20,937][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458962 effective words) took 0.3s, 4827306 effective words/s
[2023-02-07 13:44:21,240][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458962 effective words) took 0.3s, 4839980 effective words/s
[2023-02-07 13:44:21,540][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458962 effective words) took 0.3s, 4874829 effective words/s
[2023-02-07 13:44:21,846][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458962 effective words) took 0.3s, 4796410 effective words/s
[2023-02-07 13:44:22,150][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458962 effective words) took 0.3s, 4823585 effective words/s
[2023-02-07 13:44:22,453][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458962 effective words) took 0.3s, 4832596 effective words/s
[2023-02-07 13:44:22,759][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458962 effective words) took 0.3s, 4812364 effective words/s
[2023-02-07 13:44:23,060][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458962 effective words) took 0.3s, 4863740 effective words/s
[2023-02-07 13:44:23,356][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458962 effective words) took 0.3s, 4941877 effective words/s
[2023-02-07 13:44:23,654][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458962 effective words) took 0.3s, 4915049 effective words/s
[2023-02-07 13:44:23,954][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458962 effective words) took 0.3s, 4885362 effective words/s
[2023-02-07 13:44:24,256][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458962 effective words) took 0.3s, 4852854 effective words/s
[2023-02-07 13:44:24,559][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458962 effective words) took 0.3s, 4839571 effective words/s
[2023-02-07 13:44:24,859][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458962 effective words) took 0.3s, 4884351 effective words/s
[2023-02-07 13:44:25,151][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458962 effective words) took 0.3s, 5019107 effective words/s
[2023-02-07 13:44:25,446][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458962 effective words) took 0.3s, 4954647 effective words/s
[2023-02-07 13:44:25,742][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458962 effective words) took 0.3s, 4949053 effective words/s
[2023-02-07 13:44:26,035][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458962 effective words) took 0.3s, 5002737 effective words/s
[2023-02-07 13:44:26,328][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458962 effective words) took 0.3s, 5006250 effective words/s
[2023-02-07 13:44:26,624][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458962 effective words) took 0.3s, 4939799 effective words/s
[2023-02-07 13:44:26,932][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458962 effective words) took 0.3s, 4788663 effective words/s
[2023-02-07 13:44:27,236][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458962 effective words) took 0.3s, 4836061 effective words/s
[2023-02-07 13:44:27,536][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458962 effective words) took 0.3s, 4876914 effective words/s
[2023-02-07 13:44:27,837][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458962 effective words) took 0.3s, 4872956 effective words/s
[2023-02-07 13:44:28,141][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458962 effective words) took 0.3s, 4819377 effective words/s
[2023-02-07 13:44:28,443][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458962 effective words) took 0.3s, 4851627 effective words/s
[2023-02-07 13:44:28,747][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458962 effective words) took 0.3s, 4809447 effective words/s
[2023-02-07 13:44:29,044][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458962 effective words) took 0.3s, 4940742 effective words/s
[2023-02-07 13:44:29,338][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458962 effective words) took 0.3s, 4985099 effective words/s
[2023-02-07 13:44:29,634][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458962 effective words) took 0.3s, 4951309 effective words/s
[2023-02-07 13:44:29,930][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458962 effective words) took 0.3s, 4938498 effective words/s
[2023-02-07 13:44:30,230][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458962 effective words) took 0.3s, 4900720 effective words/s
[2023-02-07 13:44:30,536][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1458962 effective words) took 0.3s, 4793664 effective words/s
[2023-02-07 13:44:30,841][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1458962 effective words) took 0.3s, 4791975 effective words/s
[2023-02-07 13:44:31,143][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1458962 effective words) took 0.3s, 4854875 effective words/s
[2023-02-07 13:44:31,443][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1458962 effective words) took 0.3s, 4887694 effective words/s
[2023-02-07 13:44:31,741][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1458962 effective words) took 0.3s, 4917825 effective words/s
[2023-02-07 13:44:32,038][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1458962 effective words) took 0.3s, 4941831 effective words/s
[2023-02-07 13:44:32,341][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1458962 effective words) took 0.3s, 4829959 effective words/s
[2023-02-07 13:44:32,642][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1458962 effective words) took 0.3s, 4870880 effective words/s
[2023-02-07 13:44:32,949][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1458962 effective words) took 0.3s, 4776542 effective words/s
[2023-02-07 13:44:33,250][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1458962 effective words) took 0.3s, 4866865 effective words/s
[2023-02-07 13:44:33,547][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1458962 effective words) took 0.3s, 4930627 effective words/s
[2023-02-07 13:44:33,845][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1458962 effective words) took 0.3s, 4909121 effective words/s
[2023-02-07 13:44:34,145][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1458962 effective words) took 0.3s, 4899532 effective words/s
[2023-02-07 13:44:34,438][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1458962 effective words) took 0.3s, 4996628 effective words/s
[2023-02-07 13:44:34,730][gensim.models.word2vec][INFO] - EPOCH 70: training on 1455748 raw words (1458962 effective words) took 0.3s, 5025981 effective words/s
[2023-02-07 13:44:35,022][gensim.models.word2vec][INFO] - EPOCH 71: training on 1455748 raw words (1458962 effective words) took 0.3s, 5015845 effective words/s
[2023-02-07 13:44:35,322][gensim.models.word2vec][INFO] - EPOCH 72: training on 1455748 raw words (1458962 effective words) took 0.3s, 4883828 effective words/s
[2023-02-07 13:44:35,619][gensim.models.word2vec][INFO] - EPOCH 73: training on 1455748 raw words (1458962 effective words) took 0.3s, 4937235 effective words/s
[2023-02-07 13:44:35,917][gensim.models.word2vec][INFO] - EPOCH 74: training on 1455748 raw words (1458962 effective words) took 0.3s, 4904771 effective words/s
[2023-02-07 13:44:36,215][gensim.models.word2vec][INFO] - EPOCH 75: training on 1455748 raw words (1458962 effective words) took 0.3s, 4909376 effective words/s
[2023-02-07 13:44:36,512][gensim.models.word2vec][INFO] - EPOCH 76: training on 1455748 raw words (1458962 effective words) took 0.3s, 4942590 effective words/s
[2023-02-07 13:44:36,810][gensim.models.word2vec][INFO] - EPOCH 77: training on 1455748 raw words (1458962 effective words) took 0.3s, 4923110 effective words/s
[2023-02-07 13:44:37,110][gensim.models.word2vec][INFO] - EPOCH 78: training on 1455748 raw words (1458962 effective words) took 0.3s, 4879312 effective words/s
[2023-02-07 13:44:37,409][gensim.models.word2vec][INFO] - EPOCH 79: training on 1455748 raw words (1458962 effective words) took 0.3s, 4898441 effective words/s
[2023-02-07 13:44:37,709][gensim.models.word2vec][INFO] - EPOCH 80: training on 1455748 raw words (1458962 effective words) took 0.3s, 4891898 effective words/s
[2023-02-07 13:44:38,008][gensim.models.word2vec][INFO] - EPOCH 81: training on 1455748 raw words (1458962 effective words) took 0.3s, 4905589 effective words/s
[2023-02-07 13:44:38,303][gensim.models.word2vec][INFO] - EPOCH 82: training on 1455748 raw words (1458962 effective words) took 0.3s, 4961154 effective words/s
[2023-02-07 13:44:38,603][gensim.models.word2vec][INFO] - EPOCH 83: training on 1455748 raw words (1458962 effective words) took 0.3s, 4885564 effective words/s
[2023-02-07 13:44:38,901][gensim.models.word2vec][INFO] - EPOCH 84: training on 1455748 raw words (1458962 effective words) took 0.3s, 4910409 effective words/s
[2023-02-07 13:44:39,200][gensim.models.word2vec][INFO] - EPOCH 85: training on 1455748 raw words (1458962 effective words) took 0.3s, 4910612 effective words/s
[2023-02-07 13:44:39,502][gensim.models.word2vec][INFO] - EPOCH 86: training on 1455748 raw words (1458962 effective words) took 0.3s, 4836765 effective words/s
[2023-02-07 13:44:39,801][gensim.models.word2vec][INFO] - EPOCH 87: training on 1455748 raw words (1458962 effective words) took 0.3s, 4899123 effective words/s
[2023-02-07 13:44:40,099][gensim.models.word2vec][INFO] - EPOCH 88: training on 1455748 raw words (1458962 effective words) took 0.3s, 4921792 effective words/s
[2023-02-07 13:44:40,398][gensim.models.word2vec][INFO] - EPOCH 89: training on 1455748 raw words (1458962 effective words) took 0.3s, 4910011 effective words/s
[2023-02-07 13:44:40,696][gensim.models.word2vec][INFO] - EPOCH 90: training on 1455748 raw words (1458962 effective words) took 0.3s, 4903668 effective words/s
[2023-02-07 13:44:40,998][gensim.models.word2vec][INFO] - EPOCH 91: training on 1455748 raw words (1458962 effective words) took 0.3s, 4847664 effective words/s
[2023-02-07 13:44:41,301][gensim.models.word2vec][INFO] - EPOCH 92: training on 1455748 raw words (1458962 effective words) took 0.3s, 4847708 effective words/s
[2023-02-07 13:44:41,611][gensim.models.word2vec][INFO] - EPOCH 93: training on 1455748 raw words (1458962 effective words) took 0.3s, 4722363 effective words/s
[2023-02-07 13:44:41,915][gensim.models.word2vec][INFO] - EPOCH 94: training on 1455748 raw words (1458962 effective words) took 0.3s, 4814447 effective words/s
[2023-02-07 13:44:42,223][gensim.models.word2vec][INFO] - EPOCH 95: training on 1455748 raw words (1458962 effective words) took 0.3s, 4772094 effective words/s
[2023-02-07 13:44:42,531][gensim.models.word2vec][INFO] - EPOCH 96: training on 1455748 raw words (1458962 effective words) took 0.3s, 4743125 effective words/s
[2023-02-07 13:44:42,850][gensim.models.word2vec][INFO] - EPOCH 97: training on 1455748 raw words (1458962 effective words) took 0.3s, 4590580 effective words/s
[2023-02-07 13:44:43,157][gensim.models.word2vec][INFO] - EPOCH 98: training on 1455748 raw words (1458962 effective words) took 0.3s, 4780651 effective words/s
[2023-02-07 13:44:43,157][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 144119052 raw words (144437238 effective words) took 29.5s, 4895807 effective words/s', 'datetime': '2023-02-07T13:44:43.157545', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:44:43.157 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:44:45,433][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134358-y3vjo6tu/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:44:45.433837', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:44:45,435][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:44:45,439][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134358-y3vjo6tu/files/../tmp/embedding_model.pt
2023-02-07 13:44:45.439 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:44:46.246 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:44:46.590 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:44:46.847 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2325467560708936, 'test_mae': 1.1129072486920042, 'test_r2': -0.056128099351221206}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.46
wandb: percentage 0.04654
wandb:   test_mae 1.11291
wandb:   test_mse 2.23255
wandb:    test_r2 -0.05613
wandb: 
wandb: üöÄ View run stellar-sweep-54 at: https://wandb.ai/xiaoqiz/mof2vec/runs/y3vjo6tu
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_134358-y3vjo6tu/logs
wandb: Agent Starting Run: 70xy55nr with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 875
wandb: 	model.gensim.alpha: 0.000532246047610158
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 93
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.495960219833928
wandb: 	model.gensim.vector_size: 145
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.5315365703288399
wandb: 	model.sklearn.max_depth: 21
wandb: 	model.sklearn.min_child_weight: 0.004563822600256857
wandb: 	model.sklearn.n_estimators: 3661
wandb: 	model.sklearn.num_leaves: 499
wandb: 	model.sklearn.reg_alpha: 0.279873951750235
wandb: 	model.sklearn.reg_lambda: 0.10160974033280744
wandb: 	model.sklearn.subsample: 0.6822976635360507
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134456-70xy55nr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-55
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/70xy55nr
2023-02-07 13:45:04.840 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 13:45:04.841 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 875 for sweep.
2023-02-07 13:45:04.841 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.000532246047610158 for sweep.
2023-02-07 13:45:04.842 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:45:04.842 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 93 for sweep.
2023-02-07 13:45:04.842 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 13:45:04.842 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.495960219833928 for sweep.
2023-02-07 13:45:04.842 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 145 for sweep.
2023-02-07 13:45:04.843 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 13:45:04.843 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.5315365703288399 for sweep.
2023-02-07 13:45:04.844 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 21 for sweep.
2023-02-07 13:45:04.844 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.004563822600256857 for sweep.
2023-02-07 13:45:04.844 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3661 for sweep.
2023-02-07 13:45:04.844 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 499 for sweep.
2023-02-07 13:45:04.844 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.279873951750235 for sweep.
2023-02-07 13:45:04.845 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.10160974033280744 for sweep.
2023-02-07 13:45:04.845 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6822976635360507 for sweep.
2023-02-07 13:45:04.845 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:45:04.850 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134456-70xy55nr/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 875, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 145, 'window': 3, 'min_count': 3, 'dm': 0, 'sample': 0.495960219833928, 'workers': 4, 'alpha': 0.000532246047610158, 'epochs': 93}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3661, 'max_depth': 21, 'num_leaves': 499, 'reg_alpha': 0.279873951750235, 'reg_lambda': 0.10160974033280744, 'subsample': 0.6822976635360507, 'min_child_weight': 0.004563822600256857, 'n_jobs': 4, 'learning_rate': 0.5315365703288399}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 27/3257 [00:00<00:12, 261.47it/s]  2%|‚ñè         | 54/3257 [00:00<00:12, 256.42it/s]  3%|‚ñé         | 84/3257 [00:00<00:11, 273.11it/s]  3%|‚ñé         | 112/3257 [00:00<00:11, 263.01it/s]  4%|‚ñç         | 144/3257 [00:00<00:11, 281.85it/s]  5%|‚ñå         | 173/3257 [00:00<00:11, 269.43it/s]  6%|‚ñå         | 201/3257 [00:00<00:11, 272.64it/s]  7%|‚ñã         | 236/3257 [00:00<00:10, 295.42it/s]  8%|‚ñä         | 266/3257 [00:00<00:10, 286.47it/s]  9%|‚ñâ         | 299/3257 [00:01<00:09, 298.17it/s] 10%|‚ñà         | 329/3257 [00:01<00:09, 294.06it/s] 11%|‚ñà         | 359/3257 [00:01<00:09, 289.88it/s] 12%|‚ñà‚ñè        | 389/3257 [00:01<00:10, 274.49it/s] 13%|‚ñà‚ñé        | 420/3257 [00:01<00:10, 279.35it/s] 14%|‚ñà‚ñç        | 449/3257 [00:01<00:10, 258.93it/s] 15%|‚ñà‚ñç        | 479/3257 [00:01<00:10, 266.35it/s] 16%|‚ñà‚ñå        | 512/3257 [00:01<00:09, 280.67it/s] 17%|‚ñà‚ñã        | 541/3257 [00:01<00:09, 281.79it/s] 18%|‚ñà‚ñä        | 570/3257 [00:02<00:10, 268.05it/s] 18%|‚ñà‚ñä        | 598/3257 [00:02<00:09, 267.71it/s] 19%|‚ñà‚ñâ        | 625/3257 [00:02<00:09, 264.53it/s] 20%|‚ñà‚ñà        | 654/3257 [00:02<00:09, 269.54it/s] 21%|‚ñà‚ñà        | 682/3257 [00:02<00:09, 267.98it/s] 22%|‚ñà‚ñà‚ñè       | 709/3257 [00:02<00:09, 267.58it/s] 23%|‚ñà‚ñà‚ñé       | 736/3257 [00:02<00:09, 259.07it/s] 24%|‚ñà‚ñà‚ñé       | 767/3257 [00:02<00:09, 273.51it/s] 24%|‚ñà‚ñà‚ñç       | 795/3257 [00:02<00:09, 268.76it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:03<00:09, 264.33it/s] 26%|‚ñà‚ñà‚ñå       | 850/3257 [00:03<00:09, 256.20it/s] 27%|‚ñà‚ñà‚ñã       | 877/3257 [00:03<00:09, 257.45it/s] 28%|‚ñà‚ñà‚ñä       | 908/3257 [00:03<00:08, 269.65it/s] 29%|‚ñà‚ñà‚ñâ       | 937/3257 [00:03<00:08, 272.96it/s] 30%|‚ñà‚ñà‚ñâ       | 968/3257 [00:03<00:08, 282.00it/s] 31%|‚ñà‚ñà‚ñà       | 997/3257 [00:03<00:08, 271.15it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1025/3257 [00:03<00:08, 261.43it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1052/3257 [00:03<00:08, 245.61it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1077/3257 [00:04<00:12, 178.74it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1098/3257 [00:04<00:11, 184.73it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1124/3257 [00:04<00:10, 202.37it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:04<00:10, 207.43it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1175/3257 [00:04<00:09, 221.65it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1199/3257 [00:04<00:09, 217.46it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1224/3257 [00:04<00:09, 225.16it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1252/3257 [00:04<00:08, 239.87it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1277/3257 [00:04<00:08, 239.51it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1302/3257 [00:05<00:08, 239.68it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1331/3257 [00:05<00:07, 251.59it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1358/3257 [00:05<00:07, 256.69it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:05<00:07, 252.27it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1417/3257 [00:05<00:06, 273.39it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1446/3257 [00:05<00:06, 278.04it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1478/3257 [00:05<00:06, 289.93it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:05<00:05, 300.08it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1542/3257 [00:05<00:06, 278.01it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1571/3257 [00:06<00:06, 273.38it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1600/3257 [00:06<00:05, 276.84it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1631/3257 [00:06<00:05, 285.98it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1660/3257 [00:06<00:05, 267.58it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1688/3257 [00:06<00:05, 262.25it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1716/3257 [00:06<00:05, 263.88it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1743/3257 [00:06<00:05, 252.37it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1774/3257 [00:06<00:05, 266.40it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1803/3257 [00:06<00:05, 271.55it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1831/3257 [00:07<00:05, 260.64it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1863/3257 [00:07<00:05, 274.96it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1891/3257 [00:07<00:04, 275.35it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1919/3257 [00:07<00:04, 273.50it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1955/3257 [00:07<00:04, 298.06it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1985/3257 [00:07<00:04, 290.65it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2015/3257 [00:07<00:04, 291.34it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2045/3257 [00:07<00:04, 290.90it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2075/3257 [00:07<00:04, 277.77it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2103/3257 [00:07<00:04, 270.12it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2131/3257 [00:08<00:04, 261.39it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2159/3257 [00:08<00:04, 265.67it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2186/3257 [00:08<00:04, 266.70it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2213/3257 [00:08<00:03, 266.44it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2241/3257 [00:08<00:03, 269.76it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2270/3257 [00:08<00:03, 270.07it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2300/3257 [00:08<00:03, 275.71it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2335/3257 [00:08<00:03, 295.93it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2368/3257 [00:08<00:02, 305.14it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2404/3257 [00:09<00:02, 317.94it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2436/3257 [00:09<00:02, 302.50it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2467/3257 [00:09<00:03, 211.20it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2503/3257 [00:09<00:03, 243.92it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2539/3257 [00:09<00:02, 270.55it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2570/3257 [00:09<00:02, 270.21it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2600/3257 [00:09<00:02, 277.87it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2638/3257 [00:09<00:02, 303.86it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:10<00:01, 300.51it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2702/3257 [00:10<00:01, 285.03it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:10<00:01, 284.96it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2765/3257 [00:10<00:01, 295.64it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2796/3257 [00:10<00:01, 296.44it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2826/3257 [00:10<00:01, 288.93it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2861/3257 [00:10<00:01, 305.15it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2894/3257 [00:10<00:01, 307.93it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:10<00:01, 315.55it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2960/3257 [00:10<00:00, 309.85it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2992/3257 [00:11<00:00, 309.97it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3026/3257 [00:11<00:00, 316.63it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3060/3257 [00:11<00:00, 322.13it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3093/3257 [00:11<00:00, 320.20it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3130/3257 [00:11<00:00, 332.22it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3164/3257 [00:11<00:00, 316.29it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3197/3257 [00:11<00:00, 313.64it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3229/3257 [00:11<00:00, 314.24it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 273.60it/s]
2023-02-07 13:45:17.088 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:45:17,089][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d145,n5,mc3,s0.49596,t4>', 'datetime': '2023-02-07T13:45:17.089381', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:45:17,089][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:45:17,089][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:45:17,340][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 13:45:17,341][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:45:17,353][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 4875 unique words (73.18% of original 6662, drops 1787)', 'datetime': '2023-02-07T13:45:17.353030', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:45:17,354][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 2908813 word corpus (99.91% of original 2911496, drops 2683)', 'datetime': '2023-02-07T13:45:17.354713', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:45:17,370][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 13:45:17,370][gensim.models.word2vec][INFO] - sample=0.49596 downsamples 0 most-common words
[2023-02-07 13:45:17,370][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2908813 word corpus (100.0%% of prior 2908813)', 'datetime': '2023-02-07T13:45:17.370684', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:45:17,397][gensim.models.word2vec][INFO] - estimated required memory for 4875 words and 145 dimensions: 10632960 bytes
[2023-02-07 13:45:17,398][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:45:17,403][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 4875 vocabulary and 145 features, using sg=1 hs=0 sample=0.495960219833928 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T13:45:17.403503', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:45:18,409][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 97.39% examples, 2829582 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:18,429][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2912070 effective words) took 1.0s, 2844051 effective words/s
[2023-02-07 13:45:19,434][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 93.00% examples, 2714842 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:19,499][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2912070 effective words) took 1.1s, 2722997 effective words/s
[2023-02-07 13:45:20,503][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 90.70% examples, 2652239 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:20,593][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2912070 effective words) took 1.1s, 2666937 effective words/s
[2023-02-07 13:45:21,597][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 91.53% examples, 2679828 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:21,674][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2912070 effective words) took 1.1s, 2696363 effective words/s
[2023-02-07 13:45:22,677][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 91.53% examples, 2682745 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:22,757][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2912070 effective words) took 1.1s, 2692738 effective words/s
[2023-02-07 13:45:23,759][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 93.00% examples, 2722427 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:23,823][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2912070 effective words) took 1.1s, 2735407 effective words/s
[2023-02-07 13:45:24,829][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 92.39% examples, 2693646 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:24,902][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2912070 effective words) took 1.1s, 2703481 effective words/s
[2023-02-07 13:45:25,904][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 91.86% examples, 2693256 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:25,979][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2912070 effective words) took 1.1s, 2707001 effective words/s
[2023-02-07 13:45:26,987][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 93.00% examples, 2711608 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:27,051][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2912070 effective words) took 1.1s, 2721805 effective words/s
[2023-02-07 13:45:28,057][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 91.86% examples, 2686351 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:28,132][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2912070 effective words) took 1.1s, 2698473 effective words/s
[2023-02-07 13:45:29,135][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 93.00% examples, 2722227 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:29,196][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2912070 effective words) took 1.1s, 2740475 effective words/s
[2023-02-07 13:45:30,197][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 94.38% examples, 2755340 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:30,252][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2912070 effective words) took 1.1s, 2760456 effective words/s
[2023-02-07 13:45:31,256][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 91.86% examples, 2691646 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:31,329][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2912070 effective words) took 1.1s, 2710357 effective words/s
[2023-02-07 13:45:32,333][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 93.00% examples, 2718612 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:32,396][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2912070 effective words) took 1.1s, 2731591 effective words/s
[2023-02-07 13:45:33,401][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 95.12% examples, 2763923 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:33,444][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2912070 effective words) took 1.0s, 2781380 effective words/s
[2023-02-07 13:45:34,450][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 95.46% examples, 2772073 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:34,492][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2912070 effective words) took 1.0s, 2781525 effective words/s
[2023-02-07 13:45:35,495][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 91.86% examples, 2694021 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:35,570][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2912070 effective words) took 1.1s, 2705972 effective words/s
[2023-02-07 13:45:36,577][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 93.00% examples, 2710242 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:36,639][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2912070 effective words) took 1.1s, 2726866 effective words/s
[2023-02-07 13:45:37,642][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 94.69% examples, 2760931 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:37,689][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2912070 effective words) took 1.0s, 2777767 effective words/s
[2023-02-07 13:45:38,691][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 95.89% examples, 2790114 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:38,730][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2912070 effective words) took 1.0s, 2799473 effective words/s
[2023-02-07 13:45:39,732][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 93.00% examples, 2723213 words/s, in_qsize 6, out_qsize 1
[2023-02-07 13:45:39,793][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2912070 effective words) took 1.1s, 2742791 effective words/s
[2023-02-07 13:45:40,798][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 95.46% examples, 2775254 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:40,841][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2912070 effective words) took 1.0s, 2784700 effective words/s
[2023-02-07 13:45:41,844][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 95.89% examples, 2789808 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:41,883][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2912070 effective words) took 1.0s, 2800409 effective words/s
[2023-02-07 13:45:42,887][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 96.47% examples, 2800737 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:42,919][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2912070 effective words) took 1.0s, 2813264 effective words/s
[2023-02-07 13:45:43,922][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 96.16% examples, 2795410 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:43,959][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2912070 effective words) took 1.0s, 2802642 effective words/s
[2023-02-07 13:45:44,963][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 90.70% examples, 2651725 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:45,053][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2912070 effective words) took 1.1s, 2663887 effective words/s
[2023-02-07 13:45:46,056][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 90.02% examples, 2633531 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:46,157][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2912070 effective words) took 1.1s, 2641097 effective words/s
[2023-02-07 13:45:47,161][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 92.39% examples, 2701617 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:47,232][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2912070 effective words) took 1.1s, 2712584 effective words/s
[2023-02-07 13:45:48,239][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 91.68% examples, 2673427 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:48,316][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2912070 effective words) took 1.1s, 2689301 effective words/s
[2023-02-07 13:45:49,325][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 91.53% examples, 2670227 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:49,404][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2912070 effective words) took 1.1s, 2683011 effective words/s
[2023-02-07 13:45:50,409][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 91.53% examples, 2678496 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:50,489][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2912070 effective words) took 1.1s, 2689413 effective words/s
[2023-02-07 13:45:51,496][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 91.53% examples, 2673114 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:51,577][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2912070 effective words) took 1.1s, 2680022 effective words/s
[2023-02-07 13:45:52,579][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 90.48% examples, 2653021 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:52,671][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2912070 effective words) took 1.1s, 2666387 effective words/s
[2023-02-07 13:45:53,679][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 93.00% examples, 2706253 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:53,741][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2912070 effective words) took 1.1s, 2725213 effective words/s
[2023-02-07 13:45:54,745][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 91.53% examples, 2680988 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:45:54,821][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2912070 effective words) took 1.1s, 2699312 effective words/s
[2023-02-07 13:45:55,825][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 93.92% examples, 2737851 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:55,883][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2912070 effective words) took 1.1s, 2746401 effective words/s
[2023-02-07 13:45:56,886][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 90.70% examples, 2655897 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:56,974][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2912070 effective words) took 1.1s, 2670867 effective words/s
[2023-02-07 13:45:57,977][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 93.00% examples, 2721104 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:58,041][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2912070 effective words) took 1.1s, 2733230 effective words/s
[2023-02-07 13:45:59,044][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 93.00% examples, 2722793 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:45:59,106][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2912070 effective words) took 1.1s, 2739309 effective words/s
[2023-02-07 13:46:00,112][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 93.49% examples, 2723557 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:00,172][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2912070 effective words) took 1.1s, 2736336 effective words/s
[2023-02-07 13:46:01,177][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 92.39% examples, 2697031 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:01,252][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2912070 effective words) took 1.1s, 2701017 effective words/s
[2023-02-07 13:46:02,255][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 91.86% examples, 2691061 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:02,331][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2912070 effective words) took 1.1s, 2701555 effective words/s
[2023-02-07 13:46:03,319][gensim.models.word2vec][INFO] - EPOCH 42: training on 2911496 raw words (2912070 effective words) took 1.0s, 2952038 effective words/s
[2023-02-07 13:46:04,290][gensim.models.word2vec][INFO] - EPOCH 43: training on 2911496 raw words (2912070 effective words) took 1.0s, 3004074 effective words/s
[2023-02-07 13:46:05,275][gensim.models.word2vec][INFO] - EPOCH 44: training on 2911496 raw words (2912070 effective words) took 1.0s, 2958003 effective words/s
[2023-02-07 13:46:06,277][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 93.92% examples, 2743946 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:46:06,333][gensim.models.word2vec][INFO] - EPOCH 45: training on 2911496 raw words (2912070 effective words) took 1.1s, 2756640 effective words/s
[2023-02-07 13:46:07,335][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 93.00% examples, 2722752 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:07,402][gensim.models.word2vec][INFO] - EPOCH 46: training on 2911496 raw words (2912070 effective words) took 1.1s, 2728033 effective words/s
[2023-02-07 13:46:08,406][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 93.00% examples, 2719169 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:08,468][gensim.models.word2vec][INFO] - EPOCH 47: training on 2911496 raw words (2912070 effective words) took 1.1s, 2734914 effective words/s
[2023-02-07 13:46:09,471][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 94.38% examples, 2750738 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:09,523][gensim.models.word2vec][INFO] - EPOCH 48: training on 2911496 raw words (2912070 effective words) took 1.1s, 2762456 effective words/s
[2023-02-07 13:46:10,526][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 95.03% examples, 2769744 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:10,571][gensim.models.word2vec][INFO] - EPOCH 49: training on 2911496 raw words (2912070 effective words) took 1.0s, 2783659 effective words/s
[2023-02-07 13:46:11,574][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 96.47% examples, 2806924 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:11,604][gensim.models.word2vec][INFO] - EPOCH 50: training on 2911496 raw words (2912070 effective words) took 1.0s, 2821606 effective words/s
[2023-02-07 13:46:12,609][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 93.95% examples, 2737127 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:12,664][gensim.models.word2vec][INFO] - EPOCH 51: training on 2911496 raw words (2912070 effective words) took 1.1s, 2750576 effective words/s
[2023-02-07 13:46:13,673][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 95.03% examples, 2755858 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:13,714][gensim.models.word2vec][INFO] - EPOCH 52: training on 2911496 raw words (2912070 effective words) took 1.0s, 2778726 effective words/s
[2023-02-07 13:46:14,717][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 96.47% examples, 2807185 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:14,747][gensim.models.word2vec][INFO] - EPOCH 53: training on 2911496 raw words (2912070 effective words) took 1.0s, 2821473 effective words/s
[2023-02-07 13:46:15,749][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 95.46% examples, 2782796 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:15,792][gensim.models.word2vec][INFO] - EPOCH 54: training on 2911496 raw words (2912070 effective words) took 1.0s, 2792708 effective words/s
[2023-02-07 13:46:16,798][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 96.16% examples, 2789436 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:16,833][gensim.models.word2vec][INFO] - EPOCH 55: training on 2911496 raw words (2912070 effective words) took 1.0s, 2801185 effective words/s
[2023-02-07 13:46:17,837][gensim.models.word2vec][INFO] - EPOCH 56 - PROGRESS: at 96.16% examples, 2793240 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:17,871][gensim.models.word2vec][INFO] - EPOCH 56: training on 2911496 raw words (2912070 effective words) took 1.0s, 2807834 effective words/s
[2023-02-07 13:46:18,874][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 95.03% examples, 2768923 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:18,919][gensim.models.word2vec][INFO] - EPOCH 57: training on 2911496 raw words (2912070 effective words) took 1.0s, 2781757 effective words/s
[2023-02-07 13:46:19,924][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 97.39% examples, 2829329 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:19,943][gensim.models.word2vec][INFO] - EPOCH 58: training on 2911496 raw words (2912070 effective words) took 1.0s, 2846366 effective words/s
[2023-02-07 13:46:20,945][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 98.13% examples, 2858038 words/s, in_qsize 6, out_qsize 0
[2023-02-07 13:46:20,960][gensim.models.word2vec][INFO] - EPOCH 59: training on 2911496 raw words (2912070 effective words) took 1.0s, 2866242 effective words/s
[2023-02-07 13:46:21,962][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 95.89% examples, 2792238 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:22,003][gensim.models.word2vec][INFO] - EPOCH 60: training on 2911496 raw words (2912070 effective words) took 1.0s, 2797323 effective words/s
[2023-02-07 13:46:23,005][gensim.models.word2vec][INFO] - EPOCH 61 - PROGRESS: at 96.47% examples, 2808959 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:23,036][gensim.models.word2vec][INFO] - EPOCH 61: training on 2911496 raw words (2912070 effective words) took 1.0s, 2821954 effective words/s
[2023-02-07 13:46:24,040][gensim.models.word2vec][INFO] - EPOCH 62 - PROGRESS: at 96.16% examples, 2794364 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:24,075][gensim.models.word2vec][INFO] - EPOCH 62: training on 2911496 raw words (2912070 effective words) took 1.0s, 2803780 effective words/s
[2023-02-07 13:46:25,077][gensim.models.word2vec][INFO] - EPOCH 63 - PROGRESS: at 97.39% examples, 2837582 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:25,097][gensim.models.word2vec][INFO] - EPOCH 63: training on 2911496 raw words (2912070 effective words) took 1.0s, 2853029 effective words/s
[2023-02-07 13:46:26,100][gensim.models.word2vec][INFO] - EPOCH 64 - PROGRESS: at 98.93% examples, 2884004 words/s, in_qsize 3, out_qsize 1
[2023-02-07 13:46:26,105][gensim.models.word2vec][INFO] - EPOCH 64: training on 2911496 raw words (2912070 effective words) took 1.0s, 2895752 effective words/s
[2023-02-07 13:46:27,108][gensim.models.word2vec][INFO] - EPOCH 65 - PROGRESS: at 98.62% examples, 2870172 words/s, in_qsize 4, out_qsize 0
[2023-02-07 13:46:27,117][gensim.models.word2vec][INFO] - EPOCH 65: training on 2911496 raw words (2912070 effective words) took 1.0s, 2879467 effective words/s
[2023-02-07 13:46:28,120][gensim.models.word2vec][INFO] - EPOCH 66 - PROGRESS: at 97.39% examples, 2835611 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:28,139][gensim.models.word2vec][INFO] - EPOCH 66: training on 2911496 raw words (2912070 effective words) took 1.0s, 2852925 effective words/s
[2023-02-07 13:46:29,142][gensim.models.word2vec][INFO] - EPOCH 67 - PROGRESS: at 97.91% examples, 2853458 words/s, in_qsize 6, out_qsize 0
[2023-02-07 13:46:29,155][gensim.models.word2vec][INFO] - EPOCH 67: training on 2911496 raw words (2912070 effective words) took 1.0s, 2869176 effective words/s
[2023-02-07 13:46:30,157][gensim.models.word2vec][INFO] - EPOCH 68 - PROGRESS: at 96.71% examples, 2818885 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:30,185][gensim.models.word2vec][INFO] - EPOCH 68: training on 2911496 raw words (2912070 effective words) took 1.0s, 2829876 effective words/s
[2023-02-07 13:46:31,189][gensim.models.word2vec][INFO] - EPOCH 69 - PROGRESS: at 93.00% examples, 2718919 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:31,251][gensim.models.word2vec][INFO] - EPOCH 69: training on 2911496 raw words (2912070 effective words) took 1.1s, 2735564 effective words/s
[2023-02-07 13:46:32,256][gensim.models.word2vec][INFO] - EPOCH 70 - PROGRESS: at 93.00% examples, 2716102 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:32,318][gensim.models.word2vec][INFO] - EPOCH 70: training on 2911496 raw words (2912070 effective words) took 1.1s, 2734302 effective words/s
[2023-02-07 13:46:33,321][gensim.models.word2vec][INFO] - EPOCH 71 - PROGRESS: at 93.92% examples, 2741054 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:33,375][gensim.models.word2vec][INFO] - EPOCH 71: training on 2911496 raw words (2912070 effective words) took 1.1s, 2757026 effective words/s
[2023-02-07 13:46:34,378][gensim.models.word2vec][INFO] - EPOCH 72 - PROGRESS: at 94.38% examples, 2750747 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:34,435][gensim.models.word2vec][INFO] - EPOCH 72: training on 2911496 raw words (2912070 effective words) took 1.1s, 2751806 effective words/s
[2023-02-07 13:46:35,437][gensim.models.word2vec][INFO] - EPOCH 73 - PROGRESS: at 93.00% examples, 2723150 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:35,501][gensim.models.word2vec][INFO] - EPOCH 73: training on 2911496 raw words (2912070 effective words) took 1.1s, 2733443 effective words/s
[2023-02-07 13:46:36,503][gensim.models.word2vec][INFO] - EPOCH 74 - PROGRESS: at 93.00% examples, 2724520 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:36,565][gensim.models.word2vec][INFO] - EPOCH 74: training on 2911496 raw words (2912070 effective words) took 1.1s, 2741524 effective words/s
[2023-02-07 13:46:37,568][gensim.models.word2vec][INFO] - EPOCH 75 - PROGRESS: at 94.69% examples, 2761661 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:37,618][gensim.models.word2vec][INFO] - EPOCH 75: training on 2911496 raw words (2912070 effective words) took 1.1s, 2770782 effective words/s
[2023-02-07 13:46:38,624][gensim.models.word2vec][INFO] - EPOCH 76 - PROGRESS: at 94.69% examples, 2754072 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:38,674][gensim.models.word2vec][INFO] - EPOCH 76: training on 2911496 raw words (2912070 effective words) took 1.1s, 2763749 effective words/s
[2023-02-07 13:46:39,678][gensim.models.word2vec][INFO] - EPOCH 77 - PROGRESS: at 91.53% examples, 2680039 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:39,758][gensim.models.word2vec][INFO] - EPOCH 77: training on 2911496 raw words (2912070 effective words) took 1.1s, 2688495 effective words/s
[2023-02-07 13:46:40,764][gensim.models.word2vec][INFO] - EPOCH 78 - PROGRESS: at 94.38% examples, 2746080 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:40,817][gensim.models.word2vec][INFO] - EPOCH 78: training on 2911496 raw words (2912070 effective words) took 1.1s, 2757126 effective words/s
[2023-02-07 13:46:41,824][gensim.models.word2vec][INFO] - EPOCH 79 - PROGRESS: at 94.69% examples, 2751194 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:41,872][gensim.models.word2vec][INFO] - EPOCH 79: training on 2911496 raw words (2912070 effective words) took 1.1s, 2764330 effective words/s
[2023-02-07 13:46:42,877][gensim.models.word2vec][INFO] - EPOCH 80 - PROGRESS: at 93.92% examples, 2735938 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:42,933][gensim.models.word2vec][INFO] - EPOCH 80: training on 2911496 raw words (2912070 effective words) took 1.1s, 2748779 effective words/s
[2023-02-07 13:46:43,936][gensim.models.word2vec][INFO] - EPOCH 81 - PROGRESS: at 92.39% examples, 2705347 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:44,009][gensim.models.word2vec][INFO] - EPOCH 81: training on 2911496 raw words (2912070 effective words) took 1.1s, 2714681 effective words/s
[2023-02-07 13:46:45,012][gensim.models.word2vec][INFO] - EPOCH 82 - PROGRESS: at 93.92% examples, 2744118 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:45,068][gensim.models.word2vec][INFO] - EPOCH 82: training on 2911496 raw words (2912070 effective words) took 1.1s, 2755562 effective words/s
[2023-02-07 13:46:46,070][gensim.models.word2vec][INFO] - EPOCH 83 - PROGRESS: at 95.03% examples, 2773293 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:46,114][gensim.models.word2vec][INFO] - EPOCH 83: training on 2911496 raw words (2912070 effective words) took 1.0s, 2787005 effective words/s
[2023-02-07 13:46:47,122][gensim.models.word2vec][INFO] - EPOCH 84 - PROGRESS: at 94.69% examples, 2748746 words/s, in_qsize 8, out_qsize 1
[2023-02-07 13:46:47,168][gensim.models.word2vec][INFO] - EPOCH 84: training on 2911496 raw words (2912070 effective words) took 1.1s, 2767649 effective words/s
[2023-02-07 13:46:48,171][gensim.models.word2vec][INFO] - EPOCH 85 - PROGRESS: at 97.91% examples, 2856579 words/s, in_qsize 5, out_qsize 1
[2023-02-07 13:46:48,183][gensim.models.word2vec][INFO] - EPOCH 85: training on 2911496 raw words (2912070 effective words) took 1.0s, 2874235 effective words/s
[2023-02-07 13:46:49,137][gensim.models.word2vec][INFO] - EPOCH 86: training on 2911496 raw words (2912070 effective words) took 1.0s, 3057253 effective words/s
[2023-02-07 13:46:50,111][gensim.models.word2vec][INFO] - EPOCH 87: training on 2911496 raw words (2912070 effective words) took 1.0s, 2991723 effective words/s
[2023-02-07 13:46:51,103][gensim.models.word2vec][INFO] - EPOCH 88: training on 2911496 raw words (2912070 effective words) took 1.0s, 2940596 effective words/s
[2023-02-07 13:46:52,108][gensim.models.word2vec][INFO] - EPOCH 89 - PROGRESS: at 93.00% examples, 2715858 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:46:52,172][gensim.models.word2vec][INFO] - EPOCH 89: training on 2911496 raw words (2912070 effective words) took 1.1s, 2726598 effective words/s
[2023-02-07 13:46:53,176][gensim.models.word2vec][INFO] - EPOCH 90 - PROGRESS: at 95.89% examples, 2786433 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:53,216][gensim.models.word2vec][INFO] - EPOCH 90: training on 2911496 raw words (2912070 effective words) took 1.0s, 2794482 effective words/s
[2023-02-07 13:46:54,218][gensim.models.word2vec][INFO] - EPOCH 91 - PROGRESS: at 94.69% examples, 2763021 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:46:54,268][gensim.models.word2vec][INFO] - EPOCH 91: training on 2911496 raw words (2912070 effective words) took 1.1s, 2772860 effective words/s
[2023-02-07 13:46:55,271][gensim.models.word2vec][INFO] - EPOCH 92 - PROGRESS: at 95.89% examples, 2786902 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:46:55,312][gensim.models.word2vec][INFO] - EPOCH 92: training on 2911496 raw words (2912070 effective words) took 1.0s, 2793753 effective words/s
[2023-02-07 13:46:55,312][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 270769128 raw words (270822510 effective words) took 97.9s, 2766074 effective words/s', 'datetime': '2023-02-07T13:46:55.312419', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:46:55.312 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:47:01,295][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134456-70xy55nr/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:47:01.295504', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:47:01,297][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:47:01,320][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134456-70xy55nr/files/../tmp/embedding_model.pt
2023-02-07 13:47:01.321 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:47:02.534 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:47:02.998 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:47:03.976 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0136150792229675, 'test_mae': 1.0594305502924999, 'test_r2': 0.047439673699187224}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.050 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.050 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.9
wandb: percentage 0.26824
wandb:   test_mae 1.05943
wandb:   test_mse 2.01362
wandb:    test_r2 0.04744
wandb: 
wandb: üöÄ View run misty-sweep-55 at: https://wandb.ai/xiaoqiz/mof2vec/runs/70xy55nr
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_134456-70xy55nr/logs
wandb: Agent Starting Run: 5z15i9ex with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 145
wandb: 	model.gensim.alpha: 0.0005516618946009023
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 48
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.5383943462115237
wandb: 	model.gensim.vector_size: 32
wandb: 	model.gensim.window: 5
wandb: 	model.sklearn.learning_rate: 0.025690538232741367
wandb: 	model.sklearn.max_depth: 17
wandb: 	model.sklearn.min_child_weight: 0.05994399782240911
wandb: 	model.sklearn.n_estimators: 2475
wandb: 	model.sklearn.num_leaves: 498
wandb: 	model.sklearn.reg_alpha: 0.003909007560738621
wandb: 	model.sklearn.reg_lambda: 0.4072741311222377
wandb: 	model.sklearn.subsample: 0.4124086799242089
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134713-5z15i9ex
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-56
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/5z15i9ex
2023-02-07 13:47:21.892 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:47:21.893 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 145 for sweep.
2023-02-07 13:47:21.893 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0005516618946009023 for sweep.
2023-02-07 13:47:21.893 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:47:21.893 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 48 for sweep.
2023-02-07 13:47:21.894 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 13:47:21.894 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5383943462115237 for sweep.
2023-02-07 13:47:21.894 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 32 for sweep.
2023-02-07 13:47:21.894 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 5 for sweep.
2023-02-07 13:47:21.894 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.025690538232741367 for sweep.
2023-02-07 13:47:21.895 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 17 for sweep.
2023-02-07 13:47:21.895 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05994399782240911 for sweep.
2023-02-07 13:47:21.895 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2475 for sweep.
2023-02-07 13:47:21.895 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 498 for sweep.
2023-02-07 13:47:21.896 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.003909007560738621 for sweep.
2023-02-07 13:47:21.896 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.4072741311222377 for sweep.
2023-02-07 13:47:21.896 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.4124086799242089 for sweep.
2023-02-07 13:47:21.896 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:47:21.901 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134713-5z15i9ex/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 145, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 32, 'window': 5, 'min_count': 5, 'dm': 0, 'sample': 0.5383943462115237, 'workers': 4, 'alpha': 0.0005516618946009023, 'epochs': 48}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2475, 'max_depth': 17, 'num_leaves': 498, 'reg_alpha': 0.003909007560738621, 'reg_lambda': 0.4072741311222377, 'subsample': 0.4124086799242089, 'min_child_weight': 0.05994399782240911, 'n_jobs': 4, 'learning_rate': 0.025690538232741367}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 37/3257 [00:00<00:08, 369.31it/s]  2%|‚ñè         | 76/3257 [00:00<00:08, 377.75it/s]  4%|‚ñé         | 114/3257 [00:00<00:08, 374.03it/s]  5%|‚ñç         | 158/3257 [00:00<00:07, 393.35it/s]  6%|‚ñå         | 198/3257 [00:00<00:07, 394.52it/s]  7%|‚ñã         | 243/3257 [00:00<00:07, 412.08it/s]  9%|‚ñâ         | 286/3257 [00:00<00:07, 416.68it/s] 10%|‚ñà         | 328/3257 [00:00<00:07, 415.16it/s] 11%|‚ñà‚ñè        | 370/3257 [00:00<00:06, 414.86it/s] 13%|‚ñà‚ñé        | 412/3257 [00:01<00:07, 403.02it/s] 14%|‚ñà‚ñç        | 453/3257 [00:01<00:07, 380.20it/s] 15%|‚ñà‚ñå        | 493/3257 [00:01<00:07, 384.32it/s] 16%|‚ñà‚ñã        | 535/3257 [00:01<00:06, 393.57it/s] 18%|‚ñà‚ñä        | 575/3257 [00:01<00:07, 379.44it/s] 19%|‚ñà‚ñâ        | 618/3257 [00:01<00:06, 393.37it/s] 20%|‚ñà‚ñà        | 658/3257 [00:01<00:06, 374.11it/s] 21%|‚ñà‚ñà‚ñè       | 696/3257 [00:01<00:07, 358.80it/s] 23%|‚ñà‚ñà‚ñé       | 733/3257 [00:02<00:09, 258.20it/s] 24%|‚ñà‚ñà‚ñé       | 773/3257 [00:02<00:08, 287.20it/s] 25%|‚ñà‚ñà‚ñç       | 812/3257 [00:02<00:07, 309.96it/s] 26%|‚ñà‚ñà‚ñå       | 847/3257 [00:02<00:07, 316.34it/s] 27%|‚ñà‚ñà‚ñã       | 886/3257 [00:02<00:07, 334.74it/s] 28%|‚ñà‚ñà‚ñä       | 928/3257 [00:02<00:06, 357.05it/s] 30%|‚ñà‚ñà‚ñâ       | 967/3257 [00:02<00:06, 364.53it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:02<00:06, 365.31it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1045/3257 [00:02<00:06, 366.04it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1084/3257 [00:02<00:05, 371.83it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1123/3257 [00:03<00:05, 374.45it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1162/3257 [00:03<00:05, 376.65it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1200/3257 [00:03<00:05, 359.94it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1241/3257 [00:03<00:05, 370.07it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1279/3257 [00:03<00:05, 350.62it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1315/3257 [00:03<00:05, 350.23it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1351/3257 [00:03<00:05, 348.59it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1387/3257 [00:03<00:05, 346.33it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1427/3257 [00:03<00:05, 360.22it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1466/3257 [00:04<00:04, 368.79it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1506/3257 [00:04<00:04, 376.59it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1544/3257 [00:04<00:04, 352.61it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1580/3257 [00:04<00:04, 348.41it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:04<00:04, 356.09it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1656/3257 [00:04<00:04, 357.69it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1692/3257 [00:04<00:04, 343.50it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:04<00:04, 341.37it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:04<00:04, 335.60it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1799/3257 [00:05<00:04, 342.43it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1839/3257 [00:05<00:03, 356.59it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1880/3257 [00:05<00:03, 371.75it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1919/3257 [00:05<00:03, 375.51it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1969/3257 [00:05<00:03, 411.70it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2011/3257 [00:05<00:03, 404.64it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2052/3257 [00:05<00:03, 396.97it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2092/3257 [00:05<00:02, 395.75it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2132/3257 [00:05<00:04, 277.16it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2173/3257 [00:06<00:03, 306.70it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2211/3257 [00:06<00:03, 324.05it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2253/3257 [00:06<00:02, 347.40it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2293/3257 [00:06<00:02, 360.11it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2337/3257 [00:06<00:02, 379.64it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2383/3257 [00:06<00:02, 400.15it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2425/3257 [00:06<00:02, 402.28it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2467/3257 [00:06<00:01, 398.77it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2512/3257 [00:06<00:01, 411.38it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2554/3257 [00:06<00:01, 410.53it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2596/3257 [00:07<00:01, 398.49it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2641/3257 [00:07<00:01, 412.15it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2683/3257 [00:07<00:01, 387.62it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2723/3257 [00:07<00:01, 367.71it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:07<00:01, 388.04it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2810/3257 [00:07<00:01, 396.08it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2851/3257 [00:07<00:01, 394.02it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2895/3257 [00:07<00:00, 404.74it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2936/3257 [00:07<00:00, 404.52it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2977/3257 [00:08<00:00, 390.08it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3017/3257 [00:08<00:00, 391.44it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3063/3257 [00:08<00:00, 408.27it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3106/3257 [00:08<00:00, 412.02it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3148/3257 [00:08<00:00, 407.54it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3189/3257 [00:08<00:00, 403.24it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3230/3257 [00:08<00:00, 400.02it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 371.45it/s]
2023-02-07 13:47:30.855 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:47:30,856][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d32,n5,mc5,s0.538394,t4>', 'datetime': '2023-02-07T13:47:30.856223', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:47:30,856][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:47:30,856][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:47:30,988][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:47:30,989][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:47:30,990][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 599 unique words (64.83% of original 924, drops 325)', 'datetime': '2023-02-07T13:47:30.990677', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:47:30,990][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1454829 word corpus (99.94% of original 1455748, drops 919)', 'datetime': '2023-02-07T13:47:30.990862', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:47:30,994][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:47:30,994][gensim.models.word2vec][INFO] - sample=0.538394 downsamples 0 most-common words
[2023-02-07 13:47:30,994][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1454829 word corpus (100.0%% of prior 1454829)', 'datetime': '2023-02-07T13:47:30.994456', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:47:30,998][gensim.models.word2vec][INFO] - estimated required memory for 599 words and 32 dimensions: 1521140 bytes
[2023-02-07 13:47:30,998][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:47:30,999][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 599 vocabulary and 32 features, using sg=1 hs=0 sample=0.5383943462115237 negative=5 window=5 shrink_windows=True', 'datetime': '2023-02-07T13:47:30.999250', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:47:31,411][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458086 effective words) took 0.4s, 3551892 effective words/s
[2023-02-07 13:47:31,785][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458086 effective words) took 0.4s, 3917389 effective words/s
[2023-02-07 13:47:32,161][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458086 effective words) took 0.4s, 3887868 effective words/s
[2023-02-07 13:47:32,533][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458086 effective words) took 0.4s, 3938710 effective words/s
[2023-02-07 13:47:32,909][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458086 effective words) took 0.4s, 3895645 effective words/s
[2023-02-07 13:47:33,281][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458086 effective words) took 0.4s, 3928913 effective words/s
[2023-02-07 13:47:33,665][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458086 effective words) took 0.4s, 3810235 effective words/s
[2023-02-07 13:47:34,040][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458086 effective words) took 0.4s, 3911918 effective words/s
[2023-02-07 13:47:34,437][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458086 effective words) took 0.4s, 3684941 effective words/s
[2023-02-07 13:47:34,840][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458086 effective words) took 0.4s, 3620555 effective words/s
[2023-02-07 13:47:35,264][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458086 effective words) took 0.4s, 3455404 effective words/s
[2023-02-07 13:47:35,680][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458086 effective words) took 0.4s, 3514355 effective words/s
[2023-02-07 13:47:36,099][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458086 effective words) took 0.4s, 3490871 effective words/s
[2023-02-07 13:47:36,507][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458086 effective words) took 0.4s, 3580795 effective words/s
[2023-02-07 13:47:36,922][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458086 effective words) took 0.4s, 3528825 effective words/s
[2023-02-07 13:47:37,339][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458086 effective words) took 0.4s, 3508637 effective words/s
[2023-02-07 13:47:37,763][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458086 effective words) took 0.4s, 3446279 effective words/s
[2023-02-07 13:47:38,179][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458086 effective words) took 0.4s, 3521567 effective words/s
[2023-02-07 13:47:38,598][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458086 effective words) took 0.4s, 3490815 effective words/s
[2023-02-07 13:47:39,023][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458086 effective words) took 0.4s, 3448834 effective words/s
[2023-02-07 13:47:39,430][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458086 effective words) took 0.4s, 3593924 effective words/s
[2023-02-07 13:47:39,842][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458086 effective words) took 0.4s, 3550080 effective words/s
[2023-02-07 13:47:40,257][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458086 effective words) took 0.4s, 3520880 effective words/s
[2023-02-07 13:47:40,670][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458086 effective words) took 0.4s, 3544126 effective words/s
[2023-02-07 13:47:41,091][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458086 effective words) took 0.4s, 3468444 effective words/s
[2023-02-07 13:47:41,510][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458086 effective words) took 0.4s, 3494480 effective words/s
[2023-02-07 13:47:41,923][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458086 effective words) took 0.4s, 3541909 effective words/s
[2023-02-07 13:47:42,342][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458086 effective words) took 0.4s, 3487904 effective words/s
[2023-02-07 13:47:42,756][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458086 effective words) took 0.4s, 3535184 effective words/s
[2023-02-07 13:47:43,171][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458086 effective words) took 0.4s, 3526368 effective words/s
[2023-02-07 13:47:43,584][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458086 effective words) took 0.4s, 3542540 effective words/s
[2023-02-07 13:47:44,000][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458086 effective words) took 0.4s, 3511760 effective words/s
[2023-02-07 13:47:44,408][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458086 effective words) took 0.4s, 3595315 effective words/s
[2023-02-07 13:47:44,819][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458086 effective words) took 0.4s, 3558245 effective words/s
[2023-02-07 13:47:45,239][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458086 effective words) took 0.4s, 3485636 effective words/s
[2023-02-07 13:47:45,663][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458086 effective words) took 0.4s, 3450020 effective words/s
[2023-02-07 13:47:46,071][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458086 effective words) took 0.4s, 3586516 effective words/s
[2023-02-07 13:47:46,485][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458086 effective words) took 0.4s, 3532622 effective words/s
[2023-02-07 13:47:46,892][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458086 effective words) took 0.4s, 3598403 effective words/s
[2023-02-07 13:47:47,300][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458086 effective words) took 0.4s, 3585190 effective words/s
[2023-02-07 13:47:47,707][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458086 effective words) took 0.4s, 3596619 effective words/s
[2023-02-07 13:47:48,121][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458086 effective words) took 0.4s, 3533022 effective words/s
[2023-02-07 13:47:48,526][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458086 effective words) took 0.4s, 3611028 effective words/s
[2023-02-07 13:47:48,937][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458086 effective words) took 0.4s, 3566209 effective words/s
[2023-02-07 13:47:49,349][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458086 effective words) took 0.4s, 3553030 effective words/s
[2023-02-07 13:47:49,760][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458086 effective words) took 0.4s, 3559673 effective words/s
[2023-02-07 13:47:50,164][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458086 effective words) took 0.4s, 3619251 effective words/s
[2023-02-07 13:47:50,575][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458086 effective words) took 0.4s, 3556053 effective words/s
[2023-02-07 13:47:50,576][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 69875904 raw words (69988128 effective words) took 19.6s, 3575045 effective words/s', 'datetime': '2023-02-07T13:47:50.576327', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:47:50.576 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:47:51,595][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134713-5z15i9ex/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:47:51.595549', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:47:51,596][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:47:51,599][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134713-5z15i9ex/files/../tmp/embedding_model.pt
2023-02-07 13:47:51.599 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:47:52.437 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:47:52.772 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:47:53.045 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9696041067512398, 'test_mae': 1.0642541939624088, 'test_r2': 0.06825949508961049}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.031 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.65
wandb: percentage 0.35173
wandb:   test_mae 1.06425
wandb:   test_mse 1.9696
wandb:    test_r2 0.06826
wandb: 
wandb: üöÄ View run misty-sweep-56 at: https://wandb.ai/xiaoqiz/mof2vec/runs/5z15i9ex
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_134713-5z15i9ex/logs
wandb: Agent Starting Run: l2fltb2d with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 283
wandb: 	model.gensim.alpha: 0.002897426578858826
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 61
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.5506545239172592
wandb: 	model.gensim.vector_size: 11
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.014723856799813608
wandb: 	model.sklearn.max_depth: 88
wandb: 	model.sklearn.min_child_weight: 0.07118810925290935
wandb: 	model.sklearn.n_estimators: 425
wandb: 	model.sklearn.num_leaves: 499
wandb: 	model.sklearn.reg_alpha: 0.6529778808354287
wandb: 	model.sklearn.reg_lambda: 0.2798694705681499
wandb: 	model.sklearn.subsample: 0.2182418227462483
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134804-l2fltb2d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-57
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/l2fltb2d
2023-02-07 13:48:12.686 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 13:48:12.687 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 283 for sweep.
2023-02-07 13:48:12.687 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.002897426578858826 for sweep.
2023-02-07 13:48:12.687 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:48:12.687 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 61 for sweep.
2023-02-07 13:48:12.688 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 13:48:12.688 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5506545239172592 for sweep.
2023-02-07 13:48:12.688 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 11 for sweep.
2023-02-07 13:48:12.688 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 13:48:12.688 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.014723856799813608 for sweep.
2023-02-07 13:48:12.689 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 88 for sweep.
2023-02-07 13:48:12.689 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.07118810925290935 for sweep.
2023-02-07 13:48:12.689 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 425 for sweep.
2023-02-07 13:48:12.689 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 499 for sweep.
2023-02-07 13:48:12.689 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.6529778808354287 for sweep.
2023-02-07 13:48:12.690 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.2798694705681499 for sweep.
2023-02-07 13:48:12.690 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2182418227462483 for sweep.
2023-02-07 13:48:12.690 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:48:12.696 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134804-l2fltb2d/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 283, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 11, 'window': 3, 'min_count': 3, 'dm': 0, 'sample': 0.5506545239172592, 'workers': 4, 'alpha': 0.002897426578858826, 'epochs': 61}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 425, 'max_depth': 88, 'num_leaves': 499, 'reg_alpha': 0.6529778808354287, 'reg_lambda': 0.2798694705681499, 'subsample': 0.2182418227462483, 'min_child_weight': 0.07118810925290935, 'n_jobs': 4, 'learning_rate': 0.014723856799813608}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 20/3257 [00:00<00:16, 197.74it/s]  1%|‚ñè         | 41/3257 [00:00<00:15, 204.40it/s]  2%|‚ñè         | 66/3257 [00:00<00:14, 222.31it/s]  3%|‚ñé         | 90/3257 [00:00<00:14, 219.54it/s]  3%|‚ñé         | 112/3257 [00:00<00:14, 210.77it/s]  4%|‚ñç         | 136/3257 [00:00<00:14, 218.47it/s]  5%|‚ñç         | 160/3257 [00:00<00:13, 222.64it/s]  6%|‚ñå         | 183/3257 [00:00<00:13, 220.20it/s]  6%|‚ñã         | 206/3257 [00:00<00:14, 217.75it/s]  7%|‚ñã         | 232/3257 [00:01<00:13, 228.69it/s]  8%|‚ñä         | 255/3257 [00:01<00:17, 169.36it/s]  9%|‚ñä         | 282/3257 [00:01<00:15, 191.76it/s]  9%|‚ñâ         | 306/3257 [00:01<00:14, 203.14it/s] 10%|‚ñà         | 332/3257 [00:01<00:13, 216.10it/s] 11%|‚ñà         | 357/3257 [00:01<00:12, 225.23it/s] 12%|‚ñà‚ñè        | 381/3257 [00:01<00:13, 214.77it/s] 12%|‚ñà‚ñè        | 406/3257 [00:01<00:12, 222.59it/s] 13%|‚ñà‚ñé        | 429/3257 [00:02<00:13, 208.59it/s] 14%|‚ñà‚ñç        | 453/3257 [00:02<00:12, 215.91it/s] 15%|‚ñà‚ñç        | 479/3257 [00:02<00:12, 225.01it/s] 16%|‚ñà‚ñå        | 509/3257 [00:02<00:11, 245.51it/s] 16%|‚ñà‚ñã        | 534/3257 [00:02<00:11, 241.48it/s] 17%|‚ñà‚ñã        | 559/3257 [00:02<00:11, 232.28it/s] 18%|‚ñà‚ñä        | 583/3257 [00:02<00:11, 230.35it/s] 19%|‚ñà‚ñâ        | 611/3257 [00:02<00:10, 243.56it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:02<00:10, 243.22it/s] 20%|‚ñà‚ñà        | 662/3257 [00:03<00:11, 227.53it/s] 21%|‚ñà‚ñà        | 687/3257 [00:03<00:11, 233.56it/s] 22%|‚ñà‚ñà‚ñè       | 715/3257 [00:03<00:10, 245.40it/s] 23%|‚ñà‚ñà‚ñé       | 740/3257 [00:03<00:11, 223.87it/s] 24%|‚ñà‚ñà‚ñé       | 768/3257 [00:03<00:10, 238.97it/s] 24%|‚ñà‚ñà‚ñç       | 793/3257 [00:03<00:10, 228.29it/s] 25%|‚ñà‚ñà‚ñå       | 817/3257 [00:03<00:10, 230.80it/s] 26%|‚ñà‚ñà‚ñå       | 841/3257 [00:03<00:11, 213.61it/s] 27%|‚ñà‚ñà‚ñã       | 865/3257 [00:03<00:10, 219.30it/s] 27%|‚ñà‚ñà‚ñã       | 888/3257 [00:04<00:10, 218.98it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:04<00:10, 224.28it/s] 29%|‚ñà‚ñà‚ñâ       | 939/3257 [00:04<00:10, 230.97it/s] 30%|‚ñà‚ñà‚ñâ       | 966/3257 [00:04<00:09, 240.83it/s] 30%|‚ñà‚ñà‚ñà       | 991/3257 [00:04<00:09, 228.07it/s] 31%|‚ñà‚ñà‚ñà       | 1015/3257 [00:04<00:09, 226.43it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1038/3257 [00:04<00:10, 217.08it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1061/3257 [00:04<00:10, 217.37it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1083/3257 [00:04<00:09, 217.65it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1105/3257 [00:04<00:09, 216.49it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1127/3257 [00:05<00:10, 208.39it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1148/3257 [00:05<00:10, 202.46it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1172/3257 [00:05<00:09, 212.06it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1194/3257 [00:05<00:10, 192.99it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1214/3257 [00:05<00:10, 186.77it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1240/3257 [00:05<00:09, 205.44it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1261/3257 [00:05<00:09, 199.76it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1282/3257 [00:05<00:10, 187.68it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1302/3257 [00:06<00:10, 187.51it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1325/3257 [00:06<00:09, 199.03it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1346/3257 [00:06<00:09, 201.23it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1367/3257 [00:06<00:09, 193.01it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1387/3257 [00:06<00:09, 189.73it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1412/3257 [00:06<00:08, 205.30it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1434/3257 [00:06<00:08, 208.06it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1459/3257 [00:06<00:08, 219.39it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1482/3257 [00:06<00:08, 214.30it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1507/3257 [00:06<00:07, 221.77it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1530/3257 [00:07<00:08, 195.72it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1551/3257 [00:07<00:12, 132.49it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1570/3257 [00:07<00:11, 142.65it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1589/3257 [00:07<00:10, 152.26it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1612/3257 [00:07<00:09, 170.69it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:07<00:09, 174.03it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1655/3257 [00:07<00:08, 182.23it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1675/3257 [00:08<00:08, 182.61it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1700/3257 [00:08<00:07, 197.51it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1725/3257 [00:08<00:07, 211.85it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1747/3257 [00:08<00:07, 201.74it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1773/3257 [00:08<00:06, 217.10it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1797/3257 [00:08<00:06, 222.79it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1821/3257 [00:08<00:06, 225.11it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1845/3257 [00:08<00:06, 224.94it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1871/3257 [00:08<00:05, 234.22it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1895/3257 [00:08<00:05, 232.24it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1919/3257 [00:09<00:05, 231.80it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1950/3257 [00:09<00:05, 254.41it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:09<00:05, 252.53it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2003/3257 [00:09<00:04, 251.98it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2029/3257 [00:09<00:04, 252.77it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2055/3257 [00:09<00:05, 228.02it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2079/3257 [00:09<00:05, 227.07it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2103/3257 [00:09<00:05, 221.81it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2126/3257 [00:09<00:05, 217.45it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2148/3257 [00:10<00:05, 212.02it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2176/3257 [00:10<00:04, 230.28it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2200/3257 [00:10<00:04, 229.97it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2224/3257 [00:10<00:04, 223.38it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2247/3257 [00:10<00:04, 219.70it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2270/3257 [00:10<00:04, 216.83it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2295/3257 [00:10<00:04, 226.08it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2320/3257 [00:10<00:04, 232.05it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2350/3257 [00:10<00:03, 250.08it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2376/3257 [00:11<00:03, 245.48it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:11<00:03, 244.73it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2430/3257 [00:11<00:03, 238.49it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2454/3257 [00:11<00:03, 227.65it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2482/3257 [00:11<00:03, 241.46it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2512/3257 [00:11<00:02, 256.90it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2540/3257 [00:11<00:02, 254.76it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2566/3257 [00:11<00:02, 241.65it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2591/3257 [00:11<00:02, 230.43it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2623/3257 [00:12<00:02, 253.97it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2649/3257 [00:12<00:02, 247.01it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2674/3257 [00:12<00:02, 243.05it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2699/3257 [00:12<00:02, 230.19it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2723/3257 [00:12<00:02, 225.80it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2752/3257 [00:12<00:02, 242.26it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2777/3257 [00:12<00:02, 233.12it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:12<00:01, 247.72it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2832/3257 [00:12<00:01, 233.00it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2860/3257 [00:13<00:01, 244.31it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2888/3257 [00:13<00:01, 253.28it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2914/3257 [00:13<00:02, 150.33it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2936/3257 [00:13<00:01, 163.61it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2957/3257 [00:13<00:01, 169.85it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2981/3257 [00:13<00:01, 183.98it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3011/3257 [00:13<00:01, 211.09it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3036/3257 [00:14<00:01, 220.82it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3065/3257 [00:14<00:00, 239.10it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3091/3257 [00:14<00:00, 239.52it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3122/3257 [00:14<00:00, 256.95it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3149/3257 [00:14<00:00, 244.76it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3175/3257 [00:14<00:00, 244.14it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3200/3257 [00:14<00:00, 242.72it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3225/3257 [00:14<00:00, 235.93it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:14<00:00, 244.11it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:14<00:00, 218.42it/s]
2023-02-07 13:48:28.226 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:48:28,227][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d11,n5,mc3,s0.550655,t4>', 'datetime': '2023-02-07T13:48:28.227566', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:48:28,228][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:48:28,229][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:48:28,654][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 13:48:28,654][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:48:28,706][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 22723 unique words (71.45% of original 31803, drops 9080)', 'datetime': '2023-02-07T13:48:28.706600', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:48:28,706][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 5081575 word corpus (99.73% of original 5095118, drops 13543)', 'datetime': '2023-02-07T13:48:28.706938', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:48:28,774][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 13:48:28,776][gensim.models.word2vec][INFO] - sample=0.550655 downsamples 0 most-common words
[2023-02-07 13:48:28,776][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5081575 word corpus (100.0%% of prior 5081575)', 'datetime': '2023-02-07T13:48:28.776221', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:48:28,896][gensim.models.word2vec][INFO] - estimated required memory for 22723 words and 11 dimensions: 14155832 bytes
[2023-02-07 13:48:28,896][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:48:28,897][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 22723 vocabulary and 11 features, using sg=1 hs=0 sample=0.5506545239172592 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T13:48:28.897615', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:48:29,900][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 85.66% examples, 4374035 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:30,056][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5073452 effective words) took 1.2s, 4384415 effective words/s
[2023-02-07 13:48:31,060][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 86.95% examples, 4440893 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:31,193][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5073452 effective words) took 1.1s, 4464666 effective words/s
[2023-02-07 13:48:32,195][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 88.42% examples, 4503148 words/s, in_qsize 8, out_qsize 0
[2023-02-07 13:48:32,317][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5073452 effective words) took 1.1s, 4518364 effective words/s
[2023-02-07 13:48:33,319][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 88.79% examples, 4528057 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:33,437][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5073452 effective words) took 1.1s, 4537149 effective words/s
[2023-02-07 13:48:34,439][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 88.42% examples, 4503451 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:34,566][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5073452 effective words) took 1.1s, 4497468 effective words/s
[2023-02-07 13:48:35,568][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 86.03% examples, 4393514 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:35,728][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5073452 effective words) took 1.2s, 4370815 effective words/s
[2023-02-07 13:48:36,731][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 82.84% examples, 4240398 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:36,917][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5073452 effective words) took 1.2s, 4272255 effective words/s
[2023-02-07 13:48:37,919][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 85.14% examples, 4359302 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:38,078][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5073452 effective words) took 1.2s, 4376554 effective words/s
[2023-02-07 13:48:39,081][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 84.99% examples, 4344654 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:39,239][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5073452 effective words) took 1.2s, 4375174 effective words/s
[2023-02-07 13:48:40,242][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 85.66% examples, 4371192 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:40,403][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5073452 effective words) took 1.2s, 4362326 effective words/s
[2023-02-07 13:48:41,409][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 84.28% examples, 4298853 words/s, in_qsize 7, out_qsize 2
[2023-02-07 13:48:41,581][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5073452 effective words) took 1.2s, 4317064 effective words/s
[2023-02-07 13:48:42,583][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 82.47% examples, 4212488 words/s, in_qsize 6, out_qsize 0
[2023-02-07 13:48:42,778][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5073452 effective words) took 1.2s, 4240309 effective words/s
[2023-02-07 13:48:43,780][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 85.66% examples, 4379422 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:43,933][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5073452 effective words) took 1.2s, 4396964 effective words/s
[2023-02-07 13:48:44,936][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 83.39% examples, 4276670 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:45,119][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5073452 effective words) took 1.2s, 4286454 effective words/s
[2023-02-07 13:48:46,121][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 84.99% examples, 4346346 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:46,288][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5073452 effective words) took 1.2s, 4341289 effective words/s
[2023-02-07 13:48:47,291][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 84.83% examples, 4338160 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:47,462][gensim.models.word2vec][INFO] - EPOCH 15: training on 5095118 raw words (5073452 effective words) took 1.2s, 4327196 effective words/s
[2023-02-07 13:48:48,465][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 81.98% examples, 4183547 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:48,664][gensim.models.word2vec][INFO] - EPOCH 16: training on 5095118 raw words (5073452 effective words) took 1.2s, 4224673 effective words/s
[2023-02-07 13:48:49,666][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 84.99% examples, 4349782 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:49,829][gensim.models.word2vec][INFO] - EPOCH 17: training on 5095118 raw words (5073452 effective words) took 1.2s, 4359229 effective words/s
[2023-02-07 13:48:50,832][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 83.39% examples, 4277970 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:51,013][gensim.models.word2vec][INFO] - EPOCH 18: training on 5095118 raw words (5073452 effective words) took 1.2s, 4289534 effective words/s
[2023-02-07 13:48:52,018][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 84.03% examples, 4296949 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:52,195][gensim.models.word2vec][INFO] - EPOCH 19: training on 5095118 raw words (5073452 effective words) took 1.2s, 4300320 effective words/s
[2023-02-07 13:48:53,196][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 85.14% examples, 4361495 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:53,349][gensim.models.word2vec][INFO] - EPOCH 20: training on 5095118 raw words (5073452 effective words) took 1.2s, 4400628 effective words/s
[2023-02-07 13:48:54,351][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 90.36% examples, 4614440 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:54,446][gensim.models.word2vec][INFO] - EPOCH 21: training on 5095118 raw words (5073452 effective words) took 1.1s, 4630398 effective words/s
[2023-02-07 13:48:55,451][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 90.82% examples, 4622802 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:55,540][gensim.models.word2vec][INFO] - EPOCH 22: training on 5095118 raw words (5073452 effective words) took 1.1s, 4641002 effective words/s
[2023-02-07 13:48:56,542][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 89.22% examples, 4555422 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:56,654][gensim.models.word2vec][INFO] - EPOCH 23: training on 5095118 raw words (5073452 effective words) took 1.1s, 4559614 effective words/s
[2023-02-07 13:48:57,657][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 87.53% examples, 4471691 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:57,792][gensim.models.word2vec][INFO] - EPOCH 24: training on 5095118 raw words (5073452 effective words) took 1.1s, 4464133 effective words/s
[2023-02-07 13:48:58,793][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 86.21% examples, 4406628 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:48:58,941][gensim.models.word2vec][INFO] - EPOCH 25: training on 5095118 raw words (5073452 effective words) took 1.1s, 4418754 effective words/s
[2023-02-07 13:48:59,945][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 86.21% examples, 4393082 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:00,092][gensim.models.word2vec][INFO] - EPOCH 26: training on 5095118 raw words (5073452 effective words) took 1.2s, 4411484 effective words/s
[2023-02-07 13:49:01,095][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 86.95% examples, 4445516 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:01,227][gensim.models.word2vec][INFO] - EPOCH 27: training on 5095118 raw words (5073452 effective words) took 1.1s, 4471843 effective words/s
[2023-02-07 13:49:02,229][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 88.42% examples, 4506818 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:02,353][gensim.models.word2vec][INFO] - EPOCH 28: training on 5095118 raw words (5073452 effective words) took 1.1s, 4510995 effective words/s
[2023-02-07 13:49:03,354][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 89.87% examples, 4580856 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:03,464][gensim.models.word2vec][INFO] - EPOCH 29: training on 5095118 raw words (5073452 effective words) took 1.1s, 4570180 effective words/s
[2023-02-07 13:49:04,465][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 84.99% examples, 4353192 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:04,628][gensim.models.word2vec][INFO] - EPOCH 30: training on 5095118 raw words (5073452 effective words) took 1.2s, 4365302 effective words/s
[2023-02-07 13:49:05,630][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 84.99% examples, 4348556 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:05,788][gensim.models.word2vec][INFO] - EPOCH 31: training on 5095118 raw words (5073452 effective words) took 1.2s, 4375940 effective words/s
[2023-02-07 13:49:06,792][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 86.77% examples, 4431180 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:06,933][gensim.models.word2vec][INFO] - EPOCH 32: training on 5095118 raw words (5073452 effective words) took 1.1s, 4437151 effective words/s
[2023-02-07 13:49:07,935][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 86.40% examples, 4413773 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:08,080][gensim.models.word2vec][INFO] - EPOCH 33: training on 5095118 raw words (5073452 effective words) took 1.1s, 4429594 effective words/s
[2023-02-07 13:49:09,083][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 83.39% examples, 4271718 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:09,267][gensim.models.word2vec][INFO] - EPOCH 34: training on 5095118 raw words (5073452 effective words) took 1.2s, 4275684 effective words/s
[2023-02-07 13:49:10,270][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 80.84% examples, 4124696 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:10,498][gensim.models.word2vec][INFO] - EPOCH 35: training on 5095118 raw words (5073452 effective words) took 1.2s, 4124718 effective words/s
[2023-02-07 13:49:11,501][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 82.68% examples, 4216641 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:11,696][gensim.models.word2vec][INFO] - EPOCH 36: training on 5095118 raw words (5073452 effective words) took 1.2s, 4239035 effective words/s
[2023-02-07 13:49:12,703][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 84.03% examples, 4283384 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:12,878][gensim.models.word2vec][INFO] - EPOCH 37: training on 5095118 raw words (5073452 effective words) took 1.2s, 4300747 effective words/s
[2023-02-07 13:49:13,879][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 84.03% examples, 4302618 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:14,056][gensim.models.word2vec][INFO] - EPOCH 38: training on 5095118 raw words (5073452 effective words) took 1.2s, 4309271 effective words/s
[2023-02-07 13:49:15,057][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 84.03% examples, 4304217 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:15,241][gensim.models.word2vec][INFO] - EPOCH 39: training on 5095118 raw words (5073452 effective words) took 1.2s, 4285271 effective words/s
[2023-02-07 13:49:16,243][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 80.84% examples, 4130104 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:16,464][gensim.models.word2vec][INFO] - EPOCH 40: training on 5095118 raw words (5073452 effective words) took 1.2s, 4153612 effective words/s
[2023-02-07 13:49:17,465][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 82.78% examples, 4230317 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:17,656][gensim.models.word2vec][INFO] - EPOCH 41: training on 5095118 raw words (5073452 effective words) took 1.2s, 4259236 effective words/s
[2023-02-07 13:49:18,658][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 82.90% examples, 4251041 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:18,847][gensim.models.word2vec][INFO] - EPOCH 42: training on 5095118 raw words (5073452 effective words) took 1.2s, 4267005 effective words/s
[2023-02-07 13:49:19,848][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 81.24% examples, 4153733 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:20,065][gensim.models.word2vec][INFO] - EPOCH 43: training on 5095118 raw words (5073452 effective words) took 1.2s, 4165914 effective words/s
[2023-02-07 13:49:21,067][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 83.67% examples, 4290615 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:21,249][gensim.models.word2vec][INFO] - EPOCH 44: training on 5095118 raw words (5073452 effective words) took 1.2s, 4290056 effective words/s
[2023-02-07 13:49:22,254][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 84.03% examples, 4290608 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:22,430][gensim.models.word2vec][INFO] - EPOCH 45: training on 5095118 raw words (5073452 effective words) took 1.2s, 4300667 effective words/s
[2023-02-07 13:49:23,435][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 82.16% examples, 4189893 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:23,637][gensim.models.word2vec][INFO] - EPOCH 46: training on 5095118 raw words (5073452 effective words) took 1.2s, 4212297 effective words/s
[2023-02-07 13:49:24,638][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 81.39% examples, 4164651 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:24,850][gensim.models.word2vec][INFO] - EPOCH 47: training on 5095118 raw words (5073452 effective words) took 1.2s, 4187777 effective words/s
[2023-02-07 13:49:25,853][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 84.83% examples, 4337014 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:26,021][gensim.models.word2vec][INFO] - EPOCH 48: training on 5095118 raw words (5073452 effective words) took 1.2s, 4336116 effective words/s
[2023-02-07 13:49:27,026][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 84.03% examples, 4288739 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:27,205][gensim.models.word2vec][INFO] - EPOCH 49: training on 5095118 raw words (5073452 effective words) took 1.2s, 4290685 effective words/s
[2023-02-07 13:49:28,208][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 82.78% examples, 4221464 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:28,405][gensim.models.word2vec][INFO] - EPOCH 50: training on 5095118 raw words (5073452 effective words) took 1.2s, 4231935 effective words/s
[2023-02-07 13:49:29,408][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 83.39% examples, 4270892 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:29,587][gensim.models.word2vec][INFO] - EPOCH 51: training on 5095118 raw words (5073452 effective words) took 1.2s, 4296104 effective words/s
[2023-02-07 13:49:30,589][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 82.90% examples, 4248882 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:30,776][gensim.models.word2vec][INFO] - EPOCH 52: training on 5095118 raw words (5073452 effective words) took 1.2s, 4271720 effective words/s
[2023-02-07 13:49:31,778][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 84.28% examples, 4311314 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:31,948][gensim.models.word2vec][INFO] - EPOCH 53: training on 5095118 raw words (5073452 effective words) took 1.2s, 4332136 effective words/s
[2023-02-07 13:49:32,952][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 84.03% examples, 4293771 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:33,134][gensim.models.word2vec][INFO] - EPOCH 54: training on 5095118 raw words (5073452 effective words) took 1.2s, 4281124 effective words/s
[2023-02-07 13:49:34,137][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 81.58% examples, 4168988 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:34,354][gensim.models.word2vec][INFO] - EPOCH 55: training on 5095118 raw words (5073452 effective words) took 1.2s, 4163781 effective words/s
[2023-02-07 13:49:35,357][gensim.models.word2vec][INFO] - EPOCH 56 - PROGRESS: at 81.39% examples, 4155976 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:35,566][gensim.models.word2vec][INFO] - EPOCH 56: training on 5095118 raw words (5073452 effective words) took 1.2s, 4189356 effective words/s
[2023-02-07 13:49:36,568][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 82.47% examples, 4212609 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:36,767][gensim.models.word2vec][INFO] - EPOCH 57: training on 5095118 raw words (5073452 effective words) took 1.2s, 4229850 effective words/s
[2023-02-07 13:49:37,771][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 85.66% examples, 4364986 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:37,927][gensim.models.word2vec][INFO] - EPOCH 58: training on 5095118 raw words (5073452 effective words) took 1.2s, 4375388 effective words/s
[2023-02-07 13:49:38,928][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 83.39% examples, 4281576 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:39,117][gensim.models.word2vec][INFO] - EPOCH 59: training on 5095118 raw words (5073452 effective words) took 1.2s, 4269162 effective words/s
[2023-02-07 13:49:40,120][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 82.16% examples, 4195326 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:49:40,325][gensim.models.word2vec][INFO] - EPOCH 60: training on 5095118 raw words (5073452 effective words) took 1.2s, 4205027 effective words/s
[2023-02-07 13:49:40,325][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 310802198 raw words (309480572 effective words) took 71.4s, 4332787 effective words/s', 'datetime': '2023-02-07T13:49:40.325474', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:49:40.325 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:49:46,212][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134804-l2fltb2d/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:49:46.212649', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:49:46,214][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:49:46,235][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_134804-l2fltb2d/files/../tmp/embedding_model.pt
2023-02-07 13:49:46.235 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:49:47.089 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:49:47.406 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:49:47.564 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.1934965536996027, 'test_mae': 1.1241527399179623, 'test_r2': -0.03765501882221378}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.46
wandb: percentage 0.28551
wandb:   test_mae 1.12415
wandb:   test_mse 2.1935
wandb:    test_r2 -0.03766
wandb: 
wandb: üöÄ View run valiant-sweep-57 at: https://wandb.ai/xiaoqiz/mof2vec/runs/l2fltb2d
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_134804-l2fltb2d/logs
wandb: Agent Starting Run: 5ylmnwd7 with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 700
wandb: 	model.gensim.alpha: 0.004393489502695135
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 88
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.32822882546836996
wandb: 	model.gensim.vector_size: 122
wandb: 	model.gensim.window: 8
wandb: 	model.sklearn.learning_rate: 0.003070683655760773
wandb: 	model.sklearn.max_depth: 44
wandb: 	model.sklearn.min_child_weight: 0.06835354109713128
wandb: 	model.sklearn.n_estimators: 2705
wandb: 	model.sklearn.num_leaves: 469
wandb: 	model.sklearn.reg_alpha: 0.22374685690165105
wandb: 	model.sklearn.reg_lambda: 0.8223596797610538
wandb: 	model.sklearn.subsample: 0.3186892430086837
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135000-5ylmnwd7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-58
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/5ylmnwd7
2023-02-07 13:50:08.122 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 13:50:08.123 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 700 for sweep.
2023-02-07 13:50:08.123 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.004393489502695135 for sweep.
2023-02-07 13:50:08.124 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:50:08.124 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 88 for sweep.
2023-02-07 13:50:08.124 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 13:50:08.124 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.32822882546836996 for sweep.
2023-02-07 13:50:08.124 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 122 for sweep.
2023-02-07 13:50:08.125 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 8 for sweep.
2023-02-07 13:50:08.126 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.003070683655760773 for sweep.
2023-02-07 13:50:08.126 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 44 for sweep.
2023-02-07 13:50:08.126 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.06835354109713128 for sweep.
2023-02-07 13:50:08.126 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2705 for sweep.
2023-02-07 13:50:08.126 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 469 for sweep.
2023-02-07 13:50:08.127 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.22374685690165105 for sweep.
2023-02-07 13:50:08.127 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.8223596797610538 for sweep.
2023-02-07 13:50:08.127 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3186892430086837 for sweep.
2023-02-07 13:50:08.127 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:50:08.133 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135000-5ylmnwd7/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 700, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 122, 'window': 8, 'min_count': 6, 'dm': 0, 'sample': 0.32822882546836996, 'workers': 4, 'alpha': 0.004393489502695135, 'epochs': 88}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2705, 'max_depth': 44, 'num_leaves': 469, 'reg_alpha': 0.22374685690165105, 'reg_lambda': 0.8223596797610538, 'subsample': 0.3186892430086837, 'min_child_weight': 0.06835354109713128, 'n_jobs': 4, 'learning_rate': 0.003070683655760773}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 29/3257 [00:00<00:11, 287.10it/s]  2%|‚ñè         | 58/3257 [00:00<00:11, 284.19it/s]  3%|‚ñé         | 90/3257 [00:00<00:10, 298.46it/s]  4%|‚ñé         | 120/3257 [00:00<00:10, 294.97it/s]  5%|‚ñç         | 153/3257 [00:00<00:10, 307.23it/s]  6%|‚ñå         | 184/3257 [00:00<00:10, 300.93it/s]  7%|‚ñã         | 219/3257 [00:00<00:09, 316.05it/s]  8%|‚ñä         | 253/3257 [00:00<00:09, 323.17it/s]  9%|‚ñâ         | 290/3257 [00:00<00:08, 335.63it/s] 10%|‚ñâ         | 324/3257 [00:01<00:08, 336.66it/s] 11%|‚ñà         | 358/3257 [00:01<00:08, 333.22it/s] 12%|‚ñà‚ñè        | 392/3257 [00:01<00:09, 310.21it/s] 13%|‚ñà‚ñé        | 425/3257 [00:01<00:09, 303.81it/s] 14%|‚ñà‚ñç        | 456/3257 [00:01<00:09, 296.96it/s] 15%|‚ñà‚ñç        | 487/3257 [00:01<00:09, 298.69it/s] 16%|‚ñà‚ñå        | 522/3257 [00:01<00:08, 311.66it/s] 17%|‚ñà‚ñã        | 554/3257 [00:01<00:08, 311.81it/s] 18%|‚ñà‚ñä        | 586/3257 [00:01<00:09, 293.05it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:02<00:08, 299.88it/s] 20%|‚ñà‚ñà        | 652/3257 [00:02<00:08, 304.14it/s] 21%|‚ñà‚ñà        | 683/3257 [00:02<00:08, 293.83it/s] 22%|‚ñà‚ñà‚ñè       | 718/3257 [00:02<00:08, 306.00it/s] 23%|‚ñà‚ñà‚ñé       | 749/3257 [00:02<00:08, 292.89it/s] 24%|‚ñà‚ñà‚ñç       | 780/3257 [00:02<00:08, 293.30it/s] 25%|‚ñà‚ñà‚ñç       | 812/3257 [00:02<00:08, 297.32it/s] 26%|‚ñà‚ñà‚ñå       | 842/3257 [00:02<00:08, 282.32it/s] 27%|‚ñà‚ñà‚ñã       | 871/3257 [00:02<00:08, 283.44it/s] 28%|‚ñà‚ñà‚ñä       | 900/3257 [00:03<00:11, 203.57it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:03<00:10, 223.93it/s] 30%|‚ñà‚ñà‚ñâ       | 962/3257 [00:03<00:09, 244.03it/s] 30%|‚ñà‚ñà‚ñà       | 990/3257 [00:03<00:09, 250.14it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1019/3257 [00:03<00:08, 258.78it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:03<00:08, 254.11it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:03<00:08, 262.21it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1110/3257 [00:03<00:07, 271.63it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1138/3257 [00:03<00:07, 269.50it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1166/3257 [00:04<00:07, 272.16it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1194/3257 [00:04<00:08, 254.58it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1220/3257 [00:04<00:08, 252.01it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1251/3257 [00:04<00:07, 266.99it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1278/3257 [00:04<00:07, 264.69it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1305/3257 [00:04<00:07, 263.19it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1336/3257 [00:04<00:06, 275.81it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1364/3257 [00:04<00:06, 273.07it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1392/3257 [00:04<00:06, 267.96it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1426/3257 [00:05<00:06, 286.14it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1458/3257 [00:05<00:06, 295.86it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1489/3257 [00:05<00:05, 298.38it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:05<00:05, 296.22it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1549/3257 [00:05<00:06, 281.14it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1578/3257 [00:05<00:06, 274.68it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1609/3257 [00:05<00:05, 283.70it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1638/3257 [00:05<00:05, 277.79it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1666/3257 [00:05<00:05, 265.32it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1693/3257 [00:06<00:05, 263.59it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1720/3257 [00:06<00:05, 264.27it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1747/3257 [00:06<00:05, 256.84it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1776/3257 [00:06<00:05, 263.97it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:06<00:05, 266.38it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1832/3257 [00:06<00:05, 260.75it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1862/3257 [00:06<00:05, 271.28it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1891/3257 [00:06<00:05, 272.32it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1920/3257 [00:06<00:04, 275.21it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1957/3257 [00:06<00:04, 301.44it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1988/3257 [00:07<00:04, 295.69it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2019/3257 [00:07<00:04, 299.20it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2049/3257 [00:07<00:04, 285.73it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2078/3257 [00:07<00:04, 280.42it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2107/3257 [00:07<00:04, 279.33it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2136/3257 [00:07<00:04, 272.37it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2164/3257 [00:07<00:04, 273.00it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2194/3257 [00:07<00:03, 279.41it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2223/3257 [00:08<00:05, 186.53it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2250/3257 [00:08<00:04, 203.16it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2275/3257 [00:08<00:04, 213.02it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2307/3257 [00:08<00:03, 239.58it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2342/3257 [00:08<00:03, 268.09it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2375/3257 [00:08<00:03, 283.73it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2406/3257 [00:08<00:02, 286.33it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2436/3257 [00:08<00:02, 282.20it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2468/3257 [00:08<00:02, 291.35it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2501/3257 [00:09<00:02, 301.38it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2535/3257 [00:09<00:02, 309.67it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2567/3257 [00:09<00:02, 289.94it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2597/3257 [00:09<00:02, 282.17it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2634/3257 [00:09<00:02, 304.45it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2665/3257 [00:09<00:02, 293.90it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2695/3257 [00:09<00:01, 286.33it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2724/3257 [00:09<00:01, 274.83it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2757/3257 [00:09<00:01, 288.60it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2787/3257 [00:10<00:01, 290.70it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2817/3257 [00:10<00:01, 285.36it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2846/3257 [00:10<00:01, 283.55it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2885/3257 [00:10<00:01, 312.69it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2917/3257 [00:10<00:01, 292.74it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2947/3257 [00:10<00:01, 285.30it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:10<00:00, 282.84it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3007/3257 [00:10<00:00, 289.80it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3038/3257 [00:10<00:00, 293.64it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3072/3257 [00:10<00:00, 305.03it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3104/3257 [00:11<00:00, 308.74it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3135/3257 [00:11<00:00, 306.26it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3166/3257 [00:11<00:00, 298.77it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3197/3257 [00:11<00:00, 294.92it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3227/3257 [00:11<00:00, 293.03it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 281.10it/s]
2023-02-07 13:50:20.055 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:50:20,056][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d122,n5,mc6,s0.328229,t4>', 'datetime': '2023-02-07T13:50:20.056579', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:50:20,056][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:50:20,057][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:50:20,304][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 13:50:20,305][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:50:20,314][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 3569 unique words (53.57% of original 6662, drops 3093)', 'datetime': '2023-02-07T13:50:20.314954', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:50:20,316][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 2903654 word corpus (99.73% of original 2911496, drops 7842)', 'datetime': '2023-02-07T13:50:20.316754', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:50:20,328][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 13:50:20,329][gensim.models.word2vec][INFO] - sample=0.328229 downsamples 0 most-common words
[2023-02-07 13:50:20,329][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2903654 word corpus (100.0%% of prior 2903654)', 'datetime': '2023-02-07T13:50:20.329377', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:50:20,349][gensim.models.word2vec][INFO] - estimated required memory for 3569 words and 122 dimensions: 7508660 bytes
[2023-02-07 13:50:20,349][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:50:20,353][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 3569 vocabulary and 122 features, using sg=1 hs=0 sample=0.32822882546836996 negative=5 window=8 shrink_windows=True', 'datetime': '2023-02-07T13:50:20.353246', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:50:21,355][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2906911 effective words) took 1.0s, 2909641 effective words/s
[2023-02-07 13:50:22,226][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2906911 effective words) took 0.9s, 3344882 effective words/s
[2023-02-07 13:50:23,113][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2906911 effective words) took 0.9s, 3282624 effective words/s
[2023-02-07 13:50:23,991][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2906911 effective words) took 0.9s, 3317252 effective words/s
[2023-02-07 13:50:24,866][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2906911 effective words) took 0.9s, 3328738 effective words/s
[2023-02-07 13:50:25,716][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2906911 effective words) took 0.8s, 3426932 effective words/s
[2023-02-07 13:50:26,555][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2906911 effective words) took 0.8s, 3467634 effective words/s
[2023-02-07 13:50:27,385][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2906911 effective words) took 0.8s, 3507987 effective words/s
[2023-02-07 13:50:28,209][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2906911 effective words) took 0.8s, 3531465 effective words/s
[2023-02-07 13:50:29,034][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2906911 effective words) took 0.8s, 3527737 effective words/s
[2023-02-07 13:50:29,865][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2906911 effective words) took 0.8s, 3506041 effective words/s
[2023-02-07 13:50:30,684][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2906911 effective words) took 0.8s, 3554142 effective words/s
[2023-02-07 13:50:31,500][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2906911 effective words) took 0.8s, 3562827 effective words/s
[2023-02-07 13:50:32,319][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2906911 effective words) took 0.8s, 3555466 effective words/s
[2023-02-07 13:50:33,139][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2906911 effective words) took 0.8s, 3549431 effective words/s
[2023-02-07 13:50:33,953][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2906911 effective words) took 0.8s, 3578701 effective words/s
[2023-02-07 13:50:34,765][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2906911 effective words) took 0.8s, 3587659 effective words/s
[2023-02-07 13:50:35,572][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2906911 effective words) took 0.8s, 3603604 effective words/s
[2023-02-07 13:50:36,367][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2906911 effective words) took 0.8s, 3662144 effective words/s
[2023-02-07 13:50:37,168][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2906911 effective words) took 0.8s, 3635120 effective words/s
[2023-02-07 13:50:38,001][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2906911 effective words) took 0.8s, 3494322 effective words/s
[2023-02-07 13:50:38,803][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2906911 effective words) took 0.8s, 3629366 effective words/s
[2023-02-07 13:50:39,655][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2906911 effective words) took 0.9s, 3417837 effective words/s
[2023-02-07 13:50:40,470][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2906911 effective words) took 0.8s, 3570536 effective words/s
[2023-02-07 13:50:41,298][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2906911 effective words) took 0.8s, 3518381 effective words/s
[2023-02-07 13:50:42,094][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2906911 effective words) took 0.8s, 3657937 effective words/s
[2023-02-07 13:50:42,902][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2906911 effective words) took 0.8s, 3604325 effective words/s
[2023-02-07 13:50:43,710][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2906911 effective words) took 0.8s, 3601784 effective words/s
[2023-02-07 13:50:44,504][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2906911 effective words) took 0.8s, 3664983 effective words/s
[2023-02-07 13:50:45,298][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2906911 effective words) took 0.8s, 3665228 effective words/s
[2023-02-07 13:50:46,101][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2906911 effective words) took 0.8s, 3626208 effective words/s
[2023-02-07 13:50:46,902][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2906911 effective words) took 0.8s, 3635342 effective words/s
[2023-02-07 13:50:47,706][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2906911 effective words) took 0.8s, 3620459 effective words/s
[2023-02-07 13:50:48,508][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2906911 effective words) took 0.8s, 3629146 effective words/s
[2023-02-07 13:50:49,307][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2906911 effective words) took 0.8s, 3646941 effective words/s
[2023-02-07 13:50:50,107][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2906911 effective words) took 0.8s, 3639759 effective words/s
[2023-02-07 13:50:50,912][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2906911 effective words) took 0.8s, 3615923 effective words/s
[2023-02-07 13:50:51,711][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2906911 effective words) took 0.8s, 3640786 effective words/s
[2023-02-07 13:50:52,505][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2906911 effective words) took 0.8s, 3667579 effective words/s
[2023-02-07 13:50:53,297][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2906911 effective words) took 0.8s, 3677423 effective words/s
[2023-02-07 13:50:54,092][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2906911 effective words) took 0.8s, 3661185 effective words/s
[2023-02-07 13:50:54,890][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2906911 effective words) took 0.8s, 3648490 effective words/s
[2023-02-07 13:50:55,714][gensim.models.word2vec][INFO] - EPOCH 42: training on 2911496 raw words (2906911 effective words) took 0.8s, 3532837 effective words/s
[2023-02-07 13:50:56,539][gensim.models.word2vec][INFO] - EPOCH 43: training on 2911496 raw words (2906911 effective words) took 0.8s, 3526748 effective words/s
[2023-02-07 13:50:57,355][gensim.models.word2vec][INFO] - EPOCH 44: training on 2911496 raw words (2906911 effective words) took 0.8s, 3568585 effective words/s
[2023-02-07 13:50:58,214][gensim.models.word2vec][INFO] - EPOCH 45: training on 2911496 raw words (2906911 effective words) took 0.9s, 3388863 effective words/s
[2023-02-07 13:50:59,064][gensim.models.word2vec][INFO] - EPOCH 46: training on 2911496 raw words (2906911 effective words) took 0.8s, 3423718 effective words/s
[2023-02-07 13:50:59,923][gensim.models.word2vec][INFO] - EPOCH 47: training on 2911496 raw words (2906911 effective words) took 0.9s, 3387825 effective words/s
[2023-02-07 13:51:00,774][gensim.models.word2vec][INFO] - EPOCH 48: training on 2911496 raw words (2906911 effective words) took 0.8s, 3421046 effective words/s
[2023-02-07 13:51:01,640][gensim.models.word2vec][INFO] - EPOCH 49: training on 2911496 raw words (2906911 effective words) took 0.9s, 3365438 effective words/s
[2023-02-07 13:51:02,486][gensim.models.word2vec][INFO] - EPOCH 50: training on 2911496 raw words (2906911 effective words) took 0.8s, 3441612 effective words/s
[2023-02-07 13:51:03,340][gensim.models.word2vec][INFO] - EPOCH 51: training on 2911496 raw words (2906911 effective words) took 0.9s, 3407055 effective words/s
[2023-02-07 13:51:04,181][gensim.models.word2vec][INFO] - EPOCH 52: training on 2911496 raw words (2906911 effective words) took 0.8s, 3462631 effective words/s
[2023-02-07 13:51:05,015][gensim.models.word2vec][INFO] - EPOCH 53: training on 2911496 raw words (2906911 effective words) took 0.8s, 3492816 effective words/s
[2023-02-07 13:51:05,854][gensim.models.word2vec][INFO] - EPOCH 54: training on 2911496 raw words (2906911 effective words) took 0.8s, 3469007 effective words/s
[2023-02-07 13:51:06,706][gensim.models.word2vec][INFO] - EPOCH 55: training on 2911496 raw words (2906911 effective words) took 0.9s, 3415807 effective words/s
[2023-02-07 13:51:07,556][gensim.models.word2vec][INFO] - EPOCH 56: training on 2911496 raw words (2906911 effective words) took 0.8s, 3428403 effective words/s
[2023-02-07 13:51:08,390][gensim.models.word2vec][INFO] - EPOCH 57: training on 2911496 raw words (2906911 effective words) took 0.8s, 3489524 effective words/s
[2023-02-07 13:51:09,219][gensim.models.word2vec][INFO] - EPOCH 58: training on 2911496 raw words (2906911 effective words) took 0.8s, 3512772 effective words/s
[2023-02-07 13:51:10,056][gensim.models.word2vec][INFO] - EPOCH 59: training on 2911496 raw words (2906911 effective words) took 0.8s, 3478740 effective words/s
[2023-02-07 13:51:10,898][gensim.models.word2vec][INFO] - EPOCH 60: training on 2911496 raw words (2906911 effective words) took 0.8s, 3456096 effective words/s
[2023-02-07 13:51:11,748][gensim.models.word2vec][INFO] - EPOCH 61: training on 2911496 raw words (2906911 effective words) took 0.8s, 3424341 effective words/s
[2023-02-07 13:51:12,597][gensim.models.word2vec][INFO] - EPOCH 62: training on 2911496 raw words (2906911 effective words) took 0.8s, 3428667 effective words/s
[2023-02-07 13:51:13,435][gensim.models.word2vec][INFO] - EPOCH 63: training on 2911496 raw words (2906911 effective words) took 0.8s, 3475282 effective words/s
[2023-02-07 13:51:14,260][gensim.models.word2vec][INFO] - EPOCH 64: training on 2911496 raw words (2906911 effective words) took 0.8s, 3527855 effective words/s
[2023-02-07 13:51:15,097][gensim.models.word2vec][INFO] - EPOCH 65: training on 2911496 raw words (2906911 effective words) took 0.8s, 3480852 effective words/s
[2023-02-07 13:51:15,939][gensim.models.word2vec][INFO] - EPOCH 66: training on 2911496 raw words (2906911 effective words) took 0.8s, 3460670 effective words/s
[2023-02-07 13:51:16,772][gensim.models.word2vec][INFO] - EPOCH 67: training on 2911496 raw words (2906911 effective words) took 0.8s, 3496307 effective words/s
[2023-02-07 13:51:17,605][gensim.models.word2vec][INFO] - EPOCH 68: training on 2911496 raw words (2906911 effective words) took 0.8s, 3491759 effective words/s
[2023-02-07 13:51:18,435][gensim.models.word2vec][INFO] - EPOCH 69: training on 2911496 raw words (2906911 effective words) took 0.8s, 3511066 effective words/s
[2023-02-07 13:51:19,259][gensim.models.word2vec][INFO] - EPOCH 70: training on 2911496 raw words (2906911 effective words) took 0.8s, 3529190 effective words/s
[2023-02-07 13:51:20,098][gensim.models.word2vec][INFO] - EPOCH 71: training on 2911496 raw words (2906911 effective words) took 0.8s, 3472385 effective words/s
[2023-02-07 13:51:20,928][gensim.models.word2vec][INFO] - EPOCH 72: training on 2911496 raw words (2906911 effective words) took 0.8s, 3506527 effective words/s
[2023-02-07 13:51:21,759][gensim.models.word2vec][INFO] - EPOCH 73: training on 2911496 raw words (2906911 effective words) took 0.8s, 3501514 effective words/s
[2023-02-07 13:51:22,591][gensim.models.word2vec][INFO] - EPOCH 74: training on 2911496 raw words (2906911 effective words) took 0.8s, 3503339 effective words/s
[2023-02-07 13:51:23,428][gensim.models.word2vec][INFO] - EPOCH 75: training on 2911496 raw words (2906911 effective words) took 0.8s, 3479216 effective words/s
[2023-02-07 13:51:24,260][gensim.models.word2vec][INFO] - EPOCH 76: training on 2911496 raw words (2906911 effective words) took 0.8s, 3501433 effective words/s
[2023-02-07 13:51:25,092][gensim.models.word2vec][INFO] - EPOCH 77: training on 2911496 raw words (2906911 effective words) took 0.8s, 3499888 effective words/s
[2023-02-07 13:51:25,919][gensim.models.word2vec][INFO] - EPOCH 78: training on 2911496 raw words (2906911 effective words) took 0.8s, 3521073 effective words/s
[2023-02-07 13:51:26,738][gensim.models.word2vec][INFO] - EPOCH 79: training on 2911496 raw words (2906911 effective words) took 0.8s, 3553701 effective words/s
[2023-02-07 13:51:27,567][gensim.models.word2vec][INFO] - EPOCH 80: training on 2911496 raw words (2906911 effective words) took 0.8s, 3510322 effective words/s
[2023-02-07 13:51:28,390][gensim.models.word2vec][INFO] - EPOCH 81: training on 2911496 raw words (2906911 effective words) took 0.8s, 3536706 effective words/s
[2023-02-07 13:51:29,218][gensim.models.word2vec][INFO] - EPOCH 82: training on 2911496 raw words (2906911 effective words) took 0.8s, 3516805 effective words/s
[2023-02-07 13:51:30,045][gensim.models.word2vec][INFO] - EPOCH 83: training on 2911496 raw words (2906911 effective words) took 0.8s, 3521965 effective words/s
[2023-02-07 13:51:30,874][gensim.models.word2vec][INFO] - EPOCH 84: training on 2911496 raw words (2906911 effective words) took 0.8s, 3508576 effective words/s
[2023-02-07 13:51:31,697][gensim.models.word2vec][INFO] - EPOCH 85: training on 2911496 raw words (2906911 effective words) took 0.8s, 3538234 effective words/s
[2023-02-07 13:51:32,519][gensim.models.word2vec][INFO] - EPOCH 86: training on 2911496 raw words (2906911 effective words) took 0.8s, 3539853 effective words/s
[2023-02-07 13:51:33,341][gensim.models.word2vec][INFO] - EPOCH 87: training on 2911496 raw words (2906911 effective words) took 0.8s, 3544624 effective words/s
[2023-02-07 13:51:33,341][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 256211648 raw words (255808168 effective words) took 73.0s, 3504835 effective words/s', 'datetime': '2023-02-07T13:51:33.341750', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:51:33.341 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:51:39,088][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135000-5ylmnwd7/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:51:39.088851', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:51:39,090][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:51:39,109][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135000-5ylmnwd7/files/../tmp/embedding_model.pt
2023-02-07 13:51:39.109 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:51:40.290 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:51:40.741 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:51:41.604 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8851597809971758, 'test_mae': 1.0510771821018743, 'test_r2': 0.10820671008840932}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.92
wandb: percentage 0.46427
wandb:   test_mae 1.05108
wandb:   test_mse 1.88516
wandb:    test_r2 0.10821
wandb: 
wandb: üöÄ View run polished-sweep-58 at: https://wandb.ai/xiaoqiz/mof2vec/runs/5ylmnwd7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_135000-5ylmnwd7/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: rpxy1uki with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 155
wandb: 	model.gensim.alpha: 0.0005611406623540757
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 97
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.5116388239250553
wandb: 	model.gensim.vector_size: 129
wandb: 	model.gensim.window: 12
wandb: 	model.sklearn.learning_rate: 0.4092121432306745
wandb: 	model.sklearn.max_depth: 32
wandb: 	model.sklearn.min_child_weight: 0.06129374210408517
wandb: 	model.sklearn.n_estimators: 2667
wandb: 	model.sklearn.num_leaves: 478
wandb: 	model.sklearn.reg_alpha: 0.006074117603692388
wandb: 	model.sklearn.reg_lambda: 0.3699742427105907
wandb: 	model.sklearn.subsample: 0.40461321743773687
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135200-rpxy1uki
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-sweep-59
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/rpxy1uki
2023-02-07 13:52:07.914 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:52:07.914 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 155 for sweep.
2023-02-07 13:52:07.915 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0005611406623540757 for sweep.
2023-02-07 13:52:07.915 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:52:07.915 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 97 for sweep.
2023-02-07 13:52:07.915 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 13:52:07.916 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5116388239250553 for sweep.
2023-02-07 13:52:07.917 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 129 for sweep.
2023-02-07 13:52:07.917 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 12 for sweep.
2023-02-07 13:52:07.917 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.4092121432306745 for sweep.
2023-02-07 13:52:07.918 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 32 for sweep.
2023-02-07 13:52:07.918 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.06129374210408517 for sweep.
2023-02-07 13:52:07.919 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2667 for sweep.
2023-02-07 13:52:07.919 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 478 for sweep.
2023-02-07 13:52:07.919 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.006074117603692388 for sweep.
2023-02-07 13:52:07.919 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.3699742427105907 for sweep.
2023-02-07 13:52:07.919 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.40461321743773687 for sweep.
2023-02-07 13:52:07.920 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:52:07.923 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135200-rpxy1uki/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 155, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 129, 'window': 12, 'min_count': 3, 'dm': 0, 'sample': 0.5116388239250553, 'workers': 4, 'alpha': 0.0005611406623540757, 'epochs': 97}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2667, 'max_depth': 32, 'num_leaves': 478, 'reg_alpha': 0.006074117603692388, 'reg_lambda': 0.3699742427105907, 'subsample': 0.40461321743773687, 'min_child_weight': 0.06129374210408517, 'n_jobs': 4, 'learning_rate': 0.4092121432306745}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 40/3257 [00:00<00:08, 398.80it/s]  2%|‚ñè         | 81/3257 [00:00<00:07, 403.77it/s]  4%|‚ñé         | 122/3257 [00:00<00:08, 389.56it/s]  5%|‚ñå         | 168/3257 [00:00<00:07, 414.55it/s]  7%|‚ñã         | 215/3257 [00:00<00:07, 431.85it/s]  8%|‚ñä         | 262/3257 [00:00<00:06, 444.57it/s] 10%|‚ñâ         | 313/3257 [00:00<00:06, 461.38it/s] 11%|‚ñà         | 363/3257 [00:00<00:06, 470.82it/s] 13%|‚ñà‚ñé        | 411/3257 [00:00<00:06, 459.45it/s] 14%|‚ñà‚ñç        | 458/3257 [00:01<00:08, 334.60it/s] 16%|‚ñà‚ñå        | 505/3257 [00:01<00:07, 364.80it/s] 17%|‚ñà‚ñã        | 550/3257 [00:01<00:07, 383.67it/s] 18%|‚ñà‚ñä        | 592/3257 [00:01<00:06, 388.29it/s] 20%|‚ñà‚ñâ        | 639/3257 [00:01<00:06, 409.79it/s] 21%|‚ñà‚ñà        | 682/3257 [00:01<00:06, 413.28it/s] 22%|‚ñà‚ñà‚ñè       | 725/3257 [00:01<00:06, 415.97it/s] 24%|‚ñà‚ñà‚ñé       | 770/3257 [00:01<00:05, 423.55it/s] 25%|‚ñà‚ñà‚ñå       | 815/3257 [00:01<00:05, 428.71it/s] 26%|‚ñà‚ñà‚ñã       | 859/3257 [00:02<00:05, 423.52it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:02<00:05, 431.57it/s] 29%|‚ñà‚ñà‚ñâ       | 950/3257 [00:02<00:05, 435.82it/s] 31%|‚ñà‚ñà‚ñà       | 994/3257 [00:02<00:05, 435.34it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1038/3257 [00:02<00:05, 429.45it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1082/3257 [00:02<00:05, 425.37it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1127/3257 [00:02<00:04, 428.20it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:02<00:04, 430.98it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1215/3257 [00:02<00:04, 413.02it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1263/3257 [00:03<00:04, 430.39it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1307/3257 [00:03<00:04, 421.85it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1351/3257 [00:03<00:04, 426.85it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1395/3257 [00:03<00:04, 429.93it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1443/3257 [00:03<00:04, 444.51it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1494/3257 [00:03<00:03, 461.81it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1541/3257 [00:03<00:03, 450.00it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1587/3257 [00:03<00:03, 447.55it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:03<00:03, 447.13it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1679/3257 [00:03<00:03, 440.40it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1724/3257 [00:04<00:04, 329.90it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1764/3257 [00:04<00:04, 345.45it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1810/3257 [00:04<00:03, 374.04it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1851/3257 [00:04<00:03, 383.31it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1895/3257 [00:04<00:03, 397.94it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1943/3257 [00:04<00:03, 419.95it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1987/3257 [00:04<00:02, 425.60it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2035/3257 [00:04<00:02, 440.87it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2080/3257 [00:04<00:02, 431.51it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2124/3257 [00:05<00:02, 418.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:05<00:02, 427.08it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2213/3257 [00:05<00:02, 426.89it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2259/3257 [00:05<00:02, 435.63it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2303/3257 [00:05<00:02, 436.50it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:05<00:01, 465.81it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2407/3257 [00:05<00:01, 466.30it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2454/3257 [00:05<00:01, 452.54it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2500/3257 [00:05<00:01, 451.36it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2547/3257 [00:06<00:01, 455.12it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2593/3257 [00:06<00:01, 435.87it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2644/3257 [00:06<00:01, 453.99it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2690/3257 [00:06<00:01, 455.15it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2736/3257 [00:06<00:01, 439.63it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2782/3257 [00:06<00:01, 444.71it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2827/3257 [00:06<00:00, 444.92it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:06<00:00, 467.53it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:06<00:00, 457.24it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2974/3257 [00:06<00:00, 452.55it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3020/3257 [00:07<00:00, 450.93it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3072/3257 [00:07<00:00, 470.68it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3123/3257 [00:07<00:00, 479.60it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3172/3257 [00:07<00:00, 463.44it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3219/3257 [00:07<00:00, 454.59it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:07<00:00, 429.58it/s]
2023-02-07 13:52:15.673 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:52:15,674][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d129,n5,mc3,s0.511639,t4>', 'datetime': '2023-02-07T13:52:15.674425', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:52:15,674][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:52:15,674][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:52:15,801][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:52:15,802][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:52:15,804][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 768 unique words (83.12% of original 924, drops 156)', 'datetime': '2023-02-07T13:52:15.804311', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:52:15,804][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 1455479 word corpus (99.98% of original 1455748, drops 269)', 'datetime': '2023-02-07T13:52:15.804470', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:52:15,807][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:52:15,807][gensim.models.word2vec][INFO] - sample=0.511639 downsamples 0 most-common words
[2023-02-07 13:52:15,807][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455479 word corpus (100.0%% of prior 1455479)', 'datetime': '2023-02-07T13:52:15.807302', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:52:15,811][gensim.models.word2vec][INFO] - estimated required memory for 768 words and 129 dimensions: 3508588 bytes
[2023-02-07 13:52:15,811][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:52:15,814][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 768 vocabulary and 129 features, using sg=1 hs=0 sample=0.5116388239250553 negative=5 window=12 shrink_windows=True', 'datetime': '2023-02-07T13:52:15.814042', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:52:16,445][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458736 effective words) took 0.6s, 2317345 effective words/s
[2023-02-07 13:52:17,068][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458736 effective words) took 0.6s, 2347750 effective words/s
[2023-02-07 13:52:17,696][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458736 effective words) took 0.6s, 2325436 effective words/s
[2023-02-07 13:52:18,320][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458736 effective words) took 0.6s, 2343026 effective words/s
[2023-02-07 13:52:18,948][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458736 effective words) took 0.6s, 2330279 effective words/s
[2023-02-07 13:52:19,578][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458736 effective words) took 0.6s, 2319762 effective words/s
[2023-02-07 13:52:20,204][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458736 effective words) took 0.6s, 2335105 effective words/s
[2023-02-07 13:52:20,830][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458736 effective words) took 0.6s, 2334712 effective words/s
[2023-02-07 13:52:21,456][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458736 effective words) took 0.6s, 2332345 effective words/s
[2023-02-07 13:52:22,081][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458736 effective words) took 0.6s, 2339116 effective words/s
[2023-02-07 13:52:22,708][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458736 effective words) took 0.6s, 2331017 effective words/s
[2023-02-07 13:52:23,326][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458736 effective words) took 0.6s, 2365600 effective words/s
[2023-02-07 13:52:23,955][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458736 effective words) took 0.6s, 2328327 effective words/s
[2023-02-07 13:52:24,579][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458736 effective words) took 0.6s, 2338634 effective words/s
[2023-02-07 13:52:25,204][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458736 effective words) took 0.6s, 2342270 effective words/s
[2023-02-07 13:52:25,825][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458736 effective words) took 0.6s, 2350708 effective words/s
[2023-02-07 13:52:26,446][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458736 effective words) took 0.6s, 2356034 effective words/s
[2023-02-07 13:52:27,063][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458736 effective words) took 0.6s, 2365908 effective words/s
[2023-02-07 13:52:27,689][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458736 effective words) took 0.6s, 2338486 effective words/s
[2023-02-07 13:52:28,309][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458736 effective words) took 0.6s, 2353458 effective words/s
[2023-02-07 13:52:28,937][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458736 effective words) took 0.6s, 2328783 effective words/s
[2023-02-07 13:52:29,554][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458736 effective words) took 0.6s, 2368965 effective words/s
[2023-02-07 13:52:30,170][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458736 effective words) took 0.6s, 2374520 effective words/s
[2023-02-07 13:52:30,791][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458736 effective words) took 0.6s, 2352656 effective words/s
[2023-02-07 13:52:31,410][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458736 effective words) took 0.6s, 2359695 effective words/s
[2023-02-07 13:52:32,041][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458736 effective words) took 0.6s, 2316174 effective words/s
[2023-02-07 13:52:32,709][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458736 effective words) took 0.7s, 2187298 effective words/s
[2023-02-07 13:52:33,365][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458736 effective words) took 0.7s, 2232122 effective words/s
[2023-02-07 13:52:34,032][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458736 effective words) took 0.7s, 2190265 effective words/s
[2023-02-07 13:52:34,675][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458736 effective words) took 0.6s, 2274531 effective words/s
[2023-02-07 13:52:35,340][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458736 effective words) took 0.7s, 2197540 effective words/s
[2023-02-07 13:52:35,986][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458736 effective words) took 0.6s, 2263072 effective words/s
[2023-02-07 13:52:36,631][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458736 effective words) took 0.6s, 2268460 effective words/s
[2023-02-07 13:52:37,272][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458736 effective words) took 0.6s, 2279508 effective words/s
[2023-02-07 13:52:37,913][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458736 effective words) took 0.6s, 2280541 effective words/s
[2023-02-07 13:52:38,562][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458736 effective words) took 0.6s, 2251431 effective words/s
[2023-02-07 13:52:39,217][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458736 effective words) took 0.7s, 2231365 effective words/s
[2023-02-07 13:52:39,869][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458736 effective words) took 0.7s, 2242796 effective words/s
[2023-02-07 13:52:40,506][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458736 effective words) took 0.6s, 2298746 effective words/s
[2023-02-07 13:52:41,151][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458736 effective words) took 0.6s, 2265696 effective words/s
[2023-02-07 13:52:41,804][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458736 effective words) took 0.7s, 2240175 effective words/s
[2023-02-07 13:52:42,461][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458736 effective words) took 0.7s, 2224181 effective words/s
[2023-02-07 13:52:43,142][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458736 effective words) took 0.7s, 2147265 effective words/s
[2023-02-07 13:52:43,825][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458736 effective words) took 0.7s, 2138963 effective words/s
[2023-02-07 13:52:44,513][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458736 effective words) took 0.7s, 2125107 effective words/s
[2023-02-07 13:52:45,201][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458736 effective words) took 0.7s, 2124758 effective words/s
[2023-02-07 13:52:45,893][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458736 effective words) took 0.7s, 2111540 effective words/s
[2023-02-07 13:52:46,585][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458736 effective words) took 0.7s, 2113699 effective words/s
[2023-02-07 13:52:47,284][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458736 effective words) took 0.7s, 2090783 effective words/s
[2023-02-07 13:52:47,971][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458736 effective words) took 0.7s, 2130882 effective words/s
[2023-02-07 13:52:48,645][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458736 effective words) took 0.7s, 2169400 effective words/s
[2023-02-07 13:52:49,328][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458736 effective words) took 0.7s, 2139067 effective words/s
[2023-02-07 13:52:50,020][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458736 effective words) took 0.7s, 2113499 effective words/s
[2023-02-07 13:52:50,707][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458736 effective words) took 0.7s, 2129113 effective words/s
[2023-02-07 13:52:51,376][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458736 effective words) took 0.7s, 2184031 effective words/s
[2023-02-07 13:52:52,060][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458736 effective words) took 0.7s, 2136854 effective words/s
[2023-02-07 13:52:52,731][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1458736 effective words) took 0.7s, 2178654 effective words/s
[2023-02-07 13:52:53,415][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1458736 effective words) took 0.7s, 2135789 effective words/s
[2023-02-07 13:52:54,105][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1458736 effective words) took 0.7s, 2119339 effective words/s
[2023-02-07 13:52:54,805][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1458736 effective words) took 0.7s, 2086639 effective words/s
[2023-02-07 13:52:55,503][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1458736 effective words) took 0.7s, 2095174 effective words/s
[2023-02-07 13:52:56,202][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1458736 effective words) took 0.7s, 2089749 effective words/s
[2023-02-07 13:52:56,899][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1458736 effective words) took 0.7s, 2096606 effective words/s
[2023-02-07 13:52:57,598][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1458736 effective words) took 0.7s, 2095485 effective words/s
[2023-02-07 13:52:58,301][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1458736 effective words) took 0.7s, 2077595 effective words/s
[2023-02-07 13:52:58,981][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1458736 effective words) took 0.7s, 2148373 effective words/s
[2023-02-07 13:52:59,676][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1458736 effective words) took 0.7s, 2104812 effective words/s
[2023-02-07 13:53:00,394][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1458736 effective words) took 0.7s, 2035855 effective words/s
[2023-02-07 13:53:01,091][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1458736 effective words) took 0.7s, 2096391 effective words/s
[2023-02-07 13:53:01,791][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1458736 effective words) took 0.7s, 2086546 effective words/s
[2023-02-07 13:53:02,460][gensim.models.word2vec][INFO] - EPOCH 70: training on 1455748 raw words (1458736 effective words) took 0.7s, 2184863 effective words/s
[2023-02-07 13:53:03,127][gensim.models.word2vec][INFO] - EPOCH 71: training on 1455748 raw words (1458736 effective words) took 0.7s, 2194015 effective words/s
[2023-02-07 13:53:03,791][gensim.models.word2vec][INFO] - EPOCH 72: training on 1455748 raw words (1458736 effective words) took 0.7s, 2198035 effective words/s
[2023-02-07 13:53:04,472][gensim.models.word2vec][INFO] - EPOCH 73: training on 1455748 raw words (1458736 effective words) took 0.7s, 2147784 effective words/s
[2023-02-07 13:53:05,140][gensim.models.word2vec][INFO] - EPOCH 74: training on 1455748 raw words (1458736 effective words) took 0.7s, 2187989 effective words/s
[2023-02-07 13:53:05,813][gensim.models.word2vec][INFO] - EPOCH 75: training on 1455748 raw words (1458736 effective words) took 0.7s, 2172953 effective words/s
[2023-02-07 13:53:06,482][gensim.models.word2vec][INFO] - EPOCH 76: training on 1455748 raw words (1458736 effective words) took 0.7s, 2184821 effective words/s
[2023-02-07 13:53:07,161][gensim.models.word2vec][INFO] - EPOCH 77: training on 1455748 raw words (1458736 effective words) took 0.7s, 2151409 effective words/s
[2023-02-07 13:53:07,837][gensim.models.word2vec][INFO] - EPOCH 78: training on 1455748 raw words (1458736 effective words) took 0.7s, 2167628 effective words/s
[2023-02-07 13:53:08,511][gensim.models.word2vec][INFO] - EPOCH 79: training on 1455748 raw words (1458736 effective words) took 0.7s, 2167719 effective words/s
[2023-02-07 13:53:09,182][gensim.models.word2vec][INFO] - EPOCH 80: training on 1455748 raw words (1458736 effective words) took 0.7s, 2178988 effective words/s
[2023-02-07 13:53:09,857][gensim.models.word2vec][INFO] - EPOCH 81: training on 1455748 raw words (1458736 effective words) took 0.7s, 2168190 effective words/s
[2023-02-07 13:53:10,532][gensim.models.word2vec][INFO] - EPOCH 82: training on 1455748 raw words (1458736 effective words) took 0.7s, 2163947 effective words/s
[2023-02-07 13:53:11,205][gensim.models.word2vec][INFO] - EPOCH 83: training on 1455748 raw words (1458736 effective words) took 0.7s, 2172225 effective words/s
[2023-02-07 13:53:11,885][gensim.models.word2vec][INFO] - EPOCH 84: training on 1455748 raw words (1458736 effective words) took 0.7s, 2148707 effective words/s
[2023-02-07 13:53:12,554][gensim.models.word2vec][INFO] - EPOCH 85: training on 1455748 raw words (1458736 effective words) took 0.7s, 2186200 effective words/s
[2023-02-07 13:53:13,232][gensim.models.word2vec][INFO] - EPOCH 86: training on 1455748 raw words (1458736 effective words) took 0.7s, 2155322 effective words/s
[2023-02-07 13:53:13,906][gensim.models.word2vec][INFO] - EPOCH 87: training on 1455748 raw words (1458736 effective words) took 0.7s, 2168389 effective words/s
[2023-02-07 13:53:14,575][gensim.models.word2vec][INFO] - EPOCH 88: training on 1455748 raw words (1458736 effective words) took 0.7s, 2184782 effective words/s
[2023-02-07 13:53:15,249][gensim.models.word2vec][INFO] - EPOCH 89: training on 1455748 raw words (1458736 effective words) took 0.7s, 2172679 effective words/s
[2023-02-07 13:53:15,930][gensim.models.word2vec][INFO] - EPOCH 90: training on 1455748 raw words (1458736 effective words) took 0.7s, 2142808 effective words/s
[2023-02-07 13:53:16,615][gensim.models.word2vec][INFO] - EPOCH 91: training on 1455748 raw words (1458736 effective words) took 0.7s, 2136025 effective words/s
[2023-02-07 13:53:17,299][gensim.models.word2vec][INFO] - EPOCH 92: training on 1455748 raw words (1458736 effective words) took 0.7s, 2136352 effective words/s
[2023-02-07 13:53:17,986][gensim.models.word2vec][INFO] - EPOCH 93: training on 1455748 raw words (1458736 effective words) took 0.7s, 2125842 effective words/s
[2023-02-07 13:53:18,669][gensim.models.word2vec][INFO] - EPOCH 94: training on 1455748 raw words (1458736 effective words) took 0.7s, 2140839 effective words/s
[2023-02-07 13:53:19,333][gensim.models.word2vec][INFO] - EPOCH 95: training on 1455748 raw words (1458736 effective words) took 0.7s, 2200855 effective words/s
[2023-02-07 13:53:20,010][gensim.models.word2vec][INFO] - EPOCH 96: training on 1455748 raw words (1458736 effective words) took 0.7s, 2160251 effective words/s
[2023-02-07 13:53:20,010][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 141207556 raw words (141497392 effective words) took 64.2s, 2204133 effective words/s', 'datetime': '2023-02-07T13:53:20.010612', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:53:20.010 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:53:22,330][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135200-rpxy1uki/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:53:22.330431', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:53:22,332][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:53:22,337][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135200-rpxy1uki/files/../tmp/embedding_model.pt
2023-02-07 13:53:22.337 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:53:23.469 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:53:23.905 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:53:24.798 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.2123757663535955, 'test_mae': 1.076097883032692, 'test_r2': -0.04658601519364014}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.81
wandb: percentage 0.16883
wandb:   test_mae 1.0761
wandb:   test_mse 2.21238
wandb:    test_r2 -0.04659
wandb: 
wandb: üöÄ View run young-sweep-59 at: https://wandb.ai/xiaoqiz/mof2vec/runs/rpxy1uki
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_135200-rpxy1uki/logs
wandb: Agent Starting Run: 5l4rv9zw with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 137
wandb: 	model.gensim.alpha: 0.00123364930136948
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 82
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.8002234126617436
wandb: 	model.gensim.vector_size: 57
wandb: 	model.gensim.window: 8
wandb: 	model.sklearn.learning_rate: 0.03407383970906447
wandb: 	model.sklearn.max_depth: 10
wandb: 	model.sklearn.min_child_weight: 0.002846078523132152
wandb: 	model.sklearn.n_estimators: 1740
wandb: 	model.sklearn.num_leaves: 463
wandb: 	model.sklearn.reg_alpha: 0.3440494102798951
wandb: 	model.sklearn.reg_lambda: 0.503852294411107
wandb: 	model.sklearn.subsample: 0.3680751930192878
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135335-5l4rv9zw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-60
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/5l4rv9zw
2023-02-07 13:53:43.459 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:53:43.459 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 137 for sweep.
2023-02-07 13:53:43.460 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.00123364930136948 for sweep.
2023-02-07 13:53:43.460 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:53:43.460 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 82 for sweep.
2023-02-07 13:53:43.460 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 13:53:43.460 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.8002234126617436 for sweep.
2023-02-07 13:53:43.461 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 57 for sweep.
2023-02-07 13:53:43.461 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 8 for sweep.
2023-02-07 13:53:43.462 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.03407383970906447 for sweep.
2023-02-07 13:53:43.462 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 10 for sweep.
2023-02-07 13:53:43.462 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.002846078523132152 for sweep.
2023-02-07 13:53:43.462 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1740 for sweep.
2023-02-07 13:53:43.462 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 463 for sweep.
2023-02-07 13:53:43.463 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.3440494102798951 for sweep.
2023-02-07 13:53:43.463 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.503852294411107 for sweep.
2023-02-07 13:53:43.463 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3680751930192878 for sweep.
2023-02-07 13:53:43.463 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:53:43.471 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135335-5l4rv9zw/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 137, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 57, 'window': 8, 'min_count': 4, 'dm': 0, 'sample': 0.8002234126617436, 'workers': 4, 'alpha': 0.00123364930136948, 'epochs': 82}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1740, 'max_depth': 10, 'num_leaves': 463, 'reg_alpha': 0.3440494102798951, 'reg_lambda': 0.503852294411107, 'subsample': 0.3680751930192878, 'min_child_weight': 0.002846078523132152, 'n_jobs': 4, 'learning_rate': 0.03407383970906447}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 36/3257 [00:00<00:08, 359.29it/s]  2%|‚ñè         | 79/3257 [00:00<00:08, 395.14it/s]  4%|‚ñé         | 119/3257 [00:00<00:08, 381.93it/s]  5%|‚ñç         | 159/3257 [00:00<00:08, 384.83it/s]  6%|‚ñå         | 198/3257 [00:00<00:08, 380.83it/s]  7%|‚ñã         | 243/3257 [00:00<00:07, 401.34it/s]  9%|‚ñâ         | 290/3257 [00:00<00:07, 421.62it/s] 10%|‚ñà         | 334/3257 [00:00<00:06, 426.45it/s] 12%|‚ñà‚ñè        | 377/3257 [00:00<00:06, 420.03it/s] 13%|‚ñà‚ñé        | 421/3257 [00:01<00:06, 423.69it/s] 14%|‚ñà‚ñç        | 464/3257 [00:01<00:06, 411.93it/s] 16%|‚ñà‚ñå        | 510/3257 [00:01<00:06, 422.60it/s] 17%|‚ñà‚ñã        | 553/3257 [00:01<00:06, 423.39it/s] 18%|‚ñà‚ñä        | 596/3257 [00:01<00:06, 411.62it/s] 20%|‚ñà‚ñâ        | 640/3257 [00:01<00:06, 417.33it/s] 21%|‚ñà‚ñà        | 682/3257 [00:01<00:06, 414.19it/s] 22%|‚ñà‚ñà‚ñè       | 724/3257 [00:01<00:06, 405.33it/s] 24%|‚ñà‚ñà‚ñé       | 769/3257 [00:01<00:05, 416.45it/s] 25%|‚ñà‚ñà‚ñç       | 811/3257 [00:01<00:05, 415.03it/s] 26%|‚ñà‚ñà‚ñå       | 853/3257 [00:02<00:05, 400.78it/s] 27%|‚ñà‚ñà‚ñã       | 895/3257 [00:02<00:05, 404.45it/s] 29%|‚ñà‚ñà‚ñâ       | 939/3257 [00:02<00:05, 412.50it/s] 30%|‚ñà‚ñà‚ñà       | 981/3257 [00:02<00:05, 414.56it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1023/3257 [00:02<00:05, 413.32it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1065/3257 [00:02<00:05, 400.93it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1106/3257 [00:02<00:05, 402.10it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:02<00:05, 397.69it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1187/3257 [00:03<00:07, 282.42it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1220/3257 [00:03<00:06, 291.71it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1261/3257 [00:03<00:06, 319.28it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1296/3257 [00:03<00:06, 322.26it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1337/3257 [00:03<00:05, 344.51it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1376/3257 [00:03<00:05, 356.85it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1421/3257 [00:03<00:04, 382.64it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1466/3257 [00:03<00:04, 400.19it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1513/3257 [00:03<00:04, 417.97it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1556/3257 [00:03<00:04, 400.60it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1598/3257 [00:04<00:04, 405.12it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1641/3257 [00:04<00:03, 409.74it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1683/3257 [00:04<00:03, 398.17it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1727/3257 [00:04<00:03, 408.62it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1769/3257 [00:04<00:03, 395.68it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1809/3257 [00:04<00:03, 389.31it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1849/3257 [00:04<00:03, 386.99it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1891/3257 [00:04<00:03, 394.69it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1937/3257 [00:04<00:03, 411.23it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1984/3257 [00:05<00:02, 426.24it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2032/3257 [00:05<00:02, 440.23it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2077/3257 [00:05<00:02, 421.13it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:05<00:02, 414.04it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2162/3257 [00:05<00:02, 409.24it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:05<00:02, 415.16it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2248/3257 [00:05<00:02, 411.42it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2290/3257 [00:05<00:02, 413.89it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2337/3257 [00:05<00:02, 428.34it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2387/3257 [00:05<00:01, 448.08it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2432/3257 [00:06<00:01, 433.34it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2478/3257 [00:06<00:01, 437.56it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2522/3257 [00:06<00:02, 314.68it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2565/3257 [00:06<00:02, 338.56it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2609/3257 [00:06<00:01, 362.56it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2654/3257 [00:06<00:01, 385.06it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2696/3257 [00:06<00:01, 391.51it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2738/3257 [00:06<00:01, 398.19it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2781/3257 [00:07<00:01, 406.80it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2824/3257 [00:07<00:01, 410.26it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2873/3257 [00:07<00:00, 432.46it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2917/3257 [00:07<00:00, 429.96it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2961/3257 [00:07<00:00, 421.10it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3004/3257 [00:07<00:00, 417.39it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3048/3257 [00:07<00:00, 419.74it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3092/3257 [00:07<00:00, 423.61it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3135/3257 [00:07<00:00, 424.46it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3178/3257 [00:07<00:00, 418.37it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3223/3257 [00:08<00:00, 425.31it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 399.92it/s]
2023-02-07 13:53:51.789 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:53:51,789][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d57,n5,mc4,s0.800223,t4>', 'datetime': '2023-02-07T13:53:51.789916', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:53:51,790][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:53:51,790][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:53:51,917][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:53:51,918][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:53:51,919][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 742 unique words (80.30% of original 924, drops 182)', 'datetime': '2023-02-07T13:53:51.919917', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:53:51,920][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 1455401 word corpus (99.98% of original 1455748, drops 347)', 'datetime': '2023-02-07T13:53:51.920087', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:53:51,923][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:53:51,923][gensim.models.word2vec][INFO] - sample=0.800223 downsamples 0 most-common words
[2023-02-07 13:53:51,923][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455401 word corpus (100.0%% of prior 1455401)', 'datetime': '2023-02-07T13:53:51.923756', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:53:51,928][gensim.models.word2vec][INFO] - estimated required memory for 742 words and 57 dimensions: 2103348 bytes
[2023-02-07 13:53:51,928][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:53:51,929][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 742 vocabulary and 57 features, using sg=1 hs=0 sample=0.8002234126617436 negative=5 window=8 shrink_windows=True', 'datetime': '2023-02-07T13:53:51.929596', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:53:52,441][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458658 effective words) took 0.5s, 2856584 effective words/s
[2023-02-07 13:53:52,947][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458658 effective words) took 0.5s, 2892819 effective words/s
[2023-02-07 13:53:53,438][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458658 effective words) took 0.5s, 2981819 effective words/s
[2023-02-07 13:53:53,900][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458658 effective words) took 0.5s, 3166803 effective words/s
[2023-02-07 13:53:54,346][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458658 effective words) took 0.4s, 3283104 effective words/s
[2023-02-07 13:53:54,812][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458658 effective words) took 0.5s, 3136062 effective words/s
[2023-02-07 13:53:55,300][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458658 effective words) took 0.5s, 3000583 effective words/s
[2023-02-07 13:53:55,816][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458658 effective words) took 0.5s, 2833561 effective words/s
[2023-02-07 13:53:56,344][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458658 effective words) took 0.5s, 2767382 effective words/s
[2023-02-07 13:53:56,870][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458658 effective words) took 0.5s, 2779984 effective words/s
[2023-02-07 13:53:57,389][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458658 effective words) took 0.5s, 2815749 effective words/s
[2023-02-07 13:53:57,914][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458658 effective words) took 0.5s, 2784983 effective words/s
[2023-02-07 13:53:58,432][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458658 effective words) took 0.5s, 2822862 effective words/s
[2023-02-07 13:53:58,954][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458658 effective words) took 0.5s, 2804568 effective words/s
[2023-02-07 13:53:59,475][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458658 effective words) took 0.5s, 2807898 effective words/s
[2023-02-07 13:53:59,994][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458658 effective words) took 0.5s, 2814475 effective words/s
[2023-02-07 13:54:00,518][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458658 effective words) took 0.5s, 2793540 effective words/s
[2023-02-07 13:54:01,045][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458658 effective words) took 0.5s, 2774043 effective words/s
[2023-02-07 13:54:01,568][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458658 effective words) took 0.5s, 2803042 effective words/s
[2023-02-07 13:54:02,088][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458658 effective words) took 0.5s, 2810457 effective words/s
[2023-02-07 13:54:02,604][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458658 effective words) took 0.5s, 2835268 effective words/s
[2023-02-07 13:54:03,129][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458658 effective words) took 0.5s, 2782747 effective words/s
[2023-02-07 13:54:03,657][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458658 effective words) took 0.5s, 2771054 effective words/s
[2023-02-07 13:54:04,184][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458658 effective words) took 0.5s, 2776510 effective words/s
[2023-02-07 13:54:04,709][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458658 effective words) took 0.5s, 2786902 effective words/s
[2023-02-07 13:54:05,234][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458658 effective words) took 0.5s, 2783809 effective words/s
[2023-02-07 13:54:05,763][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458658 effective words) took 0.5s, 2766017 effective words/s
[2023-02-07 13:54:06,291][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458658 effective words) took 0.5s, 2771152 effective words/s
[2023-02-07 13:54:06,826][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458658 effective words) took 0.5s, 2733113 effective words/s
[2023-02-07 13:54:07,354][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458658 effective words) took 0.5s, 2771974 effective words/s
[2023-02-07 13:54:07,884][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458658 effective words) took 0.5s, 2757355 effective words/s
[2023-02-07 13:54:08,416][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458658 effective words) took 0.5s, 2749080 effective words/s
[2023-02-07 13:54:08,952][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458658 effective words) took 0.5s, 2728548 effective words/s
[2023-02-07 13:54:09,467][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458658 effective words) took 0.5s, 2837002 effective words/s
[2023-02-07 13:54:09,967][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458658 effective words) took 0.5s, 2926981 effective words/s
[2023-02-07 13:54:10,473][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458658 effective words) took 0.5s, 2893726 effective words/s
[2023-02-07 13:54:10,974][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458658 effective words) took 0.5s, 2916837 effective words/s
[2023-02-07 13:54:11,482][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458658 effective words) took 0.5s, 2886128 effective words/s
[2023-02-07 13:54:11,995][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458658 effective words) took 0.5s, 2849470 effective words/s
[2023-02-07 13:54:12,501][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458658 effective words) took 0.5s, 2891427 effective words/s
[2023-02-07 13:54:13,004][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458658 effective words) took 0.5s, 2908405 effective words/s
[2023-02-07 13:54:13,511][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458658 effective words) took 0.5s, 2881876 effective words/s
[2023-02-07 13:54:14,014][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458658 effective words) took 0.5s, 2911293 effective words/s
[2023-02-07 13:54:14,521][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458658 effective words) took 0.5s, 2885693 effective words/s
[2023-02-07 13:54:15,031][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458658 effective words) took 0.5s, 2865281 effective words/s
[2023-02-07 13:54:15,543][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458658 effective words) took 0.5s, 2858704 effective words/s
[2023-02-07 13:54:16,048][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458658 effective words) took 0.5s, 2897784 effective words/s
[2023-02-07 13:54:16,560][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458658 effective words) took 0.5s, 2853931 effective words/s
[2023-02-07 13:54:17,070][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458658 effective words) took 0.5s, 2864727 effective words/s
[2023-02-07 13:54:17,573][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458658 effective words) took 0.5s, 2905340 effective words/s
[2023-02-07 13:54:18,078][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458658 effective words) took 0.5s, 2899381 effective words/s
[2023-02-07 13:54:18,582][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458658 effective words) took 0.5s, 2898750 effective words/s
[2023-02-07 13:54:19,088][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458658 effective words) took 0.5s, 2892890 effective words/s
[2023-02-07 13:54:19,595][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458658 effective words) took 0.5s, 2884140 effective words/s
[2023-02-07 13:54:20,107][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458658 effective words) took 0.5s, 2856383 effective words/s
[2023-02-07 13:54:20,615][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458658 effective words) took 0.5s, 2880375 effective words/s
[2023-02-07 13:54:21,121][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1458658 effective words) took 0.5s, 2888346 effective words/s
[2023-02-07 13:54:21,633][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1458658 effective words) took 0.5s, 2859977 effective words/s
[2023-02-07 13:54:22,149][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1458658 effective words) took 0.5s, 2834760 effective words/s
[2023-02-07 13:54:22,658][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1458658 effective words) took 0.5s, 2872857 effective words/s
[2023-02-07 13:54:23,170][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1458658 effective words) took 0.5s, 2855771 effective words/s
[2023-02-07 13:54:23,682][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1458658 effective words) took 0.5s, 2856050 effective words/s
[2023-02-07 13:54:24,196][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1458658 effective words) took 0.5s, 2842059 effective words/s
[2023-02-07 13:54:24,715][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1458658 effective words) took 0.5s, 2822457 effective words/s
[2023-02-07 13:54:25,234][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1458658 effective words) took 0.5s, 2815442 effective words/s
[2023-02-07 13:54:25,742][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1458658 effective words) took 0.5s, 2882107 effective words/s
[2023-02-07 13:54:26,242][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1458658 effective words) took 0.5s, 2921397 effective words/s
[2023-02-07 13:54:26,751][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1458658 effective words) took 0.5s, 2873965 effective words/s
[2023-02-07 13:54:27,250][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1458658 effective words) took 0.5s, 2930205 effective words/s
[2023-02-07 13:54:27,756][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1458658 effective words) took 0.5s, 2896741 effective words/s
[2023-02-07 13:54:28,252][gensim.models.word2vec][INFO] - EPOCH 70: training on 1455748 raw words (1458658 effective words) took 0.5s, 2947766 effective words/s
[2023-02-07 13:54:28,754][gensim.models.word2vec][INFO] - EPOCH 71: training on 1455748 raw words (1458658 effective words) took 0.5s, 2916076 effective words/s
[2023-02-07 13:54:29,258][gensim.models.word2vec][INFO] - EPOCH 72: training on 1455748 raw words (1458658 effective words) took 0.5s, 2899629 effective words/s
[2023-02-07 13:54:29,768][gensim.models.word2vec][INFO] - EPOCH 73: training on 1455748 raw words (1458658 effective words) took 0.5s, 2867714 effective words/s
[2023-02-07 13:54:30,290][gensim.models.word2vec][INFO] - EPOCH 74: training on 1455748 raw words (1458658 effective words) took 0.5s, 2799480 effective words/s
[2023-02-07 13:54:30,810][gensim.models.word2vec][INFO] - EPOCH 75: training on 1455748 raw words (1458658 effective words) took 0.5s, 2815112 effective words/s
[2023-02-07 13:54:31,315][gensim.models.word2vec][INFO] - EPOCH 76: training on 1455748 raw words (1458658 effective words) took 0.5s, 2893282 effective words/s
[2023-02-07 13:54:31,832][gensim.models.word2vec][INFO] - EPOCH 77: training on 1455748 raw words (1458658 effective words) took 0.5s, 2832654 effective words/s
[2023-02-07 13:54:32,339][gensim.models.word2vec][INFO] - EPOCH 78: training on 1455748 raw words (1458658 effective words) took 0.5s, 2887661 effective words/s
[2023-02-07 13:54:32,835][gensim.models.word2vec][INFO] - EPOCH 79: training on 1455748 raw words (1458658 effective words) took 0.5s, 2948425 effective words/s
[2023-02-07 13:54:33,323][gensim.models.word2vec][INFO] - EPOCH 80: training on 1455748 raw words (1458658 effective words) took 0.5s, 2998640 effective words/s
[2023-02-07 13:54:33,818][gensim.models.word2vec][INFO] - EPOCH 81: training on 1455748 raw words (1458658 effective words) took 0.5s, 2957447 effective words/s
[2023-02-07 13:54:33,819][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 119371336 raw words (119609956 effective words) took 41.9s, 2855386 effective words/s', 'datetime': '2023-02-07T13:54:33.819038', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:54:33.819 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:54:35,946][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135335-5l4rv9zw/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:54:35.946134', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:54:35,947][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:54:35,951][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135335-5l4rv9zw/files/../tmp/embedding_model.pt
2023-02-07 13:54:35.951 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:54:36.830 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:54:37.180 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:54:37.623 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9624432626537698, 'test_mae': 1.0691267541421154, 'test_r2': 0.07164700249380973}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.77
wandb: percentage 0.19697
wandb:   test_mae 1.06913
wandb:   test_mse 1.96244
wandb:    test_r2 0.07165
wandb: 
wandb: üöÄ View run sage-sweep-60 at: https://wandb.ai/xiaoqiz/mof2vec/runs/5l4rv9zw
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_135335-5l4rv9zw/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 5epn1fq9 with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 365
wandb: 	model.gensim.alpha: 0.07911569506395504
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 94
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.21115372335844668
wandb: 	model.gensim.vector_size: 169
wandb: 	model.gensim.window: 1
wandb: 	model.sklearn.learning_rate: 0.07177002960432703
wandb: 	model.sklearn.max_depth: 67
wandb: 	model.sklearn.min_child_weight: 0.02969123715437317
wandb: 	model.sklearn.n_estimators: 4912
wandb: 	model.sklearn.num_leaves: 391
wandb: 	model.sklearn.reg_alpha: 0.004624037796676935
wandb: 	model.sklearn.reg_lambda: 0.7896471345417643
wandb: 	model.sklearn.subsample: 0.2154346763689514
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135454-5epn1fq9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-61
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/5epn1fq9
2023-02-07 13:55:01.974 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:55:01.975 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 365 for sweep.
2023-02-07 13:55:01.975 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.07911569506395504 for sweep.
2023-02-07 13:55:01.975 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:55:01.975 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 94 for sweep.
2023-02-07 13:55:01.976 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 13:55:01.976 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.21115372335844668 for sweep.
2023-02-07 13:55:01.976 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 169 for sweep.
2023-02-07 13:55:01.976 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 1 for sweep.
2023-02-07 13:55:01.977 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.07177002960432703 for sweep.
2023-02-07 13:55:01.977 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 67 for sweep.
2023-02-07 13:55:01.977 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.02969123715437317 for sweep.
2023-02-07 13:55:01.977 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4912 for sweep.
2023-02-07 13:55:01.977 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 391 for sweep.
2023-02-07 13:55:01.978 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.004624037796676935 for sweep.
2023-02-07 13:55:01.978 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.7896471345417643 for sweep.
2023-02-07 13:55:01.978 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2154346763689514 for sweep.
2023-02-07 13:55:01.978 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:55:01.983 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135454-5epn1fq9/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 365, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 169, 'window': 1, 'min_count': 2, 'dm': 0, 'sample': 0.21115372335844668, 'workers': 4, 'alpha': 0.07911569506395504, 'epochs': 94}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4912, 'max_depth': 67, 'num_leaves': 391, 'reg_alpha': 0.004624037796676935, 'reg_lambda': 0.7896471345417643, 'subsample': 0.2154346763689514, 'min_child_weight': 0.02969123715437317, 'n_jobs': 4, 'learning_rate': 0.07177002960432703}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|‚ñè         | 41/3257 [00:00<00:07, 403.83it/s]  3%|‚ñé         | 84/3257 [00:00<00:07, 416.54it/s]  4%|‚ñç         | 126/3257 [00:00<00:07, 411.68it/s]  5%|‚ñå         | 171/3257 [00:00<00:07, 423.36it/s]  7%|‚ñã         | 216/3257 [00:00<00:07, 430.77it/s]  8%|‚ñä         | 261/3257 [00:00<00:06, 434.38it/s]  9%|‚ñâ         | 308/3257 [00:00<00:06, 443.88it/s] 11%|‚ñà         | 353/3257 [00:00<00:06, 441.16it/s] 12%|‚ñà‚ñè        | 398/3257 [00:00<00:06, 428.03it/s] 14%|‚ñà‚ñé        | 441/3257 [00:01<00:06, 411.40it/s] 15%|‚ñà‚ñç        | 483/3257 [00:01<00:06, 412.09it/s] 16%|‚ñà‚ñå        | 527/3257 [00:01<00:06, 419.63it/s] 18%|‚ñà‚ñä        | 570/3257 [00:01<00:06, 418.48it/s] 19%|‚ñà‚ñâ        | 613/3257 [00:01<00:06, 421.58it/s] 20%|‚ñà‚ñà        | 656/3257 [00:01<00:06, 408.61it/s] 21%|‚ñà‚ñà‚ñè       | 697/3257 [00:01<00:06, 407.05it/s] 23%|‚ñà‚ñà‚ñé       | 738/3257 [00:01<00:06, 403.35it/s] 24%|‚ñà‚ñà‚ñç       | 779/3257 [00:01<00:08, 293.70it/s] 25%|‚ñà‚ñà‚ñå       | 821/3257 [00:02<00:07, 321.80it/s] 26%|‚ñà‚ñà‚ñã       | 859/3257 [00:02<00:07, 336.17it/s] 28%|‚ñà‚ñà‚ñä       | 901/3257 [00:02<00:06, 357.77it/s] 29%|‚ñà‚ñà‚ñâ       | 943/3257 [00:02<00:06, 373.02it/s] 30%|‚ñà‚ñà‚ñà       | 985/3257 [00:02<00:05, 385.59it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1025/3257 [00:02<00:05, 384.90it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1065/3257 [00:02<00:05, 384.15it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1105/3257 [00:02<00:05, 382.59it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1144/3257 [00:02<00:05, 382.81it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1183/3257 [00:03<00:05, 383.29it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1222/3257 [00:03<00:05, 374.33it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1266/3257 [00:03<00:05, 391.76it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1306/3257 [00:03<00:05, 383.88it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1349/3257 [00:03<00:04, 395.31it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1389/3257 [00:03<00:04, 387.15it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1436/3257 [00:03<00:04, 408.41it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1482/3257 [00:03<00:04, 421.70it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1525/3257 [00:03<00:04, 422.38it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1568/3257 [00:03<00:04, 415.14it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1610/3257 [00:04<00:03, 416.53it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1652/3257 [00:04<00:03, 406.57it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1693/3257 [00:04<00:03, 403.15it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1734/3257 [00:04<00:03, 391.88it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1779/3257 [00:04<00:03, 405.45it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1822/3257 [00:04<00:03, 410.89it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1864/3257 [00:04<00:03, 411.33it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1907/3257 [00:04<00:03, 414.93it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1954/3257 [00:04<00:03, 429.31it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1998/3257 [00:04<00:02, 431.26it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2042/3257 [00:05<00:02, 422.83it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2085/3257 [00:05<00:02, 407.70it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2126/3257 [00:05<00:02, 400.11it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2167/3257 [00:05<00:03, 280.74it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2208/3257 [00:05<00:03, 309.27it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2248/3257 [00:05<00:03, 329.56it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2290/3257 [00:05<00:02, 350.88it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2336/3257 [00:05<00:02, 378.59it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2383/3257 [00:06<00:02, 402.71it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2426/3257 [00:06<00:02, 408.34it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2469/3257 [00:06<00:01, 408.94it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2516/3257 [00:06<00:01, 424.68it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2560/3257 [00:06<00:01, 420.52it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2603/3257 [00:06<00:01, 417.87it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2649/3257 [00:06<00:01, 428.75it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2693/3257 [00:06<00:01, 431.53it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2737/3257 [00:06<00:01, 418.20it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2781/3257 [00:07<00:01, 423.35it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2824/3257 [00:07<00:01, 420.74it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2872/3257 [00:07<00:00, 436.67it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2916/3257 [00:07<00:00, 430.83it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2960/3257 [00:07<00:00, 418.70it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3004/3257 [00:07<00:00, 423.42it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:07<00:00, 429.53it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3096/3257 [00:07<00:00, 438.13it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3141/3257 [00:07<00:00, 441.52it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3186/3257 [00:07<00:00, 428.84it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3231/3257 [00:08<00:00, 434.32it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 401.25it/s]
2023-02-07 13:55:10.267 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:55:10,268][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d169,n5,mc2,s0.211154,t4>', 'datetime': '2023-02-07T13:55:10.268742', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:55:10,269][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:55:10,269][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:55:10,393][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:55:10,394][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:55:10,396][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 881 unique words (95.35% of original 924, drops 43)', 'datetime': '2023-02-07T13:55:10.396775', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:55:10,396][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 1455705 word corpus (100.00% of original 1455748, drops 43)', 'datetime': '2023-02-07T13:55:10.396955', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:55:10,400][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:55:10,400][gensim.models.word2vec][INFO] - sample=0.211154 downsamples 0 most-common words
[2023-02-07 13:55:10,400][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455705 word corpus (100.0%% of prior 1455705)', 'datetime': '2023-02-07T13:55:10.400727', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:55:10,405][gensim.models.word2vec][INFO] - estimated required memory for 881 words and 169 dimensions: 4484744 bytes
[2023-02-07 13:55:10,406][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:55:10,408][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 881 vocabulary and 169 features, using sg=1 hs=0 sample=0.21115372335844668 negative=5 window=1 shrink_windows=True', 'datetime': '2023-02-07T13:55:10.408840', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:55:10,760][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458962 effective words) took 0.4s, 4167154 effective words/s
[2023-02-07 13:55:11,092][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458962 effective words) took 0.3s, 4416204 effective words/s
[2023-02-07 13:55:11,420][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458962 effective words) took 0.3s, 4465950 effective words/s
[2023-02-07 13:55:11,752][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458962 effective words) took 0.3s, 4403814 effective words/s
[2023-02-07 13:55:12,083][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458962 effective words) took 0.3s, 4427479 effective words/s
[2023-02-07 13:55:12,410][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458962 effective words) took 0.3s, 4476734 effective words/s
[2023-02-07 13:55:12,742][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458962 effective words) took 0.3s, 4411795 effective words/s
[2023-02-07 13:55:13,071][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458962 effective words) took 0.3s, 4451847 effective words/s
[2023-02-07 13:55:13,399][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458962 effective words) took 0.3s, 4461926 effective words/s
[2023-02-07 13:55:13,730][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458962 effective words) took 0.3s, 4422505 effective words/s
[2023-02-07 13:55:14,059][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458962 effective words) took 0.3s, 4461987 effective words/s
[2023-02-07 13:55:14,391][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458962 effective words) took 0.3s, 4407995 effective words/s
[2023-02-07 13:55:14,721][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458962 effective words) took 0.3s, 4433219 effective words/s
[2023-02-07 13:55:15,051][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458962 effective words) took 0.3s, 4454433 effective words/s
[2023-02-07 13:55:15,382][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458962 effective words) took 0.3s, 4418731 effective words/s
[2023-02-07 13:55:15,712][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458962 effective words) took 0.3s, 4437013 effective words/s
[2023-02-07 13:55:16,044][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458962 effective words) took 0.3s, 4424523 effective words/s
[2023-02-07 13:55:16,378][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458962 effective words) took 0.3s, 4380297 effective words/s
[2023-02-07 13:55:16,715][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458962 effective words) took 0.3s, 4343927 effective words/s
[2023-02-07 13:55:17,053][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458962 effective words) took 0.3s, 4334193 effective words/s
[2023-02-07 13:55:17,390][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458962 effective words) took 0.3s, 4343181 effective words/s
[2023-02-07 13:55:17,729][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458962 effective words) took 0.3s, 4330176 effective words/s
[2023-02-07 13:55:18,069][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458962 effective words) took 0.3s, 4303909 effective words/s
[2023-02-07 13:55:18,407][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458962 effective words) took 0.3s, 4336549 effective words/s
[2023-02-07 13:55:18,743][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458962 effective words) took 0.3s, 4349943 effective words/s
[2023-02-07 13:55:19,084][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458962 effective words) took 0.3s, 4300646 effective words/s
[2023-02-07 13:55:19,428][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458962 effective words) took 0.3s, 4252407 effective words/s
[2023-02-07 13:55:19,772][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458962 effective words) took 0.3s, 4262678 effective words/s
[2023-02-07 13:55:20,119][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458962 effective words) took 0.3s, 4213652 effective words/s
[2023-02-07 13:55:20,465][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458962 effective words) took 0.3s, 4231030 effective words/s
[2023-02-07 13:55:20,813][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458962 effective words) took 0.3s, 4206125 effective words/s
[2023-02-07 13:55:21,163][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458962 effective words) took 0.3s, 4184682 effective words/s
[2023-02-07 13:55:21,513][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458962 effective words) took 0.3s, 4187182 effective words/s
[2023-02-07 13:55:21,857][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458962 effective words) took 0.3s, 4252475 effective words/s
[2023-02-07 13:55:22,200][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458962 effective words) took 0.3s, 4271122 effective words/s
[2023-02-07 13:55:22,542][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458962 effective words) took 0.3s, 4288377 effective words/s
[2023-02-07 13:55:22,884][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458962 effective words) took 0.3s, 4277936 effective words/s
[2023-02-07 13:55:23,237][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458962 effective words) took 0.4s, 4142105 effective words/s
[2023-02-07 13:55:23,593][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458962 effective words) took 0.4s, 4130984 effective words/s
[2023-02-07 13:55:23,941][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458962 effective words) took 0.3s, 4205671 effective words/s
[2023-02-07 13:55:24,293][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458962 effective words) took 0.4s, 4153132 effective words/s
[2023-02-07 13:55:24,639][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458962 effective words) took 0.3s, 4230614 effective words/s
[2023-02-07 13:55:24,985][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458962 effective words) took 0.3s, 4244357 effective words/s
[2023-02-07 13:55:25,332][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458962 effective words) took 0.3s, 4215156 effective words/s
[2023-02-07 13:55:25,687][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458962 effective words) took 0.4s, 4137257 effective words/s
[2023-02-07 13:55:26,035][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458962 effective words) took 0.3s, 4216292 effective words/s
[2023-02-07 13:55:26,387][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458962 effective words) took 0.4s, 4153049 effective words/s
[2023-02-07 13:55:26,737][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458962 effective words) took 0.3s, 4187914 effective words/s
[2023-02-07 13:55:27,087][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458962 effective words) took 0.3s, 4185137 effective words/s
[2023-02-07 13:55:27,450][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458962 effective words) took 0.4s, 4029092 effective words/s
[2023-02-07 13:55:27,805][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458962 effective words) took 0.4s, 4123633 effective words/s
[2023-02-07 13:55:28,165][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458962 effective words) took 0.4s, 4070292 effective words/s
[2023-02-07 13:55:28,514][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458962 effective words) took 0.3s, 4197269 effective words/s
[2023-02-07 13:55:28,857][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458962 effective words) took 0.3s, 4266548 effective words/s
[2023-02-07 13:55:29,201][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458962 effective words) took 0.3s, 4255496 effective words/s
[2023-02-07 13:55:29,545][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458962 effective words) took 0.3s, 4261441 effective words/s
[2023-02-07 13:55:29,994][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1458962 effective words) took 0.4s, 3250933 effective words/s
[2023-02-07 13:55:30,377][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1458962 effective words) took 0.4s, 3826882 effective words/s
[2023-02-07 13:55:30,769][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1458962 effective words) took 0.4s, 3730888 effective words/s
[2023-02-07 13:55:31,160][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1458962 effective words) took 0.4s, 3748332 effective words/s
[2023-02-07 13:55:31,548][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1458962 effective words) took 0.4s, 3769369 effective words/s
[2023-02-07 13:55:31,942][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1458962 effective words) took 0.4s, 3720825 effective words/s
[2023-02-07 13:55:32,327][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1458962 effective words) took 0.4s, 3799682 effective words/s
[2023-02-07 13:55:32,724][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1458962 effective words) took 0.4s, 3692590 effective words/s
[2023-02-07 13:55:33,113][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1458962 effective words) took 0.4s, 3767770 effective words/s
[2023-02-07 13:55:33,505][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1458962 effective words) took 0.4s, 3733543 effective words/s
[2023-02-07 13:55:33,895][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1458962 effective words) took 0.4s, 3755220 effective words/s
[2023-02-07 13:55:34,289][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1458962 effective words) took 0.4s, 3721514 effective words/s
[2023-02-07 13:55:34,678][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1458962 effective words) took 0.4s, 3765149 effective words/s
[2023-02-07 13:55:35,082][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1458962 effective words) took 0.4s, 3622561 effective words/s
[2023-02-07 13:55:35,468][gensim.models.word2vec][INFO] - EPOCH 70: training on 1455748 raw words (1458962 effective words) took 0.4s, 3791375 effective words/s
[2023-02-07 13:55:35,873][gensim.models.word2vec][INFO] - EPOCH 71: training on 1455748 raw words (1458962 effective words) took 0.4s, 3617489 effective words/s
[2023-02-07 13:55:36,264][gensim.models.word2vec][INFO] - EPOCH 72: training on 1455748 raw words (1458962 effective words) took 0.4s, 3746204 effective words/s
[2023-02-07 13:55:36,666][gensim.models.word2vec][INFO] - EPOCH 73: training on 1455748 raw words (1458962 effective words) took 0.4s, 3650375 effective words/s
[2023-02-07 13:55:37,067][gensim.models.word2vec][INFO] - EPOCH 74: training on 1455748 raw words (1458962 effective words) took 0.4s, 3649656 effective words/s
[2023-02-07 13:55:37,481][gensim.models.word2vec][INFO] - EPOCH 75: training on 1455748 raw words (1458962 effective words) took 0.4s, 3538741 effective words/s
[2023-02-07 13:55:37,875][gensim.models.word2vec][INFO] - EPOCH 76: training on 1455748 raw words (1458962 effective words) took 0.4s, 3713591 effective words/s
[2023-02-07 13:55:38,285][gensim.models.word2vec][INFO] - EPOCH 77: training on 1455748 raw words (1458962 effective words) took 0.4s, 3571334 effective words/s
[2023-02-07 13:55:38,675][gensim.models.word2vec][INFO] - EPOCH 78: training on 1455748 raw words (1458962 effective words) took 0.4s, 3754795 effective words/s
[2023-02-07 13:55:39,073][gensim.models.word2vec][INFO] - EPOCH 79: training on 1455748 raw words (1458962 effective words) took 0.4s, 3683758 effective words/s
[2023-02-07 13:55:39,471][gensim.models.word2vec][INFO] - EPOCH 80: training on 1455748 raw words (1458962 effective words) took 0.4s, 3678772 effective words/s
[2023-02-07 13:55:39,865][gensim.models.word2vec][INFO] - EPOCH 81: training on 1455748 raw words (1458962 effective words) took 0.4s, 3715292 effective words/s
[2023-02-07 13:55:40,264][gensim.models.word2vec][INFO] - EPOCH 82: training on 1455748 raw words (1458962 effective words) took 0.4s, 3675119 effective words/s
[2023-02-07 13:55:40,660][gensim.models.word2vec][INFO] - EPOCH 83: training on 1455748 raw words (1458962 effective words) took 0.4s, 3698848 effective words/s
[2023-02-07 13:55:41,061][gensim.models.word2vec][INFO] - EPOCH 84: training on 1455748 raw words (1458962 effective words) took 0.4s, 3644438 effective words/s
[2023-02-07 13:55:41,464][gensim.models.word2vec][INFO] - EPOCH 85: training on 1455748 raw words (1458962 effective words) took 0.4s, 3641287 effective words/s
[2023-02-07 13:55:41,863][gensim.models.word2vec][INFO] - EPOCH 86: training on 1455748 raw words (1458962 effective words) took 0.4s, 3671241 effective words/s
[2023-02-07 13:55:42,265][gensim.models.word2vec][INFO] - EPOCH 87: training on 1455748 raw words (1458962 effective words) took 0.4s, 3638891 effective words/s
[2023-02-07 13:55:42,669][gensim.models.word2vec][INFO] - EPOCH 88: training on 1455748 raw words (1458962 effective words) took 0.4s, 3619845 effective words/s
[2023-02-07 13:55:43,078][gensim.models.word2vec][INFO] - EPOCH 89: training on 1455748 raw words (1458962 effective words) took 0.4s, 3580961 effective words/s
[2023-02-07 13:55:43,486][gensim.models.word2vec][INFO] - EPOCH 90: training on 1455748 raw words (1458962 effective words) took 0.4s, 3593449 effective words/s
[2023-02-07 13:55:43,897][gensim.models.word2vec][INFO] - EPOCH 91: training on 1455748 raw words (1458962 effective words) took 0.4s, 3555138 effective words/s
[2023-02-07 13:55:44,307][gensim.models.word2vec][INFO] - EPOCH 92: training on 1455748 raw words (1458962 effective words) took 0.4s, 3577962 effective words/s
[2023-02-07 13:55:44,717][gensim.models.word2vec][INFO] - EPOCH 93: training on 1455748 raw words (1458962 effective words) took 0.4s, 3572875 effective words/s
[2023-02-07 13:55:44,718][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 136840312 raw words (137142428 effective words) took 34.3s, 3997256 effective words/s', 'datetime': '2023-02-07T13:55:44.718231', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:55:44.718 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:55:46,830][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135454-5epn1fq9/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:55:46.830843', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:55:46,831][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:55:46,839][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135454-5epn1fq9/files/../tmp/embedding_model.pt
2023-02-07 13:55:46.840 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:55:48.145 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:55:48.657 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:55:49.859 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.019215555678045, 'test_mae': 1.08344977375897, 'test_r2': 0.0447903134840526}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.41
wandb: percentage 0.04654
wandb:   test_mae 1.08345
wandb:   test_mse 2.01922
wandb:    test_r2 0.04479
wandb: 
wandb: üöÄ View run divine-sweep-61 at: https://wandb.ai/xiaoqiz/mof2vec/runs/5epn1fq9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_135454-5epn1fq9/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: qve0a2q4 with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 199
wandb: 	model.gensim.alpha: 0.003659103079773738
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 84
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.918404632740088
wandb: 	model.gensim.vector_size: 198
wandb: 	model.gensim.window: 2
wandb: 	model.sklearn.learning_rate: 0.03714277749066333
wandb: 	model.sklearn.max_depth: 85
wandb: 	model.sklearn.min_child_weight: 0.00586304594850253
wandb: 	model.sklearn.n_estimators: 3607
wandb: 	model.sklearn.num_leaves: 470
wandb: 	model.sklearn.reg_alpha: 0.006718561193683006
wandb: 	model.sklearn.reg_lambda: 0.17538448398505685
wandb: 	model.sklearn.subsample: 0.22597775751519
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135606-qve0a2q4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-sweep-62
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/qve0a2q4
2023-02-07 13:56:14.499 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:56:14.499 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 199 for sweep.
2023-02-07 13:56:14.500 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.003659103079773738 for sweep.
2023-02-07 13:56:14.500 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:56:14.500 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 84 for sweep.
2023-02-07 13:56:14.501 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 13:56:14.501 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.918404632740088 for sweep.
2023-02-07 13:56:14.501 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 198 for sweep.
2023-02-07 13:56:14.501 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 2 for sweep.
2023-02-07 13:56:14.501 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.03714277749066333 for sweep.
2023-02-07 13:56:14.502 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 85 for sweep.
2023-02-07 13:56:14.502 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.00586304594850253 for sweep.
2023-02-07 13:56:14.502 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3607 for sweep.
2023-02-07 13:56:14.502 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 470 for sweep.
2023-02-07 13:56:14.503 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.006718561193683006 for sweep.
2023-02-07 13:56:14.503 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.17538448398505685 for sweep.
2023-02-07 13:56:14.503 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.22597775751519 for sweep.
2023-02-07 13:56:14.503 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:56:14.508 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135606-qve0a2q4/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 199, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 198, 'window': 2, 'min_count': 2, 'dm': 0, 'sample': 0.918404632740088, 'workers': 4, 'alpha': 0.003659103079773738, 'epochs': 84}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3607, 'max_depth': 85, 'num_leaves': 470, 'reg_alpha': 0.006718561193683006, 'reg_lambda': 0.17538448398505685, 'subsample': 0.22597775751519, 'min_child_weight': 0.00586304594850253, 'n_jobs': 4, 'learning_rate': 0.03714277749066333}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 40/3257 [00:00<00:08, 395.02it/s]  2%|‚ñè         | 81/3257 [00:00<00:07, 403.76it/s]  4%|‚ñé         | 122/3257 [00:00<00:07, 396.95it/s]  5%|‚ñå         | 164/3257 [00:00<00:07, 405.04it/s]  6%|‚ñã         | 207/3257 [00:00<00:07, 412.20it/s]  8%|‚ñä         | 253/3257 [00:00<00:07, 424.80it/s]  9%|‚ñâ         | 298/3257 [00:00<00:06, 431.69it/s] 11%|‚ñà         | 342/3257 [00:00<00:06, 422.50it/s] 12%|‚ñà‚ñè        | 385/3257 [00:00<00:06, 417.61it/s] 13%|‚ñà‚ñé        | 427/3257 [00:01<00:09, 288.04it/s] 14%|‚ñà‚ñç        | 468/3257 [00:01<00:08, 314.99it/s] 16%|‚ñà‚ñå        | 510/3257 [00:01<00:08, 340.10it/s] 17%|‚ñà‚ñã        | 550/3257 [00:01<00:07, 354.08it/s] 18%|‚ñà‚ñä        | 589/3257 [00:01<00:07, 353.04it/s] 19%|‚ñà‚ñâ        | 633/3257 [00:01<00:06, 375.75it/s] 21%|‚ñà‚ñà        | 673/3257 [00:01<00:06, 375.26it/s] 22%|‚ñà‚ñà‚ñè       | 715/3257 [00:01<00:06, 386.63it/s] 23%|‚ñà‚ñà‚ñé       | 755/3257 [00:02<00:06, 382.18it/s] 24%|‚ñà‚ñà‚ñç       | 794/3257 [00:02<00:06, 383.87it/s] 26%|‚ñà‚ñà‚ñå       | 834/3257 [00:02<00:06, 386.86it/s] 27%|‚ñà‚ñà‚ñã       | 873/3257 [00:02<00:06, 383.10it/s] 28%|‚ñà‚ñà‚ñä       | 915/3257 [00:02<00:05, 391.41it/s] 29%|‚ñà‚ñà‚ñâ       | 959/3257 [00:02<00:05, 402.01it/s] 31%|‚ñà‚ñà‚ñà       | 1000/3257 [00:02<00:05, 400.13it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1041/3257 [00:02<00:05, 384.52it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1081/3257 [00:02<00:05, 388.36it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1121/3257 [00:02<00:05, 391.17it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1161/3257 [00:03<00:05, 390.56it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1201/3257 [00:03<00:05, 371.61it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1243/3257 [00:03<00:05, 384.28it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1282/3257 [00:03<00:05, 376.56it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1322/3257 [00:03<00:05, 382.28it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1363/3257 [00:03<00:04, 388.46it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1402/3257 [00:03<00:04, 383.15it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1445/3257 [00:03<00:04, 396.37it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1487/3257 [00:03<00:04, 402.78it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1528/3257 [00:03<00:04, 400.36it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1569/3257 [00:04<00:04, 391.05it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1610/3257 [00:04<00:04, 394.15it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1650/3257 [00:04<00:04, 383.03it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1689/3257 [00:04<00:04, 382.06it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:04<00:04, 381.00it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1767/3257 [00:04<00:05, 267.17it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1807/3257 [00:04<00:04, 295.02it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1847/3257 [00:04<00:04, 319.82it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1888/3257 [00:05<00:03, 342.28it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1926/3257 [00:05<00:03, 352.31it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1974/3257 [00:05<00:03, 385.21it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2015/3257 [00:05<00:03, 375.51it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2054/3257 [00:05<00:03, 368.80it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2092/3257 [00:05<00:03, 357.28it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2129/3257 [00:05<00:03, 348.72it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:05<00:03, 361.84it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2208/3257 [00:05<00:02, 368.93it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2246/3257 [00:06<00:02, 367.50it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2286/3257 [00:06<00:02, 375.02it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2328/3257 [00:06<00:02, 385.94it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2368/3257 [00:06<00:02, 389.55it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2408/3257 [00:06<00:02, 391.27it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2448/3257 [00:06<00:02, 385.16it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2494/3257 [00:06<00:01, 406.45it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2540/3257 [00:06<00:01, 417.79it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2582/3257 [00:06<00:01, 401.72it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2629/3257 [00:06<00:01, 418.65it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2672/3257 [00:07<00:01, 412.85it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2714/3257 [00:07<00:01, 394.41it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2758/3257 [00:07<00:01, 405.08it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2801/3257 [00:07<00:01, 410.75it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2843/3257 [00:07<00:01, 393.11it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2890/3257 [00:07<00:00, 412.08it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2932/3257 [00:07<00:00, 407.14it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2973/3257 [00:07<00:00, 400.99it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3014/3257 [00:07<00:00, 401.99it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3060/3257 [00:08<00:00, 417.12it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3105/3257 [00:08<00:00, 426.20it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3148/3257 [00:08<00:00, 419.01it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3190/3257 [00:08<00:00, 414.17it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3232/3257 [00:08<00:00, 285.96it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 376.35it/s]
2023-02-07 13:56:23.334 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:56:23,335][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d198,n5,mc2,s0.918405,t4>', 'datetime': '2023-02-07T13:56:23.335233', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:56:23,335][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:56:23,335][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:56:23,471][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:56:23,471][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:56:23,474][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 881 unique words (95.35% of original 924, drops 43)', 'datetime': '2023-02-07T13:56:23.474140', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:56:23,474][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 1455705 word corpus (100.00% of original 1455748, drops 43)', 'datetime': '2023-02-07T13:56:23.474320', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:56:23,478][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:56:23,478][gensim.models.word2vec][INFO] - sample=0.918405 downsamples 0 most-common words
[2023-02-07 13:56:23,478][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455705 word corpus (100.0%% of prior 1455705)', 'datetime': '2023-02-07T13:56:23.478887', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:56:23,485][gensim.models.word2vec][INFO] - estimated required memory for 881 words and 198 dimensions: 5066948 bytes
[2023-02-07 13:56:23,485][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:56:23,488][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 881 vocabulary and 198 features, using sg=1 hs=0 sample=0.918404632740088 negative=5 window=2 shrink_windows=True', 'datetime': '2023-02-07T13:56:23.488530', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:56:24,223][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458962 effective words) took 0.7s, 1990784 effective words/s
[2023-02-07 13:56:24,742][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458962 effective words) took 0.5s, 2816116 effective words/s
[2023-02-07 13:56:25,249][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458962 effective words) took 0.5s, 2890507 effective words/s
[2023-02-07 13:56:25,750][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458962 effective words) took 0.5s, 2926194 effective words/s
[2023-02-07 13:56:26,247][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458962 effective words) took 0.5s, 2943818 effective words/s
[2023-02-07 13:56:26,741][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458962 effective words) took 0.5s, 2961195 effective words/s
[2023-02-07 13:56:27,232][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458962 effective words) took 0.5s, 2978410 effective words/s
[2023-02-07 13:56:27,728][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458962 effective words) took 0.5s, 2950674 effective words/s
[2023-02-07 13:56:28,221][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458962 effective words) took 0.5s, 2969204 effective words/s
[2023-02-07 13:56:28,722][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458962 effective words) took 0.5s, 2920965 effective words/s
[2023-02-07 13:56:29,217][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458962 effective words) took 0.5s, 2956744 effective words/s
[2023-02-07 13:56:29,717][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458962 effective words) took 0.5s, 2922767 effective words/s
[2023-02-07 13:56:30,214][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458962 effective words) took 0.5s, 2941683 effective words/s
[2023-02-07 13:56:30,717][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458962 effective words) took 0.5s, 2912066 effective words/s
[2023-02-07 13:56:31,206][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458962 effective words) took 0.5s, 2986288 effective words/s
[2023-02-07 13:56:31,692][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458962 effective words) took 0.5s, 3016032 effective words/s
[2023-02-07 13:56:32,169][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458962 effective words) took 0.5s, 3065459 effective words/s
[2023-02-07 13:56:32,656][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458962 effective words) took 0.5s, 3005020 effective words/s
[2023-02-07 13:56:33,145][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458962 effective words) took 0.5s, 2995235 effective words/s
[2023-02-07 13:56:33,635][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458962 effective words) took 0.5s, 2989879 effective words/s
[2023-02-07 13:56:34,117][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458962 effective words) took 0.5s, 3036520 effective words/s
[2023-02-07 13:56:34,604][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458962 effective words) took 0.5s, 3004956 effective words/s
[2023-02-07 13:56:35,080][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458962 effective words) took 0.5s, 3071218 effective words/s
[2023-02-07 13:56:35,554][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458962 effective words) took 0.5s, 3085745 effective words/s
[2023-02-07 13:56:36,031][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458962 effective words) took 0.5s, 3070353 effective words/s
[2023-02-07 13:56:36,505][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458962 effective words) took 0.5s, 3088058 effective words/s
[2023-02-07 13:56:36,979][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458962 effective words) took 0.5s, 3085602 effective words/s
[2023-02-07 13:56:37,462][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458962 effective words) took 0.5s, 3027681 effective words/s
[2023-02-07 13:56:37,942][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458962 effective words) took 0.5s, 3043622 effective words/s
[2023-02-07 13:56:38,421][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458962 effective words) took 0.5s, 3054491 effective words/s
[2023-02-07 13:56:38,902][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458962 effective words) took 0.5s, 3042339 effective words/s
[2023-02-07 13:56:39,381][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458962 effective words) took 0.5s, 3057407 effective words/s
[2023-02-07 13:56:39,859][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458962 effective words) took 0.5s, 3064641 effective words/s
[2023-02-07 13:56:40,334][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458962 effective words) took 0.5s, 3080243 effective words/s
[2023-02-07 13:56:40,814][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458962 effective words) took 0.5s, 3054117 effective words/s
[2023-02-07 13:56:41,290][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458962 effective words) took 0.5s, 3075539 effective words/s
[2023-02-07 13:56:41,770][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458962 effective words) took 0.5s, 3049870 effective words/s
[2023-02-07 13:56:42,239][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458962 effective words) took 0.5s, 3123021 effective words/s
[2023-02-07 13:56:42,710][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458962 effective words) took 0.5s, 3106702 effective words/s
[2023-02-07 13:56:43,184][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458962 effective words) took 0.5s, 3088631 effective words/s
[2023-02-07 13:56:43,656][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458962 effective words) took 0.5s, 3098363 effective words/s
[2023-02-07 13:56:44,127][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458962 effective words) took 0.5s, 3106801 effective words/s
[2023-02-07 13:56:44,599][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458962 effective words) took 0.5s, 3095598 effective words/s
[2023-02-07 13:56:45,071][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458962 effective words) took 0.5s, 3106139 effective words/s
[2023-02-07 13:56:45,548][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458962 effective words) took 0.5s, 3072090 effective words/s
[2023-02-07 13:56:46,024][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458962 effective words) took 0.5s, 3073637 effective words/s
[2023-02-07 13:56:46,500][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458962 effective words) took 0.5s, 3080436 effective words/s
[2023-02-07 13:56:46,974][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458962 effective words) took 0.5s, 3086175 effective words/s
[2023-02-07 13:56:47,444][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458962 effective words) took 0.5s, 3110590 effective words/s
[2023-02-07 13:56:47,915][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458962 effective words) took 0.5s, 3105310 effective words/s
[2023-02-07 13:56:48,391][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458962 effective words) took 0.5s, 3071068 effective words/s
[2023-02-07 13:56:48,872][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458962 effective words) took 0.5s, 3046850 effective words/s
[2023-02-07 13:56:49,349][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458962 effective words) took 0.5s, 3068238 effective words/s
[2023-02-07 13:56:49,830][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458962 effective words) took 0.5s, 3038515 effective words/s
[2023-02-07 13:56:50,308][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458962 effective words) took 0.5s, 3062248 effective words/s
[2023-02-07 13:56:50,787][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458962 effective words) took 0.5s, 3056176 effective words/s
[2023-02-07 13:56:51,260][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1458962 effective words) took 0.5s, 3092298 effective words/s
[2023-02-07 13:56:51,736][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1458962 effective words) took 0.5s, 3077132 effective words/s
[2023-02-07 13:56:52,216][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1458962 effective words) took 0.5s, 3050913 effective words/s
[2023-02-07 13:56:52,704][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1458962 effective words) took 0.5s, 2998982 effective words/s
[2023-02-07 13:56:53,182][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1458962 effective words) took 0.5s, 3056231 effective words/s
[2023-02-07 13:56:53,674][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1458962 effective words) took 0.5s, 2977520 effective words/s
[2023-02-07 13:56:54,154][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1458962 effective words) took 0.5s, 3048663 effective words/s
[2023-02-07 13:56:54,644][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1458962 effective words) took 0.5s, 2989080 effective words/s
[2023-02-07 13:56:55,120][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1458962 effective words) took 0.5s, 3072682 effective words/s
[2023-02-07 13:56:55,597][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1458962 effective words) took 0.5s, 3070159 effective words/s
[2023-02-07 13:56:56,071][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1458962 effective words) took 0.5s, 3082704 effective words/s
[2023-02-07 13:56:56,557][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1458962 effective words) took 0.5s, 3013929 effective words/s
[2023-02-07 13:56:57,043][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1458962 effective words) took 0.5s, 3009737 effective words/s
[2023-02-07 13:56:57,527][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1458962 effective words) took 0.5s, 3026970 effective words/s
[2023-02-07 13:56:58,012][gensim.models.word2vec][INFO] - EPOCH 70: training on 1455748 raw words (1458962 effective words) took 0.5s, 3015905 effective words/s
[2023-02-07 13:56:58,493][gensim.models.word2vec][INFO] - EPOCH 71: training on 1455748 raw words (1458962 effective words) took 0.5s, 3038176 effective words/s
[2023-02-07 13:56:58,973][gensim.models.word2vec][INFO] - EPOCH 72: training on 1455748 raw words (1458962 effective words) took 0.5s, 3052523 effective words/s
[2023-02-07 13:56:59,458][gensim.models.word2vec][INFO] - EPOCH 73: training on 1455748 raw words (1458962 effective words) took 0.5s, 3015256 effective words/s
[2023-02-07 13:56:59,942][gensim.models.word2vec][INFO] - EPOCH 74: training on 1455748 raw words (1458962 effective words) took 0.5s, 3025372 effective words/s
[2023-02-07 13:57:00,431][gensim.models.word2vec][INFO] - EPOCH 75: training on 1455748 raw words (1458962 effective words) took 0.5s, 2992343 effective words/s
[2023-02-07 13:57:00,921][gensim.models.word2vec][INFO] - EPOCH 76: training on 1455748 raw words (1458962 effective words) took 0.5s, 2987785 effective words/s
[2023-02-07 13:57:01,408][gensim.models.word2vec][INFO] - EPOCH 77: training on 1455748 raw words (1458962 effective words) took 0.5s, 3004708 effective words/s
[2023-02-07 13:57:01,901][gensim.models.word2vec][INFO] - EPOCH 78: training on 1455748 raw words (1458962 effective words) took 0.5s, 2966772 effective words/s
[2023-02-07 13:57:02,393][gensim.models.word2vec][INFO] - EPOCH 79: training on 1455748 raw words (1458962 effective words) took 0.5s, 2974755 effective words/s
[2023-02-07 13:57:02,890][gensim.models.word2vec][INFO] - EPOCH 80: training on 1455748 raw words (1458962 effective words) took 0.5s, 2945856 effective words/s
[2023-02-07 13:57:03,385][gensim.models.word2vec][INFO] - EPOCH 81: training on 1455748 raw words (1458962 effective words) took 0.5s, 2956536 effective words/s
[2023-02-07 13:57:03,889][gensim.models.word2vec][INFO] - EPOCH 82: training on 1455748 raw words (1458962 effective words) took 0.5s, 2906912 effective words/s
[2023-02-07 13:57:04,393][gensim.models.word2vec][INFO] - EPOCH 83: training on 1455748 raw words (1458962 effective words) took 0.5s, 2904341 effective words/s
[2023-02-07 13:57:04,393][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 122282832 raw words (122552808 effective words) took 40.9s, 2996050 effective words/s', 'datetime': '2023-02-07T13:57:04.393590', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:57:04.393 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:57:06,423][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135606-qve0a2q4/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:57:06.423390', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:57:06,424][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:57:06,431][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135606-qve0a2q4/files/../tmp/embedding_model.pt
2023-02-07 13:57:06.432 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:57:07.849 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:57:08.394 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:57:09.787 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8584358158986563, 'test_mae': 1.0606796014596898, 'test_r2': 0.12084874340299923}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: | 0.034 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: / 0.034 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: - 0.034 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: \ 0.037 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: | 0.037 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: / 0.037 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: - 0.037 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.69
wandb: percentage 0.04654
wandb:   test_mae 1.06068
wandb:   test_mse 1.85844
wandb:    test_r2 0.12085
wandb: 
wandb: üöÄ View run toasty-sweep-62 at: https://wandb.ai/xiaoqiz/mof2vec/runs/qve0a2q4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_135606-qve0a2q4/logs
wandb: Agent Starting Run: g4m9a9a7 with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 152
wandb: 	model.gensim.alpha: 0.00045581572579083687
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 28
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.508252260391731
wandb: 	model.gensim.vector_size: 67
wandb: 	model.gensim.window: 5
wandb: 	model.sklearn.learning_rate: 0.007559425439043184
wandb: 	model.sklearn.max_depth: 56
wandb: 	model.sklearn.min_child_weight: 0.008665574272072783
wandb: 	model.sklearn.n_estimators: 2822
wandb: 	model.sklearn.num_leaves: 487
wandb: 	model.sklearn.reg_alpha: 0.0030755250404095108
wandb: 	model.sklearn.reg_lambda: 0.003111599282909514
wandb: 	model.sklearn.subsample: 0.322899733117481
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135727-g4m9a9a7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-63
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/g4m9a9a7
2023-02-07 13:57:35.736 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 13:57:35.736 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 152 for sweep.
2023-02-07 13:57:35.737 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.00045581572579083687 for sweep.
2023-02-07 13:57:35.737 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:57:35.737 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 28 for sweep.
2023-02-07 13:57:35.737 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 13:57:35.737 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.508252260391731 for sweep.
2023-02-07 13:57:35.738 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 67 for sweep.
2023-02-07 13:57:35.738 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 5 for sweep.
2023-02-07 13:57:35.738 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.007559425439043184 for sweep.
2023-02-07 13:57:35.739 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 56 for sweep.
2023-02-07 13:57:35.739 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.008665574272072783 for sweep.
2023-02-07 13:57:35.740 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2822 for sweep.
2023-02-07 13:57:35.740 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 487 for sweep.
2023-02-07 13:57:35.740 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.0030755250404095108 for sweep.
2023-02-07 13:57:35.740 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.003111599282909514 for sweep.
2023-02-07 13:57:35.740 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.322899733117481 for sweep.
2023-02-07 13:57:35.741 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:57:35.745 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135727-g4m9a9a7/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 152, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 67, 'window': 5, 'min_count': 1, 'dm': 0, 'sample': 0.508252260391731, 'workers': 4, 'alpha': 0.00045581572579083687, 'epochs': 28}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2822, 'max_depth': 56, 'num_leaves': 487, 'reg_alpha': 0.0030755250404095108, 'reg_lambda': 0.003111599282909514, 'subsample': 0.322899733117481, 'min_child_weight': 0.008665574272072783, 'n_jobs': 4, 'learning_rate': 0.007559425439043184}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 33/3257 [00:00<00:10, 320.12it/s]  2%|‚ñè         | 67/3257 [00:00<00:09, 319.80it/s]  3%|‚ñé         | 100/3257 [00:00<00:09, 323.81it/s]  4%|‚ñç         | 134/3257 [00:00<00:09, 324.87it/s]  5%|‚ñå         | 169/3257 [00:00<00:09, 333.12it/s]  6%|‚ñå         | 203/3257 [00:00<00:09, 325.27it/s]  7%|‚ñã         | 242/3257 [00:00<00:08, 344.36it/s]  9%|‚ñä         | 277/3257 [00:00<00:08, 345.87it/s] 10%|‚ñâ         | 313/3257 [00:00<00:08, 347.06it/s] 11%|‚ñà         | 349/3257 [00:01<00:08, 349.65it/s] 12%|‚ñà‚ñè        | 384/3257 [00:01<00:08, 344.18it/s] 13%|‚ñà‚ñé        | 419/3257 [00:01<00:08, 342.53it/s] 14%|‚ñà‚ñç        | 454/3257 [00:01<00:08, 317.40it/s] 15%|‚ñà‚ñç        | 488/3257 [00:01<00:08, 322.58it/s] 16%|‚ñà‚ñå        | 524/3257 [00:01<00:08, 332.86it/s] 17%|‚ñà‚ñã        | 558/3257 [00:01<00:08, 313.15it/s] 18%|‚ñà‚ñä        | 590/3257 [00:01<00:08, 308.84it/s] 19%|‚ñà‚ñâ        | 624/3257 [00:01<00:08, 317.53it/s] 20%|‚ñà‚ñà        | 656/3257 [00:02<00:08, 317.05it/s] 21%|‚ñà‚ñà        | 688/3257 [00:02<00:08, 316.26it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:02<00:07, 321.41it/s] 23%|‚ñà‚ñà‚ñé       | 755/3257 [00:02<00:07, 322.96it/s] 24%|‚ñà‚ñà‚ñç       | 788/3257 [00:02<00:07, 324.58it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:02<00:07, 323.85it/s] 26%|‚ñà‚ñà‚ñã       | 856/3257 [00:02<00:07, 319.71it/s] 27%|‚ñà‚ñà‚ñã       | 890/3257 [00:02<00:07, 324.19it/s] 28%|‚ñà‚ñà‚ñä       | 927/3257 [00:02<00:06, 336.07it/s] 30%|‚ñà‚ñà‚ñâ       | 961/3257 [00:02<00:06, 337.04it/s] 31%|‚ñà‚ñà‚ñà       | 995/3257 [00:03<00:06, 332.14it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1029/3257 [00:03<00:06, 328.19it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1062/3257 [00:03<00:06, 323.75it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1095/3257 [00:03<00:06, 323.25it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1128/3257 [00:03<00:06, 324.56it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1163/3257 [00:03<00:06, 330.92it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1197/3257 [00:03<00:09, 223.31it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1232/3257 [00:03<00:08, 248.29it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1265/3257 [00:04<00:07, 266.99it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1296/3257 [00:04<00:07, 268.01it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1331/3257 [00:04<00:06, 287.07it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1364/3257 [00:04<00:06, 297.61it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1397/3257 [00:04<00:06, 305.24it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1434/3257 [00:04<00:05, 322.01it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1472/3257 [00:04<00:05, 338.00it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:04<00:04, 349.43it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1547/3257 [00:04<00:05, 326.63it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1581/3257 [00:04<00:05, 320.76it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1617/3257 [00:05<00:04, 331.46it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1651/3257 [00:05<00:05, 313.50it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1683/3257 [00:05<00:05, 309.70it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1716/3257 [00:05<00:04, 314.11it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1748/3257 [00:05<00:04, 307.61it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1784/3257 [00:05<00:04, 320.61it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1817/3257 [00:05<00:04, 317.07it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1850/3257 [00:05<00:04, 318.64it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1885/3257 [00:05<00:04, 326.90it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1918/3257 [00:06<00:04, 324.86it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1960/3257 [00:06<00:03, 350.58it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1996/3257 [00:06<00:03, 347.55it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2032/3257 [00:06<00:03, 347.93it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2067/3257 [00:06<00:03, 320.50it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2100/3257 [00:06<00:03, 319.61it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2133/3257 [00:06<00:03, 315.60it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2166/3257 [00:06<00:03, 317.19it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2200/3257 [00:06<00:03, 322.41it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2233/3257 [00:07<00:03, 320.22it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2266/3257 [00:07<00:03, 315.08it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:07<00:03, 315.01it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2339/3257 [00:07<00:02, 337.48it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2375/3257 [00:07<00:02, 343.09it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2410/3257 [00:07<00:02, 340.27it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2445/3257 [00:07<00:02, 335.98it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2481/3257 [00:07<00:02, 340.03it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2519/3257 [00:07<00:02, 350.57it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2555/3257 [00:07<00:02, 344.70it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2590/3257 [00:08<00:02, 329.61it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2632/3257 [00:08<00:01, 352.98it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2668/3257 [00:08<00:01, 344.66it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2703/3257 [00:08<00:02, 227.09it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2738/3257 [00:08<00:02, 252.20it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2772/3257 [00:08<00:01, 270.90it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2809/3257 [00:08<00:01, 295.00it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2842/3257 [00:08<00:01, 296.13it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2883/3257 [00:09<00:01, 326.37it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2918/3257 [00:09<00:01, 322.91it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2952/3257 [00:09<00:00, 312.87it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2986/3257 [00:09<00:00, 315.93it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3022/3257 [00:09<00:00, 325.52it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3063/3257 [00:09<00:00, 346.80it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3099/3257 [00:09<00:00, 348.13it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3135/3257 [00:09<00:00, 349.73it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3171/3257 [00:09<00:00, 342.78it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3206/3257 [00:10<00:00, 338.83it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3241/3257 [00:10<00:00, 340.68it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:10<00:00, 319.78it/s]
2023-02-07 13:57:46.195 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:57:46,198][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d67,n5,s0.508252,t4>', 'datetime': '2023-02-07T13:57:46.198014', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:57:46,198][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:57:46,198][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:57:46,401][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 13:57:46,401][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:57:46,408][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 2819 unique words (100.00% of original 2819, drops 0)', 'datetime': '2023-02-07T13:57:46.408716', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:57:46,409][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 2183622 word corpus (100.00% of original 2183622, drops 0)', 'datetime': '2023-02-07T13:57:46.409969', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:57:46,419][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 13:57:46,420][gensim.models.word2vec][INFO] - sample=0.508252 downsamples 0 most-common words
[2023-02-07 13:57:46,420][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2183622 word corpus (100.0%% of prior 2183622)', 'datetime': '2023-02-07T13:57:46.420243', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:57:46,438][gensim.models.word2vec][INFO] - estimated required memory for 2819 words and 67 dimensions: 4444760 bytes
[2023-02-07 13:57:46,438][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:57:46,440][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2819 vocabulary and 67 features, using sg=1 hs=0 sample=0.508252260391731 negative=5 window=5 shrink_windows=True', 'datetime': '2023-02-07T13:57:46.440848', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:57:47,135][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2186879 effective words) took 0.7s, 3158736 effective words/s
[2023-02-07 13:57:47,818][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2186879 effective words) took 0.7s, 3211833 effective words/s
[2023-02-07 13:57:48,494][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2186879 effective words) took 0.7s, 3242732 effective words/s
[2023-02-07 13:57:49,167][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2186879 effective words) took 0.7s, 3253056 effective words/s
[2023-02-07 13:57:49,846][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2186879 effective words) took 0.7s, 3229477 effective words/s
[2023-02-07 13:57:50,530][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2186879 effective words) took 0.7s, 3201371 effective words/s
[2023-02-07 13:57:51,208][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2186879 effective words) took 0.7s, 3236631 effective words/s
[2023-02-07 13:57:51,891][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2186879 effective words) took 0.7s, 3208803 effective words/s
[2023-02-07 13:57:52,579][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2186879 effective words) took 0.7s, 3181845 effective words/s
[2023-02-07 13:57:53,259][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2186879 effective words) took 0.7s, 3223548 effective words/s
[2023-02-07 13:57:53,937][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2186879 effective words) took 0.7s, 3230923 effective words/s
[2023-02-07 13:57:54,620][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2186879 effective words) took 0.7s, 3210662 effective words/s
[2023-02-07 13:57:55,300][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2186879 effective words) took 0.7s, 3222289 effective words/s
[2023-02-07 13:57:55,982][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2186879 effective words) took 0.7s, 3210628 effective words/s
[2023-02-07 13:57:56,655][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2186879 effective words) took 0.7s, 3257145 effective words/s
[2023-02-07 13:57:57,335][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2186879 effective words) took 0.7s, 3221595 effective words/s
[2023-02-07 13:57:58,013][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2186879 effective words) took 0.7s, 3232463 effective words/s
[2023-02-07 13:57:58,695][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2186879 effective words) took 0.7s, 3213856 effective words/s
[2023-02-07 13:57:59,374][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2186879 effective words) took 0.7s, 3225050 effective words/s
[2023-02-07 13:58:00,052][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2186879 effective words) took 0.7s, 3234523 effective words/s
[2023-02-07 13:58:00,733][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2186879 effective words) took 0.7s, 3214350 effective words/s
[2023-02-07 13:58:01,419][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2186879 effective words) took 0.7s, 3199198 effective words/s
[2023-02-07 13:58:02,119][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2186879 effective words) took 0.7s, 3128028 effective words/s
[2023-02-07 13:58:02,817][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2186879 effective words) took 0.7s, 3140306 effective words/s
[2023-02-07 13:58:03,506][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2186879 effective words) took 0.7s, 3180579 effective words/s
[2023-02-07 13:58:04,198][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2186879 effective words) took 0.7s, 3162997 effective words/s
[2023-02-07 13:58:04,880][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2186879 effective words) took 0.7s, 3218241 effective words/s
[2023-02-07 13:58:05,570][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2186879 effective words) took 0.7s, 3176103 effective words/s
[2023-02-07 13:58:05,570][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 61141416 raw words (61232612 effective words) took 19.1s, 3200977 effective words/s', 'datetime': '2023-02-07T13:58:05.570488', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:58:05.570 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:58:06,755][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135727-g4m9a9a7/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:58:06.755770', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:58:06,756][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:58:06,762][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135727-g4m9a9a7/files/../tmp/embedding_model.pt
2023-02-07 13:58:06.762 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:58:07.794 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:58:08.206 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:58:08.750 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.01921798856539, 'test_mae': 1.0653567922992941, 'test_r2': 0.04478916258287635}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.014 MB of 0.028 MB uploaded (0.000 MB deduped)wandb: / 0.014 MB of 0.028 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.4
wandb: percentage 0.0
wandb:   test_mae 1.06536
wandb:   test_mse 2.01922
wandb:    test_r2 0.04479
wandb: 
wandb: üöÄ View run prime-sweep-63 at: https://wandb.ai/xiaoqiz/mof2vec/runs/g4m9a9a7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_135727-g4m9a9a7/logs
wandb: Agent Starting Run: 2anmtur2 with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 499
wandb: 	model.gensim.alpha: 0.00106465877399146
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.epochs: 22
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.3772583336696213
wandb: 	model.gensim.vector_size: 144
wandb: 	model.gensim.window: 2
wandb: 	model.sklearn.learning_rate: 0.20685297072637795
wandb: 	model.sklearn.max_depth: 25
wandb: 	model.sklearn.min_child_weight: 0.027186283895550452
wandb: 	model.sklearn.n_estimators: 696
wandb: 	model.sklearn.num_leaves: 318
wandb: 	model.sklearn.reg_alpha: 0.017835462318320495
wandb: 	model.sklearn.reg_lambda: 0.8491637691736089
wandb: 	model.sklearn.subsample: 0.2703100736949816
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135821-2anmtur2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-64
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/2anmtur2
2023-02-07 13:58:31.670 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 13:58:31.671 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 499 for sweep.
2023-02-07 13:58:31.671 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.00106465877399146 for sweep.
2023-02-07 13:58:31.671 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 13:58:31.671 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 22 for sweep.
2023-02-07 13:58:31.672 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 13:58:31.672 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3772583336696213 for sweep.
2023-02-07 13:58:31.672 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 144 for sweep.
2023-02-07 13:58:31.672 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 2 for sweep.
2023-02-07 13:58:31.673 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.20685297072637795 for sweep.
2023-02-07 13:58:31.673 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 25 for sweep.
2023-02-07 13:58:31.673 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.027186283895550452 for sweep.
2023-02-07 13:58:31.673 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 696 for sweep.
2023-02-07 13:58:31.673 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 318 for sweep.
2023-02-07 13:58:31.674 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.017835462318320495 for sweep.
2023-02-07 13:58:31.674 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.8491637691736089 for sweep.
2023-02-07 13:58:31.674 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2703100736949816 for sweep.
2023-02-07 13:58:31.674 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:58:31.681 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135821-2anmtur2/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 499, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 144, 'window': 2, 'min_count': 4, 'dm': 1, 'sample': 0.3772583336696213, 'workers': 4, 'alpha': 0.00106465877399146, 'epochs': 22}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 696, 'max_depth': 25, 'num_leaves': 318, 'reg_alpha': 0.017835462318320495, 'reg_lambda': 0.8491637691736089, 'subsample': 0.2703100736949816, 'min_child_weight': 0.027186283895550452, 'n_jobs': 4, 'learning_rate': 0.20685297072637795}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 25/3257 [00:00<00:12, 249.20it/s]  2%|‚ñè         | 51/3257 [00:00<00:12, 255.11it/s]  2%|‚ñè         | 79/3257 [00:00<00:11, 265.40it/s]  3%|‚ñé         | 106/3257 [00:00<00:12, 254.23it/s]  4%|‚ñç         | 134/3257 [00:00<00:11, 260.37it/s]  5%|‚ñå         | 163/3257 [00:00<00:11, 264.22it/s]  6%|‚ñå         | 193/3257 [00:00<00:11, 274.06it/s]  7%|‚ñã         | 223/3257 [00:00<00:10, 281.29it/s]  8%|‚ñä         | 253/3257 [00:00<00:10, 284.14it/s]  9%|‚ñâ         | 285/3257 [00:01<00:10, 293.30it/s] 10%|‚ñâ         | 315/3257 [00:01<00:10, 288.57it/s] 11%|‚ñà         | 344/3257 [00:01<00:10, 288.02it/s] 12%|‚ñà‚ñè        | 375/3257 [00:01<00:10, 286.65it/s] 12%|‚ñà‚ñè        | 404/3257 [00:01<00:10, 277.04it/s] 13%|‚ñà‚ñé        | 432/3257 [00:01<00:10, 260.96it/s] 14%|‚ñà‚ñç        | 460/3257 [00:01<00:10, 265.80it/s] 15%|‚ñà‚ñå        | 490/3257 [00:01<00:10, 273.34it/s] 16%|‚ñà‚ñå        | 522/3257 [00:01<00:09, 281.34it/s] 17%|‚ñà‚ñã        | 551/3257 [00:02<00:09, 278.26it/s] 18%|‚ñà‚ñä        | 579/3257 [00:02<00:10, 259.34it/s] 19%|‚ñà‚ñä        | 609/3257 [00:02<00:09, 268.11it/s] 20%|‚ñà‚ñâ        | 638/3257 [00:02<00:09, 273.81it/s] 20%|‚ñà‚ñà        | 666/3257 [00:02<00:09, 263.19it/s] 21%|‚ñà‚ñà‚ñè       | 695/3257 [00:02<00:09, 268.84it/s] 22%|‚ñà‚ñà‚ñè       | 723/3257 [00:02<00:09, 268.83it/s] 23%|‚ñà‚ñà‚ñé       | 751/3257 [00:02<00:09, 269.50it/s] 24%|‚ñà‚ñà‚ñç       | 781/3257 [00:02<00:08, 275.93it/s] 25%|‚ñà‚ñà‚ñç       | 811/3257 [00:02<00:08, 282.34it/s] 26%|‚ñà‚ñà‚ñå       | 840/3257 [00:03<00:12, 195.11it/s] 27%|‚ñà‚ñà‚ñã       | 866/3257 [00:03<00:11, 209.42it/s] 27%|‚ñà‚ñà‚ñã       | 895/3257 [00:03<00:10, 227.78it/s] 28%|‚ñà‚ñà‚ñä       | 927/3257 [00:03<00:09, 250.68it/s] 29%|‚ñà‚ñà‚ñâ       | 957/3257 [00:03<00:08, 262.60it/s] 30%|‚ñà‚ñà‚ñà       | 986/3257 [00:03<00:08, 266.18it/s] 31%|‚ñà‚ñà‚ñà       | 1014/3257 [00:03<00:08, 267.21it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1042/3257 [00:03<00:08, 264.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1076/3257 [00:04<00:07, 284.72it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1105/3257 [00:04<00:07, 279.09it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1134/3257 [00:04<00:07, 281.06it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1163/3257 [00:04<00:07, 281.14it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1192/3257 [00:04<00:07, 265.91it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1219/3257 [00:04<00:07, 264.26it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1251/3257 [00:04<00:07, 277.70it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1279/3257 [00:04<00:07, 274.35it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1307/3257 [00:04<00:07, 274.74it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:04<00:06, 283.11it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1368/3257 [00:05<00:06, 279.96it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1397/3257 [00:05<00:06, 281.15it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:05<00:06, 294.53it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1466/3257 [00:05<00:05, 307.15it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1501/3257 [00:05<00:05, 317.05it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1533/3257 [00:05<00:05, 299.00it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1564/3257 [00:05<00:05, 293.63it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1594/3257 [00:05<00:05, 294.24it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1627/3257 [00:05<00:05, 303.45it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1658/3257 [00:06<00:05, 288.80it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1688/3257 [00:06<00:05, 286.67it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1718/3257 [00:06<00:05, 289.59it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1748/3257 [00:06<00:05, 281.08it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1779/3257 [00:06<00:05, 288.92it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1809/3257 [00:06<00:05, 287.83it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1840/3257 [00:06<00:04, 294.04it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1872/3257 [00:06<00:04, 300.38it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1903/3257 [00:06<00:04, 301.45it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1935/3257 [00:07<00:04, 304.96it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1973/3257 [00:07<00:03, 324.77it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2006/3257 [00:07<00:03, 312.96it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2039/3257 [00:07<00:03, 313.88it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2071/3257 [00:07<00:05, 205.20it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2099/3257 [00:07<00:05, 219.66it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2127/3257 [00:07<00:04, 233.05it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2155/3257 [00:07<00:04, 242.96it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2185/3257 [00:08<00:04, 257.33it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2215/3257 [00:08<00:03, 267.87it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2244/3257 [00:08<00:03, 271.79it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2273/3257 [00:08<00:03, 271.01it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2306/3257 [00:08<00:03, 287.60it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2343/3257 [00:08<00:02, 310.27it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2377/3257 [00:08<00:02, 317.73it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2410/3257 [00:08<00:02, 313.97it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2442/3257 [00:08<00:02, 305.35it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2476/3257 [00:08<00:02, 314.33it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2513/3257 [00:09<00:02, 328.25it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2546/3257 [00:09<00:02, 323.58it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2579/3257 [00:09<00:02, 300.38it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2614/3257 [00:09<00:02, 314.01it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2646/3257 [00:09<00:01, 314.59it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2678/3257 [00:09<00:01, 312.81it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2710/3257 [00:09<00:01, 293.24it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2744/3257 [00:09<00:01, 305.57it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2775/3257 [00:09<00:01, 300.39it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2810/3257 [00:10<00:01, 312.01it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2842/3257 [00:10<00:01, 299.91it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2881/3257 [00:10<00:01, 324.03it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2914/3257 [00:10<00:01, 314.08it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2946/3257 [00:10<00:01, 303.89it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2977/3257 [00:10<00:00, 298.71it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3011/3257 [00:10<00:00, 308.00it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3044/3257 [00:10<00:00, 314.02it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:10<00:00, 323.66it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3113/3257 [00:10<00:00, 327.94it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3146/3257 [00:11<00:00, 315.01it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3178/3257 [00:11<00:00, 304.11it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3212/3257 [00:11<00:00, 313.45it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3245/3257 [00:11<00:00, 317.30it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 284.09it/s]
2023-02-07 13:58:43.518 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:58:43,519][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d144,n5,w2,mc4,s0.377258,t4>', 'datetime': '2023-02-07T13:58:43.519592', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:58:43,520][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:58:43,520][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:58:43,775][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 13:58:43,776][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:58:43,788][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 4674 unique words (70.16% of original 6662, drops 1988)', 'datetime': '2023-02-07T13:58:43.788870', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:58:43,789][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 2908210 word corpus (99.89% of original 2911496, drops 3286)', 'datetime': '2023-02-07T13:58:43.789196', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:58:43,804][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 13:58:43,805][gensim.models.word2vec][INFO] - sample=0.377258 downsamples 0 most-common words
[2023-02-07 13:58:43,805][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2908210 word corpus (100.0%% of prior 2908210)', 'datetime': '2023-02-07T13:58:43.805158', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:58:43,832][gensim.models.word2vec][INFO] - estimated required memory for 4674 words and 144 dimensions: 10248880 bytes
[2023-02-07 13:58:43,832][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:58:43,836][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 4674 vocabulary and 144 features, using sg=0 hs=0 sample=0.3772583336696213 negative=5 window=2 shrink_windows=True', 'datetime': '2023-02-07T13:58:43.836817', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:58:44,839][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 90.48% examples, 2651390 words/s, in_qsize 7, out_qsize 0
[2023-02-07 13:58:44,927][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2911467 effective words) took 1.1s, 2674188 effective words/s
[2023-02-07 13:58:45,918][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2911467 effective words) took 1.0s, 2940548 effective words/s
[2023-02-07 13:58:46,880][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2911467 effective words) took 1.0s, 3030081 effective words/s
[2023-02-07 13:58:47,825][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2911467 effective words) took 0.9s, 3085952 effective words/s
[2023-02-07 13:58:48,769][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2911467 effective words) took 0.9s, 3087771 effective words/s
[2023-02-07 13:58:49,717][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2911467 effective words) took 0.9s, 3075769 effective words/s
[2023-02-07 13:58:50,661][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2911467 effective words) took 0.9s, 3086278 effective words/s
[2023-02-07 13:58:51,589][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2911467 effective words) took 0.9s, 3140738 effective words/s
[2023-02-07 13:58:52,510][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2911467 effective words) took 0.9s, 3165815 effective words/s
[2023-02-07 13:58:53,448][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2911467 effective words) took 0.9s, 3105763 effective words/s
[2023-02-07 13:58:54,365][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2911467 effective words) took 0.9s, 3180918 effective words/s
[2023-02-07 13:58:55,286][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2911467 effective words) took 0.9s, 3165307 effective words/s
[2023-02-07 13:58:56,214][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2911467 effective words) took 0.9s, 3139722 effective words/s
[2023-02-07 13:58:57,131][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2911467 effective words) took 0.9s, 3179643 effective words/s
[2023-02-07 13:58:58,047][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2911467 effective words) took 0.9s, 3181318 effective words/s
[2023-02-07 13:58:58,966][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2911467 effective words) took 0.9s, 3173567 effective words/s
[2023-02-07 13:58:59,871][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2911467 effective words) took 0.9s, 3221302 effective words/s
[2023-02-07 13:59:00,802][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2911467 effective words) took 0.9s, 3129655 effective words/s
[2023-02-07 13:59:01,715][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2911467 effective words) took 0.9s, 3194767 effective words/s
[2023-02-07 13:59:02,627][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2911467 effective words) took 0.9s, 3195821 effective words/s
[2023-02-07 13:59:03,540][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2911467 effective words) took 0.9s, 3193616 effective words/s
[2023-02-07 13:59:04,474][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2911467 effective words) took 0.9s, 3121159 effective words/s
[2023-02-07 13:59:04,474][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 64052912 raw words (64052274 effective words) took 20.6s, 3103675 effective words/s', 'datetime': '2023-02-07T13:59:04.474680', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 13:59:04.474 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 13:59:06,258][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135821-2anmtur2/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T13:59:06.258339', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 13:59:06,259][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 13:59:06,279][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135821-2anmtur2/files/../tmp/embedding_model.pt
2023-02-07 13:59:06.279 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 13:59:07.574 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 13:59:08.044 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 13:59:09.056 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0025358430267244, 'test_mae': 1.0875946936180438, 'test_r2': 0.052680814846347124}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.59
wandb: percentage 0.29841
wandb:   test_mae 1.08759
wandb:   test_mse 2.00254
wandb:    test_r2 0.05268
wandb: 
wandb: üöÄ View run divine-sweep-64 at: https://wandb.ai/xiaoqiz/mof2vec/runs/2anmtur2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_135821-2anmtur2/logs
wandb: Agent Starting Run: 8j01lo85 with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 594
wandb: 	model.gensim.alpha: 0.00035782233190787845
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 65
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.30186666604970713
wandb: 	model.gensim.vector_size: 212
wandb: 	model.gensim.window: 1
wandb: 	model.sklearn.learning_rate: 0.004404241399927319
wandb: 	model.sklearn.max_depth: 48
wandb: 	model.sklearn.min_child_weight: 0.01895851881924575
wandb: 	model.sklearn.n_estimators: 823
wandb: 	model.sklearn.num_leaves: 497
wandb: 	model.sklearn.reg_alpha: 0.013423600340209191
wandb: 	model.sklearn.reg_lambda: 0.20634978110550312
wandb: 	model.sklearn.subsample: 0.367746455163102
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135918-8j01lo85
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-sweep-65
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/8j01lo85
2023-02-07 13:59:26.995 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 13:59:26.996 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 594 for sweep.
2023-02-07 13:59:26.996 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.00035782233190787845 for sweep.
2023-02-07 13:59:26.996 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 13:59:26.997 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 65 for sweep.
2023-02-07 13:59:26.997 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 13:59:26.998 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.30186666604970713 for sweep.
2023-02-07 13:59:26.998 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 212 for sweep.
2023-02-07 13:59:26.998 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 1 for sweep.
2023-02-07 13:59:26.999 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.004404241399927319 for sweep.
2023-02-07 13:59:26.999 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 48 for sweep.
2023-02-07 13:59:26.999 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.01895851881924575 for sweep.
2023-02-07 13:59:26.999 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 823 for sweep.
2023-02-07 13:59:26.999 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 497 for sweep.
2023-02-07 13:59:27.000 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.013423600340209191 for sweep.
2023-02-07 13:59:27.000 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.20634978110550312 for sweep.
2023-02-07 13:59:27.000 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.367746455163102 for sweep.
2023-02-07 13:59:27.000 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 13:59:27.006 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135918-8j01lo85/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 594, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 212, 'window': 1, 'min_count': 3, 'dm': 0, 'sample': 0.30186666604970713, 'workers': 4, 'alpha': 0.00035782233190787845, 'epochs': 65}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 823, 'max_depth': 48, 'num_leaves': 497, 'reg_alpha': 0.013423600340209191, 'reg_lambda': 0.20634978110550312, 'subsample': 0.367746455163102, 'min_child_weight': 0.01895851881924575, 'n_jobs': 4, 'learning_rate': 0.004404241399927319}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 39/3257 [00:00<00:08, 387.80it/s]  2%|‚ñè         | 78/3257 [00:00<00:14, 222.74it/s]  4%|‚ñé         | 117/3257 [00:00<00:11, 276.44it/s]  5%|‚ñç         | 161/3257 [00:00<00:09, 326.44it/s]  6%|‚ñå         | 201/3257 [00:00<00:08, 348.41it/s]  8%|‚ñä         | 248/3257 [00:00<00:07, 385.58it/s]  9%|‚ñâ         | 294/3257 [00:00<00:07, 405.97it/s] 10%|‚ñà         | 337/3257 [00:00<00:07, 407.70it/s] 12%|‚ñà‚ñè        | 379/3257 [00:01<00:07, 407.68it/s] 13%|‚ñà‚ñé        | 421/3257 [00:01<00:06, 411.06it/s] 14%|‚ñà‚ñç        | 463/3257 [00:01<00:07, 396.37it/s] 16%|‚ñà‚ñå        | 505/3257 [00:01<00:06, 400.48it/s] 17%|‚ñà‚ñã        | 547/3257 [00:01<00:06, 403.74it/s] 18%|‚ñà‚ñä        | 588/3257 [00:01<00:06, 393.42it/s] 19%|‚ñà‚ñâ        | 632/3257 [00:01<00:06, 406.16it/s] 21%|‚ñà‚ñà        | 673/3257 [00:01<00:06, 397.51it/s] 22%|‚ñà‚ñà‚ñè       | 715/3257 [00:01<00:06, 401.59it/s] 23%|‚ñà‚ñà‚ñé       | 756/3257 [00:01<00:06, 395.34it/s] 24%|‚ñà‚ñà‚ñç       | 797/3257 [00:02<00:06, 397.57it/s] 26%|‚ñà‚ñà‚ñå       | 837/3257 [00:02<00:06, 392.48it/s] 27%|‚ñà‚ñà‚ñã       | 877/3257 [00:02<00:06, 387.44it/s] 28%|‚ñà‚ñà‚ñä       | 923/3257 [00:02<00:05, 406.41it/s] 30%|‚ñà‚ñà‚ñâ       | 966/3257 [00:02<00:05, 411.63it/s] 31%|‚ñà‚ñà‚ñà       | 1008/3257 [00:02<00:05, 397.43it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1048/3257 [00:02<00:05, 389.83it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1088/3257 [00:02<00:05, 392.60it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1128/3257 [00:02<00:05, 390.54it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1168/3257 [00:03<00:05, 387.08it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1207/3257 [00:03<00:05, 368.26it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1248/3257 [00:03<00:05, 379.98it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1287/3257 [00:03<00:07, 275.42it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:03<00:06, 303.33it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1368/3257 [00:03<00:05, 326.16it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1410/3257 [00:03<00:05, 348.60it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1454/3257 [00:03<00:04, 372.69it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1501/3257 [00:03<00:04, 398.23it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1543/3257 [00:04<00:04, 389.99it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1584/3257 [00:04<00:04, 387.38it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1624/3257 [00:04<00:04, 390.45it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1664/3257 [00:04<00:04, 380.70it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1705/3257 [00:04<00:04, 387.77it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1745/3257 [00:04<00:04, 376.92it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1790/3257 [00:04<00:03, 397.58it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1831/3257 [00:04<00:03, 393.78it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1876/3257 [00:04<00:03, 408.25it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1918/3257 [00:05<00:03, 406.74it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1969/3257 [00:05<00:02, 434.44it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2013/3257 [00:05<00:02, 424.02it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2056/3257 [00:05<00:02, 406.89it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2097/3257 [00:05<00:02, 404.04it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2138/3257 [00:05<00:02, 400.37it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2180/3257 [00:05<00:02, 403.72it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2222/3257 [00:05<00:02, 405.10it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2263/3257 [00:05<00:02, 402.50it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2304/3257 [00:06<00:02, 402.68it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2354/3257 [00:06<00:02, 431.00it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2399/3257 [00:06<00:01, 434.98it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2443/3257 [00:06<00:01, 417.45it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2489/3257 [00:06<00:01, 427.24it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2537/3257 [00:06<00:01, 439.67it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2582/3257 [00:06<00:01, 420.83it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2629/3257 [00:06<00:01, 431.55it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2673/3257 [00:07<00:01, 296.18it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2710/3257 [00:07<00:01, 312.14it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2757/3257 [00:07<00:01, 348.14it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2801/3257 [00:07<00:01, 370.51it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2842/3257 [00:07<00:01, 373.57it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2890/3257 [00:07<00:00, 402.12it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2933/3257 [00:07<00:00, 403.57it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2975/3257 [00:07<00:00, 404.77it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3017/3257 [00:07<00:00, 402.24it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3064/3257 [00:07<00:00, 420.46it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3109/3257 [00:08<00:00, 428.72it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3153/3257 [00:08<00:00, 426.22it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3197/3257 [00:08<00:00, 426.06it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3242/3257 [00:08<00:00, 432.64it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 388.75it/s]
2023-02-07 13:59:35.552 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 13:59:35,553][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d212,n5,mc3,s0.301867,t4>', 'datetime': '2023-02-07T13:59:35.553244', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 13:59:35,553][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 13:59:35,553][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 13:59:35,685][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 13:59:35,686][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 13:59:35,688][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 768 unique words (83.12% of original 924, drops 156)', 'datetime': '2023-02-07T13:59:35.688005', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:59:35,689][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 1455479 word corpus (99.98% of original 1455748, drops 269)', 'datetime': '2023-02-07T13:59:35.689489', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:59:35,693][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 13:59:35,694][gensim.models.word2vec][INFO] - sample=0.301867 downsamples 0 most-common words
[2023-02-07 13:59:35,694][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455479 word corpus (100.0%% of prior 1455479)', 'datetime': '2023-02-07T13:59:35.694159', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 13:59:35,698][gensim.models.word2vec][INFO] - estimated required memory for 768 words and 212 dimensions: 5099864 bytes
[2023-02-07 13:59:35,698][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 13:59:35,702][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 768 vocabulary and 212 features, using sg=1 hs=0 sample=0.30186666604970713 negative=5 window=1 shrink_windows=True', 'datetime': '2023-02-07T13:59:35.702172', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 13:59:36,492][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458736 effective words) took 0.8s, 1851151 effective words/s
[2023-02-07 13:59:37,273][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458736 effective words) took 0.8s, 1871622 effective words/s
[2023-02-07 13:59:38,060][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458736 effective words) took 0.8s, 1856956 effective words/s
[2023-02-07 13:59:38,847][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458736 effective words) took 0.8s, 1857583 effective words/s
[2023-02-07 13:59:39,640][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458736 effective words) took 0.8s, 1843615 effective words/s
[2023-02-07 13:59:40,428][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458736 effective words) took 0.8s, 1854543 effective words/s
[2023-02-07 13:59:41,216][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458736 effective words) took 0.8s, 1855234 effective words/s
[2023-02-07 13:59:42,011][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458736 effective words) took 0.8s, 1839237 effective words/s
[2023-02-07 13:59:42,816][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458736 effective words) took 0.8s, 1815912 effective words/s
[2023-02-07 13:59:43,616][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458736 effective words) took 0.8s, 1828824 effective words/s
[2023-02-07 13:59:44,399][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458736 effective words) took 0.8s, 1866566 effective words/s
[2023-02-07 13:59:45,198][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458736 effective words) took 0.8s, 1830383 effective words/s
[2023-02-07 13:59:45,978][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458736 effective words) took 0.8s, 1871683 effective words/s
[2023-02-07 13:59:46,759][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458736 effective words) took 0.8s, 1873389 effective words/s
[2023-02-07 13:59:47,542][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458736 effective words) took 0.8s, 1865667 effective words/s
[2023-02-07 13:59:48,337][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458736 effective words) took 0.8s, 1840304 effective words/s
[2023-02-07 13:59:49,154][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458736 effective words) took 0.8s, 1788635 effective words/s
[2023-02-07 13:59:49,986][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458736 effective words) took 0.8s, 1759404 effective words/s
[2023-02-07 13:59:50,832][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458736 effective words) took 0.8s, 1726291 effective words/s
[2023-02-07 13:59:51,653][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458736 effective words) took 0.8s, 1780439 effective words/s
[2023-02-07 13:59:52,486][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458736 effective words) took 0.8s, 1755460 effective words/s
[2023-02-07 13:59:53,312][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458736 effective words) took 0.8s, 1769683 effective words/s
[2023-02-07 13:59:54,154][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458736 effective words) took 0.8s, 1736636 effective words/s
[2023-02-07 13:59:54,999][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458736 effective words) took 0.8s, 1728761 effective words/s
[2023-02-07 13:59:55,829][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458736 effective words) took 0.8s, 1762614 effective words/s
[2023-02-07 13:59:56,658][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458736 effective words) took 0.8s, 1763789 effective words/s
[2023-02-07 13:59:57,480][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458736 effective words) took 0.8s, 1776800 effective words/s
[2023-02-07 13:59:58,314][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458736 effective words) took 0.8s, 1754047 effective words/s
[2023-02-07 13:59:59,141][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458736 effective words) took 0.8s, 1767010 effective words/s
[2023-02-07 13:59:59,985][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458736 effective words) took 0.8s, 1732990 effective words/s
[2023-02-07 14:00:00,823][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458736 effective words) took 0.8s, 1745200 effective words/s
[2023-02-07 14:00:01,649][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458736 effective words) took 0.8s, 1770866 effective words/s
[2023-02-07 14:00:02,472][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458736 effective words) took 0.8s, 1774098 effective words/s
[2023-02-07 14:00:03,300][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458736 effective words) took 0.8s, 1766711 effective words/s
[2023-02-07 14:00:04,134][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458736 effective words) took 0.8s, 1753662 effective words/s
[2023-02-07 14:00:04,961][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458736 effective words) took 0.8s, 1766375 effective words/s
[2023-02-07 14:00:05,789][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458736 effective words) took 0.8s, 1766342 effective words/s
[2023-02-07 14:00:06,609][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458736 effective words) took 0.8s, 1782842 effective words/s
[2023-02-07 14:00:07,437][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458736 effective words) took 0.8s, 1764072 effective words/s
[2023-02-07 14:00:08,271][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458736 effective words) took 0.8s, 1752401 effective words/s
[2023-02-07 14:00:09,102][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458736 effective words) took 0.8s, 1761366 effective words/s
[2023-02-07 14:00:09,928][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458736 effective words) took 0.8s, 1769826 effective words/s
[2023-02-07 14:00:10,750][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458736 effective words) took 0.8s, 1776912 effective words/s
[2023-02-07 14:00:11,578][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458736 effective words) took 0.8s, 1765288 effective words/s
[2023-02-07 14:00:12,399][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458736 effective words) took 0.8s, 1780598 effective words/s
[2023-02-07 14:00:13,226][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458736 effective words) took 0.8s, 1767871 effective words/s
[2023-02-07 14:00:14,051][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458736 effective words) took 0.8s, 1773451 effective words/s
[2023-02-07 14:00:14,869][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458736 effective words) took 0.8s, 1786041 effective words/s
[2023-02-07 14:00:15,694][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458736 effective words) took 0.8s, 1771389 effective words/s
[2023-02-07 14:00:16,522][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458736 effective words) took 0.8s, 1766549 effective words/s
[2023-02-07 14:00:17,345][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458736 effective words) took 0.8s, 1777705 effective words/s
[2023-02-07 14:00:18,159][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458736 effective words) took 0.8s, 1797104 effective words/s
[2023-02-07 14:00:18,983][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458736 effective words) took 0.8s, 1772428 effective words/s
[2023-02-07 14:00:19,808][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458736 effective words) took 0.8s, 1772961 effective words/s
[2023-02-07 14:00:20,620][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458736 effective words) took 0.8s, 1798163 effective words/s
[2023-02-07 14:00:21,450][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458736 effective words) took 0.8s, 1761260 effective words/s
[2023-02-07 14:00:22,280][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1458736 effective words) took 0.8s, 1761534 effective words/s
[2023-02-07 14:00:23,090][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1458736 effective words) took 0.8s, 1804081 effective words/s
[2023-02-07 14:00:23,920][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1458736 effective words) took 0.8s, 1761551 effective words/s
[2023-02-07 14:00:24,738][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1458736 effective words) took 0.8s, 1785398 effective words/s
[2023-02-07 14:00:25,565][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1458736 effective words) took 0.8s, 1767879 effective words/s
[2023-02-07 14:00:26,392][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1458736 effective words) took 0.8s, 1769039 effective words/s
[2023-02-07 14:00:27,234][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1458736 effective words) took 0.8s, 1735470 effective words/s
[2023-02-07 14:00:28,055][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1458736 effective words) took 0.8s, 1781764 effective words/s
[2023-02-07 14:00:28,877][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1458736 effective words) took 0.8s, 1779153 effective words/s
[2023-02-07 14:00:28,877][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 94623620 raw words (94817840 effective words) took 53.2s, 1783156 effective words/s', 'datetime': '2023-02-07T14:00:28.877514', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:00:28.878 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:00:31,094][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135918-8j01lo85/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:00:31.094195', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:00:31,095][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:00:31,113][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_135918-8j01lo85/files/../tmp/embedding_model.pt
2023-02-07 14:00:31.113 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:00:32.615 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:00:33.176 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:00:34.642 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.993258468369921, 'test_mae': 1.0474378609447026, 'test_r2': 0.05706956775224492}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.54
wandb: percentage 0.16883
wandb:   test_mae 1.04744
wandb:   test_mse 1.99326
wandb:    test_r2 0.05707
wandb: 
wandb: üöÄ View run vivid-sweep-65 at: https://wandb.ai/xiaoqiz/mof2vec/runs/8j01lo85
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_135918-8j01lo85/logs
wandb: Agent Starting Run: o4r9z02y with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 956
wandb: 	model.gensim.alpha: 0.005463304321260348
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 31
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.2751610178673632
wandb: 	model.gensim.vector_size: 165
wandb: 	model.gensim.window: 4
wandb: 	model.sklearn.learning_rate: 0.03105181239190887
wandb: 	model.sklearn.max_depth: 57
wandb: 	model.sklearn.min_child_weight: 0.00928053179119534
wandb: 	model.sklearn.n_estimators: 1097
wandb: 	model.sklearn.num_leaves: 345
wandb: 	model.sklearn.reg_alpha: 0.007242506888204706
wandb: 	model.sklearn.reg_lambda: 0.8879293641599139
wandb: 	model.sklearn.subsample: 0.4767161102634012
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140044-o4r9z02y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-66
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/o4r9z02y
2023-02-07 14:00:52.656 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 14:00:52.656 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 956 for sweep.
2023-02-07 14:00:52.657 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.005463304321260348 for sweep.
2023-02-07 14:00:52.657 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:00:52.657 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 31 for sweep.
2023-02-07 14:00:52.657 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 14:00:52.658 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2751610178673632 for sweep.
2023-02-07 14:00:52.658 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 165 for sweep.
2023-02-07 14:00:52.658 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 4 for sweep.
2023-02-07 14:00:52.658 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.03105181239190887 for sweep.
2023-02-07 14:00:52.659 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 57 for sweep.
2023-02-07 14:00:52.659 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.00928053179119534 for sweep.
2023-02-07 14:00:52.659 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1097 for sweep.
2023-02-07 14:00:52.660 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 345 for sweep.
2023-02-07 14:00:52.660 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.007242506888204706 for sweep.
2023-02-07 14:00:52.660 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.8879293641599139 for sweep.
2023-02-07 14:00:52.660 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.4767161102634012 for sweep.
2023-02-07 14:00:52.661 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:00:52.666 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140044-o4r9z02y/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 956, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 165, 'window': 4, 'min_count': 4, 'dm': 0, 'sample': 0.2751610178673632, 'workers': 4, 'alpha': 0.005463304321260348, 'epochs': 31}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1097, 'max_depth': 57, 'num_leaves': 345, 'reg_alpha': 0.007242506888204706, 'reg_lambda': 0.8879293641599139, 'subsample': 0.4767161102634012, 'min_child_weight': 0.00928053179119534, 'n_jobs': 4, 'learning_rate': 0.03105181239190887}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 36/3257 [00:00<00:08, 358.94it/s]  2%|‚ñè         | 74/3257 [00:00<00:08, 370.90it/s]  3%|‚ñé         | 112/3257 [00:00<00:08, 372.26it/s]  5%|‚ñç         | 154/3257 [00:00<00:07, 389.46it/s]  6%|‚ñå         | 193/3257 [00:00<00:07, 388.99it/s]  7%|‚ñã         | 237/3257 [00:00<00:07, 404.04it/s]  9%|‚ñä         | 282/3257 [00:00<00:07, 416.97it/s] 10%|‚ñâ         | 325/3257 [00:00<00:06, 419.86it/s] 11%|‚ñà‚ñè        | 367/3257 [00:00<00:07, 409.52it/s] 13%|‚ñà‚ñé        | 408/3257 [00:01<00:07, 399.60it/s] 14%|‚ñà‚ñç        | 449/3257 [00:01<00:07, 375.58it/s] 15%|‚ñà‚ñç        | 487/3257 [00:01<00:07, 376.62it/s] 16%|‚ñà‚ñå        | 529/3257 [00:01<00:07, 388.84it/s] 17%|‚ñà‚ñã        | 569/3257 [00:01<00:07, 382.38it/s] 19%|‚ñà‚ñä        | 608/3257 [00:01<00:06, 381.26it/s] 20%|‚ñà‚ñâ        | 647/3257 [00:01<00:09, 270.97it/s] 21%|‚ñà‚ñà        | 683/3257 [00:01<00:08, 288.91it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:02<00:08, 312.07it/s] 23%|‚ñà‚ñà‚ñé       | 760/3257 [00:02<00:07, 329.41it/s] 25%|‚ñà‚ñà‚ñç       | 799/3257 [00:02<00:07, 343.75it/s] 26%|‚ñà‚ñà‚ñå       | 836/3257 [00:02<00:06, 346.69it/s] 27%|‚ñà‚ñà‚ñã       | 872/3257 [00:02<00:06, 349.76it/s] 28%|‚ñà‚ñà‚ñä       | 912/3257 [00:02<00:06, 360.63it/s] 29%|‚ñà‚ñà‚ñâ       | 950/3257 [00:02<00:06, 365.14it/s] 30%|‚ñà‚ñà‚ñà       | 988/3257 [00:02<00:06, 362.64it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1025/3257 [00:02<00:06, 364.55it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1062/3257 [00:02<00:06, 354.43it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1098/3257 [00:03<00:06, 355.68it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1135/3257 [00:03<00:05, 358.65it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:03<00:05, 362.92it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1210/3257 [00:03<00:05, 341.80it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1250/3257 [00:03<00:05, 357.13it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1287/3257 [00:03<00:05, 352.98it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1327/3257 [00:03<00:05, 366.00it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1364/3257 [00:03<00:05, 366.06it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1402/3257 [00:03<00:05, 368.33it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1445/3257 [00:03<00:04, 385.95it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1488/3257 [00:04<00:04, 396.91it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1528/3257 [00:04<00:04, 392.23it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1568/3257 [00:04<00:04, 385.52it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1607/3257 [00:04<00:04, 386.58it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1646/3257 [00:04<00:04, 381.54it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1685/3257 [00:04<00:04, 369.80it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1725/3257 [00:04<00:04, 377.08it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:04<00:04, 363.55it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1804/3257 [00:04<00:03, 374.71it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1842/3257 [00:05<00:05, 263.81it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1882/3257 [00:05<00:04, 294.11it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1919/3257 [00:05<00:04, 311.38it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1966/3257 [00:05<00:03, 351.52it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2006/3257 [00:05<00:03, 363.58it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2046/3257 [00:05<00:03, 373.07it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2085/3257 [00:05<00:03, 361.67it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2123/3257 [00:05<00:03, 359.76it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2161/3257 [00:06<00:03, 363.27it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2203/3257 [00:06<00:02, 377.72it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2242/3257 [00:06<00:02, 367.82it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2280/3257 [00:06<00:02, 366.93it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2325/3257 [00:06<00:02, 390.27it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2370/3257 [00:06<00:02, 405.29it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2411/3257 [00:06<00:02, 403.14it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2452/3257 [00:06<00:02, 390.57it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2496/3257 [00:06<00:01, 404.64it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2540/3257 [00:06<00:01, 409.12it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2582/3257 [00:07<00:01, 392.73it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2627/3257 [00:07<00:01, 407.45it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2668/3257 [00:07<00:01, 397.77it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2708/3257 [00:07<00:01, 380.80it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2751/3257 [00:07<00:01, 390.26it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2791/3257 [00:07<00:01, 392.07it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2831/3257 [00:07<00:01, 381.31it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2878/3257 [00:07<00:00, 405.41it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2919/3257 [00:07<00:00, 392.30it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2959/3257 [00:08<00:00, 380.27it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3001/3257 [00:08<00:00, 390.60it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3042/3257 [00:08<00:00, 394.30it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3086/3257 [00:08<00:00, 407.50it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3129/3257 [00:08<00:00, 413.95it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3171/3257 [00:08<00:00, 404.79it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3212/3257 [00:08<00:00, 397.70it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3254/3257 [00:08<00:00, 402.69it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 371.77it/s]
2023-02-07 14:01:01.622 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:01:01,623][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d165,n5,mc4,s0.275161,t4>', 'datetime': '2023-02-07T14:01:01.623595', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:01:01,623][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:01:01,624][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:01:01,761][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 14:01:01,762][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:01:01,764][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 742 unique words (80.30% of original 924, drops 182)', 'datetime': '2023-02-07T14:01:01.764030', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:01:01,764][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 1455401 word corpus (99.98% of original 1455748, drops 347)', 'datetime': '2023-02-07T14:01:01.764221', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:01:01,766][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 14:01:01,766][gensim.models.word2vec][INFO] - sample=0.275161 downsamples 0 most-common words
[2023-02-07 14:01:01,767][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455401 word corpus (100.0%% of prior 1455401)', 'datetime': '2023-02-07T14:01:01.767067', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:01:01,771][gensim.models.word2vec][INFO] - estimated required memory for 742 words and 165 dimensions: 4151460 bytes
[2023-02-07 14:01:01,771][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:01:01,774][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 742 vocabulary and 165 features, using sg=1 hs=0 sample=0.2751610178673632 negative=5 window=4 shrink_windows=True', 'datetime': '2023-02-07T14:01:01.774577', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:01:02,380][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458658 effective words) took 0.6s, 2414014 effective words/s
[2023-02-07 14:01:02,836][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458658 effective words) took 0.5s, 3220556 effective words/s
[2023-02-07 14:01:03,305][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458658 effective words) took 0.5s, 3122233 effective words/s
[2023-02-07 14:01:03,768][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458658 effective words) took 0.5s, 3168263 effective words/s
[2023-02-07 14:01:04,232][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458658 effective words) took 0.5s, 3154447 effective words/s
[2023-02-07 14:01:04,681][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458658 effective words) took 0.4s, 3255877 effective words/s
[2023-02-07 14:01:05,133][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458658 effective words) took 0.5s, 3239238 effective words/s
[2023-02-07 14:01:05,578][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458658 effective words) took 0.4s, 3289250 effective words/s
[2023-02-07 14:01:06,024][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458658 effective words) took 0.4s, 3283403 effective words/s
[2023-02-07 14:01:06,470][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458658 effective words) took 0.4s, 3284109 effective words/s
[2023-02-07 14:01:06,922][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458658 effective words) took 0.5s, 3237635 effective words/s
[2023-02-07 14:01:07,370][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458658 effective words) took 0.4s, 3264996 effective words/s
[2023-02-07 14:01:07,818][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458658 effective words) took 0.4s, 3268300 effective words/s
[2023-02-07 14:01:08,262][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458658 effective words) took 0.4s, 3297160 effective words/s
[2023-02-07 14:01:08,702][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458658 effective words) took 0.4s, 3321442 effective words/s
[2023-02-07 14:01:09,153][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458658 effective words) took 0.4s, 3249454 effective words/s
[2023-02-07 14:01:09,599][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458658 effective words) took 0.4s, 3276623 effective words/s
[2023-02-07 14:01:10,041][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458658 effective words) took 0.4s, 3319438 effective words/s
[2023-02-07 14:01:10,483][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458658 effective words) took 0.4s, 3305586 effective words/s
[2023-02-07 14:01:10,930][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458658 effective words) took 0.4s, 3278691 effective words/s
[2023-02-07 14:01:11,373][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458658 effective words) took 0.4s, 3302358 effective words/s
[2023-02-07 14:01:11,813][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458658 effective words) took 0.4s, 3327355 effective words/s
[2023-02-07 14:01:12,262][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458658 effective words) took 0.4s, 3260972 effective words/s
[2023-02-07 14:01:12,709][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458658 effective words) took 0.4s, 3273166 effective words/s
[2023-02-07 14:01:13,153][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458658 effective words) took 0.4s, 3294893 effective words/s
[2023-02-07 14:01:13,603][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458658 effective words) took 0.4s, 3259744 effective words/s
[2023-02-07 14:01:14,051][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458658 effective words) took 0.4s, 3266091 effective words/s
[2023-02-07 14:01:14,500][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458658 effective words) took 0.4s, 3279362 effective words/s
[2023-02-07 14:01:14,947][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458658 effective words) took 0.4s, 3283949 effective words/s
[2023-02-07 14:01:15,394][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458658 effective words) took 0.4s, 3272512 effective words/s
[2023-02-07 14:01:15,841][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458658 effective words) took 0.4s, 3274306 effective words/s
[2023-02-07 14:01:15,842][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 45128188 raw words (45218398 effective words) took 14.1s, 3214396 effective words/s', 'datetime': '2023-02-07T14:01:15.842330', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:01:15.842 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:01:16,752][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140044-o4r9z02y/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:01:16.752017', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:01:16,753][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:01:16,772][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140044-o4r9z02y/files/../tmp/embedding_model.pt
2023-02-07 14:01:16.773 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:01:18.146 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:01:18.662 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:01:19.832 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8355799975838423, 'test_mae': 1.0239583971721629, 'test_r2': 0.1316609120128175}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.53
wandb: percentage 0.19697
wandb:   test_mae 1.02396
wandb:   test_mse 1.83558
wandb:    test_r2 0.13166
wandb: 
wandb: üöÄ View run efficient-sweep-66 at: https://wandb.ai/xiaoqiz/mof2vec/runs/o4r9z02y
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_140044-o4r9z02y/logs
wandb: Agent Starting Run: fbt69apn with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 826
wandb: 	model.gensim.alpha: 0.0021243444185834225
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 55
wandb: 	model.gensim.min_count: 7
wandb: 	model.gensim.sample: 0.7623028539259833
wandb: 	model.gensim.vector_size: 107
wandb: 	model.gensim.window: 6
wandb: 	model.sklearn.learning_rate: 0.5566104560655425
wandb: 	model.sklearn.max_depth: 16
wandb: 	model.sklearn.min_child_weight: 0.02782659302394139
wandb: 	model.sklearn.n_estimators: 3791
wandb: 	model.sklearn.num_leaves: 462
wandb: 	model.sklearn.reg_alpha: 0.004849168045942509
wandb: 	model.sklearn.reg_lambda: 0.19965158107454084
wandb: 	model.sklearn.subsample: 0.20874412986364785
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140142-fbt69apn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-67
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/fbt69apn
2023-02-07 14:01:50.204 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 14:01:50.204 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 826 for sweep.
2023-02-07 14:01:50.205 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0021243444185834225 for sweep.
2023-02-07 14:01:50.205 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:01:50.205 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 55 for sweep.
2023-02-07 14:01:50.205 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 7 for sweep.
2023-02-07 14:01:50.206 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7623028539259833 for sweep.
2023-02-07 14:01:50.206 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 107 for sweep.
2023-02-07 14:01:50.206 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 6 for sweep.
2023-02-07 14:01:50.206 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.5566104560655425 for sweep.
2023-02-07 14:01:50.207 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 16 for sweep.
2023-02-07 14:01:50.207 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.02782659302394139 for sweep.
2023-02-07 14:01:50.207 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3791 for sweep.
2023-02-07 14:01:50.207 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 462 for sweep.
2023-02-07 14:01:50.207 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.004849168045942509 for sweep.
2023-02-07 14:01:50.208 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.19965158107454084 for sweep.
2023-02-07 14:01:50.208 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.20874412986364785 for sweep.
2023-02-07 14:01:50.208 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:01:50.252 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140142-fbt69apn/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 826, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 107, 'window': 6, 'min_count': 7, 'dm': 0, 'sample': 0.7623028539259833, 'workers': 4, 'alpha': 0.0021243444185834225, 'epochs': 55}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3791, 'max_depth': 16, 'num_leaves': 462, 'reg_alpha': 0.004849168045942509, 'reg_lambda': 0.19965158107454084, 'subsample': 0.20874412986364785, 'min_child_weight': 0.02782659302394139, 'n_jobs': 4, 'learning_rate': 0.5566104560655425}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 7/3257 [00:00<00:48, 67.49it/s]  1%|          | 18/3257 [00:00<00:35, 90.28it/s]  1%|          | 28/3257 [00:00<00:48, 66.79it/s]  1%|          | 38/3257 [00:00<00:42, 76.47it/s]  1%|‚ñè         | 47/3257 [00:00<00:50, 63.32it/s]  2%|‚ñè         | 56/3257 [00:00<00:47, 67.83it/s]  2%|‚ñè         | 64/3257 [00:00<00:49, 64.08it/s]  2%|‚ñè         | 74/3257 [00:01<00:43, 72.98it/s]  3%|‚ñé         | 83/3257 [00:01<00:41, 76.88it/s]  3%|‚ñé         | 93/3257 [00:01<00:38, 82.80it/s]  3%|‚ñé         | 104/3257 [00:01<00:35, 88.54it/s]  4%|‚ñé         | 114/3257 [00:01<00:38, 81.01it/s]  4%|‚ñç         | 123/3257 [00:01<00:44, 71.01it/s]  4%|‚ñç         | 133/3257 [00:01<00:40, 76.68it/s]  4%|‚ñç         | 142/3257 [00:01<00:39, 79.29it/s]  5%|‚ñç         | 151/3257 [00:02<00:46, 66.95it/s]  5%|‚ñå         | 163/3257 [00:02<00:39, 77.47it/s]  5%|‚ñå         | 172/3257 [00:02<00:43, 70.36it/s]  6%|‚ñå         | 181/3257 [00:02<00:41, 73.69it/s]  6%|‚ñå         | 189/3257 [00:02<00:43, 71.25it/s]  6%|‚ñå         | 197/3257 [00:02<00:42, 72.25it/s]  6%|‚ñã         | 205/3257 [00:02<00:45, 66.44it/s]  7%|‚ñã         | 215/3257 [00:02<00:42, 72.30it/s]  7%|‚ñã         | 223/3257 [00:03<00:42, 71.11it/s]  7%|‚ñã         | 231/3257 [00:03<00:44, 68.04it/s]  7%|‚ñã         | 240/3257 [00:03<00:42, 71.65it/s]  8%|‚ñä         | 248/3257 [00:03<00:41, 72.72it/s]  8%|‚ñä         | 256/3257 [00:03<00:42, 70.26it/s]  8%|‚ñä         | 264/3257 [00:03<00:50, 59.75it/s]  8%|‚ñä         | 271/3257 [00:03<00:52, 56.75it/s]  9%|‚ñä         | 280/3257 [00:03<00:47, 62.36it/s]  9%|‚ñâ         | 287/3257 [00:04<00:53, 55.53it/s]  9%|‚ñâ         | 297/3257 [00:04<00:44, 65.82it/s]  9%|‚ñâ         | 307/3257 [00:04<00:39, 73.76it/s] 10%|‚ñâ         | 316/3257 [00:04<00:37, 77.68it/s] 10%|‚ñà         | 328/3257 [00:04<00:33, 88.70it/s] 10%|‚ñà         | 338/3257 [00:04<00:33, 87.66it/s] 11%|‚ñà         | 349/3257 [00:04<00:31, 91.31it/s] 11%|‚ñà         | 359/3257 [00:04<00:32, 89.76it/s] 11%|‚ñà‚ñè        | 369/3257 [00:04<00:31, 90.43it/s] 12%|‚ñà‚ñè        | 379/3257 [00:05<00:37, 75.97it/s] 12%|‚ñà‚ñè        | 388/3257 [00:05<00:45, 63.34it/s] 12%|‚ñà‚ñè        | 400/3257 [00:05<00:37, 75.21it/s] 13%|‚ñà‚ñé        | 410/3257 [00:05<00:36, 78.37it/s] 13%|‚ñà‚ñé        | 423/3257 [00:05<00:31, 90.85it/s] 13%|‚ñà‚ñé        | 433/3257 [00:05<00:31, 89.99it/s] 14%|‚ñà‚ñé        | 443/3257 [00:05<00:32, 85.51it/s] 14%|‚ñà‚ñç        | 452/3257 [00:06<00:33, 83.96it/s] 14%|‚ñà‚ñç        | 465/3257 [00:06<00:29, 95.55it/s] 15%|‚ñà‚ñç        | 479/3257 [00:06<00:26, 103.43it/s] 15%|‚ñà‚ñå        | 490/3257 [00:06<00:32, 83.91it/s]  15%|‚ñà‚ñå        | 501/3257 [00:06<00:32, 84.83it/s] 16%|‚ñà‚ñå        | 510/3257 [00:06<00:34, 80.62it/s] 16%|‚ñà‚ñå        | 519/3257 [00:06<00:33, 80.74it/s] 16%|‚ñà‚ñã        | 531/3257 [00:06<00:30, 89.00it/s] 17%|‚ñà‚ñã        | 542/3257 [00:07<00:29, 92.89it/s] 17%|‚ñà‚ñã        | 553/3257 [00:07<00:28, 94.36it/s] 17%|‚ñà‚ñã        | 564/3257 [00:07<00:27, 97.53it/s] 18%|‚ñà‚ñä        | 574/3257 [00:07<00:30, 89.09it/s] 18%|‚ñà‚ñä        | 584/3257 [00:07<00:34, 77.85it/s] 18%|‚ñà‚ñä        | 599/3257 [00:07<00:30, 86.38it/s] 19%|‚ñà‚ñä        | 608/3257 [00:07<00:30, 85.67it/s] 19%|‚ñà‚ñâ        | 617/3257 [00:07<00:36, 72.13it/s] 19%|‚ñà‚ñâ        | 625/3257 [00:08<00:40, 64.85it/s] 20%|‚ñà‚ñâ        | 636/3257 [00:08<00:36, 72.41it/s] 20%|‚ñà‚ñâ        | 644/3257 [00:08<00:39, 65.95it/s] 20%|‚ñà‚ñà        | 654/3257 [00:08<00:35, 72.69it/s] 20%|‚ñà‚ñà        | 662/3257 [00:08<00:36, 71.34it/s] 21%|‚ñà‚ñà        | 675/3257 [00:08<00:30, 85.09it/s] 21%|‚ñà‚ñà        | 685/3257 [00:08<00:29, 87.18it/s] 21%|‚ñà‚ñà‚ñè       | 695/3257 [00:09<00:31, 80.74it/s] 22%|‚ñà‚ñà‚ñè       | 704/3257 [00:09<00:30, 82.72it/s] 22%|‚ñà‚ñà‚ñè       | 718/3257 [00:09<00:26, 96.82it/s] 22%|‚ñà‚ñà‚ñè       | 730/3257 [00:09<00:24, 101.78it/s] 23%|‚ñà‚ñà‚ñé       | 741/3257 [00:09<00:26, 95.28it/s]  23%|‚ñà‚ñà‚ñé       | 755/3257 [00:09<00:23, 106.88it/s] 24%|‚ñà‚ñà‚ñé       | 770/3257 [00:09<00:21, 114.33it/s] 24%|‚ñà‚ñà‚ñç       | 784/3257 [00:09<00:20, 120.22it/s] 24%|‚ñà‚ñà‚ñç       | 797/3257 [00:09<00:26, 94.26it/s]  25%|‚ñà‚ñà‚ñç       | 812/3257 [00:10<00:23, 105.71it/s] 25%|‚ñà‚ñà‚ñå       | 824/3257 [00:10<00:32, 75.54it/s]  26%|‚ñà‚ñà‚ñå       | 834/3257 [00:10<00:31, 76.09it/s] 26%|‚ñà‚ñà‚ñå       | 844/3257 [00:10<00:30, 78.37it/s] 26%|‚ñà‚ñà‚ñã       | 857/3257 [00:10<00:27, 88.67it/s] 27%|‚ñà‚ñà‚ñã       | 868/3257 [00:10<00:27, 87.01it/s] 27%|‚ñà‚ñà‚ñã       | 880/3257 [00:10<00:25, 93.59it/s] 27%|‚ñà‚ñà‚ñã       | 894/3257 [00:11<00:22, 104.14it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:11<00:24, 97.34it/s]  28%|‚ñà‚ñà‚ñä       | 917/3257 [00:11<00:24, 95.67it/s] 28%|‚ñà‚ñà‚ñä       | 928/3257 [00:11<00:24, 94.89it/s] 29%|‚ñà‚ñà‚ñâ       | 940/3257 [00:11<00:23, 99.61it/s] 29%|‚ñà‚ñà‚ñâ       | 951/3257 [00:11<00:22, 101.62it/s] 30%|‚ñà‚ñà‚ñâ       | 964/3257 [00:11<00:21, 108.08it/s] 30%|‚ñà‚ñà‚ñà       | 979/3257 [00:11<00:19, 116.06it/s] 30%|‚ñà‚ñà‚ñà       | 991/3257 [00:12<00:21, 104.50it/s] 31%|‚ñà‚ñà‚ñà       | 1002/3257 [00:12<00:21, 104.10it/s] 31%|‚ñà‚ñà‚ñà       | 1013/3257 [00:12<00:22, 101.73it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:12<00:20, 110.98it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1039/3257 [00:12<00:20, 110.72it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1053/3257 [00:12<00:18, 117.33it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1071/3257 [00:12<00:16, 133.04it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:12<00:14, 150.46it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1107/3257 [00:12<00:14, 147.38it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1122/3257 [00:12<00:15, 139.89it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1137/3257 [00:13<00:14, 142.23it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1152/3257 [00:13<00:16, 130.07it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1166/3257 [00:13<00:17, 120.53it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1179/3257 [00:13<00:16, 122.62it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1192/3257 [00:13<00:17, 115.78it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1204/3257 [00:13<00:17, 116.62it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1219/3257 [00:13<00:16, 124.75it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1232/3257 [00:14<00:21, 94.04it/s]  38%|‚ñà‚ñà‚ñà‚ñä      | 1247/3257 [00:14<00:19, 105.46it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1260/3257 [00:14<00:17, 111.38it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1273/3257 [00:14<00:17, 110.80it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1287/3257 [00:14<00:16, 116.47it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1302/3257 [00:14<00:15, 123.26it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1315/3257 [00:14<00:15, 123.89it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:14<00:24, 78.80it/s]  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1344/3257 [00:15<00:20, 93.77it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1359/3257 [00:15<00:18, 104.56it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1372/3257 [00:15<00:17, 104.75it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1385/3257 [00:15<00:16, 110.22it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1398/3257 [00:15<00:17, 108.12it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1410/3257 [00:15<00:16, 111.11it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1426/3257 [00:15<00:14, 123.46it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1439/3257 [00:15<00:14, 124.45it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1456/3257 [00:15<00:13, 136.04it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1472/3257 [00:16<00:12, 140.55it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1487/3257 [00:16<00:12, 137.38it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1506/3257 [00:16<00:11, 152.02it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1522/3257 [00:16<00:11, 147.24it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1537/3257 [00:16<00:12, 135.56it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1551/3257 [00:16<00:13, 128.81it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1565/3257 [00:16<00:12, 131.24it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1579/3257 [00:16<00:15, 111.19it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1595/3257 [00:17<00:13, 120.86it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1611/3257 [00:17<00:12, 130.77it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1631/3257 [00:17<00:11, 146.60it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1647/3257 [00:17<00:11, 135.06it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1662/3257 [00:17<00:11, 137.73it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1677/3257 [00:17<00:12, 130.32it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1693/3257 [00:17<00:11, 137.89it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1708/3257 [00:17<00:11, 136.39it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1727/3257 [00:17<00:10, 147.20it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1742/3257 [00:18<00:11, 127.71it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1762/3257 [00:18<00:10, 144.34it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1780/3257 [00:18<00:09, 152.48it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1796/3257 [00:18<00:09, 152.09it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1812/3257 [00:18<00:10, 142.08it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1828/3257 [00:18<00:10, 141.55it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1843/3257 [00:18<00:10, 139.15it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1865/3257 [00:18<00:08, 155.87it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1883/3257 [00:18<00:08, 159.96it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1900/3257 [00:19<00:08, 152.34it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1918/3257 [00:19<00:08, 159.16it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1935/3257 [00:19<00:08, 157.37it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1952/3257 [00:19<00:08, 157.95it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1973/3257 [00:19<00:07, 169.68it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1991/3257 [00:19<00:07, 165.15it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2008/3257 [00:19<00:07, 164.16it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2025/3257 [00:19<00:08, 152.44it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2041/3257 [00:20<00:08, 142.67it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2057/3257 [00:20<00:08, 146.02it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2072/3257 [00:20<00:08, 145.35it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2087/3257 [00:20<00:08, 141.41it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2102/3257 [00:20<00:09, 122.26it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2121/3257 [00:20<00:08, 138.34it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2136/3257 [00:20<00:08, 138.89it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2151/3257 [00:20<00:07, 141.81it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2170/3257 [00:20<00:07, 153.70it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2187/3257 [00:21<00:06, 156.24it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2204/3257 [00:21<00:06, 159.66it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2222/3257 [00:21<00:06, 163.07it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2239/3257 [00:21<00:06, 162.76it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2256/3257 [00:21<00:06, 159.89it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2273/3257 [00:21<00:06, 151.17it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2289/3257 [00:21<00:06, 147.33it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2304/3257 [00:21<00:06, 136.74it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2325/3257 [00:21<00:05, 155.34it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2341/3257 [00:22<00:06, 151.85it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2362/3257 [00:22<00:05, 167.33it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2380/3257 [00:22<00:05, 164.38it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2397/3257 [00:22<00:05, 163.83it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2414/3257 [00:22<00:05, 162.58it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2431/3257 [00:22<00:05, 156.99it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2447/3257 [00:22<00:05, 143.11it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2466/3257 [00:22<00:05, 154.70it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2489/3257 [00:22<00:04, 169.75it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2509/3257 [00:23<00:04, 177.74it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2530/3257 [00:23<00:03, 184.13it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2549/3257 [00:23<00:03, 177.37it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2567/3257 [00:23<00:04, 172.07it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2587/3257 [00:23<00:03, 178.75it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2608/3257 [00:23<00:03, 184.74it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2627/3257 [00:23<00:03, 179.34it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2646/3257 [00:23<00:03, 181.68it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2665/3257 [00:23<00:03, 176.32it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2684/3257 [00:24<00:03, 174.12it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2702/3257 [00:24<00:03, 171.35it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2722/3257 [00:24<00:02, 179.19it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2740/3257 [00:24<00:04, 128.19it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2760/3257 [00:24<00:03, 143.82it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2777/3257 [00:24<00:03, 145.71it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2801/3257 [00:24<00:02, 168.56it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2822/3257 [00:24<00:02, 178.31it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2843/3257 [00:24<00:02, 185.71it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2872/3257 [00:25<00:01, 212.40it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2894/3257 [00:25<00:01, 211.75it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2918/3257 [00:25<00:01, 218.09it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2941/3257 [00:25<00:01, 217.39it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2963/3257 [00:25<00:01, 215.18it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:25<00:01, 211.41it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:25<00:01, 224.13it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3037/3257 [00:25<00:00, 228.91it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3063/3257 [00:25<00:00, 236.63it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3088/3257 [00:26<00:00, 240.26it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3116/3257 [00:26<00:00, 251.33it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3142/3257 [00:26<00:00, 246.94it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3167/3257 [00:26<00:00, 237.82it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3191/3257 [00:26<00:00, 234.34it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3217/3257 [00:26<00:00, 239.61it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:26<00:00, 253.18it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:26<00:00, 122.04it/s]
2023-02-07 14:02:17.245 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:02:17,246][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d107,n5,mc7,s0.762303,t4>', 'datetime': '2023-02-07T14:02:17.246452', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:02:17,246][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:02:17,246][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:02:17,449][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 14:02:17,449][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:02:17,453][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 retains 1451 unique words (51.47% of original 2819, drops 1368)', 'datetime': '2023-02-07T14:02:17.453515', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:02:17,454][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 leaves 2179471 word corpus (99.81% of original 2183622, drops 4151)', 'datetime': '2023-02-07T14:02:17.454902', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:02:17,460][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 14:02:17,460][gensim.models.word2vec][INFO] - sample=0.762303 downsamples 0 most-common words
[2023-02-07 14:02:17,460][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2179471 word corpus (100.0%% of prior 2179471)', 'datetime': '2023-02-07T14:02:17.460355', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:02:17,470][gensim.models.word2vec][INFO] - estimated required memory for 1451 words and 107 dimensions: 4012952 bytes
[2023-02-07 14:02:17,470][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:02:17,472][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 1451 vocabulary and 107 features, using sg=1 hs=0 sample=0.7623028539259833 negative=5 window=6 shrink_windows=True', 'datetime': '2023-02-07T14:02:17.472926', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:02:18,287][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2182728 effective words) took 0.8s, 2687463 effective words/s
[2023-02-07 14:02:18,978][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2182728 effective words) took 0.7s, 3169477 effective words/s
[2023-02-07 14:02:19,642][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2182728 effective words) took 0.7s, 3295853 effective words/s
[2023-02-07 14:02:20,303][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2182728 effective words) took 0.7s, 3305207 effective words/s
[2023-02-07 14:02:20,964][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2182728 effective words) took 0.7s, 3311977 effective words/s
[2023-02-07 14:02:21,628][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2182728 effective words) took 0.7s, 3298627 effective words/s
[2023-02-07 14:02:22,288][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2182728 effective words) took 0.7s, 3312961 effective words/s
[2023-02-07 14:02:22,938][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2182728 effective words) took 0.6s, 3365618 effective words/s
[2023-02-07 14:02:23,595][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2182728 effective words) took 0.7s, 3330334 effective words/s
[2023-02-07 14:02:24,254][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2182728 effective words) took 0.7s, 3316013 effective words/s
[2023-02-07 14:02:24,901][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2182728 effective words) took 0.6s, 3381480 effective words/s
[2023-02-07 14:02:25,552][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2182728 effective words) took 0.6s, 3363496 effective words/s
[2023-02-07 14:02:26,200][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2182728 effective words) took 0.6s, 3373299 effective words/s
[2023-02-07 14:02:26,856][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2182728 effective words) took 0.7s, 3332786 effective words/s
[2023-02-07 14:02:27,511][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2182728 effective words) took 0.7s, 3344512 effective words/s
[2023-02-07 14:02:28,162][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2182728 effective words) took 0.6s, 3360999 effective words/s
[2023-02-07 14:02:28,809][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2182728 effective words) took 0.6s, 3377804 effective words/s
[2023-02-07 14:02:29,457][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2182728 effective words) took 0.6s, 3378737 effective words/s
[2023-02-07 14:02:30,099][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2182728 effective words) took 0.6s, 3402629 effective words/s
[2023-02-07 14:02:30,744][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2182728 effective words) took 0.6s, 3395598 effective words/s
[2023-02-07 14:02:31,380][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2182728 effective words) took 0.6s, 3434094 effective words/s
[2023-02-07 14:02:32,018][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2182728 effective words) took 0.6s, 3437818 effective words/s
[2023-02-07 14:02:32,662][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2182728 effective words) took 0.6s, 3393821 effective words/s
[2023-02-07 14:02:33,307][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2182728 effective words) took 0.6s, 3393374 effective words/s
[2023-02-07 14:02:33,951][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2182728 effective words) took 0.6s, 3392688 effective words/s
[2023-02-07 14:02:34,593][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2182728 effective words) took 0.6s, 3410505 effective words/s
[2023-02-07 14:02:35,231][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2182728 effective words) took 0.6s, 3427502 effective words/s
[2023-02-07 14:02:35,866][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2182728 effective words) took 0.6s, 3442235 effective words/s
[2023-02-07 14:02:36,503][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2182728 effective words) took 0.6s, 3437007 effective words/s
[2023-02-07 14:02:37,148][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2182728 effective words) took 0.6s, 3392219 effective words/s
[2023-02-07 14:02:37,785][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2182728 effective words) took 0.6s, 3430957 effective words/s
[2023-02-07 14:02:38,421][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2182728 effective words) took 0.6s, 3445082 effective words/s
[2023-02-07 14:02:39,055][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2182728 effective words) took 0.6s, 3448926 effective words/s
[2023-02-07 14:02:39,686][gensim.models.word2vec][INFO] - EPOCH 33: training on 2183622 raw words (2182728 effective words) took 0.6s, 3464167 effective words/s
[2023-02-07 14:02:40,314][gensim.models.word2vec][INFO] - EPOCH 34: training on 2183622 raw words (2182728 effective words) took 0.6s, 3487888 effective words/s
[2023-02-07 14:02:40,946][gensim.models.word2vec][INFO] - EPOCH 35: training on 2183622 raw words (2182728 effective words) took 0.6s, 3460104 effective words/s
[2023-02-07 14:02:41,577][gensim.models.word2vec][INFO] - EPOCH 36: training on 2183622 raw words (2182728 effective words) took 0.6s, 3466790 effective words/s
[2023-02-07 14:02:42,213][gensim.models.word2vec][INFO] - EPOCH 37: training on 2183622 raw words (2182728 effective words) took 0.6s, 3443110 effective words/s
[2023-02-07 14:02:42,843][gensim.models.word2vec][INFO] - EPOCH 38: training on 2183622 raw words (2182728 effective words) took 0.6s, 3474424 effective words/s
[2023-02-07 14:02:43,467][gensim.models.word2vec][INFO] - EPOCH 39: training on 2183622 raw words (2182728 effective words) took 0.6s, 3505998 effective words/s
[2023-02-07 14:02:44,098][gensim.models.word2vec][INFO] - EPOCH 40: training on 2183622 raw words (2182728 effective words) took 0.6s, 3465705 effective words/s
[2023-02-07 14:02:44,720][gensim.models.word2vec][INFO] - EPOCH 41: training on 2183622 raw words (2182728 effective words) took 0.6s, 3523048 effective words/s
[2023-02-07 14:02:45,347][gensim.models.word2vec][INFO] - EPOCH 42: training on 2183622 raw words (2182728 effective words) took 0.6s, 3488970 effective words/s
[2023-02-07 14:02:45,975][gensim.models.word2vec][INFO] - EPOCH 43: training on 2183622 raw words (2182728 effective words) took 0.6s, 3480087 effective words/s
[2023-02-07 14:02:46,603][gensim.models.word2vec][INFO] - EPOCH 44: training on 2183622 raw words (2182728 effective words) took 0.6s, 3487367 effective words/s
[2023-02-07 14:02:47,235][gensim.models.word2vec][INFO] - EPOCH 45: training on 2183622 raw words (2182728 effective words) took 0.6s, 3461049 effective words/s
[2023-02-07 14:02:47,866][gensim.models.word2vec][INFO] - EPOCH 46: training on 2183622 raw words (2182728 effective words) took 0.6s, 3465856 effective words/s
[2023-02-07 14:02:48,498][gensim.models.word2vec][INFO] - EPOCH 47: training on 2183622 raw words (2182728 effective words) took 0.6s, 3465474 effective words/s
[2023-02-07 14:02:49,123][gensim.models.word2vec][INFO] - EPOCH 48: training on 2183622 raw words (2182728 effective words) took 0.6s, 3499029 effective words/s
[2023-02-07 14:02:49,753][gensim.models.word2vec][INFO] - EPOCH 49: training on 2183622 raw words (2182728 effective words) took 0.6s, 3479438 effective words/s
[2023-02-07 14:02:50,383][gensim.models.word2vec][INFO] - EPOCH 50: training on 2183622 raw words (2182728 effective words) took 0.6s, 3471574 effective words/s
[2023-02-07 14:02:51,018][gensim.models.word2vec][INFO] - EPOCH 51: training on 2183622 raw words (2182728 effective words) took 0.6s, 3442737 effective words/s
[2023-02-07 14:02:51,647][gensim.models.word2vec][INFO] - EPOCH 52: training on 2183622 raw words (2182728 effective words) took 0.6s, 3477245 effective words/s
[2023-02-07 14:02:52,275][gensim.models.word2vec][INFO] - EPOCH 53: training on 2183622 raw words (2182728 effective words) took 0.6s, 3481703 effective words/s
[2023-02-07 14:02:52,900][gensim.models.word2vec][INFO] - EPOCH 54: training on 2183622 raw words (2182728 effective words) took 0.6s, 3502169 effective words/s
[2023-02-07 14:02:52,900][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 120099210 raw words (120050040 effective words) took 35.4s, 3388626 effective words/s', 'datetime': '2023-02-07T14:02:52.900519', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:02:52.900 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:02:55,122][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140142-fbt69apn/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:02:55.122452', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:02:55,123][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:02:55,130][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140142-fbt69apn/files/../tmp/embedding_model.pt
2023-02-07 14:02:55.130 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:02:56.277 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:02:56.748 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:02:57.545 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.7998539902335253, 'test_mae': 1.012551188926024, 'test_r2': 0.1485614495436426}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.86
wandb: percentage 0.48528
wandb:   test_mae 1.01255
wandb:   test_mse 1.79985
wandb:    test_r2 0.14856
wandb: 
wandb: üöÄ View run exalted-sweep-67 at: https://wandb.ai/xiaoqiz/mof2vec/runs/fbt69apn
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_140142-fbt69apn/logs
wandb: Agent Starting Run: qoxtw66y with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 141
wandb: 	model.gensim.alpha: 0.05059027350691527
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 77
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.5619776640448702
wandb: 	model.gensim.vector_size: 85
wandb: 	model.gensim.window: 1
wandb: 	model.sklearn.learning_rate: 0.07013866863061279
wandb: 	model.sklearn.max_depth: 74
wandb: 	model.sklearn.min_child_weight: 0.03394661541954934
wandb: 	model.sklearn.n_estimators: 1724
wandb: 	model.sklearn.num_leaves: 338
wandb: 	model.sklearn.reg_alpha: 0.1016132459240833
wandb: 	model.sklearn.reg_lambda: 0.08684797182899825
wandb: 	model.sklearn.subsample: 0.27294767131514264
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140307-qoxtw66y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-68
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/qoxtw66y
2023-02-07 14:03:15.616 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 14:03:15.617 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 141 for sweep.
2023-02-07 14:03:15.617 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.05059027350691527 for sweep.
2023-02-07 14:03:15.617 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:03:15.617 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 77 for sweep.
2023-02-07 14:03:15.618 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 14:03:15.618 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5619776640448702 for sweep.
2023-02-07 14:03:15.618 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 85 for sweep.
2023-02-07 14:03:15.618 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 1 for sweep.
2023-02-07 14:03:15.619 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.07013866863061279 for sweep.
2023-02-07 14:03:15.619 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 74 for sweep.
2023-02-07 14:03:15.619 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03394661541954934 for sweep.
2023-02-07 14:03:15.620 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1724 for sweep.
2023-02-07 14:03:15.620 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 338 for sweep.
2023-02-07 14:03:15.620 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.1016132459240833 for sweep.
2023-02-07 14:03:15.620 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.08684797182899825 for sweep.
2023-02-07 14:03:15.620 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.27294767131514264 for sweep.
2023-02-07 14:03:15.621 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:03:15.625 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140307-qoxtw66y/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 141, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 85, 'window': 1, 'min_count': 3, 'dm': 0, 'sample': 0.5619776640448702, 'workers': 4, 'alpha': 0.05059027350691527, 'epochs': 77}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1724, 'max_depth': 74, 'num_leaves': 338, 'reg_alpha': 0.1016132459240833, 'reg_lambda': 0.08684797182899825, 'subsample': 0.27294767131514264, 'min_child_weight': 0.03394661541954934, 'n_jobs': 4, 'learning_rate': 0.07013866863061279}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 24/3257 [00:00<00:14, 229.10it/s]  2%|‚ñè         | 53/3257 [00:00<00:12, 253.78it/s]  3%|‚ñé         | 82/3257 [00:00<00:11, 267.69it/s]  3%|‚ñé         | 109/3257 [00:00<00:12, 252.29it/s]  4%|‚ñç         | 139/3257 [00:00<00:11, 267.12it/s]  5%|‚ñå         | 166/3257 [00:00<00:11, 264.21it/s]  6%|‚ñå         | 197/3257 [00:00<00:11, 273.25it/s]  7%|‚ñã         | 230/3257 [00:00<00:10, 290.27it/s]  8%|‚ñä         | 260/3257 [00:00<00:10, 281.45it/s]  9%|‚ñâ         | 294/3257 [00:01<00:10, 294.86it/s] 10%|‚ñà         | 326/3257 [00:01<00:09, 299.76it/s] 11%|‚ñà         | 357/3257 [00:01<00:09, 295.66it/s] 12%|‚ñà‚ñè        | 387/3257 [00:01<00:10, 283.33it/s] 13%|‚ñà‚ñé        | 419/3257 [00:01<00:09, 291.46it/s] 14%|‚ñà‚ñç        | 449/3257 [00:01<00:10, 262.54it/s] 15%|‚ñà‚ñç        | 478/3257 [00:01<00:10, 269.67it/s] 16%|‚ñà‚ñå        | 508/3257 [00:01<00:09, 277.34it/s] 16%|‚ñà‚ñã        | 537/3257 [00:01<00:09, 275.75it/s] 17%|‚ñà‚ñã        | 565/3257 [00:02<00:10, 268.47it/s] 18%|‚ñà‚ñä        | 593/3257 [00:02<00:09, 267.79it/s] 19%|‚ñà‚ñâ        | 621/3257 [00:02<00:09, 270.03it/s] 20%|‚ñà‚ñâ        | 651/3257 [00:02<00:09, 275.33it/s] 21%|‚ñà‚ñà        | 679/3257 [00:02<00:09, 276.51it/s] 22%|‚ñà‚ñà‚ñè       | 707/3257 [00:02<00:09, 276.15it/s] 23%|‚ñà‚ñà‚ñé       | 736/3257 [00:02<00:09, 273.54it/s] 24%|‚ñà‚ñà‚ñé       | 769/3257 [00:02<00:08, 289.86it/s] 25%|‚ñà‚ñà‚ñç       | 799/3257 [00:02<00:08, 286.63it/s] 25%|‚ñà‚ñà‚ñå       | 828/3257 [00:02<00:08, 281.70it/s] 26%|‚ñà‚ñà‚ñã       | 857/3257 [00:03<00:12, 194.56it/s] 27%|‚ñà‚ñà‚ñã       | 884/3257 [00:03<00:11, 210.01it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:03<00:10, 231.44it/s] 29%|‚ñà‚ñà‚ñâ       | 946/3257 [00:03<00:09, 252.78it/s] 30%|‚ñà‚ñà‚ñâ       | 977/3257 [00:03<00:08, 267.08it/s] 31%|‚ñà‚ñà‚ñà       | 1006/3257 [00:03<00:08, 267.39it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1034/3257 [00:03<00:08, 263.89it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1062/3257 [00:03<00:08, 265.77it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1091/3257 [00:04<00:08, 267.45it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1119/3257 [00:04<00:07, 269.47it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:04<00:07, 268.42it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1176/3257 [00:04<00:07, 273.01it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1204/3257 [00:04<00:07, 256.96it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1235/3257 [00:04<00:07, 270.21it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1263/3257 [00:04<00:07, 272.63it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1291/3257 [00:04<00:07, 254.88it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1320/3257 [00:04<00:07, 264.52it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:05<00:07, 269.59it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1378/3257 [00:05<00:06, 269.33it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1406/3257 [00:05<00:06, 270.70it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1438/3257 [00:05<00:06, 284.20it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1472/3257 [00:05<00:05, 300.17it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1506/3257 [00:05<00:05, 310.12it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1538/3257 [00:05<00:05, 288.29it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1568/3257 [00:05<00:05, 287.13it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1598/3257 [00:05<00:05, 289.75it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1630/3257 [00:05<00:05, 295.64it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1660/3257 [00:06<00:05, 282.29it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1689/3257 [00:06<00:05, 275.82it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1718/3257 [00:06<00:05, 278.23it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1746/3257 [00:06<00:05, 268.93it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1776/3257 [00:06<00:05, 277.43it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:06<00:05, 278.97it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1833/3257 [00:06<00:05, 274.12it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1864/3257 [00:06<00:04, 283.46it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1893/3257 [00:06<00:04, 280.84it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1923/3257 [00:07<00:04, 286.07it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1962/3257 [00:07<00:04, 315.78it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1994/3257 [00:07<00:04, 312.74it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2026/3257 [00:07<00:03, 313.01it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2058/3257 [00:07<00:04, 295.02it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2089/3257 [00:07<00:03, 298.09it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:07<00:03, 291.20it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2150/3257 [00:07<00:03, 280.29it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2182/3257 [00:07<00:03, 285.39it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2211/3257 [00:08<00:03, 285.65it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2240/3257 [00:08<00:05, 197.55it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2270/3257 [00:08<00:04, 218.84it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2300/3257 [00:08<00:04, 237.45it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2333/3257 [00:08<00:03, 259.98it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2366/3257 [00:08<00:03, 277.72it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2399/3257 [00:08<00:02, 290.11it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2430/3257 [00:08<00:02, 287.51it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2460/3257 [00:09<00:02, 288.46it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2494/3257 [00:09<00:02, 303.02it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2528/3257 [00:09<00:02, 313.01it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2560/3257 [00:09<00:02, 307.73it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2592/3257 [00:09<00:02, 295.59it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:09<00:01, 318.51it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2664/3257 [00:09<00:01, 302.97it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2695/3257 [00:09<00:01, 302.07it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2726/3257 [00:09<00:01, 290.73it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:09<00:01, 302.57it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2794/3257 [00:10<00:01, 310.12it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2826/3257 [00:10<00:01, 298.99it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2860/3257 [00:10<00:01, 310.00it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2893/3257 [00:10<00:01, 308.06it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2926/3257 [00:10<00:01, 313.35it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2958/3257 [00:10<00:01, 291.98it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2989/3257 [00:10<00:00, 296.86it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3022/3257 [00:10<00:00, 305.73it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3058/3257 [00:10<00:00, 321.32it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3091/3257 [00:11<00:00, 323.63it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3127/3257 [00:11<00:00, 333.09it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3161/3257 [00:11<00:00, 314.95it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3193/3257 [00:11<00:00, 312.87it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3225/3257 [00:11<00:00, 304.93it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 281.27it/s]
2023-02-07 14:03:27.491 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:03:27,492][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d85,n5,mc3,s0.561978,t4>', 'datetime': '2023-02-07T14:03:27.492517', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:03:27,492][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:03:27,493][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:03:27,697][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 14:03:27,697][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:03:27,703][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 2159 unique words (76.59% of original 2819, drops 660)', 'datetime': '2023-02-07T14:03:27.703194', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:03:27,703][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 2182579 word corpus (99.95% of original 2183622, drops 1043)', 'datetime': '2023-02-07T14:03:27.703530', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:03:27,712][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 14:03:27,712][gensim.models.word2vec][INFO] - sample=0.561978 downsamples 0 most-common words
[2023-02-07 14:03:27,712][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2182579 word corpus (100.0%% of prior 2182579)', 'datetime': '2023-02-07T14:03:27.712948', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:03:27,726][gensim.models.word2vec][INFO] - estimated required memory for 2159 words and 85 dimensions: 4306400 bytes
[2023-02-07 14:03:27,726][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:03:27,729][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2159 vocabulary and 85 features, using sg=1 hs=0 sample=0.5619776640448702 negative=5 window=1 shrink_windows=True', 'datetime': '2023-02-07T14:03:27.729008', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:03:28,334][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2185836 effective words) took 0.6s, 3624860 effective words/s
[2023-02-07 14:03:28,891][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2185836 effective words) took 0.6s, 3933867 effective words/s
[2023-02-07 14:03:29,445][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2185836 effective words) took 0.6s, 3955725 effective words/s
[2023-02-07 14:03:29,993][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2185836 effective words) took 0.5s, 3995622 effective words/s
[2023-02-07 14:03:30,548][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2185836 effective words) took 0.6s, 3951025 effective words/s
[2023-02-07 14:03:31,102][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2185836 effective words) took 0.6s, 3954805 effective words/s
[2023-02-07 14:03:31,647][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2185836 effective words) took 0.5s, 4018687 effective words/s
[2023-02-07 14:03:32,197][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2185836 effective words) took 0.5s, 3983503 effective words/s
[2023-02-07 14:03:32,747][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2185836 effective words) took 0.5s, 3984660 effective words/s
[2023-02-07 14:03:33,293][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2185836 effective words) took 0.5s, 4017312 effective words/s
[2023-02-07 14:03:33,841][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2185836 effective words) took 0.5s, 3996723 effective words/s
[2023-02-07 14:03:34,385][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2185836 effective words) took 0.5s, 4028654 effective words/s
[2023-02-07 14:03:34,928][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2185836 effective words) took 0.5s, 4036381 effective words/s
[2023-02-07 14:03:35,472][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2185836 effective words) took 0.5s, 4031763 effective words/s
[2023-02-07 14:03:36,021][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2185836 effective words) took 0.5s, 3987028 effective words/s
[2023-02-07 14:03:36,564][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2185836 effective words) took 0.5s, 4038045 effective words/s
[2023-02-07 14:03:37,115][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2185836 effective words) took 0.5s, 3981442 effective words/s
[2023-02-07 14:03:37,659][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2185836 effective words) took 0.5s, 4030036 effective words/s
[2023-02-07 14:03:38,202][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2185836 effective words) took 0.5s, 4041431 effective words/s
[2023-02-07 14:03:38,745][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2185836 effective words) took 0.5s, 4035212 effective words/s
[2023-02-07 14:03:39,289][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2185836 effective words) took 0.5s, 4030409 effective words/s
[2023-02-07 14:03:39,838][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2185836 effective words) took 0.5s, 3991722 effective words/s
[2023-02-07 14:03:40,382][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2185836 effective words) took 0.5s, 4026159 effective words/s
[2023-02-07 14:03:40,920][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2185836 effective words) took 0.5s, 4078540 effective words/s
[2023-02-07 14:03:41,463][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2185836 effective words) took 0.5s, 4034672 effective words/s
[2023-02-07 14:03:42,009][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2185836 effective words) took 0.5s, 4014317 effective words/s
[2023-02-07 14:03:42,558][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2185836 effective words) took 0.5s, 3995740 effective words/s
[2023-02-07 14:03:43,107][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2185836 effective words) took 0.5s, 3991314 effective words/s
[2023-02-07 14:03:43,656][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2185836 effective words) took 0.5s, 3987408 effective words/s
[2023-02-07 14:03:44,206][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2185836 effective words) took 0.5s, 3985782 effective words/s
[2023-02-07 14:03:44,755][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2185836 effective words) took 0.5s, 3995478 effective words/s
[2023-02-07 14:03:45,307][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2185836 effective words) took 0.5s, 3974498 effective words/s
[2023-02-07 14:03:45,859][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2185836 effective words) took 0.6s, 3964777 effective words/s
[2023-02-07 14:03:46,410][gensim.models.word2vec][INFO] - EPOCH 33: training on 2183622 raw words (2185836 effective words) took 0.5s, 3979862 effective words/s
[2023-02-07 14:03:46,954][gensim.models.word2vec][INFO] - EPOCH 34: training on 2183622 raw words (2185836 effective words) took 0.5s, 4026362 effective words/s
[2023-02-07 14:03:47,496][gensim.models.word2vec][INFO] - EPOCH 35: training on 2183622 raw words (2185836 effective words) took 0.5s, 4046561 effective words/s
[2023-02-07 14:03:48,041][gensim.models.word2vec][INFO] - EPOCH 36: training on 2183622 raw words (2185836 effective words) took 0.5s, 4023897 effective words/s
[2023-02-07 14:03:48,589][gensim.models.word2vec][INFO] - EPOCH 37: training on 2183622 raw words (2185836 effective words) took 0.5s, 3999706 effective words/s
[2023-02-07 14:03:49,132][gensim.models.word2vec][INFO] - EPOCH 38: training on 2183622 raw words (2185836 effective words) took 0.5s, 4033765 effective words/s
[2023-02-07 14:03:49,675][gensim.models.word2vec][INFO] - EPOCH 39: training on 2183622 raw words (2185836 effective words) took 0.5s, 4030976 effective words/s
[2023-02-07 14:03:50,222][gensim.models.word2vec][INFO] - EPOCH 40: training on 2183622 raw words (2185836 effective words) took 0.5s, 4007426 effective words/s
[2023-02-07 14:03:50,773][gensim.models.word2vec][INFO] - EPOCH 41: training on 2183622 raw words (2185836 effective words) took 0.5s, 3979369 effective words/s
[2023-02-07 14:03:51,318][gensim.models.word2vec][INFO] - EPOCH 42: training on 2183622 raw words (2185836 effective words) took 0.5s, 4015653 effective words/s
[2023-02-07 14:03:51,868][gensim.models.word2vec][INFO] - EPOCH 43: training on 2183622 raw words (2185836 effective words) took 0.5s, 3988046 effective words/s
[2023-02-07 14:03:52,414][gensim.models.word2vec][INFO] - EPOCH 44: training on 2183622 raw words (2185836 effective words) took 0.5s, 4023980 effective words/s
[2023-02-07 14:03:52,964][gensim.models.word2vec][INFO] - EPOCH 45: training on 2183622 raw words (2185836 effective words) took 0.5s, 3983255 effective words/s
[2023-02-07 14:03:53,515][gensim.models.word2vec][INFO] - EPOCH 46: training on 2183622 raw words (2185836 effective words) took 0.5s, 3977793 effective words/s
[2023-02-07 14:03:54,057][gensim.models.word2vec][INFO] - EPOCH 47: training on 2183622 raw words (2185836 effective words) took 0.5s, 4036651 effective words/s
[2023-02-07 14:03:54,601][gensim.models.word2vec][INFO] - EPOCH 48: training on 2183622 raw words (2185836 effective words) took 0.5s, 4030511 effective words/s
[2023-02-07 14:03:55,147][gensim.models.word2vec][INFO] - EPOCH 49: training on 2183622 raw words (2185836 effective words) took 0.5s, 4015529 effective words/s
[2023-02-07 14:03:55,693][gensim.models.word2vec][INFO] - EPOCH 50: training on 2183622 raw words (2185836 effective words) took 0.5s, 4008259 effective words/s
[2023-02-07 14:03:56,233][gensim.models.word2vec][INFO] - EPOCH 51: training on 2183622 raw words (2185836 effective words) took 0.5s, 4063296 effective words/s
[2023-02-07 14:03:56,777][gensim.models.word2vec][INFO] - EPOCH 52: training on 2183622 raw words (2185836 effective words) took 0.5s, 4025290 effective words/s
[2023-02-07 14:03:57,324][gensim.models.word2vec][INFO] - EPOCH 53: training on 2183622 raw words (2185836 effective words) took 0.5s, 4003405 effective words/s
[2023-02-07 14:03:57,871][gensim.models.word2vec][INFO] - EPOCH 54: training on 2183622 raw words (2185836 effective words) took 0.5s, 4007846 effective words/s
[2023-02-07 14:03:58,414][gensim.models.word2vec][INFO] - EPOCH 55: training on 2183622 raw words (2185836 effective words) took 0.5s, 4037081 effective words/s
[2023-02-07 14:03:58,962][gensim.models.word2vec][INFO] - EPOCH 56: training on 2183622 raw words (2185836 effective words) took 0.5s, 4001206 effective words/s
[2023-02-07 14:03:59,519][gensim.models.word2vec][INFO] - EPOCH 57: training on 2183622 raw words (2185836 effective words) took 0.6s, 3937497 effective words/s
[2023-02-07 14:04:00,069][gensim.models.word2vec][INFO] - EPOCH 58: training on 2183622 raw words (2185836 effective words) took 0.5s, 3985366 effective words/s
[2023-02-07 14:04:00,624][gensim.models.word2vec][INFO] - EPOCH 59: training on 2183622 raw words (2185836 effective words) took 0.6s, 3950884 effective words/s
[2023-02-07 14:04:01,180][gensim.models.word2vec][INFO] - EPOCH 60: training on 2183622 raw words (2185836 effective words) took 0.6s, 3939500 effective words/s
[2023-02-07 14:04:01,740][gensim.models.word2vec][INFO] - EPOCH 61: training on 2183622 raw words (2185836 effective words) took 0.6s, 3914066 effective words/s
[2023-02-07 14:04:02,298][gensim.models.word2vec][INFO] - EPOCH 62: training on 2183622 raw words (2185836 effective words) took 0.6s, 3930534 effective words/s
[2023-02-07 14:04:02,845][gensim.models.word2vec][INFO] - EPOCH 63: training on 2183622 raw words (2185836 effective words) took 0.5s, 4005136 effective words/s
[2023-02-07 14:04:03,400][gensim.models.word2vec][INFO] - EPOCH 64: training on 2183622 raw words (2185836 effective words) took 0.6s, 3947193 effective words/s
[2023-02-07 14:04:03,948][gensim.models.word2vec][INFO] - EPOCH 65: training on 2183622 raw words (2185836 effective words) took 0.5s, 3999853 effective words/s
[2023-02-07 14:04:04,500][gensim.models.word2vec][INFO] - EPOCH 66: training on 2183622 raw words (2185836 effective words) took 0.6s, 3970661 effective words/s
[2023-02-07 14:04:05,059][gensim.models.word2vec][INFO] - EPOCH 67: training on 2183622 raw words (2185836 effective words) took 0.6s, 3913074 effective words/s
[2023-02-07 14:04:05,618][gensim.models.word2vec][INFO] - EPOCH 68: training on 2183622 raw words (2185836 effective words) took 0.6s, 3923916 effective words/s
[2023-02-07 14:04:06,182][gensim.models.word2vec][INFO] - EPOCH 69: training on 2183622 raw words (2185836 effective words) took 0.6s, 3894065 effective words/s
[2023-02-07 14:04:06,744][gensim.models.word2vec][INFO] - EPOCH 70: training on 2183622 raw words (2185836 effective words) took 0.6s, 3902872 effective words/s
[2023-02-07 14:04:07,306][gensim.models.word2vec][INFO] - EPOCH 71: training on 2183622 raw words (2185836 effective words) took 0.6s, 3900186 effective words/s
[2023-02-07 14:04:07,862][gensim.models.word2vec][INFO] - EPOCH 72: training on 2183622 raw words (2185836 effective words) took 0.6s, 3935408 effective words/s
[2023-02-07 14:04:08,434][gensim.models.word2vec][INFO] - EPOCH 73: training on 2183622 raw words (2185836 effective words) took 0.6s, 3831613 effective words/s
[2023-02-07 14:04:09,020][gensim.models.word2vec][INFO] - EPOCH 74: training on 2183622 raw words (2185836 effective words) took 0.6s, 3739974 effective words/s
[2023-02-07 14:04:09,599][gensim.models.word2vec][INFO] - EPOCH 75: training on 2183622 raw words (2185836 effective words) took 0.6s, 3781056 effective words/s
[2023-02-07 14:04:10,183][gensim.models.word2vec][INFO] - EPOCH 76: training on 2183622 raw words (2185836 effective words) took 0.6s, 3753067 effective words/s
[2023-02-07 14:04:10,184][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 168138894 raw words (168309372 effective words) took 42.5s, 3964424 effective words/s', 'datetime': '2023-02-07T14:04:10.184251', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:04:10.184 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:04:13,326][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140307-qoxtw66y/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:04:13.326027', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:04:13,327][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:04:13,335][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140307-qoxtw66y/files/../tmp/embedding_model.pt
2023-02-07 14:04:13.335 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:04:14.430 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:04:14.862 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:04:15.495 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9620078649214343, 'test_mae': 1.0664765915370609, 'test_r2': 0.07185297165358795}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.9
wandb: percentage 0.23413
wandb:   test_mae 1.06648
wandb:   test_mse 1.96201
wandb:    test_r2 0.07185
wandb: 
wandb: üöÄ View run fine-sweep-68 at: https://wandb.ai/xiaoqiz/mof2vec/runs/qoxtw66y
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_140307-qoxtw66y/logs
wandb: Agent Starting Run: t44mq0xg with config:
wandb: 	data.data.wl_step: 5
wandb: 	data.nn.batch_size: 305
wandb: 	model.gensim.alpha: 0.03297105609867235
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 77
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.4339363966031245
wandb: 	model.gensim.vector_size: 75
wandb: 	model.gensim.window: 4
wandb: 	model.sklearn.learning_rate: 0.10307730606778412
wandb: 	model.sklearn.max_depth: 17
wandb: 	model.sklearn.min_child_weight: 0.06233264293175846
wandb: 	model.sklearn.n_estimators: 3330
wandb: 	model.sklearn.num_leaves: 366
wandb: 	model.sklearn.reg_alpha: 0.1557811761014033
wandb: 	model.sklearn.reg_lambda: 0.27345690049476773
wandb: 	model.sklearn.subsample: 0.5210403283823495
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140426-t44mq0xg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-69
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/t44mq0xg
2023-02-07 14:04:34.107 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 5 for sweep.
2023-02-07 14:04:34.108 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 305 for sweep.
2023-02-07 14:04:34.108 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.03297105609867235 for sweep.
2023-02-07 14:04:34.108 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:04:34.109 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 77 for sweep.
2023-02-07 14:04:34.109 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 14:04:34.109 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4339363966031245 for sweep.
2023-02-07 14:04:34.109 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 75 for sweep.
2023-02-07 14:04:34.109 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 4 for sweep.
2023-02-07 14:04:34.110 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.10307730606778412 for sweep.
2023-02-07 14:04:34.110 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 17 for sweep.
2023-02-07 14:04:34.110 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.06233264293175846 for sweep.
2023-02-07 14:04:34.110 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3330 for sweep.
2023-02-07 14:04:34.111 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 366 for sweep.
2023-02-07 14:04:34.111 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.1557811761014033 for sweep.
2023-02-07 14:04:34.111 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.27345690049476773 for sweep.
2023-02-07 14:04:34.111 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5210403283823495 for sweep.
2023-02-07 14:04:34.112 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:04:34.116 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 5}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140426-t44mq0xg/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 305, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 75, 'window': 4, 'min_count': 2, 'dm': 0, 'sample': 0.4339363966031245, 'workers': 4, 'alpha': 0.03297105609867235, 'epochs': 77}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3330, 'max_depth': 17, 'num_leaves': 366, 'reg_alpha': 0.1557811761014033, 'reg_lambda': 0.27345690049476773, 'subsample': 0.5210403283823495, 'min_child_weight': 0.06233264293175846, 'n_jobs': 4, 'learning_rate': 0.10307730606778412}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 20/3257 [00:00<00:16, 197.32it/s]  1%|‚ñè         | 42/3257 [00:00<00:15, 206.43it/s]  2%|‚ñè         | 67/3257 [00:00<00:15, 209.06it/s]  3%|‚ñé         | 92/3257 [00:00<00:14, 222.77it/s]  4%|‚ñé         | 115/3257 [00:00<00:14, 213.12it/s]  4%|‚ñç         | 138/3257 [00:00<00:14, 217.37it/s]  5%|‚ñç         | 161/3257 [00:00<00:14, 220.77it/s]  6%|‚ñå         | 184/3257 [00:00<00:14, 214.96it/s]  6%|‚ñã         | 210/3257 [00:00<00:13, 226.71it/s]  7%|‚ñã         | 237/3257 [00:01<00:12, 234.46it/s]  8%|‚ñä         | 261/3257 [00:01<00:13, 230.29it/s]  9%|‚ñâ         | 290/3257 [00:01<00:12, 245.55it/s] 10%|‚ñâ         | 315/3257 [00:01<00:12, 235.42it/s] 10%|‚ñà         | 339/3257 [00:01<00:12, 235.08it/s] 11%|‚ñà         | 364/3257 [00:01<00:12, 236.35it/s] 12%|‚ñà‚ñè        | 388/3257 [00:01<00:13, 219.55it/s] 13%|‚ñà‚ñé        | 411/3257 [00:01<00:17, 162.31it/s] 13%|‚ñà‚ñé        | 430/3257 [00:02<00:17, 161.20it/s] 14%|‚ñà‚ñç        | 451/3257 [00:02<00:16, 172.19it/s] 15%|‚ñà‚ñç        | 475/3257 [00:02<00:14, 186.70it/s] 15%|‚ñà‚ñå        | 499/3257 [00:02<00:13, 199.96it/s] 16%|‚ñà‚ñå        | 523/3257 [00:02<00:13, 208.19it/s] 17%|‚ñà‚ñã        | 546/3257 [00:02<00:13, 206.96it/s] 17%|‚ñà‚ñã        | 568/3257 [00:02<00:13, 202.69it/s] 18%|‚ñà‚ñä        | 589/3257 [00:02<00:13, 196.23it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:02<00:12, 211.31it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:03<00:12, 211.28it/s] 20%|‚ñà‚ñà        | 659/3257 [00:03<00:13, 194.93it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:12, 199.87it/s] 22%|‚ñà‚ñà‚ñè       | 706/3257 [00:03<00:12, 207.00it/s] 22%|‚ñà‚ñà‚ñè       | 727/3257 [00:03<00:12, 202.21it/s] 23%|‚ñà‚ñà‚ñé       | 748/3257 [00:03<00:12, 202.39it/s] 24%|‚ñà‚ñà‚ñé       | 772/3257 [00:03<00:11, 210.56it/s] 24%|‚ñà‚ñà‚ñç       | 794/3257 [00:03<00:11, 208.24it/s] 25%|‚ñà‚ñà‚ñå       | 816/3257 [00:03<00:11, 210.60it/s] 26%|‚ñà‚ñà‚ñå       | 838/3257 [00:04<00:12, 201.29it/s] 26%|‚ñà‚ñà‚ñã       | 859/3257 [00:04<00:12, 199.16it/s] 27%|‚ñà‚ñà‚ñã       | 880/3257 [00:04<00:11, 198.50it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:04<00:11, 209.27it/s] 29%|‚ñà‚ñà‚ñä       | 929/3257 [00:04<00:10, 216.45it/s] 29%|‚ñà‚ñà‚ñâ       | 951/3257 [00:04<00:10, 215.59it/s] 30%|‚ñà‚ñà‚ñâ       | 973/3257 [00:04<00:10, 214.94it/s] 31%|‚ñà‚ñà‚ñà       | 995/3257 [00:04<00:10, 208.32it/s] 31%|‚ñà‚ñà‚ñà       | 1016/3257 [00:04<00:10, 208.26it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1037/3257 [00:05<00:10, 202.23it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1058/3257 [00:05<00:11, 197.73it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:05<00:10, 201.44it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1102/3257 [00:05<00:10, 205.25it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1123/3257 [00:05<00:10, 202.58it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1144/3257 [00:05<00:10, 200.12it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1168/3257 [00:05<00:09, 209.79it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1190/3257 [00:05<00:11, 186.37it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1210/3257 [00:05<00:10, 187.95it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1236/3257 [00:05<00:09, 206.63it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1258/3257 [00:06<00:09, 201.40it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1279/3257 [00:06<00:10, 196.78it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1299/3257 [00:06<00:09, 196.78it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1322/3257 [00:06<00:09, 205.34it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1346/3257 [00:06<00:09, 211.98it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1368/3257 [00:06<00:09, 205.69it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1389/3257 [00:06<00:09, 196.81it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1419/3257 [00:06<00:08, 223.29it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1442/3257 [00:06<00:08, 217.44it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1468/3257 [00:07<00:07, 228.60it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1492/3257 [00:07<00:07, 230.99it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1517/3257 [00:07<00:07, 233.04it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1541/3257 [00:07<00:08, 213.43it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1563/3257 [00:07<00:11, 142.85it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1583/3257 [00:07<00:10, 154.06it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1606/3257 [00:07<00:09, 171.10it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1631/3257 [00:08<00:08, 190.17it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1653/3257 [00:08<00:08, 184.51it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1673/3257 [00:08<00:08, 185.91it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1693/3257 [00:08<00:08, 189.31it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1716/3257 [00:08<00:07, 198.23it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1737/3257 [00:08<00:07, 190.01it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1762/3257 [00:08<00:07, 205.28it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1786/3257 [00:08<00:06, 213.91it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1808/3257 [00:08<00:07, 202.93it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1829/3257 [00:08<00:07, 202.15it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1853/3257 [00:09<00:06, 212.27it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1878/3257 [00:09<00:06, 222.11it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1901/3257 [00:09<00:06, 216.38it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1923/3257 [00:09<00:06, 212.94it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1954/3257 [00:09<00:05, 237.06it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1978/3257 [00:09<00:05, 230.63it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2002/3257 [00:09<00:05, 230.91it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2026/3257 [00:09<00:05, 230.77it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2050/3257 [00:09<00:05, 214.13it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2072/3257 [00:10<00:05, 209.02it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2095/3257 [00:10<00:05, 211.87it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2117/3257 [00:10<00:05, 210.50it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2139/3257 [00:10<00:05, 199.40it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2160/3257 [00:10<00:05, 197.40it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2182/3257 [00:10<00:05, 198.26it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:10<00:05, 209.61it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2228/3257 [00:10<00:04, 207.71it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2249/3257 [00:10<00:04, 203.74it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2270/3257 [00:11<00:04, 197.44it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2293/3257 [00:11<00:04, 204.42it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2315/3257 [00:11<00:04, 203.66it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2345/3257 [00:11<00:03, 229.93it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2370/3257 [00:11<00:03, 233.45it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2396/3257 [00:11<00:03, 240.85it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2421/3257 [00:11<00:03, 227.05it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2444/3257 [00:11<00:03, 211.36it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2469/3257 [00:11<00:03, 220.81it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2492/3257 [00:12<00:03, 223.09it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2519/3257 [00:12<00:03, 233.90it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2543/3257 [00:12<00:03, 234.15it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2567/3257 [00:12<00:03, 219.67it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2590/3257 [00:12<00:03, 206.64it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2621/3257 [00:12<00:02, 232.11it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2645/3257 [00:12<00:02, 229.77it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2669/3257 [00:12<00:02, 223.94it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2692/3257 [00:12<00:02, 223.81it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2715/3257 [00:13<00:02, 195.45it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2744/3257 [00:13<00:02, 217.89it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:13<00:02, 217.90it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2793/3257 [00:13<00:02, 224.94it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2816/3257 [00:13<00:02, 219.46it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2839/3257 [00:13<00:01, 209.99it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2871/3257 [00:13<00:01, 239.70it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2896/3257 [00:14<00:02, 136.53it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2920/3257 [00:14<00:02, 155.34it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2943/3257 [00:14<00:01, 162.99it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2967/3257 [00:14<00:01, 177.12it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2988/3257 [00:14<00:01, 178.88it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3014/3257 [00:14<00:01, 197.16it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3038/3257 [00:14<00:01, 207.57it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3065/3257 [00:14<00:00, 221.99it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3089/3257 [00:14<00:00, 220.92it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3116/3257 [00:15<00:00, 233.20it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3140/3257 [00:15<00:00, 224.29it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3163/3257 [00:15<00:00, 217.20it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3186/3257 [00:15<00:00, 207.68it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3208/3257 [00:15<00:00, 208.72it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3230/3257 [00:15<00:00, 207.94it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:15<00:00, 214.02it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:15<00:00, 206.61it/s]
2023-02-07 14:04:50.474 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:04:50,475][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d75,n5,mc2,s0.433936,t4>', 'datetime': '2023-02-07T14:04:50.475626', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:04:50,475][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:04:50,476][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:04:50,874][gensim.models.doc2vec][INFO] - collected 21699 word types and 3257 unique tags from a corpus of 3257 examples and 4367244 words
[2023-02-07 14:04:50,874][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:04:50,921][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 18495 unique words (85.23% of original 21699, drops 3204)', 'datetime': '2023-02-07T14:04:50.921527', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:04:50,923][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 4364040 word corpus (99.93% of original 4367244, drops 3204)', 'datetime': '2023-02-07T14:04:50.923038', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:04:50,990][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 21699 items
[2023-02-07 14:04:50,992][gensim.models.word2vec][INFO] - sample=0.433936 downsamples 0 most-common words
[2023-02-07 14:04:50,992][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4364040 word corpus (100.0%% of prior 4364040)', 'datetime': '2023-02-07T14:04:50.992649', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:04:51,106][gensim.models.word2vec][INFO] - estimated required memory for 18495 words and 75 dimensions: 21973000 bytes
[2023-02-07 14:04:51,106][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:04:51,114][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 18495 vocabulary and 75 features, using sg=1 hs=0 sample=0.4339363966031245 negative=5 window=4 shrink_windows=True', 'datetime': '2023-02-07T14:04:51.114457', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:04:52,121][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 76.51% examples, 3370721 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:04:52,401][gensim.models.word2vec][INFO] - EPOCH 0: training on 4367244 raw words (4365521 effective words) took 1.3s, 3398443 effective words/s
[2023-02-07 14:04:53,403][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 82.68% examples, 3632771 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:04:53,608][gensim.models.word2vec][INFO] - EPOCH 1: training on 4367244 raw words (4365521 effective words) took 1.2s, 3620768 effective words/s
[2023-02-07 14:04:54,611][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 82.28% examples, 3616099 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:04:54,816][gensim.models.word2vec][INFO] - EPOCH 2: training on 4367244 raw words (4365521 effective words) took 1.2s, 3617996 effective words/s
[2023-02-07 14:04:55,821][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 83.79% examples, 3692752 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:04:55,998][gensim.models.word2vec][INFO] - EPOCH 3: training on 4367244 raw words (4365521 effective words) took 1.2s, 3705226 effective words/s
[2023-02-07 14:04:57,000][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 84.53% examples, 3718129 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:04:57,174][gensim.models.word2vec][INFO] - EPOCH 4: training on 4367244 raw words (4365521 effective words) took 1.2s, 3713699 effective words/s
[2023-02-07 14:04:58,176][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 84.59% examples, 3723331 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:04:58,348][gensim.models.word2vec][INFO] - EPOCH 5: training on 4367244 raw words (4365521 effective words) took 1.2s, 3722678 effective words/s
[2023-02-07 14:04:59,352][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 84.59% examples, 3714885 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:04:59,519][gensim.models.word2vec][INFO] - EPOCH 6: training on 4367244 raw words (4365521 effective words) took 1.2s, 3733972 effective words/s
[2023-02-07 14:05:00,521][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 86.31% examples, 3793193 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:00,666][gensim.models.word2vec][INFO] - EPOCH 7: training on 4367244 raw words (4365521 effective words) took 1.1s, 3809022 effective words/s
[2023-02-07 14:05:01,669][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 87.14% examples, 3836490 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:01,803][gensim.models.word2vec][INFO] - EPOCH 8: training on 4367244 raw words (4365521 effective words) took 1.1s, 3845486 effective words/s
[2023-02-07 14:05:02,806][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 87.96% examples, 3861118 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:02,937][gensim.models.word2vec][INFO] - EPOCH 9: training on 4367244 raw words (4365521 effective words) took 1.1s, 3851472 effective words/s
[2023-02-07 14:05:03,941][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 85.23% examples, 3749803 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:04,100][gensim.models.word2vec][INFO] - EPOCH 10: training on 4367244 raw words (4365521 effective words) took 1.2s, 3758702 effective words/s
[2023-02-07 14:05:05,105][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 87.14% examples, 3829046 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:05,238][gensim.models.word2vec][INFO] - EPOCH 11: training on 4367244 raw words (4365521 effective words) took 1.1s, 3842997 effective words/s
[2023-02-07 14:05:06,242][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 90.33% examples, 3954632 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:06,340][gensim.models.word2vec][INFO] - EPOCH 12: training on 4367244 raw words (4365521 effective words) took 1.1s, 3965166 effective words/s
[2023-02-07 14:05:07,342][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 89.62% examples, 3934272 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:07,452][gensim.models.word2vec][INFO] - EPOCH 13: training on 4367244 raw words (4365521 effective words) took 1.1s, 3933398 effective words/s
[2023-02-07 14:05:08,454][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 87.38% examples, 3847367 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:08,589][gensim.models.word2vec][INFO] - EPOCH 14: training on 4367244 raw words (4365521 effective words) took 1.1s, 3842450 effective words/s
[2023-02-07 14:05:09,591][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 87.84% examples, 3864101 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:09,716][gensim.models.word2vec][INFO] - EPOCH 15: training on 4367244 raw words (4365521 effective words) took 1.1s, 3880771 effective words/s
[2023-02-07 14:05:10,720][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 89.87% examples, 3931342 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:10,823][gensim.models.word2vec][INFO] - EPOCH 16: training on 4367244 raw words (4365521 effective words) took 1.1s, 3950894 effective words/s
[2023-02-07 14:05:11,826][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 90.76% examples, 3986909 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:11,921][gensim.models.word2vec][INFO] - EPOCH 17: training on 4367244 raw words (4365521 effective words) took 1.1s, 3981071 effective words/s
[2023-02-07 14:05:12,926][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 88.89% examples, 3893210 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:13,042][gensim.models.word2vec][INFO] - EPOCH 18: training on 4367244 raw words (4365521 effective words) took 1.1s, 3896683 effective words/s
[2023-02-07 14:05:14,044][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 89.07% examples, 3915072 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:14,156][gensim.models.word2vec][INFO] - EPOCH 19: training on 4367244 raw words (4365521 effective words) took 1.1s, 3925267 effective words/s
[2023-02-07 14:05:15,158][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 89.07% examples, 3912786 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:15,270][gensim.models.word2vec][INFO] - EPOCH 20: training on 4367244 raw words (4365521 effective words) took 1.1s, 3924679 effective words/s
[2023-02-07 14:05:16,273][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 90.36% examples, 3967947 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:16,369][gensim.models.word2vec][INFO] - EPOCH 21: training on 4367244 raw words (4365521 effective words) took 1.1s, 3978481 effective words/s
[2023-02-07 14:05:17,373][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 88.42% examples, 3869471 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:17,496][gensim.models.word2vec][INFO] - EPOCH 22: training on 4367244 raw words (4365521 effective words) took 1.1s, 3879126 effective words/s
[2023-02-07 14:05:18,499][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 90.76% examples, 3987699 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:18,589][gensim.models.word2vec][INFO] - EPOCH 23: training on 4367244 raw words (4365521 effective words) took 1.1s, 3999552 effective words/s
[2023-02-07 14:05:19,593][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 91.25% examples, 4001769 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:19,680][gensim.models.word2vec][INFO] - EPOCH 24: training on 4367244 raw words (4365521 effective words) took 1.1s, 4005700 effective words/s
[2023-02-07 14:05:20,682][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 91.25% examples, 4007468 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:20,768][gensim.models.word2vec][INFO] - EPOCH 25: training on 4367244 raw words (4365521 effective words) took 1.1s, 4018105 effective words/s
[2023-02-07 14:05:21,772][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 89.07% examples, 3906093 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:21,885][gensim.models.word2vec][INFO] - EPOCH 26: training on 4367244 raw words (4365521 effective words) took 1.1s, 3914434 effective words/s
[2023-02-07 14:05:22,886][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 91.07% examples, 4000401 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:22,974][gensim.models.word2vec][INFO] - EPOCH 27: training on 4367244 raw words (4365521 effective words) took 1.1s, 4011769 effective words/s
[2023-02-07 14:05:23,976][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 91.77% examples, 4038654 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:24,053][gensim.models.word2vec][INFO] - EPOCH 28: training on 4367244 raw words (4365521 effective words) took 1.1s, 4055281 effective words/s
[2023-02-07 14:05:25,055][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 92.14% examples, 4043414 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:25,131][gensim.models.word2vec][INFO] - EPOCH 29: training on 4367244 raw words (4365521 effective words) took 1.1s, 4054302 effective words/s
[2023-02-07 14:05:26,133][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 90.33% examples, 3959467 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:26,234][gensim.models.word2vec][INFO] - EPOCH 30: training on 4367244 raw words (4365521 effective words) took 1.1s, 3962387 effective words/s
[2023-02-07 14:05:27,238][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 90.33% examples, 3954280 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:27,335][gensim.models.word2vec][INFO] - EPOCH 31: training on 4367244 raw words (4365521 effective words) took 1.1s, 3970286 effective words/s
[2023-02-07 14:05:28,339][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 93.00% examples, 4074307 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:28,402][gensim.models.word2vec][INFO] - EPOCH 32: training on 4367244 raw words (4365521 effective words) took 1.1s, 4097239 effective words/s
[2023-02-07 14:05:29,405][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 93.00% examples, 4079741 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:29,468][gensim.models.word2vec][INFO] - EPOCH 33: training on 4367244 raw words (4365521 effective words) took 1.1s, 4100953 effective words/s
[2023-02-07 14:05:30,469][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 91.07% examples, 3999419 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:30,558][gensim.models.word2vec][INFO] - EPOCH 34: training on 4367244 raw words (4365521 effective words) took 1.1s, 4008850 effective words/s
[2023-02-07 14:05:31,560][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 89.87% examples, 3939796 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:31,664][gensim.models.word2vec][INFO] - EPOCH 35: training on 4367244 raw words (4365521 effective words) took 1.1s, 3953633 effective words/s
[2023-02-07 14:05:32,666][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 89.07% examples, 3914712 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:32,779][gensim.models.word2vec][INFO] - EPOCH 36: training on 4367244 raw words (4365521 effective words) took 1.1s, 3921030 effective words/s
[2023-02-07 14:05:33,782][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 89.28% examples, 3914619 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:33,891][gensim.models.word2vec][INFO] - EPOCH 37: training on 4367244 raw words (4365521 effective words) took 1.1s, 3928984 effective words/s
[2023-02-07 14:05:34,893][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 91.07% examples, 3996912 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:34,981][gensim.models.word2vec][INFO] - EPOCH 38: training on 4367244 raw words (4365521 effective words) took 1.1s, 4011304 effective words/s
[2023-02-07 14:05:35,985][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 92.14% examples, 4036105 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:36,060][gensim.models.word2vec][INFO] - EPOCH 39: training on 4367244 raw words (4365521 effective words) took 1.1s, 4050471 effective words/s
[2023-02-07 14:05:37,064][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 88.89% examples, 3897557 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:37,180][gensim.models.word2vec][INFO] - EPOCH 40: training on 4367244 raw words (4365521 effective words) took 1.1s, 3904027 effective words/s
[2023-02-07 14:05:38,181][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 94.20% examples, 4125709 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:38,232][gensim.models.word2vec][INFO] - EPOCH 41: training on 4367244 raw words (4365521 effective words) took 1.1s, 4155221 effective words/s
[2023-02-07 14:05:39,220][gensim.models.word2vec][INFO] - EPOCH 42: training on 4367244 raw words (4365521 effective words) took 1.0s, 4421194 effective words/s
[2023-02-07 14:05:40,211][gensim.models.word2vec][INFO] - EPOCH 43: training on 4367244 raw words (4365521 effective words) took 1.0s, 4413463 effective words/s
[2023-02-07 14:05:41,198][gensim.models.word2vec][INFO] - EPOCH 44: training on 4367244 raw words (4365521 effective words) took 1.0s, 4427259 effective words/s
[2023-02-07 14:05:42,184][gensim.models.word2vec][INFO] - EPOCH 45: training on 4367244 raw words (4365521 effective words) took 1.0s, 4431557 effective words/s
[2023-02-07 14:05:43,179][gensim.models.word2vec][INFO] - EPOCH 46: training on 4367244 raw words (4365521 effective words) took 1.0s, 4394681 effective words/s
[2023-02-07 14:05:44,181][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 100.00% examples, 4359734 words/s, in_qsize 0, out_qsize 1
[2023-02-07 14:05:44,182][gensim.models.word2vec][INFO] - EPOCH 47: training on 4367244 raw words (4365521 effective words) took 1.0s, 4358267 effective words/s
[2023-02-07 14:05:45,171][gensim.models.word2vec][INFO] - EPOCH 48: training on 4367244 raw words (4365521 effective words) took 1.0s, 4417298 effective words/s
[2023-02-07 14:05:46,157][gensim.models.word2vec][INFO] - EPOCH 49: training on 4367244 raw words (4365521 effective words) took 1.0s, 4430553 effective words/s
[2023-02-07 14:05:47,138][gensim.models.word2vec][INFO] - EPOCH 50: training on 4367244 raw words (4365521 effective words) took 1.0s, 4453555 effective words/s
[2023-02-07 14:05:48,120][gensim.models.word2vec][INFO] - EPOCH 51: training on 4367244 raw words (4365521 effective words) took 1.0s, 4452671 effective words/s
[2023-02-07 14:05:49,102][gensim.models.word2vec][INFO] - EPOCH 52: training on 4367244 raw words (4365521 effective words) took 1.0s, 4452678 effective words/s
[2023-02-07 14:05:50,074][gensim.models.word2vec][INFO] - EPOCH 53: training on 4367244 raw words (4365521 effective words) took 1.0s, 4495019 effective words/s
[2023-02-07 14:05:51,052][gensim.models.word2vec][INFO] - EPOCH 54: training on 4367244 raw words (4365521 effective words) took 1.0s, 4472117 effective words/s
[2023-02-07 14:05:52,031][gensim.models.word2vec][INFO] - EPOCH 55: training on 4367244 raw words (4365521 effective words) took 1.0s, 4463019 effective words/s
[2023-02-07 14:05:53,014][gensim.models.word2vec][INFO] - EPOCH 56: training on 4367244 raw words (4365521 effective words) took 1.0s, 4444507 effective words/s
[2023-02-07 14:05:54,016][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 98.46% examples, 4302915 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:54,030][gensim.models.word2vec][INFO] - EPOCH 57: training on 4367244 raw words (4365521 effective words) took 1.0s, 4305794 effective words/s
[2023-02-07 14:05:55,033][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 97.54% examples, 4256877 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:55,054][gensim.models.word2vec][INFO] - EPOCH 58: training on 4367244 raw words (4365521 effective words) took 1.0s, 4265838 effective words/s
[2023-02-07 14:05:56,057][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 97.54% examples, 4261610 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:56,077][gensim.models.word2vec][INFO] - EPOCH 59: training on 4367244 raw words (4365521 effective words) took 1.0s, 4272521 effective words/s
[2023-02-07 14:05:57,080][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 97.76% examples, 4269841 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:57,097][gensim.models.word2vec][INFO] - EPOCH 60: training on 4367244 raw words (4365521 effective words) took 1.0s, 4283717 effective words/s
[2023-02-07 14:05:58,091][gensim.models.word2vec][INFO] - EPOCH 61: training on 4367244 raw words (4365521 effective words) took 1.0s, 4395326 effective words/s
[2023-02-07 14:05:59,093][gensim.models.word2vec][INFO] - EPOCH 62 - PROGRESS: at 96.41% examples, 4210484 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:05:59,127][gensim.models.word2vec][INFO] - EPOCH 62: training on 4367244 raw words (4365521 effective words) took 1.0s, 4220690 effective words/s
[2023-02-07 14:06:00,130][gensim.models.word2vec][INFO] - EPOCH 63 - PROGRESS: at 97.54% examples, 4258549 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:06:00,152][gensim.models.word2vec][INFO] - EPOCH 63: training on 4367244 raw words (4365521 effective words) took 1.0s, 4265716 effective words/s
[2023-02-07 14:06:01,153][gensim.models.word2vec][INFO] - EPOCH 64 - PROGRESS: at 98.13% examples, 4285661 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:06:01,170][gensim.models.word2vec][INFO] - EPOCH 64: training on 4367244 raw words (4365521 effective words) took 1.0s, 4291392 effective words/s
[2023-02-07 14:06:02,175][gensim.models.word2vec][INFO] - EPOCH 65 - PROGRESS: at 96.10% examples, 4181917 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:06:02,213][gensim.models.word2vec][INFO] - EPOCH 65: training on 4367244 raw words (4365521 effective words) took 1.0s, 4188783 effective words/s
[2023-02-07 14:06:03,214][gensim.models.word2vec][INFO] - EPOCH 66 - PROGRESS: at 94.38% examples, 4134299 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:06:03,267][gensim.models.word2vec][INFO] - EPOCH 66: training on 4367244 raw words (4365521 effective words) took 1.1s, 4147213 effective words/s
[2023-02-07 14:06:04,270][gensim.models.word2vec][INFO] - EPOCH 67 - PROGRESS: at 94.38% examples, 4128544 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:06:04,322][gensim.models.word2vec][INFO] - EPOCH 67: training on 4367244 raw words (4365521 effective words) took 1.1s, 4141049 effective words/s
[2023-02-07 14:06:05,328][gensim.models.word2vec][INFO] - EPOCH 68 - PROGRESS: at 93.00% examples, 4068075 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:06:05,393][gensim.models.word2vec][INFO] - EPOCH 68: training on 4367244 raw words (4365521 effective words) took 1.1s, 4082208 effective words/s
[2023-02-07 14:06:06,397][gensim.models.word2vec][INFO] - EPOCH 69 - PROGRESS: at 93.00% examples, 4077272 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:06:06,461][gensim.models.word2vec][INFO] - EPOCH 69: training on 4367244 raw words (4365521 effective words) took 1.1s, 4092655 effective words/s
[2023-02-07 14:06:07,463][gensim.models.word2vec][INFO] - EPOCH 70 - PROGRESS: at 93.00% examples, 4082678 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:06:07,527][gensim.models.word2vec][INFO] - EPOCH 70: training on 4367244 raw words (4365521 effective words) took 1.1s, 4099212 effective words/s
[2023-02-07 14:06:08,529][gensim.models.word2vec][INFO] - EPOCH 71 - PROGRESS: at 93.00% examples, 4084557 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:06:08,595][gensim.models.word2vec][INFO] - EPOCH 71: training on 4367244 raw words (4365521 effective words) took 1.1s, 4096445 effective words/s
[2023-02-07 14:06:09,601][gensim.models.word2vec][INFO] - EPOCH 72 - PROGRESS: at 93.00% examples, 4064732 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:06:09,666][gensim.models.word2vec][INFO] - EPOCH 72: training on 4367244 raw words (4365521 effective words) took 1.1s, 4080263 effective words/s
[2023-02-07 14:06:10,668][gensim.models.word2vec][INFO] - EPOCH 73 - PROGRESS: at 91.40% examples, 4015196 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:06:10,752][gensim.models.word2vec][INFO] - EPOCH 73: training on 4367244 raw words (4365521 effective words) took 1.1s, 4021062 effective words/s
[2023-02-07 14:06:11,758][gensim.models.word2vec][INFO] - EPOCH 74 - PROGRESS: at 92.14% examples, 4031980 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:06:11,831][gensim.models.word2vec][INFO] - EPOCH 74: training on 4367244 raw words (4365521 effective words) took 1.1s, 4053460 effective words/s
[2023-02-07 14:06:12,833][gensim.models.word2vec][INFO] - EPOCH 75 - PROGRESS: at 94.20% examples, 4124210 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:06:12,889][gensim.models.word2vec][INFO] - EPOCH 75: training on 4367244 raw words (4365521 effective words) took 1.1s, 4132227 effective words/s
[2023-02-07 14:06:13,890][gensim.models.word2vec][INFO] - EPOCH 76 - PROGRESS: at 92.42% examples, 4057455 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:06:13,964][gensim.models.word2vec][INFO] - EPOCH 76: training on 4367244 raw words (4365521 effective words) took 1.1s, 4067701 effective words/s
[2023-02-07 14:06:13,964][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 336277788 raw words (336145117 effective words) took 82.8s, 4057292 effective words/s', 'datetime': '2023-02-07T14:06:13.964496', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:06:13.964 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:06:20,678][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140426-t44mq0xg/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:06:20.678895', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:06:20,679][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:06:20,721][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140426-t44mq0xg/files/../tmp/embedding_model.pt
2023-02-07 14:06:20.721 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:06:21.811 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:06:22.211 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:06:22.749 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.7407109658741104, 'test_mae': 1.014789110415278, 'test_r2': 0.17653963622069158}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.92
wandb: percentage 0.14766
wandb:   test_mae 1.01479
wandb:   test_mse 1.74071
wandb:    test_r2 0.17654
wandb: 
wandb: üöÄ View run volcanic-sweep-69 at: https://wandb.ai/xiaoqiz/mof2vec/runs/t44mq0xg
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_140426-t44mq0xg/logs
wandb: Agent Starting Run: gm83fzh7 with config:
wandb: 	data.data.wl_step: 5
wandb: 	data.nn.batch_size: 412
wandb: 	model.gensim.alpha: 0.2948422441646388
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 72
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.43349488724181734
wandb: 	model.gensim.vector_size: 30
wandb: 	model.gensim.window: 7
wandb: 	model.sklearn.learning_rate: 0.00974486658001855
wandb: 	model.sklearn.max_depth: 10
wandb: 	model.sklearn.min_child_weight: 0.05543125062661985
wandb: 	model.sklearn.n_estimators: 4028
wandb: 	model.sklearn.num_leaves: 459
wandb: 	model.sklearn.reg_alpha: 0.16151573148155007
wandb: 	model.sklearn.reg_lambda: 0.0028693692919922156
wandb: 	model.sklearn.subsample: 0.21085886384374508
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140632-gm83fzh7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-70
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/gm83fzh7
2023-02-07 14:06:39.815 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 5 for sweep.
2023-02-07 14:06:39.815 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 412 for sweep.
2023-02-07 14:06:39.816 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.2948422441646388 for sweep.
2023-02-07 14:06:39.816 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:06:39.816 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 72 for sweep.
2023-02-07 14:06:39.816 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 14:06:39.816 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.43349488724181734 for sweep.
2023-02-07 14:06:39.817 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 30 for sweep.
2023-02-07 14:06:39.817 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 7 for sweep.
2023-02-07 14:06:39.817 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.00974486658001855 for sweep.
2023-02-07 14:06:39.817 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 10 for sweep.
2023-02-07 14:06:39.818 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05543125062661985 for sweep.
2023-02-07 14:06:39.818 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4028 for sweep.
2023-02-07 14:06:39.818 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 459 for sweep.
2023-02-07 14:06:39.818 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.16151573148155007 for sweep.
2023-02-07 14:06:39.819 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.0028693692919922156 for sweep.
2023-02-07 14:06:39.819 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.21085886384374508 for sweep.
2023-02-07 14:06:39.819 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:06:39.823 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 5}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140632-gm83fzh7/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 412, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 30, 'window': 7, 'min_count': 5, 'dm': 0, 'sample': 0.43349488724181734, 'workers': 4, 'alpha': 0.2948422441646388, 'epochs': 72}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4028, 'max_depth': 10, 'num_leaves': 459, 'reg_alpha': 0.16151573148155007, 'reg_lambda': 0.0028693692919922156, 'subsample': 0.21085886384374508, 'min_child_weight': 0.05543125062661985, 'n_jobs': 4, 'learning_rate': 0.00974486658001855}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 24/3257 [00:00<00:14, 229.81it/s]  1%|‚ñè         | 48/3257 [00:00<00:13, 231.64it/s]  2%|‚ñè         | 72/3257 [00:00<00:14, 227.17it/s]  3%|‚ñé         | 96/3257 [00:00<00:13, 231.84it/s]  4%|‚ñé         | 120/3257 [00:00<00:13, 230.72it/s]  4%|‚ñç         | 146/3257 [00:00<00:12, 239.59it/s]  5%|‚ñå         | 170/3257 [00:00<00:13, 231.57it/s]  6%|‚ñå         | 194/3257 [00:00<00:13, 230.80it/s]  7%|‚ñã         | 219/3257 [00:00<00:12, 236.29it/s]  8%|‚ñä         | 247/3257 [00:01<00:12, 247.34it/s]  8%|‚ñä         | 272/3257 [00:01<00:12, 239.11it/s]  9%|‚ñâ         | 300/3257 [00:01<00:12, 246.11it/s] 10%|‚ñà         | 328/3257 [00:01<00:11, 250.22it/s] 11%|‚ñà         | 354/3257 [00:01<00:11, 247.78it/s] 12%|‚ñà‚ñè        | 379/3257 [00:01<00:12, 236.06it/s] 12%|‚ñà‚ñè        | 404/3257 [00:01<00:12, 235.66it/s] 13%|‚ñà‚ñé        | 428/3257 [00:01<00:13, 216.45it/s] 14%|‚ñà‚ñç        | 450/3257 [00:01<00:12, 216.78it/s] 15%|‚ñà‚ñç        | 476/3257 [00:02<00:12, 226.11it/s] 15%|‚ñà‚ñå        | 502/3257 [00:02<00:11, 233.91it/s] 16%|‚ñà‚ñå        | 526/3257 [00:02<00:11, 230.54it/s] 17%|‚ñà‚ñã        | 550/3257 [00:02<00:11, 231.49it/s] 18%|‚ñà‚ñä        | 574/3257 [00:02<00:12, 211.85it/s] 18%|‚ñà‚ñä        | 600/3257 [00:02<00:11, 223.94it/s] 19%|‚ñà‚ñâ        | 623/3257 [00:02<00:11, 224.14it/s] 20%|‚ñà‚ñâ        | 647/3257 [00:02<00:11, 227.89it/s] 21%|‚ñà‚ñà        | 670/3257 [00:02<00:11, 224.10it/s] 21%|‚ñà‚ñà‚ñè       | 693/3257 [00:03<00:11, 223.05it/s] 22%|‚ñà‚ñà‚ñè       | 718/3257 [00:03<00:11, 226.67it/s] 23%|‚ñà‚ñà‚ñé       | 741/3257 [00:03<00:11, 210.37it/s] 24%|‚ñà‚ñà‚ñé       | 768/3257 [00:03<00:11, 224.83it/s] 24%|‚ñà‚ñà‚ñç       | 791/3257 [00:03<00:11, 215.40it/s] 25%|‚ñà‚ñà‚ñç       | 814/3257 [00:03<00:11, 215.20it/s] 26%|‚ñà‚ñà‚ñå       | 836/3257 [00:03<00:11, 211.38it/s] 26%|‚ñà‚ñà‚ñã       | 858/3257 [00:03<00:11, 211.73it/s] 27%|‚ñà‚ñà‚ñã       | 880/3257 [00:04<00:16, 145.79it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:04<00:13, 169.29it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:04<00:12, 185.27it/s] 29%|‚ñà‚ñà‚ñâ       | 957/3257 [00:04<00:11, 201.84it/s] 30%|‚ñà‚ñà‚ñà       | 980/3257 [00:04<00:11, 206.28it/s] 31%|‚ñà‚ñà‚ñà       | 1003/3257 [00:04<00:10, 212.35it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1026/3257 [00:04<00:10, 214.54it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1049/3257 [00:04<00:10, 206.05it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1074/3257 [00:04<00:10, 217.16it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1097/3257 [00:05<00:10, 209.34it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1120/3257 [00:05<00:09, 214.87it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1142/3257 [00:05<00:09, 212.93it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1167/3257 [00:05<00:09, 222.32it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1190/3257 [00:05<00:10, 198.52it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1211/3257 [00:05<00:10, 200.68it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1238/3257 [00:05<00:09, 218.91it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1261/3257 [00:05<00:09, 218.36it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1284/3257 [00:05<00:09, 204.56it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1308/3257 [00:06<00:09, 211.34it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1333/3257 [00:06<00:08, 220.31it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1356/3257 [00:06<00:08, 218.83it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1379/3257 [00:06<00:08, 221.80it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1402/3257 [00:06<00:08, 219.66it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1431/3257 [00:06<00:07, 238.48it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1456/3257 [00:06<00:07, 241.77it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1481/3257 [00:06<00:07, 242.52it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1510/3257 [00:06<00:06, 252.78it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1536/3257 [00:06<00:07, 227.17it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1560/3257 [00:07<00:07, 221.70it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1583/3257 [00:07<00:07, 221.28it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1609/3257 [00:07<00:07, 230.47it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:07<00:07, 223.69it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1657/3257 [00:07<00:07, 220.13it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1680/3257 [00:07<00:07, 216.02it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1703/3257 [00:07<00:07, 218.09it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:07<00:07, 214.24it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1750/3257 [00:07<00:07, 212.46it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1774/3257 [00:08<00:06, 219.03it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1798/3257 [00:08<00:06, 224.38it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1821/3257 [00:08<00:06, 223.15it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1844/3257 [00:08<00:06, 222.33it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:08<00:06, 224.35it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1892/3257 [00:08<00:06, 223.88it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1917/3257 [00:08<00:05, 228.06it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1945/3257 [00:08<00:05, 240.54it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1974/3257 [00:08<00:05, 248.59it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1999/3257 [00:09<00:05, 241.28it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2024/3257 [00:09<00:05, 240.71it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2049/3257 [00:09<00:05, 226.55it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2072/3257 [00:09<00:05, 220.39it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2096/3257 [00:09<00:05, 218.72it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:09<00:05, 215.93it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2142/3257 [00:09<00:05, 210.99it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2164/3257 [00:09<00:05, 210.89it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2186/3257 [00:09<00:05, 210.25it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2208/3257 [00:10<00:05, 209.70it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2231/3257 [00:10<00:04, 214.50it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2253/3257 [00:10<00:07, 132.39it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2273/3257 [00:10<00:06, 143.25it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2300/3257 [00:10<00:05, 169.38it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2330/3257 [00:10<00:04, 198.46it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:10<00:04, 219.65it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2388/3257 [00:10<00:03, 234.80it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2414/3257 [00:11<00:03, 231.44it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2439/3257 [00:11<00:03, 226.89it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2463/3257 [00:11<00:03, 230.30it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2489/3257 [00:11<00:03, 237.89it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2519/3257 [00:11<00:02, 251.18it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2546/3257 [00:11<00:02, 254.69it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2572/3257 [00:11<00:02, 235.78it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2597/3257 [00:11<00:02, 235.16it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2630/3257 [00:11<00:02, 260.31it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2657/3257 [00:12<00:02, 246.85it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2683/3257 [00:12<00:02, 247.56it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2709/3257 [00:12<00:02, 221.20it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2735/3257 [00:12<00:02, 230.45it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:12<00:02, 234.97it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2788/3257 [00:12<00:01, 243.32it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2813/3257 [00:12<00:01, 244.37it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2838/3257 [00:12<00:01, 229.36it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2869/3257 [00:12<00:01, 250.53it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2895/3257 [00:13<00:01, 240.60it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2922/3257 [00:13<00:01, 247.52it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2948/3257 [00:13<00:01, 234.73it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2972/3257 [00:13<00:01, 235.61it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2996/3257 [00:13<00:01, 232.32it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3021/3257 [00:13<00:00, 237.13it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:13<00:00, 246.08it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3080/3257 [00:13<00:00, 262.33it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3107/3257 [00:13<00:00, 263.29it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3134/3257 [00:14<00:00, 259.82it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3161/3257 [00:14<00:00, 250.68it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3187/3257 [00:14<00:00, 241.56it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3212/3257 [00:14<00:00, 242.73it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3239/3257 [00:14<00:00, 247.91it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:14<00:00, 224.03it/s]
2023-02-07 14:06:54.949 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:06:54,950][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d30,n5,mc5,s0.433495,t4>', 'datetime': '2023-02-07T14:06:54.950393', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:06:54,951][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:06:54,951][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:06:55,341][gensim.models.doc2vec][INFO] - collected 21699 word types and 3257 unique tags from a corpus of 3257 examples and 4367244 words
[2023-02-07 14:06:55,341][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:06:55,372][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 11219 unique words (51.70% of original 21699, drops 10480)', 'datetime': '2023-02-07T14:06:55.372223', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:06:55,373][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 4341582 word corpus (99.41% of original 4367244, drops 25662)', 'datetime': '2023-02-07T14:06:55.373891', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:06:55,411][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 21699 items
[2023-02-07 14:06:55,411][gensim.models.word2vec][INFO] - sample=0.433495 downsamples 0 most-common words
[2023-02-07 14:06:55,412][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4341582 word corpus (100.0%% of prior 4341582)', 'datetime': '2023-02-07T14:06:55.412095', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:06:55,480][gensim.models.word2vec][INFO] - estimated required memory for 11219 words and 30 dimensions: 9344300 bytes
[2023-02-07 14:06:55,480][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:06:55,482][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 11219 vocabulary and 30 features, using sg=1 hs=0 sample=0.43349488724181734 negative=5 window=7 shrink_windows=True', 'datetime': '2023-02-07T14:06:55.482815', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:06:56,348][gensim.models.word2vec][INFO] - EPOCH 0: training on 4367244 raw words (4343128 effective words) took 0.9s, 5025129 effective words/s
[2023-02-07 14:06:57,102][gensim.models.word2vec][INFO] - EPOCH 1: training on 4367244 raw words (4343128 effective words) took 0.8s, 5773934 effective words/s
[2023-02-07 14:06:57,837][gensim.models.word2vec][INFO] - EPOCH 2: training on 4367244 raw words (4343128 effective words) took 0.7s, 5914817 effective words/s
[2023-02-07 14:06:58,564][gensim.models.word2vec][INFO] - EPOCH 3: training on 4367244 raw words (4343128 effective words) took 0.7s, 5982837 effective words/s
[2023-02-07 14:06:59,290][gensim.models.word2vec][INFO] - EPOCH 4: training on 4367244 raw words (4343128 effective words) took 0.7s, 5990165 effective words/s
[2023-02-07 14:07:00,016][gensim.models.word2vec][INFO] - EPOCH 5: training on 4367244 raw words (4343128 effective words) took 0.7s, 6000050 effective words/s
[2023-02-07 14:07:00,740][gensim.models.word2vec][INFO] - EPOCH 6: training on 4367244 raw words (4343128 effective words) took 0.7s, 6008090 effective words/s
[2023-02-07 14:07:01,461][gensim.models.word2vec][INFO] - EPOCH 7: training on 4367244 raw words (4343128 effective words) took 0.7s, 6033020 effective words/s
[2023-02-07 14:07:02,182][gensim.models.word2vec][INFO] - EPOCH 8: training on 4367244 raw words (4343128 effective words) took 0.7s, 6029074 effective words/s
[2023-02-07 14:07:02,899][gensim.models.word2vec][INFO] - EPOCH 9: training on 4367244 raw words (4343128 effective words) took 0.7s, 6068043 effective words/s
[2023-02-07 14:07:03,621][gensim.models.word2vec][INFO] - EPOCH 10: training on 4367244 raw words (4343128 effective words) took 0.7s, 6031700 effective words/s
[2023-02-07 14:07:04,344][gensim.models.word2vec][INFO] - EPOCH 11: training on 4367244 raw words (4343128 effective words) took 0.7s, 6013981 effective words/s
[2023-02-07 14:07:05,066][gensim.models.word2vec][INFO] - EPOCH 12: training on 4367244 raw words (4343128 effective words) took 0.7s, 6033689 effective words/s
[2023-02-07 14:07:05,788][gensim.models.word2vec][INFO] - EPOCH 13: training on 4367244 raw words (4343128 effective words) took 0.7s, 6019960 effective words/s
[2023-02-07 14:07:06,516][gensim.models.word2vec][INFO] - EPOCH 14: training on 4367244 raw words (4343128 effective words) took 0.7s, 5983876 effective words/s
[2023-02-07 14:07:07,244][gensim.models.word2vec][INFO] - EPOCH 15: training on 4367244 raw words (4343128 effective words) took 0.7s, 5976076 effective words/s
[2023-02-07 14:07:07,971][gensim.models.word2vec][INFO] - EPOCH 16: training on 4367244 raw words (4343128 effective words) took 0.7s, 5987281 effective words/s
[2023-02-07 14:07:08,699][gensim.models.word2vec][INFO] - EPOCH 17: training on 4367244 raw words (4343128 effective words) took 0.7s, 5967300 effective words/s
[2023-02-07 14:07:09,426][gensim.models.word2vec][INFO] - EPOCH 18: training on 4367244 raw words (4343128 effective words) took 0.7s, 5987529 effective words/s
[2023-02-07 14:07:10,154][gensim.models.word2vec][INFO] - EPOCH 19: training on 4367244 raw words (4343128 effective words) took 0.7s, 5977084 effective words/s
[2023-02-07 14:07:10,880][gensim.models.word2vec][INFO] - EPOCH 20: training on 4367244 raw words (4343128 effective words) took 0.7s, 5992685 effective words/s
[2023-02-07 14:07:11,612][gensim.models.word2vec][INFO] - EPOCH 21: training on 4367244 raw words (4343128 effective words) took 0.7s, 5943522 effective words/s
[2023-02-07 14:07:12,336][gensim.models.word2vec][INFO] - EPOCH 22: training on 4367244 raw words (4343128 effective words) took 0.7s, 6010330 effective words/s
[2023-02-07 14:07:13,063][gensim.models.word2vec][INFO] - EPOCH 23: training on 4367244 raw words (4343128 effective words) took 0.7s, 5980761 effective words/s
[2023-02-07 14:07:13,792][gensim.models.word2vec][INFO] - EPOCH 24: training on 4367244 raw words (4343128 effective words) took 0.7s, 5971706 effective words/s
[2023-02-07 14:07:14,520][gensim.models.word2vec][INFO] - EPOCH 25: training on 4367244 raw words (4343128 effective words) took 0.7s, 5976509 effective words/s
[2023-02-07 14:07:15,251][gensim.models.word2vec][INFO] - EPOCH 26: training on 4367244 raw words (4343128 effective words) took 0.7s, 5953061 effective words/s
[2023-02-07 14:07:15,974][gensim.models.word2vec][INFO] - EPOCH 27: training on 4367244 raw words (4343128 effective words) took 0.7s, 6015149 effective words/s
[2023-02-07 14:07:16,696][gensim.models.word2vec][INFO] - EPOCH 28: training on 4367244 raw words (4343128 effective words) took 0.7s, 6026947 effective words/s
[2023-02-07 14:07:17,415][gensim.models.word2vec][INFO] - EPOCH 29: training on 4367244 raw words (4343128 effective words) took 0.7s, 6049691 effective words/s
[2023-02-07 14:07:18,132][gensim.models.word2vec][INFO] - EPOCH 30: training on 4367244 raw words (4343128 effective words) took 0.7s, 6061130 effective words/s
[2023-02-07 14:07:18,852][gensim.models.word2vec][INFO] - EPOCH 31: training on 4367244 raw words (4343128 effective words) took 0.7s, 6049489 effective words/s
[2023-02-07 14:07:19,570][gensim.models.word2vec][INFO] - EPOCH 32: training on 4367244 raw words (4343128 effective words) took 0.7s, 6057238 effective words/s
[2023-02-07 14:07:20,289][gensim.models.word2vec][INFO] - EPOCH 33: training on 4367244 raw words (4343128 effective words) took 0.7s, 6049139 effective words/s
[2023-02-07 14:07:21,005][gensim.models.word2vec][INFO] - EPOCH 34: training on 4367244 raw words (4343128 effective words) took 0.7s, 6075621 effective words/s
[2023-02-07 14:07:21,727][gensim.models.word2vec][INFO] - EPOCH 35: training on 4367244 raw words (4343128 effective words) took 0.7s, 6028261 effective words/s
[2023-02-07 14:07:22,448][gensim.models.word2vec][INFO] - EPOCH 36: training on 4367244 raw words (4343128 effective words) took 0.7s, 6038614 effective words/s
[2023-02-07 14:07:23,166][gensim.models.word2vec][INFO] - EPOCH 37: training on 4367244 raw words (4343128 effective words) took 0.7s, 6051020 effective words/s
[2023-02-07 14:07:23,884][gensim.models.word2vec][INFO] - EPOCH 38: training on 4367244 raw words (4343128 effective words) took 0.7s, 6060579 effective words/s
[2023-02-07 14:07:24,604][gensim.models.word2vec][INFO] - EPOCH 39: training on 4367244 raw words (4343128 effective words) took 0.7s, 6049194 effective words/s
[2023-02-07 14:07:25,324][gensim.models.word2vec][INFO] - EPOCH 40: training on 4367244 raw words (4343128 effective words) took 0.7s, 6044037 effective words/s
[2023-02-07 14:07:26,048][gensim.models.word2vec][INFO] - EPOCH 41: training on 4367244 raw words (4343128 effective words) took 0.7s, 6008724 effective words/s
[2023-02-07 14:07:26,769][gensim.models.word2vec][INFO] - EPOCH 42: training on 4367244 raw words (4343128 effective words) took 0.7s, 6033597 effective words/s
[2023-02-07 14:07:27,494][gensim.models.word2vec][INFO] - EPOCH 43: training on 4367244 raw words (4343128 effective words) took 0.7s, 6001360 effective words/s
[2023-02-07 14:07:28,213][gensim.models.word2vec][INFO] - EPOCH 44: training on 4367244 raw words (4343128 effective words) took 0.7s, 6051631 effective words/s
[2023-02-07 14:07:28,934][gensim.models.word2vec][INFO] - EPOCH 45: training on 4367244 raw words (4343128 effective words) took 0.7s, 6036492 effective words/s
[2023-02-07 14:07:29,652][gensim.models.word2vec][INFO] - EPOCH 46: training on 4367244 raw words (4343128 effective words) took 0.7s, 6053792 effective words/s
[2023-02-07 14:07:30,382][gensim.models.word2vec][INFO] - EPOCH 47: training on 4367244 raw words (4343128 effective words) took 0.7s, 5956743 effective words/s
[2023-02-07 14:07:31,102][gensim.models.word2vec][INFO] - EPOCH 48: training on 4367244 raw words (4343128 effective words) took 0.7s, 6045575 effective words/s
[2023-02-07 14:07:31,821][gensim.models.word2vec][INFO] - EPOCH 49: training on 4367244 raw words (4343128 effective words) took 0.7s, 6048621 effective words/s
[2023-02-07 14:07:32,546][gensim.models.word2vec][INFO] - EPOCH 50: training on 4367244 raw words (4343128 effective words) took 0.7s, 6000296 effective words/s
[2023-02-07 14:07:33,267][gensim.models.word2vec][INFO] - EPOCH 51: training on 4367244 raw words (4343128 effective words) took 0.7s, 6038415 effective words/s
[2023-02-07 14:07:33,986][gensim.models.word2vec][INFO] - EPOCH 52: training on 4367244 raw words (4343128 effective words) took 0.7s, 6046554 effective words/s
[2023-02-07 14:07:34,706][gensim.models.word2vec][INFO] - EPOCH 53: training on 4367244 raw words (4343128 effective words) took 0.7s, 6047924 effective words/s
[2023-02-07 14:07:35,424][gensim.models.word2vec][INFO] - EPOCH 54: training on 4367244 raw words (4343128 effective words) took 0.7s, 6053430 effective words/s
[2023-02-07 14:07:36,143][gensim.models.word2vec][INFO] - EPOCH 55: training on 4367244 raw words (4343128 effective words) took 0.7s, 6055011 effective words/s
[2023-02-07 14:07:36,872][gensim.models.word2vec][INFO] - EPOCH 56: training on 4367244 raw words (4343128 effective words) took 0.7s, 5965391 effective words/s
[2023-02-07 14:07:37,592][gensim.models.word2vec][INFO] - EPOCH 57: training on 4367244 raw words (4343128 effective words) took 0.7s, 6035747 effective words/s
[2023-02-07 14:07:38,310][gensim.models.word2vec][INFO] - EPOCH 58: training on 4367244 raw words (4343128 effective words) took 0.7s, 6063801 effective words/s
[2023-02-07 14:07:39,030][gensim.models.word2vec][INFO] - EPOCH 59: training on 4367244 raw words (4343128 effective words) took 0.7s, 6041473 effective words/s
[2023-02-07 14:07:39,749][gensim.models.word2vec][INFO] - EPOCH 60: training on 4367244 raw words (4343128 effective words) took 0.7s, 6052267 effective words/s
[2023-02-07 14:07:40,472][gensim.models.word2vec][INFO] - EPOCH 61: training on 4367244 raw words (4343128 effective words) took 0.7s, 6016179 effective words/s
[2023-02-07 14:07:41,193][gensim.models.word2vec][INFO] - EPOCH 62: training on 4367244 raw words (4343128 effective words) took 0.7s, 6036983 effective words/s
[2023-02-07 14:07:41,912][gensim.models.word2vec][INFO] - EPOCH 63: training on 4367244 raw words (4343128 effective words) took 0.7s, 6052577 effective words/s
[2023-02-07 14:07:42,629][gensim.models.word2vec][INFO] - EPOCH 64: training on 4367244 raw words (4343128 effective words) took 0.7s, 6069940 effective words/s
[2023-02-07 14:07:43,354][gensim.models.word2vec][INFO] - EPOCH 65: training on 4367244 raw words (4343128 effective words) took 0.7s, 6003051 effective words/s
[2023-02-07 14:07:44,070][gensim.models.word2vec][INFO] - EPOCH 66: training on 4367244 raw words (4343128 effective words) took 0.7s, 6073571 effective words/s
[2023-02-07 14:07:44,791][gensim.models.word2vec][INFO] - EPOCH 67: training on 4367244 raw words (4343128 effective words) took 0.7s, 6039229 effective words/s
[2023-02-07 14:07:45,511][gensim.models.word2vec][INFO] - EPOCH 68: training on 4367244 raw words (4343128 effective words) took 0.7s, 6039996 effective words/s
[2023-02-07 14:07:46,229][gensim.models.word2vec][INFO] - EPOCH 69: training on 4367244 raw words (4343128 effective words) took 0.7s, 6062451 effective words/s
[2023-02-07 14:07:46,946][gensim.models.word2vec][INFO] - EPOCH 70: training on 4367244 raw words (4343128 effective words) took 0.7s, 6060691 effective words/s
[2023-02-07 14:07:47,666][gensim.models.word2vec][INFO] - EPOCH 71: training on 4367244 raw words (4343128 effective words) took 0.7s, 6043218 effective words/s
[2023-02-07 14:07:47,666][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 314441568 raw words (312705216 effective words) took 52.2s, 5992381 effective words/s', 'datetime': '2023-02-07T14:07:47.666904', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:07:47.667 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:07:53,302][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140632-gm83fzh7/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:07:53.302419', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:07:53,303][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:07:53,313][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140632-gm83fzh7/files/../tmp/embedding_model.pt
2023-02-07 14:07:53.314 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:07:54.210 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:07:54.550 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:07:54.827 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.414678443699115, 'test_mae': 1.1949819410619742, 'test_r2': -0.14228727723332413}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.0
wandb: percentage 0.48297
wandb:   test_mae 1.19498
wandb:   test_mse 2.41468
wandb:    test_r2 -0.14229
wandb: 
wandb: üöÄ View run misty-sweep-70 at: https://wandb.ai/xiaoqiz/mof2vec/runs/gm83fzh7
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_140632-gm83fzh7/logs
wandb: Agent Starting Run: 0ohkzu99 with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 628
wandb: 	model.gensim.alpha: 0.016097856612342202
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 91
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.26962202212975916
wandb: 	model.gensim.vector_size: 69
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.721539611490147
wandb: 	model.sklearn.max_depth: 7
wandb: 	model.sklearn.min_child_weight: 0.05673870018136004
wandb: 	model.sklearn.n_estimators: 1577
wandb: 	model.sklearn.num_leaves: 339
wandb: 	model.sklearn.reg_alpha: 0.8219863442913689
wandb: 	model.sklearn.reg_lambda: 0.24865522516591076
wandb: 	model.sklearn.subsample: 0.34904041819142617
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140807-0ohkzu99
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-71
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/0ohkzu99
2023-02-07 14:08:14.397 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 14:08:14.398 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 628 for sweep.
2023-02-07 14:08:14.398 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.016097856612342202 for sweep.
2023-02-07 14:08:14.398 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:08:14.399 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 91 for sweep.
2023-02-07 14:08:14.399 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 14:08:14.399 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.26962202212975916 for sweep.
2023-02-07 14:08:14.399 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 69 for sweep.
2023-02-07 14:08:14.399 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 14:08:14.400 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.721539611490147 for sweep.
2023-02-07 14:08:14.400 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 7 for sweep.
2023-02-07 14:08:14.400 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.05673870018136004 for sweep.
2023-02-07 14:08:14.400 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1577 for sweep.
2023-02-07 14:08:14.400 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 339 for sweep.
2023-02-07 14:08:14.401 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.8219863442913689 for sweep.
2023-02-07 14:08:14.401 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.24865522516591076 for sweep.
2023-02-07 14:08:14.401 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.34904041819142617 for sweep.
2023-02-07 14:08:14.401 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:08:14.405 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140807-0ohkzu99/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 628, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 69, 'window': 3, 'min_count': 4, 'dm': 0, 'sample': 0.26962202212975916, 'workers': 4, 'alpha': 0.016097856612342202, 'epochs': 91}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1577, 'max_depth': 7, 'num_leaves': 339, 'reg_alpha': 0.8219863442913689, 'reg_lambda': 0.24865522516591076, 'subsample': 0.34904041819142617, 'min_child_weight': 0.05673870018136004, 'n_jobs': 4, 'learning_rate': 0.721539611490147}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 32/3257 [00:00<00:10, 318.52it/s]  2%|‚ñè         | 65/3257 [00:00<00:09, 325.17it/s]  3%|‚ñé         | 98/3257 [00:00<00:09, 318.44it/s]  4%|‚ñç         | 131/3257 [00:00<00:09, 321.06it/s]  5%|‚ñå         | 164/3257 [00:00<00:09, 321.95it/s]  6%|‚ñå         | 198/3257 [00:00<00:09, 327.36it/s]  7%|‚ñã         | 236/3257 [00:00<00:08, 343.67it/s]  8%|‚ñä         | 271/3257 [00:00<00:08, 338.70it/s]  9%|‚ñâ         | 306/3257 [00:00<00:08, 341.85it/s] 10%|‚ñà         | 341/3257 [00:01<00:11, 246.84it/s] 12%|‚ñà‚ñè        | 375/3257 [00:01<00:10, 267.56it/s] 12%|‚ñà‚ñè        | 407/3257 [00:01<00:10, 280.08it/s] 13%|‚ñà‚ñé        | 438/3257 [00:01<00:10, 273.79it/s] 15%|‚ñà‚ñç        | 474/3257 [00:01<00:09, 292.99it/s] 16%|‚ñà‚ñå        | 510/3257 [00:01<00:08, 308.60it/s] 17%|‚ñà‚ñã        | 543/3257 [00:01<00:08, 313.03it/s] 18%|‚ñà‚ñä        | 576/3257 [00:01<00:08, 298.93it/s] 19%|‚ñà‚ñâ        | 614/3257 [00:02<00:08, 320.34it/s] 20%|‚ñà‚ñâ        | 647/3257 [00:02<00:08, 313.94it/s] 21%|‚ñà‚ñà        | 679/3257 [00:02<00:08, 312.62it/s] 22%|‚ñà‚ñà‚ñè       | 711/3257 [00:02<00:08, 314.37it/s] 23%|‚ñà‚ñà‚ñé       | 743/3257 [00:02<00:08, 303.19it/s] 24%|‚ñà‚ñà‚ñç       | 776/3257 [00:02<00:08, 309.41it/s] 25%|‚ñà‚ñà‚ñç       | 808/3257 [00:02<00:07, 311.94it/s] 26%|‚ñà‚ñà‚ñå       | 840/3257 [00:02<00:07, 308.07it/s] 27%|‚ñà‚ñà‚ñã       | 871/3257 [00:02<00:07, 306.68it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:02<00:07, 315.73it/s] 29%|‚ñà‚ñà‚ñâ       | 937/3257 [00:03<00:07, 313.49it/s] 30%|‚ñà‚ñà‚ñâ       | 972/3257 [00:03<00:07, 321.99it/s] 31%|‚ñà‚ñà‚ñà       | 1005/3257 [00:03<00:07, 316.27it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1037/3257 [00:03<00:07, 306.37it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1068/3257 [00:03<00:07, 306.30it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1099/3257 [00:03<00:07, 306.46it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1130/3257 [00:03<00:06, 306.74it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1163/3257 [00:03<00:06, 312.23it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1195/3257 [00:03<00:06, 298.20it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1228/3257 [00:03<00:06, 306.28it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1262/3257 [00:04<00:06, 314.85it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1294/3257 [00:04<00:06, 306.43it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:04<00:06, 312.79it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1363/3257 [00:04<00:05, 321.47it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1396/3257 [00:04<00:05, 318.07it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1434/3257 [00:04<00:05, 333.44it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1473/3257 [00:04<00:05, 349.60it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1509/3257 [00:04<00:07, 249.54it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1539/3257 [00:05<00:06, 249.97it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1571/3257 [00:05<00:06, 265.59it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1605/3257 [00:05<00:05, 280.89it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1635/3257 [00:05<00:05, 285.25it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1665/3257 [00:05<00:05, 288.17it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1698/3257 [00:05<00:05, 298.87it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1729/3257 [00:05<00:05, 296.82it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1760/3257 [00:05<00:05, 277.50it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1789/3257 [00:05<00:05, 269.22it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1817/3257 [00:06<00:05, 264.84it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1848/3257 [00:06<00:05, 275.11it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1880/3257 [00:06<00:04, 287.12it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1912/3257 [00:06<00:04, 295.52it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1942/3257 [00:06<00:04, 296.49it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:06<00:04, 308.46it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2008/3257 [00:06<00:04, 308.44it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2040/3257 [00:06<00:03, 304.32it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2071/3257 [00:06<00:04, 290.56it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2101/3257 [00:06<00:04, 288.16it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2130/3257 [00:07<00:03, 283.19it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2160/3257 [00:07<00:03, 283.22it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2191/3257 [00:07<00:03, 288.78it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2220/3257 [00:07<00:03, 288.89it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2249/3257 [00:07<00:03, 286.10it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2278/3257 [00:07<00:03, 279.14it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2310/3257 [00:07<00:03, 288.71it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2348/3257 [00:07<00:02, 312.72it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2381/3257 [00:07<00:02, 316.71it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2413/3257 [00:08<00:02, 308.79it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2444/3257 [00:08<00:02, 297.99it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2476/3257 [00:08<00:02, 304.08it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2511/3257 [00:08<00:02, 315.95it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2543/3257 [00:08<00:02, 315.92it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2575/3257 [00:08<00:02, 294.73it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2606/3257 [00:08<00:02, 298.99it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2641/3257 [00:08<00:01, 312.33it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2673/3257 [00:08<00:01, 304.97it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2704/3257 [00:09<00:01, 284.85it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2734/3257 [00:09<00:01, 287.45it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:09<00:01, 294.71it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2801/3257 [00:09<00:01, 303.64it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2832/3257 [00:09<00:01, 289.59it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2868/3257 [00:09<00:01, 307.67it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2900/3257 [00:09<00:01, 301.04it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2931/3257 [00:09<00:01, 202.77it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2957/3257 [00:10<00:01, 214.55it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2986/3257 [00:10<00:01, 231.71it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3019/3257 [00:10<00:00, 255.54it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3054/3257 [00:10<00:00, 279.51it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3088/3257 [00:10<00:00, 293.78it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3124/3257 [00:10<00:00, 311.97it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3157/3257 [00:10<00:00, 300.72it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3189/3257 [00:10<00:00, 294.53it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3220/3257 [00:10<00:00, 281.42it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3254/3257 [00:11<00:00, 296.41it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 295.62it/s]
2023-02-07 14:08:25.765 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:08:25,766][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d69,n5,mc4,s0.269622,t4>', 'datetime': '2023-02-07T14:08:25.766388', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:08:25,766][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:08:25,766][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:08:26,025][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 14:08:26,025][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:08:26,037][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 4674 unique words (70.16% of original 6662, drops 1988)', 'datetime': '2023-02-07T14:08:26.037456', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:08:26,037][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 2908210 word corpus (99.89% of original 2911496, drops 3286)', 'datetime': '2023-02-07T14:08:26.037804', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:08:26,053][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 14:08:26,053][gensim.models.word2vec][INFO] - sample=0.269622 downsamples 0 most-common words
[2023-02-07 14:08:26,054][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2908210 word corpus (100.0%% of prior 2908210)', 'datetime': '2023-02-07T14:08:26.054500', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:08:26,081][gensim.models.word2vec][INFO] - estimated required memory for 4674 words and 69 dimensions: 6467380 bytes
[2023-02-07 14:08:26,081][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:08:26,084][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 4674 vocabulary and 69 features, using sg=1 hs=0 sample=0.26962202212975916 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T14:08:26.084438', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:08:26,793][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2911467 effective words) took 0.7s, 4127989 effective words/s
[2023-02-07 14:08:27,462][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2911467 effective words) took 0.7s, 4360014 effective words/s
[2023-02-07 14:08:28,134][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2911467 effective words) took 0.7s, 4342475 effective words/s
[2023-02-07 14:08:28,798][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2911467 effective words) took 0.7s, 4393701 effective words/s
[2023-02-07 14:08:29,460][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2911467 effective words) took 0.7s, 4408426 effective words/s
[2023-02-07 14:08:30,110][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2911467 effective words) took 0.6s, 4488507 effective words/s
[2023-02-07 14:08:30,759][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2911467 effective words) took 0.6s, 4494155 effective words/s
[2023-02-07 14:08:31,400][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2911467 effective words) took 0.6s, 4555008 effective words/s
[2023-02-07 14:08:32,032][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2911467 effective words) took 0.6s, 4614249 effective words/s
[2023-02-07 14:08:32,658][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2911467 effective words) took 0.6s, 4661007 effective words/s
[2023-02-07 14:08:33,275][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2911467 effective words) took 0.6s, 4731828 effective words/s
[2023-02-07 14:08:33,894][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2911467 effective words) took 0.6s, 4711819 effective words/s
[2023-02-07 14:08:34,518][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2911467 effective words) took 0.6s, 4679274 effective words/s
[2023-02-07 14:08:35,144][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2911467 effective words) took 0.6s, 4657763 effective words/s
[2023-02-07 14:08:35,770][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2911467 effective words) took 0.6s, 4657634 effective words/s
[2023-02-07 14:08:36,396][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2911467 effective words) took 0.6s, 4661900 effective words/s
[2023-02-07 14:08:37,016][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2911467 effective words) took 0.6s, 4707169 effective words/s
[2023-02-07 14:08:37,631][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2911467 effective words) took 0.6s, 4737180 effective words/s
[2023-02-07 14:08:38,255][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2911467 effective words) took 0.6s, 4684573 effective words/s
[2023-02-07 14:08:38,870][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2911467 effective words) took 0.6s, 4747324 effective words/s
[2023-02-07 14:08:39,480][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2911467 effective words) took 0.6s, 4778307 effective words/s
[2023-02-07 14:08:40,093][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2911467 effective words) took 0.6s, 4759807 effective words/s
[2023-02-07 14:08:40,706][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2911467 effective words) took 0.6s, 4757804 effective words/s
[2023-02-07 14:08:41,328][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2911467 effective words) took 0.6s, 4694999 effective words/s
[2023-02-07 14:08:41,949][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2911467 effective words) took 0.6s, 4690039 effective words/s
[2023-02-07 14:08:42,567][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2911467 effective words) took 0.6s, 4726473 effective words/s
[2023-02-07 14:08:43,186][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2911467 effective words) took 0.6s, 4713900 effective words/s
[2023-02-07 14:08:43,796][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2911467 effective words) took 0.6s, 4783377 effective words/s
[2023-02-07 14:08:44,406][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2911467 effective words) took 0.6s, 4784086 effective words/s
[2023-02-07 14:08:45,014][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2911467 effective words) took 0.6s, 4794554 effective words/s
[2023-02-07 14:08:45,619][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2911467 effective words) took 0.6s, 4818187 effective words/s
[2023-02-07 14:08:46,224][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2911467 effective words) took 0.6s, 4826683 effective words/s
[2023-02-07 14:08:46,831][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2911467 effective words) took 0.6s, 4810576 effective words/s
[2023-02-07 14:08:47,439][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2911467 effective words) took 0.6s, 4798079 effective words/s
[2023-02-07 14:08:48,048][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2911467 effective words) took 0.6s, 4790394 effective words/s
[2023-02-07 14:08:48,651][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2911467 effective words) took 0.6s, 4832125 effective words/s
[2023-02-07 14:08:49,253][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2911467 effective words) took 0.6s, 4844454 effective words/s
[2023-02-07 14:08:49,861][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2911467 effective words) took 0.6s, 4802230 effective words/s
[2023-02-07 14:08:50,470][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2911467 effective words) took 0.6s, 4791681 effective words/s
[2023-02-07 14:08:51,077][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2911467 effective words) took 0.6s, 4810064 effective words/s
[2023-02-07 14:08:51,694][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2911467 effective words) took 0.6s, 4724784 effective words/s
[2023-02-07 14:08:52,308][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2911467 effective words) took 0.6s, 4755501 effective words/s
[2023-02-07 14:08:52,916][gensim.models.word2vec][INFO] - EPOCH 42: training on 2911496 raw words (2911467 effective words) took 0.6s, 4793399 effective words/s
[2023-02-07 14:08:53,522][gensim.models.word2vec][INFO] - EPOCH 43: training on 2911496 raw words (2911467 effective words) took 0.6s, 4821721 effective words/s
[2023-02-07 14:08:54,131][gensim.models.word2vec][INFO] - EPOCH 44: training on 2911496 raw words (2911467 effective words) took 0.6s, 4789555 effective words/s
[2023-02-07 14:08:54,740][gensim.models.word2vec][INFO] - EPOCH 45: training on 2911496 raw words (2911467 effective words) took 0.6s, 4793484 effective words/s
[2023-02-07 14:08:55,346][gensim.models.word2vec][INFO] - EPOCH 46: training on 2911496 raw words (2911467 effective words) took 0.6s, 4817284 effective words/s
[2023-02-07 14:08:55,955][gensim.models.word2vec][INFO] - EPOCH 47: training on 2911496 raw words (2911467 effective words) took 0.6s, 4794695 effective words/s
[2023-02-07 14:08:56,571][gensim.models.word2vec][INFO] - EPOCH 48: training on 2911496 raw words (2911467 effective words) took 0.6s, 4734089 effective words/s
[2023-02-07 14:08:57,183][gensim.models.word2vec][INFO] - EPOCH 49: training on 2911496 raw words (2911467 effective words) took 0.6s, 4765470 effective words/s
[2023-02-07 14:08:57,788][gensim.models.word2vec][INFO] - EPOCH 50: training on 2911496 raw words (2911467 effective words) took 0.6s, 4825259 effective words/s
[2023-02-07 14:08:58,394][gensim.models.word2vec][INFO] - EPOCH 51: training on 2911496 raw words (2911467 effective words) took 0.6s, 4818625 effective words/s
[2023-02-07 14:08:59,002][gensim.models.word2vec][INFO] - EPOCH 52: training on 2911496 raw words (2911467 effective words) took 0.6s, 4798182 effective words/s
[2023-02-07 14:08:59,620][gensim.models.word2vec][INFO] - EPOCH 53: training on 2911496 raw words (2911467 effective words) took 0.6s, 4714019 effective words/s
[2023-02-07 14:09:00,220][gensim.models.word2vec][INFO] - EPOCH 54: training on 2911496 raw words (2911467 effective words) took 0.6s, 4864928 effective words/s
[2023-02-07 14:09:00,835][gensim.models.word2vec][INFO] - EPOCH 55: training on 2911496 raw words (2911467 effective words) took 0.6s, 4741755 effective words/s
[2023-02-07 14:09:01,441][gensim.models.word2vec][INFO] - EPOCH 56: training on 2911496 raw words (2911467 effective words) took 0.6s, 4816055 effective words/s
[2023-02-07 14:09:02,063][gensim.models.word2vec][INFO] - EPOCH 57: training on 2911496 raw words (2911467 effective words) took 0.6s, 4692496 effective words/s
[2023-02-07 14:09:02,670][gensim.models.word2vec][INFO] - EPOCH 58: training on 2911496 raw words (2911467 effective words) took 0.6s, 4806340 effective words/s
[2023-02-07 14:09:03,292][gensim.models.word2vec][INFO] - EPOCH 59: training on 2911496 raw words (2911467 effective words) took 0.6s, 4690149 effective words/s
[2023-02-07 14:09:03,908][gensim.models.word2vec][INFO] - EPOCH 60: training on 2911496 raw words (2911467 effective words) took 0.6s, 4736346 effective words/s
[2023-02-07 14:09:04,532][gensim.models.word2vec][INFO] - EPOCH 61: training on 2911496 raw words (2911467 effective words) took 0.6s, 4670281 effective words/s
[2023-02-07 14:09:05,136][gensim.models.word2vec][INFO] - EPOCH 62: training on 2911496 raw words (2911467 effective words) took 0.6s, 4831548 effective words/s
[2023-02-07 14:09:05,756][gensim.models.word2vec][INFO] - EPOCH 63: training on 2911496 raw words (2911467 effective words) took 0.6s, 4707955 effective words/s
[2023-02-07 14:09:06,360][gensim.models.word2vec][INFO] - EPOCH 64: training on 2911496 raw words (2911467 effective words) took 0.6s, 4831103 effective words/s
[2023-02-07 14:09:06,985][gensim.models.word2vec][INFO] - EPOCH 65: training on 2911496 raw words (2911467 effective words) took 0.6s, 4663951 effective words/s
[2023-02-07 14:09:07,599][gensim.models.word2vec][INFO] - EPOCH 66: training on 2911496 raw words (2911467 effective words) took 0.6s, 4748741 effective words/s
[2023-02-07 14:09:08,225][gensim.models.word2vec][INFO] - EPOCH 67: training on 2911496 raw words (2911467 effective words) took 0.6s, 4661662 effective words/s
[2023-02-07 14:09:08,833][gensim.models.word2vec][INFO] - EPOCH 68: training on 2911496 raw words (2911467 effective words) took 0.6s, 4795699 effective words/s
[2023-02-07 14:09:09,445][gensim.models.word2vec][INFO] - EPOCH 69: training on 2911496 raw words (2911467 effective words) took 0.6s, 4762708 effective words/s
[2023-02-07 14:09:10,058][gensim.models.word2vec][INFO] - EPOCH 70: training on 2911496 raw words (2911467 effective words) took 0.6s, 4761099 effective words/s
[2023-02-07 14:09:10,675][gensim.models.word2vec][INFO] - EPOCH 71: training on 2911496 raw words (2911467 effective words) took 0.6s, 4738144 effective words/s
[2023-02-07 14:09:11,284][gensim.models.word2vec][INFO] - EPOCH 72: training on 2911496 raw words (2911467 effective words) took 0.6s, 4786076 effective words/s
[2023-02-07 14:09:11,889][gensim.models.word2vec][INFO] - EPOCH 73: training on 2911496 raw words (2911467 effective words) took 0.6s, 4823565 effective words/s
[2023-02-07 14:09:12,509][gensim.models.word2vec][INFO] - EPOCH 74: training on 2911496 raw words (2911467 effective words) took 0.6s, 4706317 effective words/s
[2023-02-07 14:09:13,129][gensim.models.word2vec][INFO] - EPOCH 75: training on 2911496 raw words (2911467 effective words) took 0.6s, 4703753 effective words/s
[2023-02-07 14:09:13,749][gensim.models.word2vec][INFO] - EPOCH 76: training on 2911496 raw words (2911467 effective words) took 0.6s, 4714993 effective words/s
[2023-02-07 14:09:14,367][gensim.models.word2vec][INFO] - EPOCH 77: training on 2911496 raw words (2911467 effective words) took 0.6s, 4720638 effective words/s
[2023-02-07 14:09:14,984][gensim.models.word2vec][INFO] - EPOCH 78: training on 2911496 raw words (2911467 effective words) took 0.6s, 4731850 effective words/s
[2023-02-07 14:09:15,595][gensim.models.word2vec][INFO] - EPOCH 79: training on 2911496 raw words (2911467 effective words) took 0.6s, 4776491 effective words/s
[2023-02-07 14:09:16,213][gensim.models.word2vec][INFO] - EPOCH 80: training on 2911496 raw words (2911467 effective words) took 0.6s, 4720110 effective words/s
[2023-02-07 14:09:16,828][gensim.models.word2vec][INFO] - EPOCH 81: training on 2911496 raw words (2911467 effective words) took 0.6s, 4739734 effective words/s
[2023-02-07 14:09:17,450][gensim.models.word2vec][INFO] - EPOCH 82: training on 2911496 raw words (2911467 effective words) took 0.6s, 4693701 effective words/s
[2023-02-07 14:09:18,080][gensim.models.word2vec][INFO] - EPOCH 83: training on 2911496 raw words (2911467 effective words) took 0.6s, 4633910 effective words/s
[2023-02-07 14:09:18,711][gensim.models.word2vec][INFO] - EPOCH 84: training on 2911496 raw words (2911467 effective words) took 0.6s, 4618856 effective words/s
[2023-02-07 14:09:19,341][gensim.models.word2vec][INFO] - EPOCH 85: training on 2911496 raw words (2911467 effective words) took 0.6s, 4629724 effective words/s
[2023-02-07 14:09:19,967][gensim.models.word2vec][INFO] - EPOCH 86: training on 2911496 raw words (2911467 effective words) took 0.6s, 4668870 effective words/s
[2023-02-07 14:09:20,592][gensim.models.word2vec][INFO] - EPOCH 87: training on 2911496 raw words (2911467 effective words) took 0.6s, 4664908 effective words/s
[2023-02-07 14:09:21,219][gensim.models.word2vec][INFO] - EPOCH 88: training on 2911496 raw words (2911467 effective words) took 0.6s, 4647789 effective words/s
[2023-02-07 14:09:21,849][gensim.models.word2vec][INFO] - EPOCH 89: training on 2911496 raw words (2911467 effective words) took 0.6s, 4630434 effective words/s
[2023-02-07 14:09:22,481][gensim.models.word2vec][INFO] - EPOCH 90: training on 2911496 raw words (2911467 effective words) took 0.6s, 4614029 effective words/s
[2023-02-07 14:09:22,482][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 264946136 raw words (264943497 effective words) took 56.4s, 4697895 effective words/s', 'datetime': '2023-02-07T14:09:22.482181', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:09:22.482 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:09:26,743][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140807-0ohkzu99/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:09:26.743625', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:09:26,744][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:09:26,755][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140807-0ohkzu99/files/../tmp/embedding_model.pt
2023-02-07 14:09:26.755 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:09:27.748 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:09:28.132 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:09:28.637 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.056100585214914, 'test_mae': 1.0909617603213693, 'test_r2': 0.027341489161176846}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.038 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.038 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.87
wandb: percentage 0.29841
wandb:   test_mae 1.09096
wandb:   test_mse 2.0561
wandb:    test_r2 0.02734
wandb: 
wandb: üöÄ View run silvery-sweep-71 at: https://wandb.ai/xiaoqiz/mof2vec/runs/0ohkzu99
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_140807-0ohkzu99/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 15rrmksk with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 125
wandb: 	model.gensim.alpha: 0.028064599920321207
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 100
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.3655639866291609
wandb: 	model.gensim.vector_size: 100
wandb: 	model.gensim.window: 1
wandb: 	model.sklearn.learning_rate: 0.4148915581894548
wandb: 	model.sklearn.max_depth: 26
wandb: 	model.sklearn.min_child_weight: 0.016612924908770248
wandb: 	model.sklearn.n_estimators: 3252
wandb: 	model.sklearn.num_leaves: 434
wandb: 	model.sklearn.reg_alpha: 0.2235608870504401
wandb: 	model.sklearn.reg_lambda: 0.7162276446677466
wandb: 	model.sklearn.subsample: 0.4759464355701763
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140946-15rrmksk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-72
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/15rrmksk
2023-02-07 14:09:55.535 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 14:09:55.535 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 125 for sweep.
2023-02-07 14:09:55.535 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.028064599920321207 for sweep.
2023-02-07 14:09:55.536 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:09:55.536 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 100 for sweep.
2023-02-07 14:09:55.536 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 14:09:55.536 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.3655639866291609 for sweep.
2023-02-07 14:09:55.537 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 100 for sweep.
2023-02-07 14:09:55.537 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 1 for sweep.
2023-02-07 14:09:55.537 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.4148915581894548 for sweep.
2023-02-07 14:09:55.537 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 26 for sweep.
2023-02-07 14:09:55.537 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.016612924908770248 for sweep.
2023-02-07 14:09:55.538 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3252 for sweep.
2023-02-07 14:09:55.539 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 434 for sweep.
2023-02-07 14:09:55.539 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.2235608870504401 for sweep.
2023-02-07 14:09:55.539 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.7162276446677466 for sweep.
2023-02-07 14:09:55.539 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.4759464355701763 for sweep.
2023-02-07 14:09:55.540 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:09:55.547 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140946-15rrmksk/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 125, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 100, 'window': 1, 'min_count': 2, 'dm': 0, 'sample': 0.3655639866291609, 'workers': 4, 'alpha': 0.028064599920321207, 'epochs': 100}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3252, 'max_depth': 26, 'num_leaves': 434, 'reg_alpha': 0.2235608870504401, 'reg_lambda': 0.7162276446677466, 'subsample': 0.4759464355701763, 'min_child_weight': 0.016612924908770248, 'n_jobs': 4, 'learning_rate': 0.4148915581894548}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:18, 170.65it/s]  1%|          | 39/3257 [00:00<00:16, 192.90it/s]  2%|‚ñè         | 59/3257 [00:00<00:17, 183.69it/s]  3%|‚ñé         | 82/3257 [00:00<00:16, 197.61it/s]  3%|‚ñé         | 102/3257 [00:00<00:15, 198.08it/s]  4%|‚ñé         | 122/3257 [00:00<00:16, 184.91it/s]  4%|‚ñç         | 146/3257 [00:00<00:15, 200.45it/s]  5%|‚ñå         | 167/3257 [00:00<00:16, 191.87it/s]  6%|‚ñå         | 187/3257 [00:00<00:15, 193.55it/s]  6%|‚ñã         | 208/3257 [00:01<00:15, 197.16it/s]  7%|‚ñã         | 234/3257 [00:01<00:14, 214.19it/s]  8%|‚ñä         | 256/3257 [00:01<00:14, 208.86it/s]  9%|‚ñä         | 277/3257 [00:01<00:14, 208.53it/s]  9%|‚ñâ         | 299/3257 [00:01<00:13, 211.85it/s] 10%|‚ñâ         | 321/3257 [00:01<00:13, 210.90it/s] 11%|‚ñà         | 343/3257 [00:01<00:14, 203.57it/s] 11%|‚ñà         | 364/3257 [00:01<00:14, 204.08it/s] 12%|‚ñà‚ñè        | 385/3257 [00:01<00:14, 195.87it/s] 12%|‚ñà‚ñè        | 405/3257 [00:02<00:14, 194.78it/s] 13%|‚ñà‚ñé        | 425/3257 [00:02<00:15, 181.45it/s] 14%|‚ñà‚ñé        | 444/3257 [00:02<00:16, 173.69it/s] 14%|‚ñà‚ñç        | 465/3257 [00:02<00:15, 182.62it/s] 15%|‚ñà‚ñç        | 484/3257 [00:02<00:15, 178.88it/s] 16%|‚ñà‚ñå        | 505/3257 [00:02<00:14, 187.19it/s] 16%|‚ñà‚ñå        | 524/3257 [00:02<00:14, 187.64it/s] 17%|‚ñà‚ñã        | 544/3257 [00:02<00:14, 191.17it/s] 17%|‚ñà‚ñã        | 564/3257 [00:02<00:15, 170.92it/s] 18%|‚ñà‚ñä        | 582/3257 [00:03<00:15, 170.21it/s] 19%|‚ñà‚ñä        | 604/3257 [00:03<00:14, 180.81it/s] 19%|‚ñà‚ñâ        | 623/3257 [00:03<00:14, 181.60it/s] 20%|‚ñà‚ñâ        | 645/3257 [00:03<00:14, 185.52it/s] 20%|‚ñà‚ñà        | 664/3257 [00:03<00:14, 178.10it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:14, 179.84it/s] 22%|‚ñà‚ñà‚ñè       | 703/3257 [00:03<00:13, 184.87it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:03<00:13, 184.09it/s] 23%|‚ñà‚ñà‚ñé       | 741/3257 [00:03<00:14, 176.39it/s] 23%|‚ñà‚ñà‚ñé       | 764/3257 [00:04<00:13, 191.06it/s] 24%|‚ñà‚ñà‚ñç       | 784/3257 [00:04<00:13, 184.05it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:04<00:13, 185.92it/s] 25%|‚ñà‚ñà‚ñå       | 825/3257 [00:04<00:13, 180.20it/s] 26%|‚ñà‚ñà‚ñå       | 844/3257 [00:04<00:13, 174.01it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:04<00:13, 177.28it/s] 27%|‚ñà‚ñà‚ñã       | 882/3257 [00:04<00:13, 177.38it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:04<00:12, 187.89it/s] 28%|‚ñà‚ñà‚ñä       | 925/3257 [00:04<00:12, 189.83it/s] 29%|‚ñà‚ñà‚ñâ       | 945/3257 [00:05<00:12, 188.41it/s] 30%|‚ñà‚ñà‚ñâ       | 967/3257 [00:05<00:11, 192.98it/s] 30%|‚ñà‚ñà‚ñà       | 987/3257 [00:05<00:12, 187.35it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:05<00:12, 182.09it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:05<00:12, 180.55it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1046/3257 [00:05<00:17, 123.79it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1065/3257 [00:05<00:15, 137.35it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1083/3257 [00:05<00:14, 146.28it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1103/3257 [00:06<00:13, 158.11it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1121/3257 [00:06<00:13, 162.73it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1139/3257 [00:06<00:12, 164.35it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1158/3257 [00:06<00:12, 169.58it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1176/3257 [00:06<00:12, 171.29it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1194/3257 [00:06<00:12, 160.40it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1211/3257 [00:06<00:12, 158.66it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1233/3257 [00:06<00:11, 173.12it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1251/3257 [00:06<00:11, 172.66it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:07<00:11, 178.50it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1290/3257 [00:07<00:11, 165.85it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1309/3257 [00:07<00:11, 168.28it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1329/3257 [00:07<00:10, 176.45it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1349/3257 [00:07<00:10, 182.48it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1368/3257 [00:07<00:10, 175.72it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1386/3257 [00:07<00:10, 171.55it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1409/3257 [00:07<00:09, 186.11it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:07<00:09, 191.33it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1453/3257 [00:08<00:09, 196.50it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1475/3257 [00:08<00:08, 202.90it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1497/3257 [00:08<00:08, 207.43it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:08<00:08, 200.39it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1540/3257 [00:08<00:08, 191.78it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1560/3257 [00:08<00:09, 187.97it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1579/3257 [00:08<00:09, 185.25it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1602/3257 [00:08<00:08, 195.30it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1622/3257 [00:08<00:08, 194.35it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1642/3257 [00:08<00:08, 189.61it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1662/3257 [00:09<00:08, 182.74it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1681/3257 [00:09<00:08, 181.29it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1701/3257 [00:09<00:08, 186.48it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1720/3257 [00:09<00:08, 186.82it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1739/3257 [00:09<00:08, 170.89it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1761/3257 [00:09<00:08, 182.50it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1781/3257 [00:09<00:07, 186.92it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1800/3257 [00:09<00:07, 186.40it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1819/3257 [00:09<00:07, 186.77it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1838/3257 [00:10<00:07, 183.98it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1858/3257 [00:10<00:07, 185.90it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1880/3257 [00:10<00:07, 194.61it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1900/3257 [00:10<00:06, 195.13it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1920/3257 [00:10<00:06, 191.85it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1946/3257 [00:10<00:06, 210.38it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1971/3257 [00:10<00:05, 220.69it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1994/3257 [00:10<00:06, 208.64it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2016/3257 [00:10<00:06, 201.54it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2038/3257 [00:11<00:05, 204.46it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2059/3257 [00:11<00:06, 182.74it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2079/3257 [00:11<00:06, 186.04it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2098/3257 [00:11<00:06, 180.87it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:11<00:06, 180.37it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2139/3257 [00:11<00:06, 179.52it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2158/3257 [00:11<00:06, 180.92it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:11<00:05, 183.46it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2199/3257 [00:11<00:05, 188.41it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2218/3257 [00:12<00:05, 182.53it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2237/3257 [00:12<00:05, 183.52it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2256/3257 [00:12<00:05, 181.66it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2275/3257 [00:12<00:05, 169.22it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:12<00:05, 185.60it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2324/3257 [00:12<00:04, 201.79it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2350/3257 [00:12<00:04, 217.34it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2372/3257 [00:12<00:04, 211.08it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2397/3257 [00:12<00:03, 220.79it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2420/3257 [00:13<00:04, 206.40it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2441/3257 [00:13<00:04, 194.65it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2462/3257 [00:13<00:04, 197.72it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2482/3257 [00:13<00:06, 121.84it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2507/3257 [00:13<00:05, 145.60it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2530/3257 [00:13<00:04, 162.95it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:13<00:04, 172.00it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2571/3257 [00:14<00:04, 166.66it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2590/3257 [00:14<00:03, 169.06it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2618/3257 [00:14<00:03, 196.85it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:14<00:03, 202.24it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2662/3257 [00:14<00:03, 191.89it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2686/3257 [00:14<00:02, 203.29it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2707/3257 [00:14<00:03, 181.17it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2726/3257 [00:14<00:02, 181.19it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2750/3257 [00:14<00:02, 196.26it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2771/3257 [00:15<00:02, 184.98it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2795/3257 [00:15<00:02, 198.41it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2816/3257 [00:15<00:02, 191.94it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2836/3257 [00:15<00:02, 183.93it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2860/3257 [00:15<00:01, 198.84it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:15<00:01, 208.04it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2908/3257 [00:15<00:01, 193.30it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2929/3257 [00:15<00:01, 196.25it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2949/3257 [00:15<00:01, 184.61it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2970/3257 [00:16<00:01, 189.89it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2990/3257 [00:16<00:01, 180.57it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:16<00:01, 191.14it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3033/3257 [00:16<00:01, 194.94it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3057/3257 [00:16<00:00, 204.96it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3080/3257 [00:16<00:00, 210.46it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3102/3257 [00:16<00:00, 209.00it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3125/3257 [00:16<00:00, 214.65it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3147/3257 [00:16<00:00, 194.64it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3167/3257 [00:17<00:00, 194.11it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3187/3257 [00:17<00:00, 189.09it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3208/3257 [00:17<00:00, 191.65it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3228/3257 [00:17<00:00, 190.20it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3252/3257 [00:17<00:00, 202.05it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:17<00:00, 185.72it/s]
2023-02-07 14:10:13.810 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:10:13,811][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d100,n5,mc2,s0.365564,t4>', 'datetime': '2023-02-07T14:10:13.811310', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:10:13,811][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:10:13,811][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:10:14,292][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 14:10:14,293][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:10:14,366][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 27186 unique words (85.48% of original 31803, drops 4617)', 'datetime': '2023-02-07T14:10:14.366510', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:10:14,366][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 5090501 word corpus (99.91% of original 5095118, drops 4617)', 'datetime': '2023-02-07T14:10:14.366907', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:10:14,463][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 14:10:14,464][gensim.models.word2vec][INFO] - sample=0.365564 downsamples 0 most-common words
[2023-02-07 14:10:14,464][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5090501 word corpus (100.0%% of prior 5090501)', 'datetime': '2023-02-07T14:10:14.464493', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:10:14,632][gensim.models.word2vec][INFO] - estimated required memory for 27186 words and 100 dimensions: 37296000 bytes
[2023-02-07 14:10:14,633][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:10:14,645][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 27186 vocabulary and 100 features, using sg=1 hs=0 sample=0.3655639866291609 negative=5 window=1 shrink_windows=True', 'datetime': '2023-02-07T14:10:14.645589', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:10:15,650][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 64.63% examples, 3341755 words/s, in_qsize 7, out_qsize 1
[2023-02-07 14:10:16,160][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5082292 effective words) took 1.5s, 3363879 effective words/s
[2023-02-07 14:10:17,166][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 69.02% examples, 3568198 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:17,589][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5082292 effective words) took 1.4s, 3562246 effective words/s
[2023-02-07 14:10:18,598][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 69.02% examples, 3558048 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:19,010][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5082292 effective words) took 1.4s, 3582249 effective words/s
[2023-02-07 14:10:20,014][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 71.88% examples, 3711892 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:20,380][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5082292 effective words) took 1.4s, 3714940 effective words/s
[2023-02-07 14:10:21,381][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 71.88% examples, 3719639 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:21,750][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5082292 effective words) took 1.4s, 3712545 effective words/s
[2023-02-07 14:10:22,753][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 71.66% examples, 3706501 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:23,116][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5082292 effective words) took 1.4s, 3723438 effective words/s
[2023-02-07 14:10:24,121][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 71.66% examples, 3698897 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:24,490][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5082292 effective words) took 1.4s, 3704360 effective words/s
[2023-02-07 14:10:25,492][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 73.17% examples, 3772590 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:25,835][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5082292 effective words) took 1.3s, 3783254 effective words/s
[2023-02-07 14:10:26,840][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 72.92% examples, 3754293 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:27,187][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5082292 effective words) took 1.3s, 3765132 effective words/s
[2023-02-07 14:10:28,190][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 74.27% examples, 3819646 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:28,526][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5082292 effective words) took 1.3s, 3798429 effective words/s
[2023-02-07 14:10:29,529][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 72.92% examples, 3756789 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:29,879][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5082292 effective words) took 1.4s, 3760612 effective words/s
[2023-02-07 14:10:30,881][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 71.66% examples, 3708213 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:31,246][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5082292 effective words) took 1.4s, 3719986 effective words/s
[2023-02-07 14:10:32,249][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 73.17% examples, 3769931 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:32,588][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5082292 effective words) took 1.3s, 3792752 effective words/s
[2023-02-07 14:10:33,591][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 74.79% examples, 3859191 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:33,903][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5082292 effective words) took 1.3s, 3872390 effective words/s
[2023-02-07 14:10:34,904][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 74.92% examples, 3866370 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:35,226][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5082292 effective words) took 1.3s, 3843046 effective words/s
[2023-02-07 14:10:36,228][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 73.81% examples, 3799926 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:36,568][gensim.models.word2vec][INFO] - EPOCH 15: training on 5095118 raw words (5082292 effective words) took 1.3s, 3790739 effective words/s
[2023-02-07 14:10:37,571][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 73.63% examples, 3788628 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:37,905][gensim.models.word2vec][INFO] - EPOCH 16: training on 5095118 raw words (5082292 effective words) took 1.3s, 3807019 effective words/s
[2023-02-07 14:10:38,909][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 75.53% examples, 3888155 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:39,207][gensim.models.word2vec][INFO] - EPOCH 17: training on 5095118 raw words (5082292 effective words) took 1.3s, 3908869 effective words/s
[2023-02-07 14:10:40,210][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 75.53% examples, 3893261 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:40,504][gensim.models.word2vec][INFO] - EPOCH 18: training on 5095118 raw words (5082292 effective words) took 1.3s, 3922410 effective words/s
[2023-02-07 14:10:41,507][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 76.27% examples, 3925235 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:41,810][gensim.models.word2vec][INFO] - EPOCH 19: training on 5095118 raw words (5082292 effective words) took 1.3s, 3897250 effective words/s
[2023-02-07 14:10:42,812][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 74.06% examples, 3815170 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:43,134][gensim.models.word2vec][INFO] - EPOCH 20: training on 5095118 raw words (5082292 effective words) took 1.3s, 3842500 effective words/s
[2023-02-07 14:10:44,137][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 74.64% examples, 3848764 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:44,446][gensim.models.word2vec][INFO] - EPOCH 21: training on 5095118 raw words (5082292 effective words) took 1.3s, 3877214 effective words/s
[2023-02-07 14:10:45,450][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 76.14% examples, 3911058 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:45,743][gensim.models.word2vec][INFO] - EPOCH 22: training on 5095118 raw words (5082292 effective words) took 1.3s, 3923251 effective words/s
[2023-02-07 14:10:46,744][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 76.14% examples, 3922336 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:10:47,034][gensim.models.word2vec][INFO] - EPOCH 23: training on 5095118 raw words (5082292 effective words) took 1.3s, 3939544 effective words/s
[2023-02-07 14:10:48,037][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 76.27% examples, 3927065 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:48,339][gensim.models.word2vec][INFO] - EPOCH 24: training on 5095118 raw words (5082292 effective words) took 1.3s, 3898747 effective words/s
[2023-02-07 14:10:49,344][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 75.22% examples, 3874472 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:49,647][gensim.models.word2vec][INFO] - EPOCH 25: training on 5095118 raw words (5082292 effective words) took 1.3s, 3890887 effective words/s
[2023-02-07 14:10:50,650][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 74.79% examples, 3853833 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:50,963][gensim.models.word2vec][INFO] - EPOCH 26: training on 5095118 raw words (5082292 effective words) took 1.3s, 3865172 effective words/s
[2023-02-07 14:10:51,966][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 76.85% examples, 3946414 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:52,256][gensim.models.word2vec][INFO] - EPOCH 27: training on 5095118 raw words (5082292 effective words) took 1.3s, 3934586 effective words/s
[2023-02-07 14:10:53,260][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 77.06% examples, 3953273 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:53,538][gensim.models.word2vec][INFO] - EPOCH 28: training on 5095118 raw words (5082292 effective words) took 1.3s, 3972242 effective words/s
[2023-02-07 14:10:54,539][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 77.31% examples, 3969156 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:10:54,821][gensim.models.word2vec][INFO] - EPOCH 29: training on 5095118 raw words (5082292 effective words) took 1.3s, 3962774 effective words/s
[2023-02-07 14:10:55,824][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 75.47% examples, 3894456 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:56,125][gensim.models.word2vec][INFO] - EPOCH 30: training on 5095118 raw words (5082292 effective words) took 1.3s, 3904694 effective words/s
[2023-02-07 14:10:57,128][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 74.24% examples, 3821602 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:57,446][gensim.models.word2vec][INFO] - EPOCH 31: training on 5095118 raw words (5082292 effective words) took 1.3s, 3851005 effective words/s
[2023-02-07 14:10:58,448][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 78.08% examples, 4004831 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:58,712][gensim.models.word2vec][INFO] - EPOCH 32: training on 5095118 raw words (5082292 effective words) took 1.3s, 4020720 effective words/s
[2023-02-07 14:10:59,716][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 78.39% examples, 4015576 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:10:59,978][gensim.models.word2vec][INFO] - EPOCH 33: training on 5095118 raw words (5082292 effective words) took 1.3s, 4020772 effective words/s
[2023-02-07 14:11:00,981][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 78.54% examples, 4025777 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:01,246][gensim.models.word2vec][INFO] - EPOCH 34: training on 5095118 raw words (5082292 effective words) took 1.3s, 4011348 effective words/s
[2023-02-07 14:11:02,251][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 77.80% examples, 3975274 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:02,524][gensim.models.word2vec][INFO] - EPOCH 35: training on 5095118 raw words (5082292 effective words) took 1.3s, 3980922 effective words/s
[2023-02-07 14:11:03,527][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 74.64% examples, 3851182 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:03,836][gensim.models.word2vec][INFO] - EPOCH 36: training on 5095118 raw words (5082292 effective words) took 1.3s, 3880632 effective words/s
[2023-02-07 14:11:04,840][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 77.06% examples, 3948530 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:05,115][gensim.models.word2vec][INFO] - EPOCH 37: training on 5095118 raw words (5082292 effective words) took 1.3s, 3977015 effective words/s
[2023-02-07 14:11:06,120][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 77.06% examples, 3950435 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:06,397][gensim.models.word2vec][INFO] - EPOCH 38: training on 5095118 raw words (5082292 effective words) took 1.3s, 3971003 effective words/s
[2023-02-07 14:11:07,399][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 77.06% examples, 3957429 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:07,683][gensim.models.word2vec][INFO] - EPOCH 39: training on 5095118 raw words (5082292 effective words) took 1.3s, 3953729 effective words/s
[2023-02-07 14:11:08,689][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 76.54% examples, 3927639 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:08,977][gensim.models.word2vec][INFO] - EPOCH 40: training on 5095118 raw words (5082292 effective words) took 1.3s, 3935940 effective words/s
[2023-02-07 14:11:09,980][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 74.79% examples, 3855142 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:10,290][gensim.models.word2vec][INFO] - EPOCH 41: training on 5095118 raw words (5082292 effective words) took 1.3s, 3877298 effective words/s
[2023-02-07 14:11:11,294][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 76.27% examples, 3920380 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:11,580][gensim.models.word2vec][INFO] - EPOCH 42: training on 5095118 raw words (5082292 effective words) took 1.3s, 3942077 effective words/s
[2023-02-07 14:11:12,582][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 75.96% examples, 3912476 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:11:12,878][gensim.models.word2vec][INFO] - EPOCH 43: training on 5095118 raw words (5082292 effective words) took 1.3s, 3921122 effective words/s
[2023-02-07 14:11:13,885][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 77.06% examples, 3941572 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:11:14,168][gensim.models.word2vec][INFO] - EPOCH 44: training on 5095118 raw words (5082292 effective words) took 1.3s, 3946106 effective words/s
[2023-02-07 14:11:15,170][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 77.49% examples, 3976655 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:15,445][gensim.models.word2vec][INFO] - EPOCH 45: training on 5095118 raw words (5082292 effective words) took 1.3s, 3985001 effective words/s
[2023-02-07 14:11:16,449][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 75.41% examples, 3886021 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:16,744][gensim.models.word2vec][INFO] - EPOCH 46: training on 5095118 raw words (5082292 effective words) took 1.3s, 3915243 effective words/s
[2023-02-07 14:11:17,746][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 76.85% examples, 3948423 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:18,026][gensim.models.word2vec][INFO] - EPOCH 47: training on 5095118 raw words (5082292 effective words) took 1.3s, 3969128 effective words/s
[2023-02-07 14:11:19,028][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 77.06% examples, 3959507 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:11:19,303][gensim.models.word2vec][INFO] - EPOCH 48: training on 5095118 raw words (5082292 effective words) took 1.3s, 3985210 effective words/s
[2023-02-07 14:11:20,306][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 78.69% examples, 4035308 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:20,568][gensim.models.word2vec][INFO] - EPOCH 49: training on 5095118 raw words (5082292 effective words) took 1.3s, 4020013 effective words/s
[2023-02-07 14:11:21,570][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 77.49% examples, 3976564 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:21,844][gensim.models.word2vec][INFO] - EPOCH 50: training on 5095118 raw words (5082292 effective words) took 1.3s, 3988043 effective words/s
[2023-02-07 14:11:22,848][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 76.54% examples, 3930303 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:23,128][gensim.models.word2vec][INFO] - EPOCH 51: training on 5095118 raw words (5082292 effective words) took 1.3s, 3961095 effective words/s
[2023-02-07 14:11:24,131][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 78.54% examples, 4025038 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:24,388][gensim.models.word2vec][INFO] - EPOCH 52: training on 5095118 raw words (5082292 effective words) took 1.3s, 4035681 effective words/s
[2023-02-07 14:11:25,390][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 77.80% examples, 3987553 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:25,662][gensim.models.word2vec][INFO] - EPOCH 53: training on 5095118 raw words (5082292 effective words) took 1.3s, 3995737 effective words/s
[2023-02-07 14:11:26,665][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 78.08% examples, 4001798 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:26,938][gensim.models.word2vec][INFO] - EPOCH 54: training on 5095118 raw words (5082292 effective words) took 1.3s, 3988084 effective words/s
[2023-02-07 14:11:27,945][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 76.08% examples, 3898078 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:28,237][gensim.models.word2vec][INFO] - EPOCH 55: training on 5095118 raw words (5082292 effective words) took 1.3s, 3916430 effective words/s
[2023-02-07 14:11:29,239][gensim.models.word2vec][INFO] - EPOCH 56 - PROGRESS: at 76.14% examples, 3920706 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:29,525][gensim.models.word2vec][INFO] - EPOCH 56: training on 5095118 raw words (5082292 effective words) took 1.3s, 3947774 effective words/s
[2023-02-07 14:11:30,528][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 77.49% examples, 3976487 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:30,800][gensim.models.word2vec][INFO] - EPOCH 57: training on 5095118 raw words (5082292 effective words) took 1.3s, 3991500 effective words/s
[2023-02-07 14:11:31,803][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 78.08% examples, 4005384 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:32,069][gensim.models.word2vec][INFO] - EPOCH 58: training on 5095118 raw words (5082292 effective words) took 1.3s, 4012237 effective words/s
[2023-02-07 14:11:33,071][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 77.06% examples, 3959111 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:33,360][gensim.models.word2vec][INFO] - EPOCH 59: training on 5095118 raw words (5082292 effective words) took 1.3s, 3939942 effective words/s
[2023-02-07 14:11:34,366][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 76.14% examples, 3907776 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:34,657][gensim.models.word2vec][INFO] - EPOCH 60: training on 5095118 raw words (5082292 effective words) took 1.3s, 3923913 effective words/s
[2023-02-07 14:11:35,659][gensim.models.word2vec][INFO] - EPOCH 61 - PROGRESS: at 76.54% examples, 3939065 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:35,939][gensim.models.word2vec][INFO] - EPOCH 61: training on 5095118 raw words (5082292 effective words) took 1.3s, 3968417 effective words/s
[2023-02-07 14:11:36,943][gensim.models.word2vec][INFO] - EPOCH 62 - PROGRESS: at 77.80% examples, 3980538 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:37,212][gensim.models.word2vec][INFO] - EPOCH 62: training on 5095118 raw words (5082292 effective words) took 1.3s, 3997401 effective words/s
[2023-02-07 14:11:38,215][gensim.models.word2vec][INFO] - EPOCH 63 - PROGRESS: at 78.17% examples, 4004648 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:11:38,479][gensim.models.word2vec][INFO] - EPOCH 63: training on 5095118 raw words (5082292 effective words) took 1.3s, 4015635 effective words/s
[2023-02-07 14:11:39,481][gensim.models.word2vec][INFO] - EPOCH 64 - PROGRESS: at 78.17% examples, 4008994 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:11:39,752][gensim.models.word2vec][INFO] - EPOCH 64: training on 5095118 raw words (5082292 effective words) took 1.3s, 3995323 effective words/s
[2023-02-07 14:11:40,755][gensim.models.word2vec][INFO] - EPOCH 65 - PROGRESS: at 76.27% examples, 3926075 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:41,043][gensim.models.word2vec][INFO] - EPOCH 65: training on 5095118 raw words (5082292 effective words) took 1.3s, 3940873 effective words/s
[2023-02-07 14:11:42,047][gensim.models.word2vec][INFO] - EPOCH 66 - PROGRESS: at 77.06% examples, 3951007 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:42,324][gensim.models.word2vec][INFO] - EPOCH 66: training on 5095118 raw words (5082292 effective words) took 1.3s, 3970813 effective words/s
[2023-02-07 14:11:43,327][gensim.models.word2vec][INFO] - EPOCH 67 - PROGRESS: at 78.39% examples, 4017416 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:43,588][gensim.models.word2vec][INFO] - EPOCH 67: training on 5095118 raw words (5082292 effective words) took 1.3s, 4027706 effective words/s
[2023-02-07 14:11:44,590][gensim.models.word2vec][INFO] - EPOCH 68 - PROGRESS: at 77.49% examples, 3977914 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:44,864][gensim.models.word2vec][INFO] - EPOCH 68: training on 5095118 raw words (5082292 effective words) took 1.3s, 3986107 effective words/s
[2023-02-07 14:11:45,866][gensim.models.word2vec][INFO] - EPOCH 69 - PROGRESS: at 78.81% examples, 4046775 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:46,126][gensim.models.word2vec][INFO] - EPOCH 69: training on 5095118 raw words (5082292 effective words) took 1.3s, 4030404 effective words/s
[2023-02-07 14:11:47,128][gensim.models.word2vec][INFO] - EPOCH 70 - PROGRESS: at 76.85% examples, 3949705 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:47,414][gensim.models.word2vec][INFO] - EPOCH 70: training on 5095118 raw words (5082292 effective words) took 1.3s, 3953150 effective words/s
[2023-02-07 14:11:48,415][gensim.models.word2vec][INFO] - EPOCH 71 - PROGRESS: at 75.96% examples, 3912022 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:48,707][gensim.models.word2vec][INFO] - EPOCH 71: training on 5095118 raw words (5082292 effective words) took 1.3s, 3932930 effective words/s
[2023-02-07 14:11:49,710][gensim.models.word2vec][INFO] - EPOCH 72 - PROGRESS: at 77.96% examples, 3991504 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:49,981][gensim.models.word2vec][INFO] - EPOCH 72: training on 5095118 raw words (5082292 effective words) took 1.3s, 3993645 effective words/s
[2023-02-07 14:11:50,985][gensim.models.word2vec][INFO] - EPOCH 73 - PROGRESS: at 77.96% examples, 3987353 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:51,254][gensim.models.word2vec][INFO] - EPOCH 73: training on 5095118 raw words (5082292 effective words) took 1.3s, 3996271 effective words/s
[2023-02-07 14:11:52,256][gensim.models.word2vec][INFO] - EPOCH 74 - PROGRESS: at 77.49% examples, 3978892 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:52,535][gensim.models.word2vec][INFO] - EPOCH 74: training on 5095118 raw words (5082292 effective words) took 1.3s, 3971266 effective words/s
[2023-02-07 14:11:53,537][gensim.models.word2vec][INFO] - EPOCH 75 - PROGRESS: at 77.96% examples, 3993242 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:53,808][gensim.models.word2vec][INFO] - EPOCH 75: training on 5095118 raw words (5082292 effective words) took 1.3s, 3997041 effective words/s
[2023-02-07 14:11:54,810][gensim.models.word2vec][INFO] - EPOCH 76 - PROGRESS: at 76.76% examples, 3950303 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:55,087][gensim.models.word2vec][INFO] - EPOCH 76: training on 5095118 raw words (5082292 effective words) took 1.3s, 3979498 effective words/s
[2023-02-07 14:11:56,092][gensim.models.word2vec][INFO] - EPOCH 77 - PROGRESS: at 78.69% examples, 4037077 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:56,349][gensim.models.word2vec][INFO] - EPOCH 77: training on 5095118 raw words (5082292 effective words) took 1.3s, 4039100 effective words/s
[2023-02-07 14:11:57,353][gensim.models.word2vec][INFO] - EPOCH 78 - PROGRESS: at 77.06% examples, 3949935 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:57,632][gensim.models.word2vec][INFO] - EPOCH 78: training on 5095118 raw words (5082292 effective words) took 1.3s, 3964800 effective words/s
[2023-02-07 14:11:58,636][gensim.models.word2vec][INFO] - EPOCH 79 - PROGRESS: at 77.96% examples, 3988629 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:11:58,911][gensim.models.word2vec][INFO] - EPOCH 79: training on 5095118 raw words (5082292 effective words) took 1.3s, 3978481 effective words/s
[2023-02-07 14:11:59,914][gensim.models.word2vec][INFO] - EPOCH 80 - PROGRESS: at 77.49% examples, 3973439 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:00,190][gensim.models.word2vec][INFO] - EPOCH 80: training on 5095118 raw words (5082292 effective words) took 1.3s, 3975838 effective words/s
[2023-02-07 14:12:01,194][gensim.models.word2vec][INFO] - EPOCH 81 - PROGRESS: at 76.27% examples, 3924449 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:01,478][gensim.models.word2vec][INFO] - EPOCH 81: training on 5095118 raw words (5082292 effective words) took 1.3s, 3952723 effective words/s
[2023-02-07 14:12:02,480][gensim.models.word2vec][INFO] - EPOCH 82 - PROGRESS: at 78.69% examples, 4036905 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:02,733][gensim.models.word2vec][INFO] - EPOCH 82: training on 5095118 raw words (5082292 effective words) took 1.3s, 4052908 effective words/s
[2023-02-07 14:12:03,736][gensim.models.word2vec][INFO] - EPOCH 83 - PROGRESS: at 78.91% examples, 4051745 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:03,988][gensim.models.word2vec][INFO] - EPOCH 83: training on 5095118 raw words (5082292 effective words) took 1.3s, 4056123 effective words/s
[2023-02-07 14:12:04,990][gensim.models.word2vec][INFO] - EPOCH 84 - PROGRESS: at 77.80% examples, 3987463 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:05,269][gensim.models.word2vec][INFO] - EPOCH 84: training on 5095118 raw words (5082292 effective words) took 1.3s, 3973743 effective words/s
[2023-02-07 14:12:06,270][gensim.models.word2vec][INFO] - EPOCH 85 - PROGRESS: at 76.36% examples, 3930923 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:06,556][gensim.models.word2vec][INFO] - EPOCH 85: training on 5095118 raw words (5082292 effective words) took 1.3s, 3953088 effective words/s
[2023-02-07 14:12:07,557][gensim.models.word2vec][INFO] - EPOCH 86 - PROGRESS: at 77.49% examples, 3978300 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:07,831][gensim.models.word2vec][INFO] - EPOCH 86: training on 5095118 raw words (5082292 effective words) took 1.3s, 3987831 effective words/s
[2023-02-07 14:12:08,835][gensim.models.word2vec][INFO] - EPOCH 87 - PROGRESS: at 76.54% examples, 3931099 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:12:09,115][gensim.models.word2vec][INFO] - EPOCH 87: training on 5095118 raw words (5082292 effective words) took 1.3s, 3962939 effective words/s
[2023-02-07 14:12:10,119][gensim.models.word2vec][INFO] - EPOCH 88 - PROGRESS: at 78.91% examples, 4046700 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:10,371][gensim.models.word2vec][INFO] - EPOCH 88: training on 5095118 raw words (5082292 effective words) took 1.3s, 4051708 effective words/s
[2023-02-07 14:12:11,373][gensim.models.word2vec][INFO] - EPOCH 89 - PROGRESS: at 78.08% examples, 4006051 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:11,645][gensim.models.word2vec][INFO] - EPOCH 89: training on 5095118 raw words (5082292 effective words) took 1.3s, 3992664 effective words/s
[2023-02-07 14:12:12,650][gensim.models.word2vec][INFO] - EPOCH 90 - PROGRESS: at 77.49% examples, 3969472 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:12,922][gensim.models.word2vec][INFO] - EPOCH 90: training on 5095118 raw words (5082292 effective words) took 1.3s, 3984894 effective words/s
[2023-02-07 14:12:13,928][gensim.models.word2vec][INFO] - EPOCH 91 - PROGRESS: at 77.10% examples, 3942650 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:14,208][gensim.models.word2vec][INFO] - EPOCH 91: training on 5095118 raw words (5082292 effective words) took 1.3s, 3958519 effective words/s
[2023-02-07 14:12:15,211][gensim.models.word2vec][INFO] - EPOCH 92 - PROGRESS: at 78.69% examples, 4032824 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:15,467][gensim.models.word2vec][INFO] - EPOCH 92: training on 5095118 raw words (5082292 effective words) took 1.3s, 4039483 effective words/s
[2023-02-07 14:12:16,473][gensim.models.word2vec][INFO] - EPOCH 93 - PROGRESS: at 79.12% examples, 4047470 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:16,720][gensim.models.word2vec][INFO] - EPOCH 93: training on 5095118 raw words (5082292 effective words) took 1.3s, 4061739 effective words/s
[2023-02-07 14:12:17,721][gensim.models.word2vec][INFO] - EPOCH 94 - PROGRESS: at 78.17% examples, 4011562 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:17,995][gensim.models.word2vec][INFO] - EPOCH 94: training on 5095118 raw words (5082292 effective words) took 1.3s, 3987909 effective words/s
[2023-02-07 14:12:18,997][gensim.models.word2vec][INFO] - EPOCH 95 - PROGRESS: at 77.31% examples, 3966950 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:19,275][gensim.models.word2vec][INFO] - EPOCH 95: training on 5095118 raw words (5082292 effective words) took 1.3s, 3975651 effective words/s
[2023-02-07 14:12:20,278][gensim.models.word2vec][INFO] - EPOCH 96 - PROGRESS: at 75.53% examples, 3889356 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:20,578][gensim.models.word2vec][INFO] - EPOCH 96: training on 5095118 raw words (5082292 effective words) took 1.3s, 3904005 effective words/s
[2023-02-07 14:12:21,579][gensim.models.word2vec][INFO] - EPOCH 97 - PROGRESS: at 75.53% examples, 3895615 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:21,877][gensim.models.word2vec][INFO] - EPOCH 97: training on 5095118 raw words (5082292 effective words) took 1.3s, 3915722 effective words/s
[2023-02-07 14:12:22,882][gensim.models.word2vec][INFO] - EPOCH 98 - PROGRESS: at 76.54% examples, 3931000 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:23,170][gensim.models.word2vec][INFO] - EPOCH 98: training on 5095118 raw words (5082292 effective words) took 1.3s, 3939299 effective words/s
[2023-02-07 14:12:24,171][gensim.models.word2vec][INFO] - EPOCH 99 - PROGRESS: at 76.85% examples, 3950042 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:12:24,466][gensim.models.word2vec][INFO] - EPOCH 99: training on 5095118 raw words (5082292 effective words) took 1.3s, 3923662 effective words/s
[2023-02-07 14:12:24,467][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 509511800 raw words (508229200 effective words) took 129.8s, 3914874 effective words/s', 'datetime': '2023-02-07T14:12:24.467311', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:12:24.467 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:12:35,126][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140946-15rrmksk/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:12:35.126344', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:12:35,127][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:12:35,189][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_140946-15rrmksk/files/../tmp/embedding_model.pt
2023-02-07 14:12:35.189 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:12:36.404 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:12:36.850 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:12:37.604 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.035513846975977, 'test_mae': 1.1003237648358557, 'test_r2': 0.03708024722705239}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.053 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.053 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.92
wandb: percentage 0.14517
wandb:   test_mae 1.10032
wandb:   test_mse 2.03551
wandb:    test_r2 0.03708
wandb: 
wandb: üöÄ View run radiant-sweep-72 at: https://wandb.ai/xiaoqiz/mof2vec/runs/15rrmksk
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_140946-15rrmksk/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: s7yg8q4n with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 805
wandb: 	model.gensim.alpha: 0.011120427599796806
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 63
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.6070110712498396
wandb: 	model.gensim.vector_size: 146
wandb: 	model.gensim.window: 12
wandb: 	model.sklearn.learning_rate: 0.21342246884036695
wandb: 	model.sklearn.max_depth: 35
wandb: 	model.sklearn.min_child_weight: 0.058168731868559274
wandb: 	model.sklearn.n_estimators: 2853
wandb: 	model.sklearn.num_leaves: 376
wandb: 	model.sklearn.reg_alpha: 0.009420597779176728
wandb: 	model.sklearn.reg_lambda: 0.22534468853732745
wandb: 	model.sklearn.subsample: 0.2312351924628919
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141300-s7yg8q4n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-sweep-73
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/s7yg8q4n
2023-02-07 14:13:11.406 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 14:13:11.407 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 805 for sweep.
2023-02-07 14:13:11.407 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.011120427599796806 for sweep.
2023-02-07 14:13:11.407 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:13:11.407 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 63 for sweep.
2023-02-07 14:13:11.408 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 14:13:11.408 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6070110712498396 for sweep.
2023-02-07 14:13:11.408 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 146 for sweep.
2023-02-07 14:13:11.408 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 12 for sweep.
2023-02-07 14:13:11.408 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.21342246884036695 for sweep.
2023-02-07 14:13:11.409 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 35 for sweep.
2023-02-07 14:13:11.409 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.058168731868559274 for sweep.
2023-02-07 14:13:11.409 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2853 for sweep.
2023-02-07 14:13:11.409 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 376 for sweep.
2023-02-07 14:13:11.410 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.009420597779176728 for sweep.
2023-02-07 14:13:11.410 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.22534468853732745 for sweep.
2023-02-07 14:13:11.410 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2312351924628919 for sweep.
2023-02-07 14:13:11.411 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:13:11.415 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141300-s7yg8q4n/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 805, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 146, 'window': 12, 'min_count': 2, 'dm': 0, 'sample': 0.6070110712498396, 'workers': 4, 'alpha': 0.011120427599796806, 'epochs': 63}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2853, 'max_depth': 35, 'num_leaves': 376, 'reg_alpha': 0.009420597779176728, 'reg_lambda': 0.22534468853732745, 'subsample': 0.2312351924628919, 'min_child_weight': 0.058168731868559274, 'n_jobs': 4, 'learning_rate': 0.21342246884036695}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 24/3257 [00:00<00:14, 229.63it/s]  1%|‚ñè         | 47/3257 [00:00<00:14, 226.72it/s]  2%|‚ñè         | 70/3257 [00:00<00:13, 228.17it/s]  3%|‚ñé         | 96/3257 [00:00<00:13, 237.59it/s]  4%|‚ñé         | 120/3257 [00:00<00:13, 236.94it/s]  5%|‚ñç         | 148/3257 [00:00<00:12, 249.55it/s]  5%|‚ñå         | 173/3257 [00:00<00:13, 237.19it/s]  6%|‚ñå         | 201/3257 [00:00<00:12, 242.50it/s]  7%|‚ñã         | 232/3257 [00:00<00:11, 262.09it/s]  8%|‚ñä         | 259/3257 [00:01<00:11, 259.83it/s]  9%|‚ñâ         | 290/3257 [00:01<00:10, 270.60it/s] 10%|‚ñâ         | 318/3257 [00:01<00:11, 261.29it/s] 11%|‚ñà         | 345/3257 [00:01<00:11, 258.70it/s] 11%|‚ñà‚ñè        | 373/3257 [00:01<00:11, 261.52it/s] 12%|‚ñà‚ñè        | 400/3257 [00:01<00:11, 244.73it/s] 13%|‚ñà‚ñé        | 425/3257 [00:01<00:11, 237.09it/s] 14%|‚ñà‚ñç        | 449/3257 [00:01<00:12, 229.28it/s] 15%|‚ñà‚ñç        | 476/3257 [00:01<00:11, 238.68it/s] 15%|‚ñà‚ñå        | 503/3257 [00:02<00:11, 246.31it/s] 16%|‚ñà‚ñå        | 528/3257 [00:02<00:11, 245.84it/s] 17%|‚ñà‚ñã        | 555/3257 [00:02<00:10, 250.63it/s] 18%|‚ñà‚ñä        | 581/3257 [00:02<00:11, 226.56it/s] 19%|‚ñà‚ñä        | 605/3257 [00:02<00:15, 167.26it/s] 19%|‚ñà‚ñâ        | 631/3257 [00:02<00:14, 187.20it/s] 20%|‚ñà‚ñà        | 654/3257 [00:02<00:13, 194.52it/s] 21%|‚ñà‚ñà        | 677/3257 [00:02<00:12, 202.65it/s] 21%|‚ñà‚ñà‚ñè       | 699/3257 [00:03<00:12, 205.67it/s] 22%|‚ñà‚ñà‚ñè       | 724/3257 [00:03<00:11, 215.67it/s] 23%|‚ñà‚ñà‚ñé       | 747/3257 [00:03<00:11, 218.92it/s] 24%|‚ñà‚ñà‚ñé       | 773/3257 [00:03<00:10, 228.03it/s] 25%|‚ñà‚ñà‚ñç       | 799/3257 [00:03<00:10, 234.95it/s] 25%|‚ñà‚ñà‚ñå       | 823/3257 [00:03<00:10, 229.17it/s] 26%|‚ñà‚ñà‚ñå       | 847/3257 [00:03<00:10, 224.61it/s] 27%|‚ñà‚ñà‚ñã       | 872/3257 [00:03<00:10, 230.75it/s] 28%|‚ñà‚ñà‚ñä       | 896/3257 [00:03<00:10, 230.74it/s] 28%|‚ñà‚ñà‚ñä       | 924/3257 [00:03<00:09, 243.70it/s] 29%|‚ñà‚ñà‚ñâ       | 949/3257 [00:04<00:09, 242.60it/s] 30%|‚ñà‚ñà‚ñâ       | 976/3257 [00:04<00:09, 249.65it/s] 31%|‚ñà‚ñà‚ñà       | 1002/3257 [00:04<00:09, 239.99it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:04<00:09, 230.61it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1051/3257 [00:04<00:09, 225.03it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1080/3257 [00:04<00:09, 229.82it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1105/3257 [00:04<00:09, 233.39it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1129/3257 [00:04<00:09, 231.41it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1153/3257 [00:04<00:09, 229.61it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1176/3257 [00:05<00:09, 224.99it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1199/3257 [00:05<00:09, 209.12it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1221/3257 [00:05<00:10, 202.77it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1246/3257 [00:05<00:09, 214.59it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1268/3257 [00:05<00:09, 215.51it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1290/3257 [00:05<00:09, 201.53it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1313/3257 [00:05<00:09, 208.86it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1339/3257 [00:05<00:08, 220.96it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1362/3257 [00:05<00:08, 222.36it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1385/3257 [00:06<00:08, 213.53it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1412/3257 [00:06<00:08, 228.76it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1438/3257 [00:06<00:07, 233.80it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1466/3257 [00:06<00:07, 245.10it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1491/3257 [00:06<00:07, 245.83it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1517/3257 [00:06<00:07, 248.35it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1542/3257 [00:06<00:07, 225.84it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1565/3257 [00:06<00:07, 219.72it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1588/3257 [00:06<00:07, 220.91it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1615/3257 [00:07<00:07, 233.14it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1639/3257 [00:07<00:07, 225.13it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1662/3257 [00:07<00:07, 214.51it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1684/3257 [00:07<00:07, 212.95it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1708/3257 [00:07<00:07, 219.61it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:07<00:07, 213.32it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1753/3257 [00:07<00:10, 143.69it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1777/3257 [00:07<00:09, 163.71it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1802/3257 [00:08<00:07, 183.64it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1824/3257 [00:08<00:07, 189.69it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1845/3257 [00:08<00:07, 192.52it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1868/3257 [00:08<00:06, 201.74it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1890/3257 [00:08<00:06, 199.04it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1915/3257 [00:08<00:06, 211.58it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1937/3257 [00:08<00:06, 207.78it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1964/3257 [00:08<00:05, 224.36it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1987/3257 [00:08<00:05, 213.74it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2011/3257 [00:09<00:05, 219.37it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2035/3257 [00:09<00:05, 224.25it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2058/3257 [00:09<00:05, 209.38it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2081/3257 [00:09<00:05, 214.44it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2103/3257 [00:09<00:05, 209.94it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2125/3257 [00:09<00:05, 209.28it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2147/3257 [00:09<00:05, 208.22it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2177/3257 [00:09<00:04, 230.50it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2202/3257 [00:09<00:04, 233.17it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2226/3257 [00:10<00:04, 229.95it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2250/3257 [00:10<00:04, 226.37it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2273/3257 [00:10<00:04, 216.20it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2300/3257 [00:10<00:04, 228.61it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2330/3257 [00:10<00:03, 247.37it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:10<00:03, 258.09it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2387/3257 [00:10<00:03, 261.32it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2414/3257 [00:10<00:03, 253.29it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2440/3257 [00:10<00:03, 245.33it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2465/3257 [00:10<00:03, 244.65it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2491/3257 [00:11<00:03, 247.92it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2519/3257 [00:11<00:02, 257.15it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2545/3257 [00:11<00:02, 256.02it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2571/3257 [00:11<00:02, 237.31it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2596/3257 [00:11<00:02, 234.63it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:11<00:02, 258.04it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2655/3257 [00:11<00:02, 249.29it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2681/3257 [00:11<00:02, 248.07it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2706/3257 [00:11<00:02, 235.32it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2731/3257 [00:12<00:02, 237.74it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2759/3257 [00:12<00:01, 249.32it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2785/3257 [00:12<00:01, 248.57it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2811/3257 [00:12<00:01, 250.21it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2837/3257 [00:12<00:01, 235.44it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2869/3257 [00:12<00:01, 257.70it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2896/3257 [00:12<00:01, 244.87it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2924/3257 [00:12<00:01, 253.97it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:12<00:01, 234.76it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:13<00:01, 235.76it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3003/3257 [00:13<00:01, 242.86it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3028/3257 [00:13<00:00, 242.47it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3058/3257 [00:13<00:00, 257.97it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3086/3257 [00:13<00:00, 261.81it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3115/3257 [00:13<00:00, 269.45it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3143/3257 [00:13<00:00, 255.30it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3169/3257 [00:13<00:00, 251.03it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3195/3257 [00:13<00:00, 252.77it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3221/3257 [00:14<00:00, 240.04it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:14<00:00, 160.56it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:14<00:00, 226.63it/s]
2023-02-07 14:13:26.283 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:13:26,284][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d146,n5,mc2,s0.607011,t4>', 'datetime': '2023-02-07T14:13:26.284178', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:13:26,284][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:13:26,284][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:13:26,623][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 14:13:26,624][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:13:26,654][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 11155 unique words (85.41% of original 13061, drops 1906)', 'datetime': '2023-02-07T14:13:26.654535', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:13:26,655][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 3637464 word corpus (99.95% of original 3639370, drops 1906)', 'datetime': '2023-02-07T14:13:26.655737', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:13:26,695][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 14:13:26,695][gensim.models.word2vec][INFO] - sample=0.607011 downsamples 0 most-common words
[2023-02-07 14:13:26,696][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3637464 word corpus (100.0%% of prior 3637464)', 'datetime': '2023-02-07T14:13:26.696086', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:13:26,764][gensim.models.word2vec][INFO] - estimated required memory for 11155 words and 146 dimensions: 21160028 bytes
[2023-02-07 14:13:26,766][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:13:26,775][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 11155 vocabulary and 146 features, using sg=1 hs=0 sample=0.6070110712498396 negative=5 window=12 shrink_windows=True', 'datetime': '2023-02-07T14:13:26.775183', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:13:27,779][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 78.35% examples, 2876267 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:28,033][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3640721 effective words) took 1.3s, 2900582 effective words/s
[2023-02-07 14:13:29,036][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 86.61% examples, 3175512 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:29,179][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3640721 effective words) took 1.1s, 3180377 effective words/s
[2023-02-07 14:13:30,186][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 86.37% examples, 3154908 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:30,334][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3640721 effective words) took 1.2s, 3158380 effective words/s
[2023-02-07 14:13:31,336][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 87.23% examples, 3202195 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:31,467][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3640721 effective words) took 1.1s, 3215295 effective words/s
[2023-02-07 14:13:32,470][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 89.19% examples, 3267363 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:13:32,587][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3640721 effective words) took 1.1s, 3255690 effective words/s
[2023-02-07 14:13:33,592][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 86.31% examples, 3159421 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:33,739][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3640721 effective words) took 1.2s, 3165363 effective words/s
[2023-02-07 14:13:34,745][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 88.61% examples, 3223359 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:34,866][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3640721 effective words) took 1.1s, 3232483 effective words/s
[2023-02-07 14:13:35,869][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 88.79% examples, 3248244 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:35,987][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3640721 effective words) took 1.1s, 3253570 effective words/s
[2023-02-07 14:13:36,991][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 90.79% examples, 3315033 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:13:37,084][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3640721 effective words) took 1.1s, 3320938 effective words/s
[2023-02-07 14:13:38,087][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 90.02% examples, 3293839 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:38,189][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3640721 effective words) took 1.1s, 3298608 effective words/s
[2023-02-07 14:13:39,193][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 92.05% examples, 3366241 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:39,270][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3640721 effective words) took 1.1s, 3373092 effective words/s
[2023-02-07 14:13:40,276][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 91.65% examples, 3350355 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:40,354][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3640721 effective words) took 1.1s, 3365223 effective words/s
[2023-02-07 14:13:41,363][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 94.11% examples, 3410534 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:41,419][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3640721 effective words) took 1.1s, 3424043 effective words/s
[2023-02-07 14:13:42,421][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 91.40% examples, 3351673 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:42,507][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3640721 effective words) took 1.1s, 3351219 effective words/s
[2023-02-07 14:13:43,508][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 91.93% examples, 3371436 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:43,584][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3640721 effective words) took 1.1s, 3382324 effective words/s
[2023-02-07 14:13:44,590][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 94.38% examples, 3432133 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:44,643][gensim.models.word2vec][INFO] - EPOCH 15: training on 3639370 raw words (3640721 effective words) took 1.1s, 3443974 effective words/s
[2023-02-07 14:13:45,646][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 93.09% examples, 3405851 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:45,710][gensim.models.word2vec][INFO] - EPOCH 16: training on 3639370 raw words (3640721 effective words) took 1.1s, 3416883 effective words/s
[2023-02-07 14:13:46,712][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 91.93% examples, 3373037 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:46,786][gensim.models.word2vec][INFO] - EPOCH 17: training on 3639370 raw words (3640721 effective words) took 1.1s, 3388088 effective words/s
[2023-02-07 14:13:47,789][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 94.41% examples, 3441357 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:47,842][gensim.models.word2vec][INFO] - EPOCH 18: training on 3639370 raw words (3640721 effective words) took 1.1s, 3452399 effective words/s
[2023-02-07 14:13:48,847][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 94.11% examples, 3423126 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:48,904][gensim.models.word2vec][INFO] - EPOCH 19: training on 3639370 raw words (3640721 effective words) took 1.1s, 3432780 effective words/s
[2023-02-07 14:13:49,910][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 95.30% examples, 3460877 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:49,954][gensim.models.word2vec][INFO] - EPOCH 20: training on 3639370 raw words (3640721 effective words) took 1.0s, 3471448 effective words/s
[2023-02-07 14:13:50,957][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 92.75% examples, 3398234 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:51,025][gensim.models.word2vec][INFO] - EPOCH 21: training on 3639370 raw words (3640721 effective words) took 1.1s, 3406609 effective words/s
[2023-02-07 14:13:52,027][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 94.69% examples, 3455929 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:52,081][gensim.models.word2vec][INFO] - EPOCH 22: training on 3639370 raw words (3640721 effective words) took 1.1s, 3452430 effective words/s
[2023-02-07 14:13:53,083][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 95.30% examples, 3473055 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:53,130][gensim.models.word2vec][INFO] - EPOCH 23: training on 3639370 raw words (3640721 effective words) took 1.0s, 3476847 effective words/s
[2023-02-07 14:13:54,132][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 95.98% examples, 3490739 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:54,173][gensim.models.word2vec][INFO] - EPOCH 24: training on 3639370 raw words (3640721 effective words) took 1.0s, 3495586 effective words/s
[2023-02-07 14:13:55,180][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 95.30% examples, 3457614 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:55,225][gensim.models.word2vec][INFO] - EPOCH 25: training on 3639370 raw words (3640721 effective words) took 1.1s, 3465169 effective words/s
[2023-02-07 14:13:56,229][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 95.49% examples, 3477293 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:13:56,270][gensim.models.word2vec][INFO] - EPOCH 26: training on 3639370 raw words (3640721 effective words) took 1.0s, 3489793 effective words/s
[2023-02-07 14:13:57,278][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 96.41% examples, 3490138 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:57,312][gensim.models.word2vec][INFO] - EPOCH 27: training on 3639370 raw words (3640721 effective words) took 1.0s, 3498539 effective words/s
[2023-02-07 14:13:58,317][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 96.41% examples, 3500077 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:58,350][gensim.models.word2vec][INFO] - EPOCH 28: training on 3639370 raw words (3640721 effective words) took 1.0s, 3508904 effective words/s
[2023-02-07 14:13:59,356][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 95.30% examples, 3461521 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:13:59,405][gensim.models.word2vec][INFO] - EPOCH 29: training on 3639370 raw words (3640721 effective words) took 1.1s, 3456774 effective words/s
[2023-02-07 14:14:00,406][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 92.54% examples, 3391304 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:00,481][gensim.models.word2vec][INFO] - EPOCH 30: training on 3639370 raw words (3640721 effective words) took 1.1s, 3386789 effective words/s
[2023-02-07 14:14:01,483][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 94.11% examples, 3433566 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:01,542][gensim.models.word2vec][INFO] - EPOCH 31: training on 3639370 raw words (3640721 effective words) took 1.1s, 3436600 effective words/s
[2023-02-07 14:14:02,551][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 95.30% examples, 3449642 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:02,593][gensim.models.word2vec][INFO] - EPOCH 32: training on 3639370 raw words (3640721 effective words) took 1.0s, 3467400 effective words/s
[2023-02-07 14:14:03,597][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 95.64% examples, 3477477 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:03,639][gensim.models.word2vec][INFO] - EPOCH 33: training on 3639370 raw words (3640721 effective words) took 1.0s, 3486938 effective words/s
[2023-02-07 14:14:04,644][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 95.30% examples, 3463226 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:04,689][gensim.models.word2vec][INFO] - EPOCH 34: training on 3639370 raw words (3640721 effective words) took 1.0s, 3471049 effective words/s
[2023-02-07 14:14:05,692][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 95.64% examples, 3480197 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:05,734][gensim.models.word2vec][INFO] - EPOCH 35: training on 3639370 raw words (3640721 effective words) took 1.0s, 3492036 effective words/s
[2023-02-07 14:14:06,737][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 96.41% examples, 3505852 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:06,770][gensim.models.word2vec][INFO] - EPOCH 36: training on 3639370 raw words (3640721 effective words) took 1.0s, 3515573 effective words/s
[2023-02-07 14:14:07,777][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 96.41% examples, 3494436 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:07,810][gensim.models.word2vec][INFO] - EPOCH 37: training on 3639370 raw words (3640721 effective words) took 1.0s, 3504357 effective words/s
[2023-02-07 14:14:08,814][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 96.68% examples, 3514019 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:14:08,844][gensim.models.word2vec][INFO] - EPOCH 38: training on 3639370 raw words (3640721 effective words) took 1.0s, 3528517 effective words/s
[2023-02-07 14:14:09,848][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 95.30% examples, 3470236 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:09,893][gensim.models.word2vec][INFO] - EPOCH 39: training on 3639370 raw words (3640721 effective words) took 1.0s, 3480739 effective words/s
[2023-02-07 14:14:10,895][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 95.30% examples, 3473107 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:10,944][gensim.models.word2vec][INFO] - EPOCH 40: training on 3639370 raw words (3640721 effective words) took 1.0s, 3470858 effective words/s
[2023-02-07 14:14:11,946][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 93.09% examples, 3405553 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:12,013][gensim.models.word2vec][INFO] - EPOCH 41: training on 3639370 raw words (3640721 effective words) took 1.1s, 3408883 effective words/s
[2023-02-07 14:14:13,014][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 94.41% examples, 3446040 words/s, in_qsize 8, out_qsize 1
[2023-02-07 14:14:13,069][gensim.models.word2vec][INFO] - EPOCH 42: training on 3639370 raw words (3640721 effective words) took 1.1s, 3452178 effective words/s
[2023-02-07 14:14:14,071][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 96.16% examples, 3503938 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:14,112][gensim.models.word2vec][INFO] - EPOCH 43: training on 3639370 raw words (3640721 effective words) took 1.0s, 3496032 effective words/s
[2023-02-07 14:14:15,119][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 96.41% examples, 3494590 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:15,152][gensim.models.word2vec][INFO] - EPOCH 44: training on 3639370 raw words (3640721 effective words) took 1.0s, 3503180 effective words/s
[2023-02-07 14:14:16,159][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 95.30% examples, 3458151 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:16,203][gensim.models.word2vec][INFO] - EPOCH 45: training on 3639370 raw words (3640721 effective words) took 1.0s, 3469026 effective words/s
[2023-02-07 14:14:17,207][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 96.41% examples, 3506930 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:14:17,241][gensim.models.word2vec][INFO] - EPOCH 46: training on 3639370 raw words (3640721 effective words) took 1.0s, 3516314 effective words/s
[2023-02-07 14:14:18,244][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 95.64% examples, 3477904 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:18,286][gensim.models.word2vec][INFO] - EPOCH 47: training on 3639370 raw words (3640721 effective words) took 1.0s, 3488252 effective words/s
[2023-02-07 14:14:19,288][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 95.30% examples, 3471650 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:19,333][gensim.models.word2vec][INFO] - EPOCH 48: training on 3639370 raw words (3640721 effective words) took 1.0s, 3481851 effective words/s
[2023-02-07 14:14:20,337][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 95.30% examples, 3469414 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:20,384][gensim.models.word2vec][INFO] - EPOCH 49: training on 3639370 raw words (3640721 effective words) took 1.0s, 3470446 effective words/s
[2023-02-07 14:14:21,390][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 95.30% examples, 3458798 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:21,436][gensim.models.word2vec][INFO] - EPOCH 50: training on 3639370 raw words (3640721 effective words) took 1.1s, 3467335 effective words/s
[2023-02-07 14:14:22,440][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 94.11% examples, 3426574 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:22,500][gensim.models.word2vec][INFO] - EPOCH 51: training on 3639370 raw words (3640721 effective words) took 1.1s, 3425443 effective words/s
[2023-02-07 14:14:23,507][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 91.65% examples, 3346510 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:23,585][gensim.models.word2vec][INFO] - EPOCH 52: training on 3639370 raw words (3640721 effective words) took 1.1s, 3360235 effective words/s
[2023-02-07 14:14:24,591][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 92.75% examples, 3386828 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:24,660][gensim.models.word2vec][INFO] - EPOCH 53: training on 3639370 raw words (3640721 effective words) took 1.1s, 3393426 effective words/s
[2023-02-07 14:14:25,664][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 94.11% examples, 3430820 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:25,722][gensim.models.word2vec][INFO] - EPOCH 54: training on 3639370 raw words (3640721 effective words) took 1.1s, 3434882 effective words/s
[2023-02-07 14:14:26,725][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 93.74% examples, 3422226 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:26,789][gensim.models.word2vec][INFO] - EPOCH 55: training on 3639370 raw words (3640721 effective words) took 1.1s, 3417222 effective words/s
[2023-02-07 14:14:27,796][gensim.models.word2vec][INFO] - EPOCH 56 - PROGRESS: at 91.65% examples, 3345889 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:27,875][gensim.models.word2vec][INFO] - EPOCH 56: training on 3639370 raw words (3640721 effective words) took 1.1s, 3357699 effective words/s
[2023-02-07 14:14:28,882][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 92.75% examples, 3383096 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:28,952][gensim.models.word2vec][INFO] - EPOCH 57: training on 3639370 raw words (3640721 effective words) took 1.1s, 3386446 effective words/s
[2023-02-07 14:14:29,954][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 93.74% examples, 3427755 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:30,017][gensim.models.word2vec][INFO] - EPOCH 58: training on 3639370 raw words (3640721 effective words) took 1.1s, 3426228 effective words/s
[2023-02-07 14:14:31,023][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 92.75% examples, 3382553 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:31,095][gensim.models.word2vec][INFO] - EPOCH 59: training on 3639370 raw words (3640721 effective words) took 1.1s, 3381395 effective words/s
[2023-02-07 14:14:32,098][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 91.40% examples, 3347649 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:32,185][gensim.models.word2vec][INFO] - EPOCH 60: training on 3639370 raw words (3640721 effective words) took 1.1s, 3344106 effective words/s
[2023-02-07 14:14:33,189][gensim.models.word2vec][INFO] - EPOCH 61 - PROGRESS: at 92.75% examples, 3387934 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:33,257][gensim.models.word2vec][INFO] - EPOCH 61: training on 3639370 raw words (3640721 effective words) took 1.1s, 3399111 effective words/s
[2023-02-07 14:14:34,261][gensim.models.word2vec][INFO] - EPOCH 62 - PROGRESS: at 94.11% examples, 3433272 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:14:34,316][gensim.models.word2vec][INFO] - EPOCH 62: training on 3639370 raw words (3640721 effective words) took 1.1s, 3445782 effective words/s
[2023-02-07 14:14:34,317][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 229280310 raw words (229365423 effective words) took 67.5s, 3395924 effective words/s', 'datetime': '2023-02-07T14:14:34.316983', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:14:34.317 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:14:39,983][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141300-s7yg8q4n/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:14:39.983360', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:14:39,985][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:14:40,022][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141300-s7yg8q4n/files/../tmp/embedding_model.pt
2023-02-07 14:14:40.023 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:14:41.365 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:14:41.862 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:14:42.921 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0171407815387687, 'test_mae': 1.0666155906542678, 'test_r2': 0.045771805702452095}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.042 MB uploaded (0.000 MB deduped)wandb: | 0.042 MB of 0.042 MB uploaded (0.000 MB deduped)wandb: / 0.042 MB of 0.042 MB uploaded (0.000 MB deduped)wandb: - 0.042 MB of 0.042 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.91
wandb: percentage 0.14593
wandb:   test_mae 1.06662
wandb:   test_mse 2.01714
wandb:    test_r2 0.04577
wandb: 
wandb: üöÄ View run helpful-sweep-73 at: https://wandb.ai/xiaoqiz/mof2vec/runs/s7yg8q4n
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_141300-s7yg8q4n/logs
wandb: Agent Starting Run: u423nndv with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 359
wandb: 	model.gensim.alpha: 0.00473094420533571
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 100
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.5641650455278568
wandb: 	model.gensim.vector_size: 41
wandb: 	model.gensim.window: 4
wandb: 	model.sklearn.learning_rate: 0.011144478048863217
wandb: 	model.sklearn.max_depth: 9
wandb: 	model.sklearn.min_child_weight: 0.03022969165184026
wandb: 	model.sklearn.n_estimators: 3609
wandb: 	model.sklearn.num_leaves: 471
wandb: 	model.sklearn.reg_alpha: 0.2063468832280503
wandb: 	model.sklearn.reg_lambda: 0.2752868088495801
wandb: 	model.sklearn.subsample: 0.24386245687256625
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141457-u423nndv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-74
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/u423nndv
2023-02-07 14:15:06.254 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 14:15:06.255 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 359 for sweep.
2023-02-07 14:15:06.255 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.00473094420533571 for sweep.
2023-02-07 14:15:06.256 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:15:06.256 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 100 for sweep.
2023-02-07 14:15:06.256 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 14:15:06.256 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5641650455278568 for sweep.
2023-02-07 14:15:06.257 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 41 for sweep.
2023-02-07 14:15:06.257 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 4 for sweep.
2023-02-07 14:15:06.257 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.011144478048863217 for sweep.
2023-02-07 14:15:06.257 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 9 for sweep.
2023-02-07 14:15:06.257 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03022969165184026 for sweep.
2023-02-07 14:15:06.258 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3609 for sweep.
2023-02-07 14:15:06.258 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 471 for sweep.
2023-02-07 14:15:06.258 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.2063468832280503 for sweep.
2023-02-07 14:15:06.258 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.2752868088495801 for sweep.
2023-02-07 14:15:06.259 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.24386245687256625 for sweep.
2023-02-07 14:15:06.259 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:15:06.279 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141457-u423nndv/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 359, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 41, 'window': 4, 'min_count': 2, 'dm': 0, 'sample': 0.5641650455278568, 'workers': 4, 'alpha': 0.00473094420533571, 'epochs': 100}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3609, 'max_depth': 9, 'num_leaves': 471, 'reg_alpha': 0.2063468832280503, 'reg_lambda': 0.2752868088495801, 'subsample': 0.24386245687256625, 'min_child_weight': 0.03022969165184026, 'n_jobs': 4, 'learning_rate': 0.011144478048863217}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 35/3257 [00:00<00:09, 348.40it/s]  2%|‚ñè         | 73/3257 [00:00<00:08, 362.44it/s]  3%|‚ñé         | 110/3257 [00:00<00:08, 363.38it/s]  5%|‚ñç         | 152/3257 [00:00<00:08, 382.77it/s]  6%|‚ñå         | 191/3257 [00:00<00:08, 380.64it/s]  7%|‚ñã         | 235/3257 [00:00<00:07, 398.46it/s]  8%|‚ñä         | 275/3257 [00:00<00:07, 396.45it/s] 10%|‚ñâ         | 318/3257 [00:00<00:07, 405.90it/s] 11%|‚ñà         | 361/3257 [00:00<00:07, 412.71it/s] 12%|‚ñà‚ñè        | 403/3257 [00:01<00:07, 396.61it/s] 14%|‚ñà‚ñé        | 443/3257 [00:01<00:07, 371.32it/s] 15%|‚ñà‚ñç        | 481/3257 [00:01<00:07, 371.23it/s] 16%|‚ñà‚ñå        | 524/3257 [00:01<00:07, 387.94it/s] 17%|‚ñà‚ñã        | 564/3257 [00:01<00:07, 378.99it/s] 19%|‚ñà‚ñä        | 604/3257 [00:01<00:06, 382.23it/s] 20%|‚ñà‚ñâ        | 644/3257 [00:01<00:06, 386.32it/s] 21%|‚ñà‚ñà        | 683/3257 [00:01<00:06, 373.57it/s] 22%|‚ñà‚ñà‚ñè       | 723/3257 [00:01<00:06, 377.47it/s] 23%|‚ñà‚ñà‚ñé       | 763/3257 [00:01<00:06, 381.52it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:02<00:06, 382.85it/s] 26%|‚ñà‚ñà‚ñå       | 843/3257 [00:02<00:06, 370.47it/s] 27%|‚ñà‚ñà‚ñã       | 882/3257 [00:02<00:06, 376.01it/s] 28%|‚ñà‚ñà‚ñä       | 924/3257 [00:02<00:06, 388.32it/s] 30%|‚ñà‚ñà‚ñâ       | 965/3257 [00:02<00:05, 394.26it/s] 31%|‚ñà‚ñà‚ñà       | 1005/3257 [00:02<00:05, 387.85it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1044/3257 [00:02<00:05, 374.40it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1083/3257 [00:02<00:05, 376.59it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1121/3257 [00:02<00:05, 377.24it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1159/3257 [00:03<00:05, 376.47it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1197/3257 [00:03<00:05, 355.27it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1233/3257 [00:03<00:08, 251.67it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:03<00:07, 279.56it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1304/3257 [00:03<00:06, 291.67it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1346/3257 [00:03<00:05, 322.23it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1381/3257 [00:03<00:05, 328.72it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1424/3257 [00:03<00:05, 354.17it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1466/3257 [00:04<00:04, 371.32it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1510/3257 [00:04<00:04, 389.11it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1550/3257 [00:04<00:04, 371.72it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1589/3257 [00:04<00:04, 375.37it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1632/3257 [00:04<00:04, 389.42it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1672/3257 [00:04<00:04, 370.60it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1712/3257 [00:04<00:04, 378.78it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1751/3257 [00:04<00:04, 366.97it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1794/3257 [00:04<00:03, 383.61it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1833/3257 [00:04<00:03, 376.73it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1876/3257 [00:05<00:03, 388.52it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:05<00:03, 385.30it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1960/3257 [00:05<00:03, 398.87it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2001/3257 [00:05<00:03, 379.10it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2040/3257 [00:05<00:03, 377.18it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2078/3257 [00:05<00:03, 372.53it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2118/3257 [00:05<00:03, 379.03it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2157/3257 [00:05<00:03, 364.51it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2197/3257 [00:05<00:02, 373.18it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2235/3257 [00:06<00:02, 374.37it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2273/3257 [00:06<00:02, 363.60it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2317/3257 [00:06<00:02, 383.61it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2362/3257 [00:06<00:02, 401.68it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:06<00:02, 401.70it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2446/3257 [00:06<00:02, 395.72it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2489/3257 [00:06<00:01, 402.84it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2532/3257 [00:06<00:01, 408.83it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2573/3257 [00:06<00:01, 391.63it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2613/3257 [00:07<00:02, 280.92it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2653/3257 [00:07<00:01, 307.27it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2693/3257 [00:07<00:01, 329.57it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2730/3257 [00:07<00:01, 333.04it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:07<00:01, 345.01it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2805/3257 [00:07<00:01, 345.94it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2841/3257 [00:07<00:01, 346.36it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:07<00:01, 370.54it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2924/3257 [00:07<00:00, 371.87it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2962/3257 [00:08<00:00, 362.68it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2999/3257 [00:08<00:00, 362.40it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3038/3257 [00:08<00:00, 368.49it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3083/3257 [00:08<00:00, 390.47it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3127/3257 [00:08<00:00, 404.62it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3168/3257 [00:08<00:00, 388.92it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3208/3257 [00:08<00:00, 373.86it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3250/3257 [00:08<00:00, 385.60it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 368.96it/s]
2023-02-07 14:15:15.296 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:15:15,297][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d41,n5,mc2,s0.564165,t4>', 'datetime': '2023-02-07T14:15:15.297174', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:15:15,297][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:15:15,297][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:15:15,440][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 14:15:15,441][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:15:15,443][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 881 unique words (95.35% of original 924, drops 43)', 'datetime': '2023-02-07T14:15:15.443761', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:15:15,445][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 1455705 word corpus (100.00% of original 1455748, drops 43)', 'datetime': '2023-02-07T14:15:15.445510', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:15:15,448][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 14:15:15,449][gensim.models.word2vec][INFO] - sample=0.564165 downsamples 0 most-common words
[2023-02-07 14:15:15,449][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455705 word corpus (100.0%% of prior 1455705)', 'datetime': '2023-02-07T14:15:15.449172', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:15:15,454][gensim.models.word2vec][INFO] - estimated required memory for 881 words and 41 dimensions: 1915016 bytes
[2023-02-07 14:15:15,454][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:15:15,456][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 881 vocabulary and 41 features, using sg=1 hs=0 sample=0.5641650455278568 negative=5 window=4 shrink_windows=True', 'datetime': '2023-02-07T14:15:15.456077', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:15:15,885][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458962 effective words) took 0.4s, 3413527 effective words/s
[2023-02-07 14:15:16,225][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458962 effective words) took 0.3s, 4308136 effective words/s
[2023-02-07 14:15:16,560][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458962 effective words) took 0.3s, 4377075 effective words/s
[2023-02-07 14:15:16,889][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458962 effective words) took 0.3s, 4447621 effective words/s
[2023-02-07 14:15:17,220][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458962 effective words) took 0.3s, 4424595 effective words/s
[2023-02-07 14:15:17,549][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458962 effective words) took 0.3s, 4455816 effective words/s
[2023-02-07 14:15:17,877][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458962 effective words) took 0.3s, 4465635 effective words/s
[2023-02-07 14:15:18,203][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458962 effective words) took 0.3s, 4505343 effective words/s
[2023-02-07 14:15:18,529][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458962 effective words) took 0.3s, 4492901 effective words/s
[2023-02-07 14:15:18,852][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458962 effective words) took 0.3s, 4544923 effective words/s
[2023-02-07 14:15:19,174][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458962 effective words) took 0.3s, 4542389 effective words/s
[2023-02-07 14:15:19,497][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458962 effective words) took 0.3s, 4548857 effective words/s
[2023-02-07 14:15:19,818][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458962 effective words) took 0.3s, 4562821 effective words/s
[2023-02-07 14:15:20,139][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458962 effective words) took 0.3s, 4559354 effective words/s
[2023-02-07 14:15:20,462][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458962 effective words) took 0.3s, 4539596 effective words/s
[2023-02-07 14:15:20,787][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458962 effective words) took 0.3s, 4514619 effective words/s
[2023-02-07 14:15:21,106][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458962 effective words) took 0.3s, 4597990 effective words/s
[2023-02-07 14:15:21,427][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458962 effective words) took 0.3s, 4558628 effective words/s
[2023-02-07 14:15:21,748][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458962 effective words) took 0.3s, 4560720 effective words/s
[2023-02-07 14:15:22,073][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458962 effective words) took 0.3s, 4518537 effective words/s
[2023-02-07 14:15:22,396][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458962 effective words) took 0.3s, 4536131 effective words/s
[2023-02-07 14:15:22,719][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458962 effective words) took 0.3s, 4529938 effective words/s
[2023-02-07 14:15:23,041][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458962 effective words) took 0.3s, 4554419 effective words/s
[2023-02-07 14:15:23,362][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458962 effective words) took 0.3s, 4567490 effective words/s
[2023-02-07 14:15:23,682][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458962 effective words) took 0.3s, 4586879 effective words/s
[2023-02-07 14:15:24,004][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458962 effective words) took 0.3s, 4543079 effective words/s
[2023-02-07 14:15:24,325][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458962 effective words) took 0.3s, 4560302 effective words/s
[2023-02-07 14:15:24,647][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458962 effective words) took 0.3s, 4546252 effective words/s
[2023-02-07 14:15:24,968][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458962 effective words) took 0.3s, 4569300 effective words/s
[2023-02-07 14:15:25,288][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458962 effective words) took 0.3s, 4588039 effective words/s
[2023-02-07 14:15:25,605][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458962 effective words) took 0.3s, 4622405 effective words/s
[2023-02-07 14:15:25,924][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458962 effective words) took 0.3s, 4596311 effective words/s
[2023-02-07 14:15:26,245][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458962 effective words) took 0.3s, 4567478 effective words/s
[2023-02-07 14:15:26,565][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458962 effective words) took 0.3s, 4577332 effective words/s
[2023-02-07 14:15:26,882][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458962 effective words) took 0.3s, 4632596 effective words/s
[2023-02-07 14:15:27,201][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458962 effective words) took 0.3s, 4595688 effective words/s
[2023-02-07 14:15:27,518][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458962 effective words) took 0.3s, 4615460 effective words/s
[2023-02-07 14:15:27,837][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458962 effective words) took 0.3s, 4599235 effective words/s
[2023-02-07 14:15:28,150][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458962 effective words) took 0.3s, 4680111 effective words/s
[2023-02-07 14:15:28,468][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458962 effective words) took 0.3s, 4608413 effective words/s
[2023-02-07 14:15:28,782][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458962 effective words) took 0.3s, 4676198 effective words/s
[2023-02-07 14:15:29,099][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458962 effective words) took 0.3s, 4637522 effective words/s
[2023-02-07 14:15:29,413][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458962 effective words) took 0.3s, 4655774 effective words/s
[2023-02-07 14:15:29,727][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458962 effective words) took 0.3s, 4670894 effective words/s
[2023-02-07 14:15:30,045][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458962 effective words) took 0.3s, 4605235 effective words/s
[2023-02-07 14:15:30,367][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458962 effective words) took 0.3s, 4561659 effective words/s
[2023-02-07 14:15:30,685][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458962 effective words) took 0.3s, 4600539 effective words/s
[2023-02-07 14:15:31,004][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458962 effective words) took 0.3s, 4601196 effective words/s
[2023-02-07 14:15:31,325][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458962 effective words) took 0.3s, 4593859 effective words/s
[2023-02-07 14:15:31,642][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458962 effective words) took 0.3s, 4623354 effective words/s
[2023-02-07 14:15:31,959][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458962 effective words) took 0.3s, 4626628 effective words/s
[2023-02-07 14:15:32,276][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458962 effective words) took 0.3s, 4629592 effective words/s
[2023-02-07 14:15:32,592][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458962 effective words) took 0.3s, 4631505 effective words/s
[2023-02-07 14:15:32,907][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458962 effective words) took 0.3s, 4652159 effective words/s
[2023-02-07 14:15:33,221][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458962 effective words) took 0.3s, 4659512 effective words/s
[2023-02-07 14:15:33,532][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458962 effective words) took 0.3s, 4709876 effective words/s
[2023-02-07 14:15:33,845][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1458962 effective words) took 0.3s, 4692559 effective words/s
[2023-02-07 14:15:34,155][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1458962 effective words) took 0.3s, 4726499 effective words/s
[2023-02-07 14:15:34,469][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1458962 effective words) took 0.3s, 4664641 effective words/s
[2023-02-07 14:15:34,781][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1458962 effective words) took 0.3s, 4707771 effective words/s
[2023-02-07 14:15:35,090][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1458962 effective words) took 0.3s, 4738923 effective words/s
[2023-02-07 14:15:35,402][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1458962 effective words) took 0.3s, 4700915 effective words/s
[2023-02-07 14:15:35,712][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1458962 effective words) took 0.3s, 4729398 effective words/s
[2023-02-07 14:15:36,025][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1458962 effective words) took 0.3s, 4673221 effective words/s
[2023-02-07 14:15:36,341][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1458962 effective words) took 0.3s, 4643349 effective words/s
[2023-02-07 14:15:36,654][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1458962 effective words) took 0.3s, 4672670 effective words/s
[2023-02-07 14:15:36,968][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1458962 effective words) took 0.3s, 4685035 effective words/s
[2023-02-07 14:15:37,281][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1458962 effective words) took 0.3s, 4682627 effective words/s
[2023-02-07 14:15:37,595][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1458962 effective words) took 0.3s, 4669817 effective words/s
[2023-02-07 14:15:37,908][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1458962 effective words) took 0.3s, 4672309 effective words/s
[2023-02-07 14:15:38,219][gensim.models.word2vec][INFO] - EPOCH 70: training on 1455748 raw words (1458962 effective words) took 0.3s, 4716933 effective words/s
[2023-02-07 14:15:38,532][gensim.models.word2vec][INFO] - EPOCH 71: training on 1455748 raw words (1458962 effective words) took 0.3s, 4674999 effective words/s
[2023-02-07 14:15:38,845][gensim.models.word2vec][INFO] - EPOCH 72: training on 1455748 raw words (1458962 effective words) took 0.3s, 4693742 effective words/s
[2023-02-07 14:15:39,152][gensim.models.word2vec][INFO] - EPOCH 73: training on 1455748 raw words (1458962 effective words) took 0.3s, 4764586 effective words/s
[2023-02-07 14:15:39,461][gensim.models.word2vec][INFO] - EPOCH 74: training on 1455748 raw words (1458962 effective words) took 0.3s, 4742370 effective words/s
[2023-02-07 14:15:39,766][gensim.models.word2vec][INFO] - EPOCH 75: training on 1455748 raw words (1458962 effective words) took 0.3s, 4808923 effective words/s
[2023-02-07 14:15:40,073][gensim.models.word2vec][INFO] - EPOCH 76: training on 1455748 raw words (1458962 effective words) took 0.3s, 4765031 effective words/s
[2023-02-07 14:15:40,384][gensim.models.word2vec][INFO] - EPOCH 77: training on 1455748 raw words (1458962 effective words) took 0.3s, 4715924 effective words/s
[2023-02-07 14:15:40,692][gensim.models.word2vec][INFO] - EPOCH 78: training on 1455748 raw words (1458962 effective words) took 0.3s, 4766726 effective words/s
[2023-02-07 14:15:40,999][gensim.models.word2vec][INFO] - EPOCH 79: training on 1455748 raw words (1458962 effective words) took 0.3s, 4778945 effective words/s
[2023-02-07 14:15:41,305][gensim.models.word2vec][INFO] - EPOCH 80: training on 1455748 raw words (1458962 effective words) took 0.3s, 4794809 effective words/s
[2023-02-07 14:15:41,615][gensim.models.word2vec][INFO] - EPOCH 81: training on 1455748 raw words (1458962 effective words) took 0.3s, 4723807 effective words/s
[2023-02-07 14:15:41,923][gensim.models.word2vec][INFO] - EPOCH 82: training on 1455748 raw words (1458962 effective words) took 0.3s, 4752002 effective words/s
[2023-02-07 14:15:42,236][gensim.models.word2vec][INFO] - EPOCH 83: training on 1455748 raw words (1458962 effective words) took 0.3s, 4707754 effective words/s
[2023-02-07 14:15:42,546][gensim.models.word2vec][INFO] - EPOCH 84: training on 1455748 raw words (1458962 effective words) took 0.3s, 4722412 effective words/s
[2023-02-07 14:15:42,856][gensim.models.word2vec][INFO] - EPOCH 85: training on 1455748 raw words (1458962 effective words) took 0.3s, 4728676 effective words/s
[2023-02-07 14:15:43,164][gensim.models.word2vec][INFO] - EPOCH 86: training on 1455748 raw words (1458962 effective words) took 0.3s, 4760529 effective words/s
[2023-02-07 14:15:43,473][gensim.models.word2vec][INFO] - EPOCH 87: training on 1455748 raw words (1458962 effective words) took 0.3s, 4756640 effective words/s
[2023-02-07 14:15:43,780][gensim.models.word2vec][INFO] - EPOCH 88: training on 1455748 raw words (1458962 effective words) took 0.3s, 4771719 effective words/s
[2023-02-07 14:15:44,091][gensim.models.word2vec][INFO] - EPOCH 89: training on 1455748 raw words (1458962 effective words) took 0.3s, 4711192 effective words/s
[2023-02-07 14:15:44,404][gensim.models.word2vec][INFO] - EPOCH 90: training on 1455748 raw words (1458962 effective words) took 0.3s, 4699851 effective words/s
[2023-02-07 14:15:44,713][gensim.models.word2vec][INFO] - EPOCH 91: training on 1455748 raw words (1458962 effective words) took 0.3s, 4730860 effective words/s
[2023-02-07 14:15:45,020][gensim.models.word2vec][INFO] - EPOCH 92: training on 1455748 raw words (1458962 effective words) took 0.3s, 4798388 effective words/s
[2023-02-07 14:15:45,333][gensim.models.word2vec][INFO] - EPOCH 93: training on 1455748 raw words (1458962 effective words) took 0.3s, 4687625 effective words/s
[2023-02-07 14:15:45,648][gensim.models.word2vec][INFO] - EPOCH 94: training on 1455748 raw words (1458962 effective words) took 0.3s, 4651830 effective words/s
[2023-02-07 14:15:45,962][gensim.models.word2vec][INFO] - EPOCH 95: training on 1455748 raw words (1458962 effective words) took 0.3s, 4662635 effective words/s
[2023-02-07 14:15:46,274][gensim.models.word2vec][INFO] - EPOCH 96: training on 1455748 raw words (1458962 effective words) took 0.3s, 4690021 effective words/s
[2023-02-07 14:15:46,590][gensim.models.word2vec][INFO] - EPOCH 97: training on 1455748 raw words (1458962 effective words) took 0.3s, 4639866 effective words/s
[2023-02-07 14:15:46,915][gensim.models.word2vec][INFO] - EPOCH 98: training on 1455748 raw words (1458962 effective words) took 0.3s, 4507592 effective words/s
[2023-02-07 14:15:47,234][gensim.models.word2vec][INFO] - EPOCH 99: training on 1455748 raw words (1458962 effective words) took 0.3s, 4591221 effective words/s
[2023-02-07 14:15:47,235][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 145574800 raw words (145896200 effective words) took 31.8s, 4590951 effective words/s', 'datetime': '2023-02-07T14:15:47.235394', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:15:47.235 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:15:49,361][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141457-u423nndv/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:15:49.361229', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:15:49,362][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:15:49,365][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141457-u423nndv/files/../tmp/embedding_model.pt
2023-02-07 14:15:49.365 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:15:50.253 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:15:50.634 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:15:51.009 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8360788527083896, 'test_mae': 1.0100963797385982, 'test_r2': 0.13142492371241254}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.7
wandb: percentage 0.04654
wandb:   test_mae 1.0101
wandb:   test_mse 1.83608
wandb:    test_r2 0.13142
wandb: 
wandb: üöÄ View run wandering-sweep-74 at: https://wandb.ai/xiaoqiz/mof2vec/runs/u423nndv
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_141457-u423nndv/logs
wandb: Agent Starting Run: 3ryhrph9 with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 367
wandb: 	model.gensim.alpha: 0.00091536372788567
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 22
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.334093892512733
wandb: 	model.gensim.vector_size: 50
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.01820566747393646
wandb: 	model.sklearn.max_depth: 17
wandb: 	model.sklearn.min_child_weight: 0.027050142380133774
wandb: 	model.sklearn.n_estimators: 711
wandb: 	model.sklearn.num_leaves: 464
wandb: 	model.sklearn.reg_alpha: 0.026709235775686888
wandb: 	model.sklearn.reg_lambda: 0.21233189498887484
wandb: 	model.sklearn.subsample: 0.9242009440560304
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141601-3ryhrph9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-sweep-75
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/3ryhrph9
2023-02-07 14:16:09.950 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 14:16:09.951 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 367 for sweep.
2023-02-07 14:16:09.951 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.00091536372788567 for sweep.
2023-02-07 14:16:09.951 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:16:09.951 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 22 for sweep.
2023-02-07 14:16:09.952 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 14:16:09.952 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.334093892512733 for sweep.
2023-02-07 14:16:09.952 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 50 for sweep.
2023-02-07 14:16:09.952 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 14:16:09.953 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.01820566747393646 for sweep.
2023-02-07 14:16:09.953 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 17 for sweep.
2023-02-07 14:16:09.953 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.027050142380133774 for sweep.
2023-02-07 14:16:09.953 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 711 for sweep.
2023-02-07 14:16:09.953 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 464 for sweep.
2023-02-07 14:16:09.954 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.026709235775686888 for sweep.
2023-02-07 14:16:09.954 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.21233189498887484 for sweep.
2023-02-07 14:16:09.954 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.9242009440560304 for sweep.
2023-02-07 14:16:09.954 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:16:09.959 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141601-3ryhrph9/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 367, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 50, 'window': 3, 'min_count': 5, 'dm': 0, 'sample': 0.334093892512733, 'workers': 4, 'alpha': 0.00091536372788567, 'epochs': 22}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 711, 'max_depth': 17, 'num_leaves': 464, 'reg_alpha': 0.026709235775686888, 'reg_lambda': 0.21233189498887484, 'subsample': 0.9242009440560304, 'min_child_weight': 0.027050142380133774, 'n_jobs': 4, 'learning_rate': 0.01820566747393646}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 27/3257 [00:00<00:11, 269.32it/s]  2%|‚ñè         | 54/3257 [00:00<00:12, 264.14it/s]  3%|‚ñé         | 84/3257 [00:00<00:11, 276.19it/s]  3%|‚ñé         | 112/3257 [00:00<00:11, 262.67it/s]  4%|‚ñç         | 142/3257 [00:00<00:11, 274.67it/s]  5%|‚ñå         | 170/3257 [00:00<00:11, 275.32it/s]  6%|‚ñå         | 198/3257 [00:00<00:11, 274.98it/s]  7%|‚ñã         | 230/3257 [00:00<00:10, 288.40it/s]  8%|‚ñä         | 259/3257 [00:00<00:10, 284.92it/s]  9%|‚ñâ         | 293/3257 [00:01<00:09, 300.19it/s] 10%|‚ñâ         | 324/3257 [00:01<00:09, 297.28it/s] 11%|‚ñà         | 354/3257 [00:01<00:09, 290.43it/s] 12%|‚ñà‚ñè        | 384/3257 [00:01<00:10, 281.61it/s] 13%|‚ñà‚ñé        | 413/3257 [00:01<00:10, 280.73it/s] 14%|‚ñà‚ñé        | 442/3257 [00:01<00:11, 252.13it/s] 14%|‚ñà‚ñç        | 472/3257 [00:01<00:10, 263.37it/s] 15%|‚ñà‚ñå        | 500/3257 [00:01<00:10, 265.55it/s] 16%|‚ñà‚ñå        | 527/3257 [00:01<00:10, 266.71it/s] 17%|‚ñà‚ñã        | 557/3257 [00:02<00:09, 274.82it/s] 18%|‚ñà‚ñä        | 585/3257 [00:02<00:10, 254.70it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:02<00:09, 264.67it/s] 20%|‚ñà‚ñâ        | 642/3257 [00:02<00:10, 260.73it/s] 21%|‚ñà‚ñà        | 669/3257 [00:02<00:10, 253.23it/s] 21%|‚ñà‚ñà‚ñè       | 695/3257 [00:02<00:10, 250.84it/s] 22%|‚ñà‚ñà‚ñè       | 722/3257 [00:02<00:09, 253.85it/s] 23%|‚ñà‚ñà‚ñé       | 748/3257 [00:02<00:09, 252.82it/s] 24%|‚ñà‚ñà‚ñç       | 774/3257 [00:03<00:13, 178.88it/s] 25%|‚ñà‚ñà‚ñç       | 803/3257 [00:03<00:12, 203.60it/s] 25%|‚ñà‚ñà‚ñå       | 827/3257 [00:03<00:11, 208.29it/s] 26%|‚ñà‚ñà‚ñå       | 850/3257 [00:03<00:11, 213.15it/s] 27%|‚ñà‚ñà‚ñã       | 876/3257 [00:03<00:10, 224.46it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:03<00:09, 244.63it/s] 29%|‚ñà‚ñà‚ñä       | 932/3257 [00:03<00:09, 247.36it/s] 29%|‚ñà‚ñà‚ñâ       | 960/3257 [00:03<00:09, 254.33it/s] 30%|‚ñà‚ñà‚ñà       | 986/3257 [00:03<00:09, 249.42it/s] 31%|‚ñà‚ñà‚ñà       | 1012/3257 [00:03<00:08, 250.22it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1038/3257 [00:04<00:08, 250.95it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1064/3257 [00:04<00:08, 253.18it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1090/3257 [00:04<00:08, 254.90it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1117/3257 [00:04<00:08, 253.93it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1143/3257 [00:04<00:08, 247.89it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:04<00:08, 252.09it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1197/3257 [00:04<00:08, 239.20it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1222/3257 [00:04<00:08, 238.69it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1250/3257 [00:04<00:08, 249.23it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1276/3257 [00:05<00:07, 250.32it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1302/3257 [00:05<00:07, 244.77it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1331/3257 [00:05<00:07, 255.25it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1358/3257 [00:05<00:07, 258.22it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:05<00:07, 252.82it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1416/3257 [00:05<00:06, 271.01it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1444/3257 [00:05<00:06, 271.52it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1475/3257 [00:05<00:06, 282.62it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1508/3257 [00:05<00:05, 295.04it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1538/3257 [00:05<00:06, 269.12it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1566/3257 [00:06<00:06, 264.71it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1595/3257 [00:06<00:06, 271.30it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1623/3257 [00:06<00:05, 273.07it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1651/3257 [00:06<00:06, 266.25it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1678/3257 [00:06<00:06, 257.33it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1706/3257 [00:06<00:05, 263.44it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1733/3257 [00:06<00:06, 249.82it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1762/3257 [00:06<00:05, 259.58it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1792/3257 [00:06<00:05, 267.22it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1819/3257 [00:07<00:05, 262.30it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1846/3257 [00:07<00:05, 260.06it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1876/3257 [00:07<00:05, 270.96it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1904/3257 [00:07<00:05, 269.53it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1932/3257 [00:07<00:04, 271.82it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1966/3257 [00:07<00:04, 290.34it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1996/3257 [00:07<00:04, 286.82it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2025/3257 [00:07<00:04, 283.18it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2054/3257 [00:07<00:04, 266.54it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2081/3257 [00:08<00:04, 264.63it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2108/3257 [00:08<00:06, 169.27it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2131/3257 [00:08<00:06, 180.99it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2157/3257 [00:08<00:05, 198.07it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2182/3257 [00:08<00:05, 209.89it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2210/3257 [00:08<00:04, 223.90it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2238/3257 [00:08<00:04, 234.93it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2265/3257 [00:08<00:04, 241.69it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2292/3257 [00:09<00:03, 248.52it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2320/3257 [00:09<00:03, 256.23it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2354/3257 [00:09<00:03, 279.39it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2383/3257 [00:09<00:03, 281.75it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2412/3257 [00:09<00:03, 274.74it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2440/3257 [00:09<00:03, 265.89it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2467/3257 [00:09<00:02, 266.07it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2494/3257 [00:09<00:02, 265.96it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2524/3257 [00:09<00:02, 274.51it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2552/3257 [00:09<00:02, 274.93it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2580/3257 [00:10<00:02, 257.23it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2610/3257 [00:10<00:02, 268.77it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2642/3257 [00:10<00:02, 279.26it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2671/3257 [00:10<00:02, 272.87it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2699/3257 [00:10<00:02, 257.43it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2725/3257 [00:10<00:02, 257.92it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:10<00:01, 270.34it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2784/3257 [00:10<00:01, 271.33it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2814/3257 [00:10<00:01, 276.76it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2842/3257 [00:11<00:01, 264.56it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2880/3257 [00:11<00:01, 295.78it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2910/3257 [00:11<00:01, 280.04it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2939/3257 [00:11<00:01, 278.25it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2968/3257 [00:11<00:01, 267.18it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2995/3257 [00:11<00:01, 259.60it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3024/3257 [00:11<00:00, 265.82it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3057/3257 [00:11<00:00, 281.66it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3087/3257 [00:11<00:00, 284.26it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3121/3257 [00:12<00:00, 298.32it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3151/3257 [00:12<00:00, 279.96it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3180/3257 [00:12<00:00, 272.09it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3209/3257 [00:12<00:00, 276.68it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3239/3257 [00:12<00:00, 280.52it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:12<00:00, 259.49it/s]
2023-02-07 14:16:22.891 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:16:22,892][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d50,n5,mc5,s0.334094,t4>', 'datetime': '2023-02-07T14:16:22.892852', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:16:22,895][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:16:22,895][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:16:23,161][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 14:16:23,162][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:16:23,172][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 3705 unique words (55.61% of original 6662, drops 2957)', 'datetime': '2023-02-07T14:16:23.172885', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:16:23,173][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 2904334 word corpus (99.75% of original 2911496, drops 7162)', 'datetime': '2023-02-07T14:16:23.173265', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:16:23,186][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 14:16:23,186][gensim.models.word2vec][INFO] - sample=0.334094 downsamples 0 most-common words
[2023-02-07 14:16:23,186][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2904334 word corpus (100.0%% of prior 2904334)', 'datetime': '2023-02-07T14:16:23.186687', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:16:23,209][gensim.models.word2vec][INFO] - estimated required memory for 3705 words and 50 dimensions: 4637300 bytes
[2023-02-07 14:16:23,209][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:16:23,212][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 3705 vocabulary and 50 features, using sg=1 hs=0 sample=0.334093892512733 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T14:16:23.212129', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:16:24,128][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2907591 effective words) took 0.9s, 3180007 effective words/s
[2023-02-07 14:16:25,039][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2907591 effective words) took 0.9s, 3202579 effective words/s
[2023-02-07 14:16:25,962][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2907591 effective words) took 0.9s, 3155049 effective words/s
[2023-02-07 14:16:26,869][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2907591 effective words) took 0.9s, 3210323 effective words/s
[2023-02-07 14:16:27,785][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2907591 effective words) took 0.9s, 3181074 effective words/s
[2023-02-07 14:16:28,693][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2907591 effective words) took 0.9s, 3209391 effective words/s
[2023-02-07 14:16:29,615][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2907591 effective words) took 0.9s, 3156955 effective words/s
[2023-02-07 14:16:30,545][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2907591 effective words) took 0.9s, 3133562 effective words/s
[2023-02-07 14:16:31,469][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2907591 effective words) took 0.9s, 3150163 effective words/s
[2023-02-07 14:16:32,395][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2907591 effective words) took 0.9s, 3148419 effective words/s
[2023-02-07 14:16:33,331][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2907591 effective words) took 0.9s, 3112149 effective words/s
[2023-02-07 14:16:34,252][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2907591 effective words) took 0.9s, 3159793 effective words/s
[2023-02-07 14:16:35,178][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2907591 effective words) took 0.9s, 3147585 effective words/s
[2023-02-07 14:16:36,106][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2907591 effective words) took 0.9s, 3136653 effective words/s
[2023-02-07 14:16:37,028][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2907591 effective words) took 0.9s, 3161023 effective words/s
[2023-02-07 14:16:37,945][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2907591 effective words) took 0.9s, 3171822 effective words/s
[2023-02-07 14:16:38,840][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2907591 effective words) took 0.9s, 3254461 effective words/s
[2023-02-07 14:16:39,681][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2907591 effective words) took 0.8s, 3463672 effective words/s
[2023-02-07 14:16:40,525][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2907591 effective words) took 0.8s, 3450316 effective words/s
[2023-02-07 14:16:41,379][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2907591 effective words) took 0.9s, 3409525 effective words/s
[2023-02-07 14:16:42,218][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2907591 effective words) took 0.8s, 3470831 effective words/s
[2023-02-07 14:16:43,056][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2907591 effective words) took 0.8s, 3475773 effective words/s
[2023-02-07 14:16:43,057][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 64052912 raw words (63967002 effective words) took 19.8s, 3223407 effective words/s', 'datetime': '2023-02-07T14:16:43.056987', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:16:43.057 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:16:44,462][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141601-3ryhrph9/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:16:44.462862', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:16:44,463][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:16:44,470][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141601-3ryhrph9/files/../tmp/embedding_model.pt
2023-02-07 14:16:44.470 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:16:45.341 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:16:45.709 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:16:46.112 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.939966429035755, 'test_mae': 1.0412690393695254, 'test_r2': 0.08227988868258795}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.57
wandb: percentage 0.44386
wandb:   test_mae 1.04127
wandb:   test_mse 1.93997
wandb:    test_r2 0.08228
wandb: 
wandb: üöÄ View run super-sweep-75 at: https://wandb.ai/xiaoqiz/mof2vec/runs/3ryhrph9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_141601-3ryhrph9/logs
wandb: Agent Starting Run: fe2iej0s with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 395
wandb: 	model.gensim.alpha: 0.001964807551583086
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 52
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.32406383720290793
wandb: 	model.gensim.vector_size: 38
wandb: 	model.gensim.window: 5
wandb: 	model.sklearn.learning_rate: 0.03083591270807009
wandb: 	model.sklearn.max_depth: 50
wandb: 	model.sklearn.min_child_weight: 0.03915622714266435
wandb: 	model.sklearn.n_estimators: 3428
wandb: 	model.sklearn.num_leaves: 469
wandb: 	model.sklearn.reg_alpha: 0.2083809658578546
wandb: 	model.sklearn.reg_lambda: 0.24553950552804135
wandb: 	model.sklearn.subsample: 0.33480590308926084
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141659-fe2iej0s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-76
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/fe2iej0s
2023-02-07 14:17:06.215 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 14:17:06.216 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 395 for sweep.
2023-02-07 14:17:06.216 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.001964807551583086 for sweep.
2023-02-07 14:17:06.216 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:17:06.218 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 52 for sweep.
2023-02-07 14:17:06.218 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 14:17:06.218 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.32406383720290793 for sweep.
2023-02-07 14:17:06.218 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 38 for sweep.
2023-02-07 14:17:06.218 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 5 for sweep.
2023-02-07 14:17:06.219 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.03083591270807009 for sweep.
2023-02-07 14:17:06.219 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 50 for sweep.
2023-02-07 14:17:06.219 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03915622714266435 for sweep.
2023-02-07 14:17:06.219 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3428 for sweep.
2023-02-07 14:17:06.219 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 469 for sweep.
2023-02-07 14:17:06.220 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.2083809658578546 for sweep.
2023-02-07 14:17:06.220 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.24553950552804135 for sweep.
2023-02-07 14:17:06.220 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.33480590308926084 for sweep.
2023-02-07 14:17:06.220 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:17:06.226 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141659-fe2iej0s/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 395, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 38, 'window': 5, 'min_count': 3, 'dm': 0, 'sample': 0.32406383720290793, 'workers': 4, 'alpha': 0.001964807551583086, 'epochs': 52}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3428, 'max_depth': 50, 'num_leaves': 469, 'reg_alpha': 0.2083809658578546, 'reg_lambda': 0.24553950552804135, 'subsample': 0.33480590308926084, 'min_child_weight': 0.03915622714266435, 'n_jobs': 4, 'learning_rate': 0.03083591270807009}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 39/3257 [00:00<00:08, 383.78it/s]  2%|‚ñè         | 78/3257 [00:00<00:08, 386.88it/s]  4%|‚ñé         | 117/3257 [00:00<00:08, 373.36it/s]  5%|‚ñç         | 155/3257 [00:00<00:08, 372.68it/s]  6%|‚ñå         | 194/3257 [00:00<00:08, 376.39it/s]  7%|‚ñã         | 237/3257 [00:00<00:07, 392.20it/s]  9%|‚ñä         | 277/3257 [00:00<00:10, 297.77it/s] 10%|‚ñâ         | 320/3257 [00:00<00:08, 330.67it/s] 11%|‚ñà         | 362/3257 [00:01<00:08, 353.49it/s] 12%|‚ñà‚ñè        | 400/3257 [00:01<00:07, 357.32it/s] 13%|‚ñà‚ñé        | 438/3257 [00:01<00:08, 336.78it/s] 15%|‚ñà‚ñç        | 475/3257 [00:01<00:08, 344.46it/s] 16%|‚ñà‚ñå        | 517/3257 [00:01<00:07, 365.02it/s] 17%|‚ñà‚ñã        | 557/3257 [00:01<00:07, 374.26it/s] 18%|‚ñà‚ñä        | 596/3257 [00:01<00:07, 364.84it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:01<00:06, 377.34it/s] 21%|‚ñà‚ñà        | 676/3257 [00:01<00:06, 372.81it/s] 22%|‚ñà‚ñà‚ñè       | 717/3257 [00:01<00:06, 381.13it/s] 23%|‚ñà‚ñà‚ñé       | 756/3257 [00:02<00:06, 375.18it/s] 24%|‚ñà‚ñà‚ñç       | 796/3257 [00:02<00:06, 379.71it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:02<00:06, 380.17it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:02<00:06, 374.28it/s] 28%|‚ñà‚ñà‚ñä       | 914/3257 [00:02<00:06, 376.16it/s] 29%|‚ñà‚ñà‚ñâ       | 956/3257 [00:02<00:05, 387.35it/s] 31%|‚ñà‚ñà‚ñà       | 995/3257 [00:02<00:05, 382.68it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1034/3257 [00:02<00:05, 378.64it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1075/3257 [00:02<00:05, 387.22it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1114/3257 [00:03<00:05, 386.27it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1153/3257 [00:03<00:05, 378.29it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:03<00:05, 368.07it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1232/3257 [00:03<00:05, 376.54it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1271/3257 [00:03<00:05, 379.45it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1310/3257 [00:03<00:05, 369.16it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:03<00:05, 377.66it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1388/3257 [00:03<00:06, 276.68it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1433/3257 [00:03<00:05, 316.24it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1477/3257 [00:04<00:05, 346.98it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:04<00:04, 363.59it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1558/3257 [00:04<00:04, 362.45it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1600/3257 [00:04<00:04, 377.30it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1640/3257 [00:04<00:04, 382.98it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1680/3257 [00:04<00:04, 377.42it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1719/3257 [00:04<00:04, 379.91it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1758/3257 [00:04<00:04, 371.34it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1799/3257 [00:04<00:03, 380.12it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1838/3257 [00:05<00:03, 381.59it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1880/3257 [00:05<00:03, 390.76it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1920/3257 [00:05<00:03, 388.33it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1969/3257 [00:05<00:03, 417.46it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2011/3257 [00:05<00:03, 409.52it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2053/3257 [00:05<00:03, 399.65it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2094/3257 [00:05<00:02, 399.02it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2134/3257 [00:05<00:02, 384.17it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2176/3257 [00:05<00:02, 392.99it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2216/3257 [00:05<00:02, 386.63it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2255/3257 [00:06<00:02, 385.40it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2295/3257 [00:06<00:02, 387.73it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2340/3257 [00:06<00:02, 404.33it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2386/3257 [00:06<00:02, 420.41it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2429/3257 [00:06<00:02, 412.55it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2471/3257 [00:06<00:01, 412.74it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2514/3257 [00:06<00:01, 417.39it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2556/3257 [00:06<00:01, 414.20it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2598/3257 [00:06<00:01, 402.74it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2643/3257 [00:06<00:01, 415.93it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2686/3257 [00:07<00:01, 418.34it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2728/3257 [00:07<00:01, 397.54it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:07<00:01, 286.49it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2812/3257 [00:07<00:01, 320.76it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2851/3257 [00:07<00:01, 335.77it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2895/3257 [00:07<00:01, 361.30it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2938/3257 [00:07<00:00, 378.83it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2978/3257 [00:07<00:00, 378.52it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3021/3257 [00:08<00:00, 390.92it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3068/3257 [00:08<00:00, 410.48it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3110/3257 [00:08<00:00, 409.56it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3152/3257 [00:08<00:00, 398.24it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3193/3257 [00:08<00:00, 396.76it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3233/3257 [00:08<00:00, 391.68it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 376.22it/s]
2023-02-07 14:17:15.135 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:17:15,136][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d38,n5,mc3,s0.324064,t4>', 'datetime': '2023-02-07T14:17:15.136294', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:17:15,136][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:17:15,136][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:17:15,315][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 14:17:15,315][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:17:15,321][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 2159 unique words (76.59% of original 2819, drops 660)', 'datetime': '2023-02-07T14:17:15.321932', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:17:15,322][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 2182579 word corpus (99.95% of original 2183622, drops 1043)', 'datetime': '2023-02-07T14:17:15.322116', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:17:15,329][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 14:17:15,329][gensim.models.word2vec][INFO] - sample=0.324064 downsamples 0 most-common words
[2023-02-07 14:17:15,329][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2182579 word corpus (100.0%% of prior 2182579)', 'datetime': '2023-02-07T14:17:15.329284', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:17:15,342][gensim.models.word2vec][INFO] - estimated required memory for 2159 words and 38 dimensions: 2882300 bytes
[2023-02-07 14:17:15,342][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:17:15,343][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2159 vocabulary and 38 features, using sg=1 hs=0 sample=0.32406383720290793 negative=5 window=5 shrink_windows=True', 'datetime': '2023-02-07T14:17:15.343902', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:17:15,875][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2185836 effective words) took 0.5s, 4123879 effective words/s
[2023-02-07 14:17:16,352][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2185836 effective words) took 0.5s, 4595601 effective words/s
[2023-02-07 14:17:16,813][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2185836 effective words) took 0.5s, 4759301 effective words/s
[2023-02-07 14:17:17,290][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2185836 effective words) took 0.5s, 4595670 effective words/s
[2023-02-07 14:17:17,754][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2185836 effective words) took 0.5s, 4719631 effective words/s
[2023-02-07 14:17:18,218][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2185836 effective words) took 0.5s, 4721831 effective words/s
[2023-02-07 14:17:18,690][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2185836 effective words) took 0.5s, 4640052 effective words/s
[2023-02-07 14:17:19,148][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2185836 effective words) took 0.5s, 4781261 effective words/s
[2023-02-07 14:17:19,611][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2185836 effective words) took 0.5s, 4734336 effective words/s
[2023-02-07 14:17:20,065][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2185836 effective words) took 0.5s, 4830092 effective words/s
[2023-02-07 14:17:20,518][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2185836 effective words) took 0.5s, 4837837 effective words/s
[2023-02-07 14:17:20,987][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2185836 effective words) took 0.5s, 4666707 effective words/s
[2023-02-07 14:17:21,469][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2185836 effective words) took 0.5s, 4554147 effective words/s
[2023-02-07 14:17:21,930][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2185836 effective words) took 0.5s, 4762919 effective words/s
[2023-02-07 14:17:22,397][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2185836 effective words) took 0.5s, 4697013 effective words/s
[2023-02-07 14:17:22,851][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2185836 effective words) took 0.5s, 4819593 effective words/s
[2023-02-07 14:17:23,294][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2185836 effective words) took 0.4s, 4951377 effective words/s
[2023-02-07 14:17:23,747][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2185836 effective words) took 0.5s, 4843879 effective words/s
[2023-02-07 14:17:24,198][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2185836 effective words) took 0.4s, 4857992 effective words/s
[2023-02-07 14:17:24,648][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2185836 effective words) took 0.4s, 4867401 effective words/s
[2023-02-07 14:17:25,098][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2185836 effective words) took 0.4s, 4876661 effective words/s
[2023-02-07 14:17:25,546][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2185836 effective words) took 0.4s, 4891068 effective words/s
[2023-02-07 14:17:25,993][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2185836 effective words) took 0.4s, 4900791 effective words/s
[2023-02-07 14:17:26,446][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2185836 effective words) took 0.5s, 4835586 effective words/s
[2023-02-07 14:17:26,897][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2185836 effective words) took 0.4s, 4859167 effective words/s
[2023-02-07 14:17:27,349][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2185836 effective words) took 0.5s, 4848420 effective words/s
[2023-02-07 14:17:27,807][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2185836 effective words) took 0.5s, 4783574 effective words/s
[2023-02-07 14:17:28,282][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2185836 effective words) took 0.5s, 4615371 effective words/s
[2023-02-07 14:17:28,770][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2185836 effective words) took 0.5s, 4490632 effective words/s
[2023-02-07 14:17:29,273][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2185836 effective words) took 0.5s, 4359594 effective words/s
[2023-02-07 14:17:29,757][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2185836 effective words) took 0.5s, 4522910 effective words/s
[2023-02-07 14:17:30,246][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2185836 effective words) took 0.5s, 4480567 effective words/s
[2023-02-07 14:17:30,739][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2185836 effective words) took 0.5s, 4449867 effective words/s
[2023-02-07 14:17:31,226][gensim.models.word2vec][INFO] - EPOCH 33: training on 2183622 raw words (2185836 effective words) took 0.5s, 4495175 effective words/s
[2023-02-07 14:17:31,710][gensim.models.word2vec][INFO] - EPOCH 34: training on 2183622 raw words (2185836 effective words) took 0.5s, 4526606 effective words/s
[2023-02-07 14:17:32,195][gensim.models.word2vec][INFO] - EPOCH 35: training on 2183622 raw words (2185836 effective words) took 0.5s, 4525715 effective words/s
[2023-02-07 14:17:32,679][gensim.models.word2vec][INFO] - EPOCH 36: training on 2183622 raw words (2185836 effective words) took 0.5s, 4520385 effective words/s
[2023-02-07 14:17:33,165][gensim.models.word2vec][INFO] - EPOCH 37: training on 2183622 raw words (2185836 effective words) took 0.5s, 4514393 effective words/s
[2023-02-07 14:17:33,655][gensim.models.word2vec][INFO] - EPOCH 38: training on 2183622 raw words (2185836 effective words) took 0.5s, 4478455 effective words/s
[2023-02-07 14:17:34,141][gensim.models.word2vec][INFO] - EPOCH 39: training on 2183622 raw words (2185836 effective words) took 0.5s, 4508760 effective words/s
[2023-02-07 14:17:34,628][gensim.models.word2vec][INFO] - EPOCH 40: training on 2183622 raw words (2185836 effective words) took 0.5s, 4498905 effective words/s
[2023-02-07 14:17:35,122][gensim.models.word2vec][INFO] - EPOCH 41: training on 2183622 raw words (2185836 effective words) took 0.5s, 4436986 effective words/s
[2023-02-07 14:17:35,619][gensim.models.word2vec][INFO] - EPOCH 42: training on 2183622 raw words (2185836 effective words) took 0.5s, 4406896 effective words/s
[2023-02-07 14:17:36,113][gensim.models.word2vec][INFO] - EPOCH 43: training on 2183622 raw words (2185836 effective words) took 0.5s, 4438241 effective words/s
[2023-02-07 14:17:36,609][gensim.models.word2vec][INFO] - EPOCH 44: training on 2183622 raw words (2185836 effective words) took 0.5s, 4415040 effective words/s
[2023-02-07 14:17:37,101][gensim.models.word2vec][INFO] - EPOCH 45: training on 2183622 raw words (2185836 effective words) took 0.5s, 4448174 effective words/s
[2023-02-07 14:17:37,593][gensim.models.word2vec][INFO] - EPOCH 46: training on 2183622 raw words (2185836 effective words) took 0.5s, 4463981 effective words/s
[2023-02-07 14:17:38,088][gensim.models.word2vec][INFO] - EPOCH 47: training on 2183622 raw words (2185836 effective words) took 0.5s, 4419219 effective words/s
[2023-02-07 14:17:38,579][gensim.models.word2vec][INFO] - EPOCH 48: training on 2183622 raw words (2185836 effective words) took 0.5s, 4464681 effective words/s
[2023-02-07 14:17:39,060][gensim.models.word2vec][INFO] - EPOCH 49: training on 2183622 raw words (2185836 effective words) took 0.5s, 4555707 effective words/s
[2023-02-07 14:17:39,550][gensim.models.word2vec][INFO] - EPOCH 50: training on 2183622 raw words (2185836 effective words) took 0.5s, 4474003 effective words/s
[2023-02-07 14:17:40,054][gensim.models.word2vec][INFO] - EPOCH 51: training on 2183622 raw words (2185836 effective words) took 0.5s, 4350463 effective words/s
[2023-02-07 14:17:40,054][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 113548344 raw words (113663472 effective words) took 24.7s, 4599840 effective words/s', 'datetime': '2023-02-07T14:17:40.054401', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:17:40.054 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:17:41,773][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141659-fe2iej0s/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:17:41.773397', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:17:41,774][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:17:41,778][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141659-fe2iej0s/files/../tmp/embedding_model.pt
2023-02-07 14:17:41.778 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:17:42.603 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:17:42.946 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:17:43.267 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9140929312123987, 'test_mae': 1.0432216424746381, 'test_r2': 0.09451960012667904}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.84
wandb: percentage 0.23413
wandb:   test_mae 1.04322
wandb:   test_mse 1.91409
wandb:    test_r2 0.09452
wandb: 
wandb: üöÄ View run effortless-sweep-76 at: https://wandb.ai/xiaoqiz/mof2vec/runs/fe2iej0s
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_141659-fe2iej0s/logs
wandb: Agent Starting Run: zqltc6bo with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 554
wandb: 	model.gensim.alpha: 0.0057116371980608874
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 91
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.9279451627498764
wandb: 	model.gensim.vector_size: 184
wandb: 	model.gensim.window: 1
wandb: 	model.sklearn.learning_rate: 0.5791907228143128
wandb: 	model.sklearn.max_depth: 76
wandb: 	model.sklearn.min_child_weight: 0.0446204175452857
wandb: 	model.sklearn.n_estimators: 4415
wandb: 	model.sklearn.num_leaves: 346
wandb: 	model.sklearn.reg_alpha: 0.0922260508645239
wandb: 	model.sklearn.reg_lambda: 0.4982847456133744
wandb: 	model.sklearn.subsample: 0.685802414022311
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141751-zqltc6bo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-sweep-77
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/zqltc6bo
2023-02-07 14:17:59.709 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 14:17:59.709 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 554 for sweep.
2023-02-07 14:17:59.710 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0057116371980608874 for sweep.
2023-02-07 14:17:59.710 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:17:59.710 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 91 for sweep.
2023-02-07 14:17:59.710 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 14:17:59.710 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.9279451627498764 for sweep.
2023-02-07 14:17:59.711 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 184 for sweep.
2023-02-07 14:17:59.711 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 1 for sweep.
2023-02-07 14:17:59.711 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.5791907228143128 for sweep.
2023-02-07 14:17:59.711 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 76 for sweep.
2023-02-07 14:17:59.711 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0446204175452857 for sweep.
2023-02-07 14:17:59.712 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4415 for sweep.
2023-02-07 14:17:59.712 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 346 for sweep.
2023-02-07 14:17:59.712 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.0922260508645239 for sweep.
2023-02-07 14:17:59.712 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.4982847456133744 for sweep.
2023-02-07 14:17:59.713 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.685802414022311 for sweep.
2023-02-07 14:17:59.713 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:17:59.718 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141751-zqltc6bo/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 554, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 184, 'window': 1, 'min_count': 2, 'dm': 0, 'sample': 0.9279451627498764, 'workers': 4, 'alpha': 0.0057116371980608874, 'epochs': 91}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4415, 'max_depth': 76, 'num_leaves': 346, 'reg_alpha': 0.0922260508645239, 'reg_lambda': 0.4982847456133744, 'subsample': 0.685802414022311, 'min_child_weight': 0.0446204175452857, 'n_jobs': 4, 'learning_rate': 0.5791907228143128}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 31/3257 [00:00<00:10, 309.89it/s]  2%|‚ñè         | 62/3257 [00:00<00:10, 307.15it/s]  3%|‚ñé         | 94/3257 [00:00<00:10, 311.90it/s]  4%|‚ñç         | 126/3257 [00:00<00:10, 303.46it/s]  5%|‚ñç         | 161/3257 [00:00<00:09, 318.65it/s]  6%|‚ñå         | 193/3257 [00:00<00:09, 317.98it/s]  7%|‚ñã         | 230/3257 [00:00<00:09, 333.93it/s]  8%|‚ñä         | 264/3257 [00:00<00:09, 329.85it/s]  9%|‚ñâ         | 301/3257 [00:00<00:08, 340.34it/s] 10%|‚ñà         | 336/3257 [00:01<00:08, 341.79it/s] 11%|‚ñà‚ñè        | 371/3257 [00:01<00:08, 342.14it/s] 12%|‚ñà‚ñè        | 406/3257 [00:01<00:08, 327.19it/s] 13%|‚ñà‚ñé        | 439/3257 [00:01<00:09, 301.05it/s] 15%|‚ñà‚ñç        | 475/3257 [00:01<00:08, 314.34it/s] 16%|‚ñà‚ñå        | 512/3257 [00:01<00:08, 327.21it/s] 17%|‚ñà‚ñã        | 546/3257 [00:01<00:08, 323.68it/s] 18%|‚ñà‚ñä        | 579/3257 [00:01<00:08, 308.55it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:01<00:08, 322.11it/s] 20%|‚ñà‚ñâ        | 648/3257 [00:02<00:08, 316.66it/s] 21%|‚ñà‚ñà        | 680/3257 [00:02<00:08, 309.15it/s] 22%|‚ñà‚ñà‚ñè       | 713/3257 [00:02<00:08, 313.90it/s] 23%|‚ñà‚ñà‚ñé       | 745/3257 [00:02<00:08, 302.71it/s] 24%|‚ñà‚ñà‚ñç       | 776/3257 [00:02<00:11, 218.49it/s] 25%|‚ñà‚ñà‚ñç       | 809/3257 [00:02<00:10, 243.07it/s] 26%|‚ñà‚ñà‚ñå       | 840/3257 [00:02<00:09, 256.38it/s] 27%|‚ñà‚ñà‚ñã       | 870/3257 [00:02<00:08, 266.46it/s] 28%|‚ñà‚ñà‚ñä       | 904/3257 [00:02<00:08, 285.01it/s] 29%|‚ñà‚ñà‚ñä       | 935/3257 [00:03<00:08, 288.47it/s] 30%|‚ñà‚ñà‚ñâ       | 970/3257 [00:03<00:07, 303.25it/s] 31%|‚ñà‚ñà‚ñà       | 1002/3257 [00:03<00:07, 303.56it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1033/3257 [00:03<00:07, 296.80it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1064/3257 [00:03<00:07, 300.10it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1095/3257 [00:03<00:07, 296.90it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1127/3257 [00:03<00:07, 299.54it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1160/3257 [00:03<00:06, 306.66it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:03<00:07, 294.01it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1221/3257 [00:04<00:06, 294.81it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1255/3257 [00:04<00:06, 307.13it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1286/3257 [00:04<00:06, 299.79it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1319/3257 [00:04<00:06, 308.20it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1351/3257 [00:04<00:06, 309.67it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1383/3257 [00:04<00:06, 308.37it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1422/3257 [00:04<00:05, 328.58it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1459/3257 [00:04<00:05, 338.02it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1494/3257 [00:04<00:05, 338.40it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1528/3257 [00:04<00:05, 327.28it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1561/3257 [00:05<00:05, 318.54it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1593/3257 [00:05<00:05, 317.94it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1629/3257 [00:05<00:04, 329.68it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1663/3257 [00:05<00:05, 310.49it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1695/3257 [00:05<00:05, 310.66it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:05<00:04, 308.01it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1760/3257 [00:05<00:04, 311.00it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1795/3257 [00:05<00:04, 320.50it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1828/3257 [00:05<00:04, 310.67it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1864/3257 [00:06<00:04, 324.61it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1897/3257 [00:06<00:04, 321.71it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1930/3257 [00:06<00:04, 322.90it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1973/3257 [00:06<00:03, 351.51it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2009/3257 [00:06<00:03, 338.01it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2043/3257 [00:06<00:03, 336.35it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2077/3257 [00:06<00:03, 327.78it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2110/3257 [00:06<00:03, 323.28it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2143/3257 [00:07<00:05, 211.34it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2178/3257 [00:07<00:04, 240.28it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2210/3257 [00:07<00:04, 255.90it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2243/3257 [00:07<00:03, 270.11it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2273/3257 [00:07<00:03, 276.83it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2308/3257 [00:07<00:03, 296.20it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2348/3257 [00:07<00:02, 323.87it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2384/3257 [00:07<00:02, 333.61it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2419/3257 [00:07<00:02, 328.06it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2453/3257 [00:08<00:02, 317.72it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2490/3257 [00:08<00:02, 331.72it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2529/3257 [00:08<00:02, 346.29it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2565/3257 [00:08<00:02, 333.44it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2599/3257 [00:08<00:02, 325.15it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:08<00:01, 345.22it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2675/3257 [00:08<00:01, 334.58it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2709/3257 [00:08<00:01, 316.57it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2745/3257 [00:08<00:01, 328.32it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2779/3257 [00:09<00:01, 321.80it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2814/3257 [00:09<00:01, 329.49it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2848/3257 [00:09<00:01, 321.10it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2887/3257 [00:09<00:01, 339.65it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2922/3257 [00:09<00:01, 329.35it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2956/3257 [00:09<00:00, 314.41it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2988/3257 [00:09<00:00, 313.48it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3022/3257 [00:09<00:00, 320.63it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3061/3257 [00:09<00:00, 340.31it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3096/3257 [00:09<00:00, 339.07it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3132/3257 [00:10<00:00, 342.80it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3167/3257 [00:10<00:00, 328.88it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3201/3257 [00:10<00:00, 325.74it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3234/3257 [00:10<00:00, 324.57it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:10<00:00, 311.50it/s]
2023-02-07 14:18:10.543 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:18:10,545][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d184,n5,mc2,s0.927945,t4>', 'datetime': '2023-02-07T14:18:10.545911', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:18:10,546][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:18:10,546][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:18:10,792][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 14:18:10,793][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:18:10,807][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 5771 unique words (86.63% of original 6662, drops 891)', 'datetime': '2023-02-07T14:18:10.807041', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:18:10,807][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 2910605 word corpus (99.97% of original 2911496, drops 891)', 'datetime': '2023-02-07T14:18:10.807277', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:18:10,825][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 14:18:10,825][gensim.models.word2vec][INFO] - sample=0.927945 downsamples 0 most-common words
[2023-02-07 14:18:10,825][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2910605 word corpus (100.0%% of prior 2910605)', 'datetime': '2023-02-07T14:18:10.825940', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:18:10,856][gensim.models.word2vec][INFO] - estimated required memory for 5771 words and 184 dimensions: 14428964 bytes
[2023-02-07 14:18:10,857][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:18:10,863][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 5771 vocabulary and 184 features, using sg=1 hs=0 sample=0.9279451627498764 negative=5 window=1 shrink_windows=True', 'datetime': '2023-02-07T14:18:10.863266', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:18:11,867][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 93.43% examples, 2731898 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:18:11,924][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2913862 effective words) took 1.1s, 2749038 effective words/s
[2023-02-07 14:18:12,838][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2913862 effective words) took 0.9s, 3195195 effective words/s
[2023-02-07 14:18:13,747][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2913862 effective words) took 0.9s, 3208152 effective words/s
[2023-02-07 14:18:14,653][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2913862 effective words) took 0.9s, 3224461 effective words/s
[2023-02-07 14:18:15,549][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2913862 effective words) took 0.9s, 3255023 effective words/s
[2023-02-07 14:18:16,448][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2913862 effective words) took 0.9s, 3245426 effective words/s
[2023-02-07 14:18:17,334][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2913862 effective words) took 0.9s, 3293454 effective words/s
[2023-02-07 14:18:18,213][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2913862 effective words) took 0.9s, 3321225 effective words/s
[2023-02-07 14:18:19,100][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2913862 effective words) took 0.9s, 3288205 effective words/s
[2023-02-07 14:18:19,992][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2913862 effective words) took 0.9s, 3272049 effective words/s
[2023-02-07 14:18:20,857][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2913862 effective words) took 0.9s, 3374838 effective words/s
[2023-02-07 14:18:21,712][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2913862 effective words) took 0.9s, 3415092 effective words/s
[2023-02-07 14:18:22,572][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2913862 effective words) took 0.9s, 3396447 effective words/s
[2023-02-07 14:18:23,419][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2913862 effective words) took 0.8s, 3443490 effective words/s
[2023-02-07 14:18:24,264][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2913862 effective words) took 0.8s, 3451585 effective words/s
[2023-02-07 14:18:25,113][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2913862 effective words) took 0.8s, 3439407 effective words/s
[2023-02-07 14:18:25,963][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2913862 effective words) took 0.8s, 3430490 effective words/s
[2023-02-07 14:18:26,814][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2913862 effective words) took 0.8s, 3432666 effective words/s
[2023-02-07 14:18:27,663][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2913862 effective words) took 0.8s, 3436126 effective words/s
[2023-02-07 14:18:28,516][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2913862 effective words) took 0.9s, 3419381 effective words/s
[2023-02-07 14:18:29,377][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2913862 effective words) took 0.9s, 3388871 effective words/s
[2023-02-07 14:18:30,238][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2913862 effective words) took 0.9s, 3387852 effective words/s
[2023-02-07 14:18:31,098][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2913862 effective words) took 0.9s, 3394875 effective words/s
[2023-02-07 14:18:31,948][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2913862 effective words) took 0.8s, 3431107 effective words/s
[2023-02-07 14:18:32,788][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2913862 effective words) took 0.8s, 3475175 effective words/s
[2023-02-07 14:18:33,614][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2913862 effective words) took 0.8s, 3532813 effective words/s
[2023-02-07 14:18:34,451][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2913862 effective words) took 0.8s, 3487179 effective words/s
[2023-02-07 14:18:35,293][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2913862 effective words) took 0.8s, 3467853 effective words/s
[2023-02-07 14:18:36,137][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2913862 effective words) took 0.8s, 3455619 effective words/s
[2023-02-07 14:18:36,979][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2913862 effective words) took 0.8s, 3469633 effective words/s
[2023-02-07 14:18:37,817][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2913862 effective words) took 0.8s, 3477990 effective words/s
[2023-02-07 14:18:38,693][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2913862 effective words) took 0.9s, 3331231 effective words/s
[2023-02-07 14:18:39,590][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2913862 effective words) took 0.9s, 3259152 effective words/s
[2023-02-07 14:18:40,488][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2913862 effective words) took 0.9s, 3246343 effective words/s
[2023-02-07 14:18:41,357][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2913862 effective words) took 0.9s, 3359523 effective words/s
[2023-02-07 14:18:42,234][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2913862 effective words) took 0.9s, 3327331 effective words/s
[2023-02-07 14:18:43,102][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2913862 effective words) took 0.9s, 3361358 effective words/s
[2023-02-07 14:18:43,962][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2913862 effective words) took 0.9s, 3392345 effective words/s
[2023-02-07 14:18:44,817][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2913862 effective words) took 0.9s, 3414800 effective words/s
[2023-02-07 14:18:45,675][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2913862 effective words) took 0.9s, 3403187 effective words/s
[2023-02-07 14:18:46,531][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2913862 effective words) took 0.9s, 3407476 effective words/s
[2023-02-07 14:18:47,399][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2913862 effective words) took 0.9s, 3364505 effective words/s
[2023-02-07 14:18:48,263][gensim.models.word2vec][INFO] - EPOCH 42: training on 2911496 raw words (2913862 effective words) took 0.9s, 3379125 effective words/s
[2023-02-07 14:18:49,130][gensim.models.word2vec][INFO] - EPOCH 43: training on 2911496 raw words (2913862 effective words) took 0.9s, 3362402 effective words/s
[2023-02-07 14:18:49,994][gensim.models.word2vec][INFO] - EPOCH 44: training on 2911496 raw words (2913862 effective words) took 0.9s, 3378194 effective words/s
[2023-02-07 14:18:50,864][gensim.models.word2vec][INFO] - EPOCH 45: training on 2911496 raw words (2913862 effective words) took 0.9s, 3355251 effective words/s
[2023-02-07 14:18:51,726][gensim.models.word2vec][INFO] - EPOCH 46: training on 2911496 raw words (2913862 effective words) took 0.9s, 3384240 effective words/s
[2023-02-07 14:18:52,590][gensim.models.word2vec][INFO] - EPOCH 47: training on 2911496 raw words (2913862 effective words) took 0.9s, 3377857 effective words/s
[2023-02-07 14:18:53,458][gensim.models.word2vec][INFO] - EPOCH 48: training on 2911496 raw words (2913862 effective words) took 0.9s, 3364630 effective words/s
[2023-02-07 14:18:54,314][gensim.models.word2vec][INFO] - EPOCH 49: training on 2911496 raw words (2913862 effective words) took 0.9s, 3407901 effective words/s
[2023-02-07 14:18:55,168][gensim.models.word2vec][INFO] - EPOCH 50: training on 2911496 raw words (2913862 effective words) took 0.9s, 3417112 effective words/s
[2023-02-07 14:18:56,016][gensim.models.word2vec][INFO] - EPOCH 51: training on 2911496 raw words (2913862 effective words) took 0.8s, 3439561 effective words/s
[2023-02-07 14:18:56,871][gensim.models.word2vec][INFO] - EPOCH 52: training on 2911496 raw words (2913862 effective words) took 0.9s, 3415944 effective words/s
[2023-02-07 14:18:57,728][gensim.models.word2vec][INFO] - EPOCH 53: training on 2911496 raw words (2913862 effective words) took 0.9s, 3401665 effective words/s
[2023-02-07 14:18:58,588][gensim.models.word2vec][INFO] - EPOCH 54: training on 2911496 raw words (2913862 effective words) took 0.9s, 3395524 effective words/s
[2023-02-07 14:18:59,462][gensim.models.word2vec][INFO] - EPOCH 55: training on 2911496 raw words (2913862 effective words) took 0.9s, 3339716 effective words/s
[2023-02-07 14:19:00,328][gensim.models.word2vec][INFO] - EPOCH 56: training on 2911496 raw words (2913862 effective words) took 0.9s, 3369393 effective words/s
[2023-02-07 14:19:01,199][gensim.models.word2vec][INFO] - EPOCH 57: training on 2911496 raw words (2913862 effective words) took 0.9s, 3352816 effective words/s
[2023-02-07 14:19:02,047][gensim.models.word2vec][INFO] - EPOCH 58: training on 2911496 raw words (2913862 effective words) took 0.8s, 3444361 effective words/s
[2023-02-07 14:19:02,892][gensim.models.word2vec][INFO] - EPOCH 59: training on 2911496 raw words (2913862 effective words) took 0.8s, 3452089 effective words/s
[2023-02-07 14:19:03,728][gensim.models.word2vec][INFO] - EPOCH 60: training on 2911496 raw words (2913862 effective words) took 0.8s, 3491822 effective words/s
[2023-02-07 14:19:04,569][gensim.models.word2vec][INFO] - EPOCH 61: training on 2911496 raw words (2913862 effective words) took 0.8s, 3468817 effective words/s
[2023-02-07 14:19:05,411][gensim.models.word2vec][INFO] - EPOCH 62: training on 2911496 raw words (2913862 effective words) took 0.8s, 3467356 effective words/s
[2023-02-07 14:19:06,261][gensim.models.word2vec][INFO] - EPOCH 63: training on 2911496 raw words (2913862 effective words) took 0.8s, 3431785 effective words/s
[2023-02-07 14:19:07,106][gensim.models.word2vec][INFO] - EPOCH 64: training on 2911496 raw words (2913862 effective words) took 0.8s, 3454935 effective words/s
[2023-02-07 14:19:07,955][gensim.models.word2vec][INFO] - EPOCH 65: training on 2911496 raw words (2913862 effective words) took 0.8s, 3438259 effective words/s
[2023-02-07 14:19:08,797][gensim.models.word2vec][INFO] - EPOCH 66: training on 2911496 raw words (2913862 effective words) took 0.8s, 3464372 effective words/s
[2023-02-07 14:19:09,645][gensim.models.word2vec][INFO] - EPOCH 67: training on 2911496 raw words (2913862 effective words) took 0.8s, 3441842 effective words/s
[2023-02-07 14:19:10,495][gensim.models.word2vec][INFO] - EPOCH 68: training on 2911496 raw words (2913862 effective words) took 0.8s, 3435100 effective words/s
[2023-02-07 14:19:11,339][gensim.models.word2vec][INFO] - EPOCH 69: training on 2911496 raw words (2913862 effective words) took 0.8s, 3457597 effective words/s
[2023-02-07 14:19:12,194][gensim.models.word2vec][INFO] - EPOCH 70: training on 2911496 raw words (2913862 effective words) took 0.9s, 3411581 effective words/s
[2023-02-07 14:19:13,053][gensim.models.word2vec][INFO] - EPOCH 71: training on 2911496 raw words (2913862 effective words) took 0.9s, 3397298 effective words/s
[2023-02-07 14:19:13,917][gensim.models.word2vec][INFO] - EPOCH 72: training on 2911496 raw words (2913862 effective words) took 0.9s, 3379268 effective words/s
[2023-02-07 14:19:14,765][gensim.models.word2vec][INFO] - EPOCH 73: training on 2911496 raw words (2913862 effective words) took 0.8s, 3442879 effective words/s
[2023-02-07 14:19:15,619][gensim.models.word2vec][INFO] - EPOCH 74: training on 2911496 raw words (2913862 effective words) took 0.9s, 3417256 effective words/s
[2023-02-07 14:19:16,465][gensim.models.word2vec][INFO] - EPOCH 75: training on 2911496 raw words (2913862 effective words) took 0.8s, 3450188 effective words/s
[2023-02-07 14:19:17,321][gensim.models.word2vec][INFO] - EPOCH 76: training on 2911496 raw words (2913862 effective words) took 0.9s, 3410795 effective words/s
[2023-02-07 14:19:18,175][gensim.models.word2vec][INFO] - EPOCH 77: training on 2911496 raw words (2913862 effective words) took 0.9s, 3417002 effective words/s
[2023-02-07 14:19:19,027][gensim.models.word2vec][INFO] - EPOCH 78: training on 2911496 raw words (2913862 effective words) took 0.9s, 3426957 effective words/s
[2023-02-07 14:19:19,863][gensim.models.word2vec][INFO] - EPOCH 79: training on 2911496 raw words (2913862 effective words) took 0.8s, 3490281 effective words/s
[2023-02-07 14:19:20,711][gensim.models.word2vec][INFO] - EPOCH 80: training on 2911496 raw words (2913862 effective words) took 0.8s, 3440229 effective words/s
[2023-02-07 14:19:21,566][gensim.models.word2vec][INFO] - EPOCH 81: training on 2911496 raw words (2913862 effective words) took 0.9s, 3410532 effective words/s
[2023-02-07 14:19:22,418][gensim.models.word2vec][INFO] - EPOCH 82: training on 2911496 raw words (2913862 effective words) took 0.8s, 3428214 effective words/s
[2023-02-07 14:19:23,272][gensim.models.word2vec][INFO] - EPOCH 83: training on 2911496 raw words (2913862 effective words) took 0.9s, 3418052 effective words/s
[2023-02-07 14:19:24,126][gensim.models.word2vec][INFO] - EPOCH 84: training on 2911496 raw words (2913862 effective words) took 0.9s, 3419480 effective words/s
[2023-02-07 14:19:24,981][gensim.models.word2vec][INFO] - EPOCH 85: training on 2911496 raw words (2913862 effective words) took 0.9s, 3414834 effective words/s
[2023-02-07 14:19:25,846][gensim.models.word2vec][INFO] - EPOCH 86: training on 2911496 raw words (2913862 effective words) took 0.9s, 3369826 effective words/s
[2023-02-07 14:19:26,716][gensim.models.word2vec][INFO] - EPOCH 87: training on 2911496 raw words (2913862 effective words) took 0.9s, 3354121 effective words/s
[2023-02-07 14:19:27,557][gensim.models.word2vec][INFO] - EPOCH 88: training on 2911496 raw words (2913862 effective words) took 0.8s, 3476444 effective words/s
[2023-02-07 14:19:28,397][gensim.models.word2vec][INFO] - EPOCH 89: training on 2911496 raw words (2913862 effective words) took 0.8s, 3473336 effective words/s
[2023-02-07 14:19:29,235][gensim.models.word2vec][INFO] - EPOCH 90: training on 2911496 raw words (2913862 effective words) took 0.8s, 3485788 effective words/s
[2023-02-07 14:19:29,235][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 264946136 raw words (265161442 effective words) took 78.4s, 3383375 effective words/s', 'datetime': '2023-02-07T14:19:29.235438', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:19:29.235 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:19:35,425][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141751-zqltc6bo/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:19:35.424896', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:19:35,425][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:19:35,453][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141751-zqltc6bo/files/../tmp/embedding_model.pt
2023-02-07 14:19:35.453 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:19:36.806 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:19:37.315 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:19:38.494 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.047556656090924, 'test_mae': 1.0968392204401867, 'test_r2': 0.031383278477424725}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.97
wandb: percentage 0.13374
wandb:   test_mae 1.09684
wandb:   test_mse 2.04756
wandb:    test_r2 0.03138
wandb: 
wandb: üöÄ View run stoic-sweep-77 at: https://wandb.ai/xiaoqiz/mof2vec/runs/zqltc6bo
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_141751-zqltc6bo/logs
wandb: Agent Starting Run: pcyxi19d with config:
wandb: 	data.data.wl_step: 5
wandb: 	data.nn.batch_size: 651
wandb: 	model.gensim.alpha: 0.00034432206952439843
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 91
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.7418589555613433
wandb: 	model.gensim.vector_size: 125
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.032924522758146064
wandb: 	model.sklearn.max_depth: 8
wandb: 	model.sklearn.min_child_weight: 0.058602094795480966
wandb: 	model.sklearn.n_estimators: 1528
wandb: 	model.sklearn.num_leaves: 446
wandb: 	model.sklearn.reg_alpha: 0.030671722625196136
wandb: 	model.sklearn.reg_lambda: 0.20883144473455073
wandb: 	model.sklearn.subsample: 0.5181297592539289
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141954-pcyxi19d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-sweep-78
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/pcyxi19d
2023-02-07 14:20:02.942 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 5 for sweep.
2023-02-07 14:20:02.943 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 651 for sweep.
2023-02-07 14:20:02.943 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.00034432206952439843 for sweep.
2023-02-07 14:20:02.943 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:20:02.944 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 91 for sweep.
2023-02-07 14:20:02.944 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 14:20:02.944 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7418589555613433 for sweep.
2023-02-07 14:20:02.944 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 125 for sweep.
2023-02-07 14:20:02.944 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 14:20:02.945 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.032924522758146064 for sweep.
2023-02-07 14:20:02.945 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 8 for sweep.
2023-02-07 14:20:02.945 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.058602094795480966 for sweep.
2023-02-07 14:20:02.945 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1528 for sweep.
2023-02-07 14:20:02.946 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 446 for sweep.
2023-02-07 14:20:02.946 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.030671722625196136 for sweep.
2023-02-07 14:20:02.946 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.20883144473455073 for sweep.
2023-02-07 14:20:02.946 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5181297592539289 for sweep.
2023-02-07 14:20:02.947 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:20:02.951 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 5}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141954-pcyxi19d/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 651, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 125, 'window': 3, 'min_count': 3, 'dm': 0, 'sample': 0.7418589555613433, 'workers': 4, 'alpha': 0.00034432206952439843, 'epochs': 91}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1528, 'max_depth': 8, 'num_leaves': 446, 'reg_alpha': 0.030671722625196136, 'reg_lambda': 0.20883144473455073, 'subsample': 0.5181297592539289, 'min_child_weight': 0.058602094795480966, 'n_jobs': 4, 'learning_rate': 0.032924522758146064}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 20/3257 [00:00<00:16, 194.38it/s]  1%|‚ñè         | 41/3257 [00:00<00:16, 198.37it/s]  2%|‚ñè         | 61/3257 [00:00<00:16, 197.40it/s]  3%|‚ñé         | 84/3257 [00:00<00:15, 209.93it/s]  3%|‚ñé         | 106/3257 [00:00<00:15, 197.87it/s]  4%|‚ñç         | 127/3257 [00:00<00:15, 201.28it/s]  5%|‚ñç         | 151/3257 [00:00<00:14, 212.32it/s]  5%|‚ñå         | 173/3257 [00:00<00:15, 203.11it/s]  6%|‚ñå         | 197/3257 [00:00<00:14, 208.66it/s]  7%|‚ñã         | 221/3257 [00:01<00:14, 216.11it/s]  8%|‚ñä         | 246/3257 [00:01<00:13, 224.74it/s]  8%|‚ñä         | 269/3257 [00:01<00:13, 215.37it/s]  9%|‚ñâ         | 297/3257 [00:01<00:12, 232.14it/s] 10%|‚ñâ         | 321/3257 [00:01<00:12, 228.44it/s] 11%|‚ñà         | 344/3257 [00:01<00:13, 221.06it/s] 11%|‚ñà‚ñè        | 367/3257 [00:01<00:12, 223.31it/s] 12%|‚ñà‚ñè        | 390/3257 [00:01<00:13, 210.56it/s] 13%|‚ñà‚ñé        | 414/3257 [00:01<00:13, 216.61it/s] 13%|‚ñà‚ñé        | 436/3257 [00:02<00:20, 137.11it/s] 14%|‚ñà‚ñç        | 459/3257 [00:02<00:18, 154.90it/s] 15%|‚ñà‚ñç        | 480/3257 [00:02<00:16, 165.28it/s] 16%|‚ñà‚ñå        | 505/3257 [00:02<00:14, 185.50it/s] 16%|‚ñà‚ñå        | 527/3257 [00:02<00:14, 191.92it/s] 17%|‚ñà‚ñã        | 550/3257 [00:02<00:13, 200.20it/s] 18%|‚ñà‚ñä        | 572/3257 [00:02<00:14, 185.42it/s] 18%|‚ñà‚ñä        | 599/3257 [00:03<00:13, 202.44it/s] 19%|‚ñà‚ñâ        | 621/3257 [00:03<00:12, 203.17it/s] 20%|‚ñà‚ñâ        | 644/3257 [00:03<00:12, 209.45it/s] 20%|‚ñà‚ñà        | 666/3257 [00:03<00:12, 199.41it/s] 21%|‚ñà‚ñà        | 687/3257 [00:03<00:12, 198.89it/s] 22%|‚ñà‚ñà‚ñè       | 711/3257 [00:03<00:12, 209.25it/s] 23%|‚ñà‚ñà‚ñé       | 733/3257 [00:03<00:12, 203.11it/s] 23%|‚ñà‚ñà‚ñé       | 754/3257 [00:03<00:12, 200.09it/s] 24%|‚ñà‚ñà‚ñç       | 776/3257 [00:03<00:12, 200.80it/s] 25%|‚ñà‚ñà‚ñç       | 799/3257 [00:03<00:11, 206.18it/s] 25%|‚ñà‚ñà‚ñå       | 820/3257 [00:04<00:11, 205.52it/s] 26%|‚ñà‚ñà‚ñå       | 841/3257 [00:04<00:12, 190.28it/s] 26%|‚ñà‚ñà‚ñã       | 863/3257 [00:04<00:12, 197.99it/s] 27%|‚ñà‚ñà‚ñã       | 884/3257 [00:04<00:12, 189.84it/s] 28%|‚ñà‚ñà‚ñä       | 909/3257 [00:04<00:11, 205.60it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:04<00:11, 205.24it/s] 29%|‚ñà‚ñà‚ñâ       | 954/3257 [00:04<00:10, 211.51it/s] 30%|‚ñà‚ñà‚ñâ       | 976/3257 [00:04<00:10, 213.04it/s] 31%|‚ñà‚ñà‚ñà       | 998/3257 [00:04<00:11, 201.92it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1020/3257 [00:05<00:10, 205.09it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1041/3257 [00:05<00:11, 190.10it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1062/3257 [00:05<00:11, 195.44it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1083/3257 [00:05<00:11, 197.47it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1105/3257 [00:05<00:10, 202.69it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1126/3257 [00:05<00:10, 200.40it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1147/3257 [00:05<00:11, 191.20it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:05<00:10, 201.58it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:05<00:11, 182.65it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1210/3257 [00:06<00:11, 181.25it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1235/3257 [00:06<00:10, 199.12it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1256/3257 [00:06<00:10, 198.50it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1277/3257 [00:06<00:10, 193.29it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1297/3257 [00:06<00:10, 186.74it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1321/3257 [00:06<00:09, 196.86it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1344/3257 [00:06<00:09, 203.21it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1365/3257 [00:06<00:09, 195.42it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1385/3257 [00:06<00:09, 192.15it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1409/3257 [00:07<00:09, 205.17it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:07<00:08, 211.33it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1458/3257 [00:07<00:08, 223.63it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1481/3257 [00:07<00:08, 221.56it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1507/3257 [00:07<00:07, 232.20it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1531/3257 [00:07<00:08, 213.27it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1553/3257 [00:07<00:08, 202.68it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1575/3257 [00:07<00:08, 202.85it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1599/3257 [00:07<00:07, 210.87it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1621/3257 [00:08<00:07, 211.06it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1643/3257 [00:08<00:07, 205.63it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1664/3257 [00:08<00:08, 197.72it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1684/3257 [00:08<00:08, 195.35it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1707/3257 [00:08<00:07, 201.72it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:08<00:07, 195.02it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1748/3257 [00:08<00:07, 194.51it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1770/3257 [00:08<00:07, 200.11it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1793/3257 [00:08<00:07, 205.10it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1814/3257 [00:09<00:07, 196.77it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1834/3257 [00:09<00:11, 125.42it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1857/3257 [00:09<00:09, 145.95it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1880/3257 [00:09<00:08, 163.34it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1901/3257 [00:09<00:07, 174.18it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1921/3257 [00:09<00:07, 180.18it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1948/3257 [00:09<00:06, 203.19it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1974/3257 [00:09<00:05, 214.15it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1997/3257 [00:10<00:05, 211.01it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2019/3257 [00:10<00:05, 213.22it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2041/3257 [00:10<00:05, 205.94it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2062/3257 [00:10<00:06, 192.18it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2084/3257 [00:10<00:05, 199.24it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2105/3257 [00:10<00:05, 199.75it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2126/3257 [00:10<00:05, 191.62it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2146/3257 [00:10<00:05, 186.94it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:10<00:05, 198.60it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2190/3257 [00:11<00:05, 200.21it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2211/3257 [00:11<00:05, 194.03it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2235/3257 [00:11<00:04, 206.75it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2256/3257 [00:11<00:05, 198.13it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2276/3257 [00:11<00:05, 186.84it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2300/3257 [00:11<00:04, 199.93it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2326/3257 [00:11<00:04, 216.25it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2354/3257 [00:11<00:03, 232.53it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2378/3257 [00:11<00:03, 224.72it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2404/3257 [00:12<00:03, 231.15it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2428/3257 [00:12<00:03, 215.26it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2450/3257 [00:12<00:04, 198.78it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2476/3257 [00:12<00:03, 213.56it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2502/3257 [00:12<00:03, 224.39it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2526/3257 [00:12<00:03, 224.59it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2549/3257 [00:12<00:03, 225.88it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2572/3257 [00:12<00:03, 204.42it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2593/3257 [00:12<00:03, 201.47it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2623/3257 [00:13<00:02, 226.21it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2647/3257 [00:13<00:02, 219.11it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:13<00:02, 218.82it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2693/3257 [00:13<00:02, 219.96it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2716/3257 [00:13<00:02, 190.55it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2743/3257 [00:13<00:02, 210.84it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2765/3257 [00:13<00:02, 210.79it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2787/3257 [00:13<00:02, 213.11it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2809/3257 [00:13<00:02, 214.20it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2831/3257 [00:14<00:02, 201.60it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2854/3257 [00:14<00:01, 209.25it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2885/3257 [00:14<00:01, 237.27it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2910/3257 [00:14<00:01, 216.80it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2933/3257 [00:14<00:01, 216.27it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2956/3257 [00:14<00:01, 206.35it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2978/3257 [00:14<00:01, 207.77it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3002/3257 [00:14<00:01, 215.31it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3024/3257 [00:14<00:01, 210.46it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:15<00:00, 218.15it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3077/3257 [00:15<00:00, 235.27it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3101/3257 [00:15<00:00, 231.07it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3127/3257 [00:15<00:00, 235.99it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3151/3257 [00:15<00:00, 218.85it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3175/3257 [00:15<00:00, 221.43it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3198/3257 [00:15<00:00, 217.45it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3220/3257 [00:15<00:00, 209.10it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:15<00:00, 222.58it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 203.35it/s]
2023-02-07 14:20:19.588 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:20:19,589][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d125,n5,mc3,s0.741859,t4>', 'datetime': '2023-02-07T14:20:19.589282', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:20:19,589][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:20:19,589][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:20:19,993][gensim.models.doc2vec][INFO] - collected 21699 word types and 3257 unique tags from a corpus of 3257 examples and 4367244 words
[2023-02-07 14:20:19,995][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:20:20,036][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 15507 unique words (71.46% of original 21699, drops 6192)', 'datetime': '2023-02-07T14:20:20.036612', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:20:20,037][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 4358064 word corpus (99.79% of original 4367244, drops 9180)', 'datetime': '2023-02-07T14:20:20.037014', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:20:20,090][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 21699 items
[2023-02-07 14:20:20,091][gensim.models.word2vec][INFO] - sample=0.741859 downsamples 0 most-common words
[2023-02-07 14:20:20,091][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4358064 word corpus (100.0%% of prior 4358064)', 'datetime': '2023-02-07T14:20:20.091606', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:20:20,189][gensim.models.word2vec][INFO] - estimated required memory for 15507 words and 125 dimensions: 25540400 bytes
[2023-02-07 14:20:20,192][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:20:20,201][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 15507 vocabulary and 125 features, using sg=1 hs=0 sample=0.7418589555613433 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T14:20:20.201669', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:20:21,207][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 53.05% examples, 2364507 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:22,022][gensim.models.word2vec][INFO] - EPOCH 0: training on 4367244 raw words (4359560 effective words) took 1.8s, 2398525 effective words/s
[2023-02-07 14:20:23,028][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 53.85% examples, 2399384 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:23,828][gensim.models.word2vec][INFO] - EPOCH 1: training on 4367244 raw words (4359560 effective words) took 1.8s, 2415619 effective words/s
[2023-02-07 14:20:24,833][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 53.21% examples, 2373982 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:25,659][gensim.models.word2vec][INFO] - EPOCH 2: training on 4367244 raw words (4359560 effective words) took 1.8s, 2383604 effective words/s
[2023-02-07 14:20:26,666][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 53.05% examples, 2357793 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:27,498][gensim.models.word2vec][INFO] - EPOCH 3: training on 4367244 raw words (4359560 effective words) took 1.8s, 2372731 effective words/s
[2023-02-07 14:20:28,502][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 53.24% examples, 2374589 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:29,327][gensim.models.word2vec][INFO] - EPOCH 4: training on 4367244 raw words (4359560 effective words) took 1.8s, 2384926 effective words/s
[2023-02-07 14:20:30,336][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 53.05% examples, 2354283 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:31,164][gensim.models.word2vec][INFO] - EPOCH 5: training on 4367244 raw words (4359560 effective words) took 1.8s, 2375243 effective words/s
[2023-02-07 14:20:32,170][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 53.85% examples, 2401257 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:32,970][gensim.models.word2vec][INFO] - EPOCH 6: training on 4367244 raw words (4359560 effective words) took 1.8s, 2416715 effective words/s
[2023-02-07 14:20:33,980][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 53.85% examples, 2391702 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:34,793][gensim.models.word2vec][INFO] - EPOCH 7: training on 4367244 raw words (4359560 effective words) took 1.8s, 2394069 effective words/s
[2023-02-07 14:20:35,799][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 53.21% examples, 2371879 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:36,622][gensim.models.word2vec][INFO] - EPOCH 8: training on 4367244 raw words (4359560 effective words) took 1.8s, 2386193 effective words/s
[2023-02-07 14:20:37,624][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 52.72% examples, 2352616 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:38,469][gensim.models.word2vec][INFO] - EPOCH 9: training on 4367244 raw words (4359560 effective words) took 1.8s, 2362300 effective words/s
[2023-02-07 14:20:39,475][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 53.05% examples, 2361067 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:40,295][gensim.models.word2vec][INFO] - EPOCH 10: training on 4367244 raw words (4359560 effective words) took 1.8s, 2388737 effective words/s
[2023-02-07 14:20:41,303][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 53.21% examples, 2369354 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:42,120][gensim.models.word2vec][INFO] - EPOCH 11: training on 4367244 raw words (4359560 effective words) took 1.8s, 2391996 effective words/s
[2023-02-07 14:20:43,126][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 53.05% examples, 2362261 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:43,960][gensim.models.word2vec][INFO] - EPOCH 12: training on 4367244 raw words (4359560 effective words) took 1.8s, 2371207 effective words/s
[2023-02-07 14:20:44,968][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 53.05% examples, 2357916 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:45,799][gensim.models.word2vec][INFO] - EPOCH 13: training on 4367244 raw words (4359560 effective words) took 1.8s, 2373236 effective words/s
[2023-02-07 14:20:46,808][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 53.05% examples, 2355121 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:47,622][gensim.models.word2vec][INFO] - EPOCH 14: training on 4367244 raw words (4359560 effective words) took 1.8s, 2393478 effective words/s
[2023-02-07 14:20:48,625][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 53.67% examples, 2397925 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:49,441][gensim.models.word2vec][INFO] - EPOCH 15: training on 4367244 raw words (4359560 effective words) took 1.8s, 2399185 effective words/s
[2023-02-07 14:20:50,443][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 53.24% examples, 2379311 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:51,269][gensim.models.word2vec][INFO] - EPOCH 16: training on 4367244 raw words (4359560 effective words) took 1.8s, 2386704 effective words/s
[2023-02-07 14:20:52,273][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 53.05% examples, 2364934 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:53,104][gensim.models.word2vec][INFO] - EPOCH 17: training on 4367244 raw words (4359560 effective words) took 1.8s, 2377869 effective words/s
[2023-02-07 14:20:54,107][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 52.44% examples, 2341177 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:54,946][gensim.models.word2vec][INFO] - EPOCH 18: training on 4367244 raw words (4359560 effective words) took 1.8s, 2368744 effective words/s
[2023-02-07 14:20:55,949][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 54.10% examples, 2414915 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:56,759][gensim.models.word2vec][INFO] - EPOCH 19: training on 4367244 raw words (4359560 effective words) took 1.8s, 2407495 effective words/s
[2023-02-07 14:20:57,765][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 53.21% examples, 2369721 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:20:58,591][gensim.models.word2vec][INFO] - EPOCH 20: training on 4367244 raw words (4359560 effective words) took 1.8s, 2381621 effective words/s
[2023-02-07 14:20:59,593][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 53.05% examples, 2371610 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:00,420][gensim.models.word2vec][INFO] - EPOCH 21: training on 4367244 raw words (4359560 effective words) took 1.8s, 2385207 effective words/s
[2023-02-07 14:21:01,422][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 52.72% examples, 2352922 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:02,263][gensim.models.word2vec][INFO] - EPOCH 22: training on 4367244 raw words (4359560 effective words) took 1.8s, 2368108 effective words/s
[2023-02-07 14:21:03,268][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 53.05% examples, 2365356 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:04,092][gensim.models.word2vec][INFO] - EPOCH 23: training on 4367244 raw words (4359560 effective words) took 1.8s, 2385584 effective words/s
[2023-02-07 14:21:05,095][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 53.85% examples, 2407365 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:05,913][gensim.models.word2vec][INFO] - EPOCH 24: training on 4367244 raw words (4359560 effective words) took 1.8s, 2396731 effective words/s
[2023-02-07 14:21:06,914][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 53.02% examples, 2362139 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:07,749][gensim.models.word2vec][INFO] - EPOCH 25: training on 4367244 raw words (4359560 effective words) took 1.8s, 2375995 effective words/s
[2023-02-07 14:21:08,751][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 53.24% examples, 2381094 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:09,559][gensim.models.word2vec][INFO] - EPOCH 26: training on 4367244 raw words (4359560 effective words) took 1.8s, 2410825 effective words/s
[2023-02-07 14:21:10,567][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 53.85% examples, 2397806 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:11,370][gensim.models.word2vec][INFO] - EPOCH 27: training on 4367244 raw words (4359560 effective words) took 1.8s, 2410880 effective words/s
[2023-02-07 14:21:12,375][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 53.85% examples, 2402471 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:13,186][gensim.models.word2vec][INFO] - EPOCH 28: training on 4367244 raw words (4359560 effective words) took 1.8s, 2402070 effective words/s
[2023-02-07 14:21:14,189][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 53.21% examples, 2378243 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:15,006][gensim.models.word2vec][INFO] - EPOCH 29: training on 4367244 raw words (4359560 effective words) took 1.8s, 2397282 effective words/s
[2023-02-07 14:21:16,008][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 53.21% examples, 2379792 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:16,819][gensim.models.word2vec][INFO] - EPOCH 30: training on 4367244 raw words (4359560 effective words) took 1.8s, 2406011 effective words/s
[2023-02-07 14:21:17,824][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 53.85% examples, 2403412 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:18,633][gensim.models.word2vec][INFO] - EPOCH 31: training on 4367244 raw words (4359560 effective words) took 1.8s, 2406795 effective words/s
[2023-02-07 14:21:19,641][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 53.85% examples, 2393799 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:20,456][gensim.models.word2vec][INFO] - EPOCH 32: training on 4367244 raw words (4359560 effective words) took 1.8s, 2393075 effective words/s
[2023-02-07 14:21:21,460][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 53.21% examples, 2376396 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:22,274][gensim.models.word2vec][INFO] - EPOCH 33: training on 4367244 raw words (4359560 effective words) took 1.8s, 2400278 effective words/s
[2023-02-07 14:21:23,280][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 53.24% examples, 2373064 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:24,091][gensim.models.word2vec][INFO] - EPOCH 34: training on 4367244 raw words (4359560 effective words) took 1.8s, 2403265 effective words/s
[2023-02-07 14:21:25,094][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 53.85% examples, 2407507 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:25,902][gensim.models.word2vec][INFO] - EPOCH 35: training on 4367244 raw words (4359560 effective words) took 1.8s, 2408743 effective words/s
[2023-02-07 14:21:26,905][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 53.21% examples, 2377934 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:27,722][gensim.models.word2vec][INFO] - EPOCH 36: training on 4367244 raw words (4359560 effective words) took 1.8s, 2398124 effective words/s
[2023-02-07 14:21:28,726][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 53.67% examples, 2396611 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:29,542][gensim.models.word2vec][INFO] - EPOCH 37: training on 4367244 raw words (4359560 effective words) took 1.8s, 2397201 effective words/s
[2023-02-07 14:21:30,546][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 53.05% examples, 2366221 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:31,370][gensim.models.word2vec][INFO] - EPOCH 38: training on 4367244 raw words (4359560 effective words) took 1.8s, 2386280 effective words/s
[2023-02-07 14:21:32,374][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 53.85% examples, 2407802 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:33,178][gensim.models.word2vec][INFO] - EPOCH 39: training on 4367244 raw words (4359560 effective words) took 1.8s, 2415185 effective words/s
[2023-02-07 14:21:34,184][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 54.10% examples, 2406119 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:34,984][gensim.models.word2vec][INFO] - EPOCH 40: training on 4367244 raw words (4359560 effective words) took 1.8s, 2415419 effective words/s
[2023-02-07 14:21:35,988][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 53.21% examples, 2377409 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:36,813][gensim.models.word2vec][INFO] - EPOCH 41: training on 4367244 raw words (4359560 effective words) took 1.8s, 2385065 effective words/s
[2023-02-07 14:21:37,817][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 53.21% examples, 2378308 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:38,626][gensim.models.word2vec][INFO] - EPOCH 42: training on 4367244 raw words (4359560 effective words) took 1.8s, 2407952 effective words/s
[2023-02-07 14:21:39,630][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 54.28% examples, 2419527 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:40,419][gensim.models.word2vec][INFO] - EPOCH 43: training on 4367244 raw words (4359560 effective words) took 1.8s, 2433102 effective words/s
[2023-02-07 14:21:41,423][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 54.10% examples, 2412802 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:42,223][gensim.models.word2vec][INFO] - EPOCH 44: training on 4367244 raw words (4359560 effective words) took 1.8s, 2419126 effective words/s
[2023-02-07 14:21:43,226][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 53.85% examples, 2406758 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:44,031][gensim.models.word2vec][INFO] - EPOCH 45: training on 4367244 raw words (4359560 effective words) took 1.8s, 2412481 effective words/s
[2023-02-07 14:21:45,039][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 54.10% examples, 2404944 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:45,823][gensim.models.word2vec][INFO] - EPOCH 46: training on 4367244 raw words (4359560 effective words) took 1.8s, 2435584 effective words/s
[2023-02-07 14:21:46,834][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 54.77% examples, 2423191 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:47,612][gensim.models.word2vec][INFO] - EPOCH 47: training on 4367244 raw words (4359560 effective words) took 1.8s, 2440136 effective words/s
[2023-02-07 14:21:48,617][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 53.85% examples, 2403374 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:21:49,425][gensim.models.word2vec][INFO] - EPOCH 48: training on 4367244 raw words (4359560 effective words) took 1.8s, 2406402 effective words/s
[2023-02-07 14:21:50,429][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 54.10% examples, 2412837 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:51,219][gensim.models.word2vec][INFO] - EPOCH 49: training on 4367244 raw words (4359560 effective words) took 1.8s, 2432691 effective words/s
[2023-02-07 14:21:52,221][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 55.02% examples, 2454060 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:52,993][gensim.models.word2vec][INFO] - EPOCH 50: training on 4367244 raw words (4359560 effective words) took 1.8s, 2459313 effective words/s
[2023-02-07 14:21:53,996][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 54.28% examples, 2424587 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:54,778][gensim.models.word2vec][INFO] - EPOCH 51: training on 4367244 raw words (4359560 effective words) took 1.8s, 2444406 effective words/s
[2023-02-07 14:21:55,782][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 55.02% examples, 2450129 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:56,569][gensim.models.word2vec][INFO] - EPOCH 52: training on 4367244 raw words (4359560 effective words) took 1.8s, 2437501 effective words/s
[2023-02-07 14:21:57,573][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 54.28% examples, 2421563 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:21:58,366][gensim.models.word2vec][INFO] - EPOCH 53: training on 4367244 raw words (4359560 effective words) took 1.8s, 2429694 effective words/s
[2023-02-07 14:21:59,368][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 53.85% examples, 2409499 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:00,170][gensim.models.word2vec][INFO] - EPOCH 54: training on 4367244 raw words (4359560 effective words) took 1.8s, 2417994 effective words/s
[2023-02-07 14:22:01,176][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 53.85% examples, 2402762 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:01,973][gensim.models.word2vec][INFO] - EPOCH 55: training on 4367244 raw words (4359560 effective words) took 1.8s, 2421427 effective words/s
[2023-02-07 14:22:02,978][gensim.models.word2vec][INFO] - EPOCH 56 - PROGRESS: at 54.28% examples, 2418161 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:03,771][gensim.models.word2vec][INFO] - EPOCH 56: training on 4367244 raw words (4359560 effective words) took 1.8s, 2426730 effective words/s
[2023-02-07 14:22:04,775][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 54.10% examples, 2412309 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:05,571][gensim.models.word2vec][INFO] - EPOCH 57: training on 4367244 raw words (4359560 effective words) took 1.8s, 2425358 effective words/s
[2023-02-07 14:22:06,574][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 53.21% examples, 2378512 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:07,388][gensim.models.word2vec][INFO] - EPOCH 58: training on 4367244 raw words (4359560 effective words) took 1.8s, 2400960 effective words/s
[2023-02-07 14:22:08,393][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 55.02% examples, 2445949 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:09,161][gensim.models.word2vec][INFO] - EPOCH 59: training on 4367244 raw words (4359560 effective words) took 1.8s, 2461308 effective words/s
[2023-02-07 14:22:10,164][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 54.10% examples, 2414272 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:10,968][gensim.models.word2vec][INFO] - EPOCH 60: training on 4367244 raw words (4359560 effective words) took 1.8s, 2414391 effective words/s
[2023-02-07 14:22:11,971][gensim.models.word2vec][INFO] - EPOCH 61 - PROGRESS: at 53.85% examples, 2409769 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:12,770][gensim.models.word2vec][INFO] - EPOCH 61: training on 4367244 raw words (4359560 effective words) took 1.8s, 2422943 effective words/s
[2023-02-07 14:22:13,782][gensim.models.word2vec][INFO] - EPOCH 62 - PROGRESS: at 54.77% examples, 2419674 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:14,550][gensim.models.word2vec][INFO] - EPOCH 62: training on 4367244 raw words (4359560 effective words) took 1.8s, 2451021 effective words/s
[2023-02-07 14:22:15,556][gensim.models.word2vec][INFO] - EPOCH 63 - PROGRESS: at 54.77% examples, 2434468 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:16,336][gensim.models.word2vec][INFO] - EPOCH 63: training on 4367244 raw words (4359560 effective words) took 1.8s, 2443220 effective words/s
[2023-02-07 14:22:17,341][gensim.models.word2vec][INFO] - EPOCH 64 - PROGRESS: at 54.77% examples, 2438393 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:18,117][gensim.models.word2vec][INFO] - EPOCH 64: training on 4367244 raw words (4359560 effective words) took 1.8s, 2450183 effective words/s
[2023-02-07 14:22:19,122][gensim.models.word2vec][INFO] - EPOCH 65 - PROGRESS: at 54.77% examples, 2437950 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:19,901][gensim.models.word2vec][INFO] - EPOCH 65: training on 4367244 raw words (4359560 effective words) took 1.8s, 2446142 effective words/s
[2023-02-07 14:22:20,904][gensim.models.word2vec][INFO] - EPOCH 66 - PROGRESS: at 53.67% examples, 2400006 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:21,703][gensim.models.word2vec][INFO] - EPOCH 66: training on 4367244 raw words (4359560 effective words) took 1.8s, 2422150 effective words/s
[2023-02-07 14:22:22,707][gensim.models.word2vec][INFO] - EPOCH 67 - PROGRESS: at 54.77% examples, 2440183 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:23,483][gensim.models.word2vec][INFO] - EPOCH 67: training on 4367244 raw words (4359560 effective words) took 1.8s, 2450513 effective words/s
[2023-02-07 14:22:24,491][gensim.models.word2vec][INFO] - EPOCH 68 - PROGRESS: at 54.77% examples, 2430803 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:25,266][gensim.models.word2vec][INFO] - EPOCH 68: training on 4367244 raw words (4359560 effective words) took 1.8s, 2447825 effective words/s
[2023-02-07 14:22:26,269][gensim.models.word2vec][INFO] - EPOCH 69 - PROGRESS: at 53.67% examples, 2399525 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:27,081][gensim.models.word2vec][INFO] - EPOCH 69: training on 4367244 raw words (4359560 effective words) took 1.8s, 2403744 effective words/s
[2023-02-07 14:22:28,091][gensim.models.word2vec][INFO] - EPOCH 70 - PROGRESS: at 55.54% examples, 2463926 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:28,842][gensim.models.word2vec][INFO] - EPOCH 70: training on 4367244 raw words (4359560 effective words) took 1.8s, 2478577 effective words/s
[2023-02-07 14:22:29,844][gensim.models.word2vec][INFO] - EPOCH 71 - PROGRESS: at 55.20% examples, 2460209 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:30,609][gensim.models.word2vec][INFO] - EPOCH 71: training on 4367244 raw words (4359560 effective words) took 1.8s, 2469188 effective words/s
[2023-02-07 14:22:31,613][gensim.models.word2vec][INFO] - EPOCH 72 - PROGRESS: at 55.20% examples, 2455795 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:32,383][gensim.models.word2vec][INFO] - EPOCH 72: training on 4367244 raw words (4359560 effective words) took 1.8s, 2458621 effective words/s
[2023-02-07 14:22:33,392][gensim.models.word2vec][INFO] - EPOCH 73 - PROGRESS: at 54.77% examples, 2428061 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:34,170][gensim.models.word2vec][INFO] - EPOCH 73: training on 4367244 raw words (4359560 effective words) took 1.8s, 2442723 effective words/s
[2023-02-07 14:22:35,175][gensim.models.word2vec][INFO] - EPOCH 74 - PROGRESS: at 54.77% examples, 2438004 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:35,953][gensim.models.word2vec][INFO] - EPOCH 74: training on 4367244 raw words (4359560 effective words) took 1.8s, 2447293 effective words/s
[2023-02-07 14:22:36,959][gensim.models.word2vec][INFO] - EPOCH 75 - PROGRESS: at 54.77% examples, 2436434 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:37,735][gensim.models.word2vec][INFO] - EPOCH 75: training on 4367244 raw words (4359560 effective words) took 1.8s, 2449674 effective words/s
[2023-02-07 14:22:38,737][gensim.models.word2vec][INFO] - EPOCH 76 - PROGRESS: at 54.77% examples, 2443607 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:39,527][gensim.models.word2vec][INFO] - EPOCH 76: training on 4367244 raw words (4359560 effective words) took 1.8s, 2435520 effective words/s
[2023-02-07 14:22:40,533][gensim.models.word2vec][INFO] - EPOCH 77 - PROGRESS: at 53.24% examples, 2371511 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:41,360][gensim.models.word2vec][INFO] - EPOCH 77: training on 4367244 raw words (4359560 effective words) took 1.8s, 2379979 effective words/s
[2023-02-07 14:22:42,367][gensim.models.word2vec][INFO] - EPOCH 78 - PROGRESS: at 53.85% examples, 2400224 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:43,154][gensim.models.word2vec][INFO] - EPOCH 78: training on 4367244 raw words (4359560 effective words) took 1.8s, 2433638 effective words/s
[2023-02-07 14:22:44,162][gensim.models.word2vec][INFO] - EPOCH 79 - PROGRESS: at 54.77% examples, 2431873 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:44,940][gensim.models.word2vec][INFO] - EPOCH 79: training on 4367244 raw words (4359560 effective words) took 1.8s, 2444345 effective words/s
[2023-02-07 14:22:45,944][gensim.models.word2vec][INFO] - EPOCH 80 - PROGRESS: at 54.77% examples, 2440220 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:46,735][gensim.models.word2vec][INFO] - EPOCH 80: training on 4367244 raw words (4359560 effective words) took 1.8s, 2432345 effective words/s
[2023-02-07 14:22:47,738][gensim.models.word2vec][INFO] - EPOCH 81 - PROGRESS: at 54.10% examples, 2413739 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:48,529][gensim.models.word2vec][INFO] - EPOCH 81: training on 4367244 raw words (4359560 effective words) took 1.8s, 2432321 effective words/s
[2023-02-07 14:22:49,531][gensim.models.word2vec][INFO] - EPOCH 82 - PROGRESS: at 54.10% examples, 2414997 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:50,318][gensim.models.word2vec][INFO] - EPOCH 82: training on 4367244 raw words (4359560 effective words) took 1.8s, 2439173 effective words/s
[2023-02-07 14:22:51,328][gensim.models.word2vec][INFO] - EPOCH 83 - PROGRESS: at 54.74% examples, 2425100 words/s, in_qsize 7, out_qsize 1
[2023-02-07 14:22:52,099][gensim.models.word2vec][INFO] - EPOCH 83: training on 4367244 raw words (4359560 effective words) took 1.8s, 2450132 effective words/s
[2023-02-07 14:22:53,104][gensim.models.word2vec][INFO] - EPOCH 84 - PROGRESS: at 54.96% examples, 2444702 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:53,879][gensim.models.word2vec][INFO] - EPOCH 84: training on 4367244 raw words (4359560 effective words) took 1.8s, 2451248 effective words/s
[2023-02-07 14:22:54,881][gensim.models.word2vec][INFO] - EPOCH 85 - PROGRESS: at 55.02% examples, 2452370 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:55,650][gensim.models.word2vec][INFO] - EPOCH 85: training on 4367244 raw words (4359560 effective words) took 1.8s, 2462939 effective words/s
[2023-02-07 14:22:56,654][gensim.models.word2vec][INFO] - EPOCH 86 - PROGRESS: at 55.02% examples, 2448970 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:57,415][gensim.models.word2vec][INFO] - EPOCH 86: training on 4367244 raw words (4359560 effective words) took 1.8s, 2472897 effective words/s
[2023-02-07 14:22:58,417][gensim.models.word2vec][INFO] - EPOCH 87 - PROGRESS: at 54.96% examples, 2451483 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:22:59,191][gensim.models.word2vec][INFO] - EPOCH 87: training on 4367244 raw words (4359560 effective words) took 1.8s, 2456370 effective words/s
[2023-02-07 14:23:00,194][gensim.models.word2vec][INFO] - EPOCH 88 - PROGRESS: at 54.77% examples, 2442169 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:23:00,971][gensim.models.word2vec][INFO] - EPOCH 88: training on 4367244 raw words (4359560 effective words) took 1.8s, 2451548 effective words/s
[2023-02-07 14:23:01,975][gensim.models.word2vec][INFO] - EPOCH 89 - PROGRESS: at 55.02% examples, 2448900 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:23:02,748][gensim.models.word2vec][INFO] - EPOCH 89: training on 4367244 raw words (4359560 effective words) took 1.8s, 2455858 effective words/s
[2023-02-07 14:23:03,753][gensim.models.word2vec][INFO] - EPOCH 90 - PROGRESS: at 55.27% examples, 2459858 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:23:04,505][gensim.models.word2vec][INFO] - EPOCH 90: training on 4367244 raw words (4359560 effective words) took 1.8s, 2483216 effective words/s
[2023-02-07 14:23:04,506][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 397419204 raw words (396719960 effective words) took 164.3s, 2414541 effective words/s', 'datetime': '2023-02-07T14:23:04.506623', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:23:04.506 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:23:17,164][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141954-pcyxi19d/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:23:17.164478', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:23:17,166][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:23:17,210][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_141954-pcyxi19d/files/../tmp/embedding_model.pt
2023-02-07 14:23:17.210 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:23:18.484 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:23:18.955 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:23:19.856 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8380318567518055, 'test_mae': 1.014565837678094, 'test_r2': 0.13050103602997576}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.050 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.050 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.86
wandb: percentage 0.28536
wandb:   test_mae 1.01457
wandb:   test_mse 1.83803
wandb:    test_r2 0.1305
wandb: 
wandb: üöÄ View run wise-sweep-78 at: https://wandb.ai/xiaoqiz/mof2vec/runs/pcyxi19d
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_141954-pcyxi19d/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: v7mvv179 with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 657
wandb: 	model.gensim.alpha: 0.0012746527724456752
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 75
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.7392379379065208
wandb: 	model.gensim.vector_size: 60
wandb: 	model.gensim.window: 10
wandb: 	model.sklearn.learning_rate: 0.10544114823624248
wandb: 	model.sklearn.max_depth: 81
wandb: 	model.sklearn.min_child_weight: 0.008316134808890228
wandb: 	model.sklearn.n_estimators: 3998
wandb: 	model.sklearn.num_leaves: 398
wandb: 	model.sklearn.reg_alpha: 0.0906397695763448
wandb: 	model.sklearn.reg_lambda: 0.23793880005331697
wandb: 	model.sklearn.subsample: 0.3988282736716432
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142337-v7mvv179
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-79
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/v7mvv179
2023-02-07 14:23:45.360 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 14:23:45.360 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 657 for sweep.
2023-02-07 14:23:45.361 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0012746527724456752 for sweep.
2023-02-07 14:23:45.361 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:23:45.361 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 75 for sweep.
2023-02-07 14:23:45.361 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 14:23:45.362 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7392379379065208 for sweep.
2023-02-07 14:23:45.362 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 60 for sweep.
2023-02-07 14:23:45.362 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 10 for sweep.
2023-02-07 14:23:45.362 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.10544114823624248 for sweep.
2023-02-07 14:23:45.363 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 81 for sweep.
2023-02-07 14:23:45.363 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.008316134808890228 for sweep.
2023-02-07 14:23:45.363 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3998 for sweep.
2023-02-07 14:23:45.363 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 398 for sweep.
2023-02-07 14:23:45.363 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.0906397695763448 for sweep.
2023-02-07 14:23:45.364 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.23793880005331697 for sweep.
2023-02-07 14:23:45.364 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3988282736716432 for sweep.
2023-02-07 14:23:45.364 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:23:45.369 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142337-v7mvv179/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 657, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 60, 'window': 10, 'min_count': 2, 'dm': 0, 'sample': 0.7392379379065208, 'workers': 4, 'alpha': 0.0012746527724456752, 'epochs': 75}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3998, 'max_depth': 81, 'num_leaves': 398, 'reg_alpha': 0.0906397695763448, 'reg_lambda': 0.23793880005331697, 'subsample': 0.3988282736716432, 'min_child_weight': 0.008316134808890228, 'n_jobs': 4, 'learning_rate': 0.10544114823624248}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  0%|          | 9/3257 [00:00<01:00, 53.96it/s]  1%|‚ñè         | 42/3257 [00:00<00:18, 178.55it/s]  2%|‚ñè         | 75/3257 [00:00<00:13, 237.88it/s]  3%|‚ñé         | 106/3257 [00:00<00:12, 261.90it/s]  4%|‚ñç         | 141/3257 [00:00<00:10, 289.73it/s]  5%|‚ñå         | 172/3257 [00:00<00:10, 293.91it/s]  6%|‚ñã         | 206/3257 [00:00<00:09, 307.08it/s]  7%|‚ñã         | 244/3257 [00:00<00:09, 324.54it/s]  9%|‚ñä         | 277/3257 [00:00<00:09, 320.05it/s] 10%|‚ñâ         | 310/3257 [00:01<00:09, 317.26it/s] 11%|‚ñà         | 342/3257 [00:01<00:09, 307.05it/s] 12%|‚ñà‚ñè        | 375/3257 [00:01<00:09, 311.83it/s] 12%|‚ñà‚ñè        | 407/3257 [00:01<00:09, 306.76it/s] 13%|‚ñà‚ñé        | 438/3257 [00:01<00:09, 288.52it/s] 15%|‚ñà‚ñç        | 473/3257 [00:01<00:09, 305.54it/s] 16%|‚ñà‚ñå        | 506/3257 [00:01<00:08, 312.27it/s] 17%|‚ñà‚ñã        | 541/3257 [00:01<00:08, 321.33it/s] 18%|‚ñà‚ñä        | 574/3257 [00:01<00:08, 299.04it/s] 19%|‚ñà‚ñä        | 610/3257 [00:02<00:08, 314.80it/s] 20%|‚ñà‚ñâ        | 643/3257 [00:02<00:08, 317.80it/s] 21%|‚ñà‚ñà        | 676/3257 [00:02<00:08, 309.56it/s] 22%|‚ñà‚ñà‚ñè       | 709/3257 [00:02<00:08, 313.61it/s] 23%|‚ñà‚ñà‚ñé       | 741/3257 [00:02<00:08, 301.58it/s] 24%|‚ñà‚ñà‚ñç       | 776/3257 [00:02<00:07, 311.30it/s] 25%|‚ñà‚ñà‚ñç       | 809/3257 [00:02<00:07, 315.67it/s] 26%|‚ñà‚ñà‚ñå       | 841/3257 [00:02<00:07, 305.79it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:02<00:07, 311.87it/s] 28%|‚ñà‚ñà‚ñä       | 908/3257 [00:03<00:07, 319.92it/s] 29%|‚ñà‚ñà‚ñâ       | 941/3257 [00:03<00:07, 320.47it/s] 30%|‚ñà‚ñà‚ñâ       | 975/3257 [00:03<00:07, 325.59it/s] 31%|‚ñà‚ñà‚ñà       | 1008/3257 [00:03<00:07, 313.40it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1040/3257 [00:03<00:07, 306.33it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1072/3257 [00:03<00:07, 308.50it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1103/3257 [00:03<00:07, 305.92it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1134/3257 [00:03<00:06, 305.73it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1166/3257 [00:03<00:06, 308.05it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1197/3257 [00:03<00:07, 284.97it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1226/3257 [00:04<00:07, 281.47it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1255/3257 [00:04<00:09, 203.96it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1283/3257 [00:04<00:08, 219.81it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1315/3257 [00:04<00:07, 242.82it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1349/3257 [00:04<00:07, 267.08it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1379/3257 [00:04<00:06, 275.27it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1413/3257 [00:04<00:06, 291.54it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1448/3257 [00:04<00:05, 305.76it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1484/3257 [00:05<00:05, 318.38it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1518/3257 [00:05<00:05, 322.17it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1551/3257 [00:05<00:05, 299.81it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1582/3257 [00:05<00:05, 300.17it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1618/3257 [00:05<00:05, 315.61it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1650/3257 [00:05<00:05, 304.20it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1681/3257 [00:05<00:05, 300.10it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:05<00:05, 307.60it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1746/3257 [00:05<00:05, 296.92it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1780/3257 [00:06<00:04, 307.96it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1811/3257 [00:06<00:04, 308.02it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1843/3257 [00:06<00:04, 310.51it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1876/3257 [00:06<00:04, 314.13it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1909/3257 [00:06<00:04, 318.77it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1941/3257 [00:06<00:04, 315.07it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:06<00:03, 324.93it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2010/3257 [00:06<00:03, 323.31it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2043/3257 [00:06<00:03, 320.84it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2076/3257 [00:06<00:03, 308.70it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2108/3257 [00:07<00:03, 308.75it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2139/3257 [00:07<00:03, 302.82it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2170/3257 [00:07<00:03, 304.26it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2204/3257 [00:07<00:03, 311.48it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:07<00:03, 306.61it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2267/3257 [00:07<00:03, 302.54it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:07<00:03, 304.34it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2337/3257 [00:07<00:02, 323.81it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2373/3257 [00:07<00:02, 331.93it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2407/3257 [00:08<00:02, 333.19it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2441/3257 [00:08<00:02, 326.30it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2477/3257 [00:08<00:02, 334.89it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2515/3257 [00:08<00:02, 346.86it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2550/3257 [00:08<00:02, 346.60it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2585/3257 [00:08<00:02, 323.54it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2624/3257 [00:08<00:01, 340.21it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2659/3257 [00:08<00:02, 229.73it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2694/3257 [00:09<00:02, 252.91it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2724/3257 [00:09<00:02, 256.12it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:09<00:01, 280.39it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2796/3257 [00:09<00:01, 296.48it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2828/3257 [00:09<00:01, 297.60it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2866/3257 [00:09<00:01, 318.77it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2900/3257 [00:09<00:01, 320.75it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2934/3257 [00:09<00:00, 326.16it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2968/3257 [00:09<00:00, 322.47it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3001/3257 [00:09<00:00, 324.23it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3034/3257 [00:10<00:00, 322.88it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3068/3257 [00:10<00:00, 327.52it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3104/3257 [00:10<00:00, 335.91it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3139/3257 [00:10<00:00, 337.87it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3173/3257 [00:10<00:00, 332.50it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:10<00:00, 329.85it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3241/3257 [00:10<00:00, 331.99it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:10<00:00, 303.55it/s]
2023-02-07 14:23:56.389 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:23:56,390][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d60,n5,mc2,s0.739238,t4>', 'datetime': '2023-02-07T14:23:56.390827', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:23:56,391][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:23:56,391][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:23:56,590][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 14:23:56,591][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:23:56,597][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 2542 unique words (90.17% of original 2819, drops 277)', 'datetime': '2023-02-07T14:23:56.597615', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:23:56,597][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 2183345 word corpus (99.99% of original 2183622, drops 277)', 'datetime': '2023-02-07T14:23:56.597935', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:23:56,606][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 14:23:56,607][gensim.models.word2vec][INFO] - sample=0.739238 downsamples 0 most-common words
[2023-02-07 14:23:56,607][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2183345 word corpus (100.0%% of prior 2183345)', 'datetime': '2023-02-07T14:23:56.607259', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:23:56,622][gensim.models.word2vec][INFO] - estimated required memory for 2542 words and 60 dimensions: 3924240 bytes
[2023-02-07 14:23:56,622][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:23:56,624][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2542 vocabulary and 60 features, using sg=1 hs=0 sample=0.7392379379065208 negative=5 window=10 shrink_windows=True', 'datetime': '2023-02-07T14:23:56.624730', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:23:57,393][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2186602 effective words) took 0.8s, 2851495 effective words/s
[2023-02-07 14:23:58,109][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2186602 effective words) took 0.7s, 3062500 effective words/s
[2023-02-07 14:23:58,851][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2186602 effective words) took 0.7s, 2951314 effective words/s
[2023-02-07 14:23:59,617][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2186602 effective words) took 0.8s, 2860409 effective words/s
[2023-02-07 14:24:00,382][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2186602 effective words) took 0.8s, 2862529 effective words/s
[2023-02-07 14:24:01,147][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2186602 effective words) took 0.8s, 2865041 effective words/s
[2023-02-07 14:24:01,910][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2186602 effective words) took 0.8s, 2874980 effective words/s
[2023-02-07 14:24:02,669][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2186602 effective words) took 0.8s, 2887474 effective words/s
[2023-02-07 14:24:03,430][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2186602 effective words) took 0.8s, 2878941 effective words/s
[2023-02-07 14:24:04,190][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2186602 effective words) took 0.8s, 2880970 effective words/s
[2023-02-07 14:24:04,954][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2186602 effective words) took 0.8s, 2867746 effective words/s
[2023-02-07 14:24:05,715][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2186602 effective words) took 0.8s, 2877988 effective words/s
[2023-02-07 14:24:06,475][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2186602 effective words) took 0.8s, 2881702 effective words/s
[2023-02-07 14:24:07,233][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2186602 effective words) took 0.8s, 2891324 effective words/s
[2023-02-07 14:24:08,007][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2186602 effective words) took 0.8s, 2832811 effective words/s
[2023-02-07 14:24:08,765][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2186602 effective words) took 0.8s, 2889773 effective words/s
[2023-02-07 14:24:09,526][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2186602 effective words) took 0.8s, 2879215 effective words/s
[2023-02-07 14:24:10,295][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2186602 effective words) took 0.8s, 2847195 effective words/s
[2023-02-07 14:24:11,065][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2186602 effective words) took 0.8s, 2845939 effective words/s
[2023-02-07 14:24:11,826][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2186602 effective words) took 0.8s, 2880528 effective words/s
[2023-02-07 14:24:12,585][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2186602 effective words) took 0.8s, 2884364 effective words/s
[2023-02-07 14:24:13,345][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2186602 effective words) took 0.8s, 2884569 effective words/s
[2023-02-07 14:24:14,101][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2186602 effective words) took 0.8s, 2896431 effective words/s
[2023-02-07 14:24:14,854][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2186602 effective words) took 0.8s, 2912181 effective words/s
[2023-02-07 14:24:15,608][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2186602 effective words) took 0.8s, 2904652 effective words/s
[2023-02-07 14:24:16,362][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2186602 effective words) took 0.8s, 2905314 effective words/s
[2023-02-07 14:24:17,116][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2186602 effective words) took 0.8s, 2905693 effective words/s
[2023-02-07 14:24:17,868][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2186602 effective words) took 0.8s, 2915123 effective words/s
[2023-02-07 14:24:18,611][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2186602 effective words) took 0.7s, 2949051 effective words/s
[2023-02-07 14:24:19,373][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2186602 effective words) took 0.8s, 2874314 effective words/s
[2023-02-07 14:24:20,123][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2186602 effective words) took 0.7s, 2922482 effective words/s
[2023-02-07 14:24:20,879][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2186602 effective words) took 0.8s, 2901807 effective words/s
[2023-02-07 14:24:21,643][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2186602 effective words) took 0.8s, 2870477 effective words/s
[2023-02-07 14:24:22,405][gensim.models.word2vec][INFO] - EPOCH 33: training on 2183622 raw words (2186602 effective words) took 0.8s, 2876005 effective words/s
[2023-02-07 14:24:23,165][gensim.models.word2vec][INFO] - EPOCH 34: training on 2183622 raw words (2186602 effective words) took 0.8s, 2879000 effective words/s
[2023-02-07 14:24:23,917][gensim.models.word2vec][INFO] - EPOCH 35: training on 2183622 raw words (2186602 effective words) took 0.8s, 2913155 effective words/s
[2023-02-07 14:24:24,672][gensim.models.word2vec][INFO] - EPOCH 36: training on 2183622 raw words (2186602 effective words) took 0.8s, 2902159 effective words/s
[2023-02-07 14:24:25,425][gensim.models.word2vec][INFO] - EPOCH 37: training on 2183622 raw words (2186602 effective words) took 0.8s, 2908963 effective words/s
[2023-02-07 14:24:26,179][gensim.models.word2vec][INFO] - EPOCH 38: training on 2183622 raw words (2186602 effective words) took 0.8s, 2903674 effective words/s
[2023-02-07 14:24:26,932][gensim.models.word2vec][INFO] - EPOCH 39: training on 2183622 raw words (2186602 effective words) took 0.8s, 2912363 effective words/s
[2023-02-07 14:24:27,687][gensim.models.word2vec][INFO] - EPOCH 40: training on 2183622 raw words (2186602 effective words) took 0.8s, 2900668 effective words/s
[2023-02-07 14:24:28,443][gensim.models.word2vec][INFO] - EPOCH 41: training on 2183622 raw words (2186602 effective words) took 0.8s, 2899614 effective words/s
[2023-02-07 14:24:29,191][gensim.models.word2vec][INFO] - EPOCH 42: training on 2183622 raw words (2186602 effective words) took 0.7s, 2928058 effective words/s
[2023-02-07 14:24:29,951][gensim.models.word2vec][INFO] - EPOCH 43: training on 2183622 raw words (2186602 effective words) took 0.8s, 2881327 effective words/s
[2023-02-07 14:24:30,712][gensim.models.word2vec][INFO] - EPOCH 44: training on 2183622 raw words (2186602 effective words) took 0.8s, 2880154 effective words/s
[2023-02-07 14:24:31,463][gensim.models.word2vec][INFO] - EPOCH 45: training on 2183622 raw words (2186602 effective words) took 0.7s, 2916279 effective words/s
[2023-02-07 14:24:32,210][gensim.models.word2vec][INFO] - EPOCH 46: training on 2183622 raw words (2186602 effective words) took 0.7s, 2933174 effective words/s
[2023-02-07 14:24:32,958][gensim.models.word2vec][INFO] - EPOCH 47: training on 2183622 raw words (2186602 effective words) took 0.7s, 2928079 effective words/s
[2023-02-07 14:24:33,713][gensim.models.word2vec][INFO] - EPOCH 48: training on 2183622 raw words (2186602 effective words) took 0.8s, 2903588 effective words/s
[2023-02-07 14:24:34,458][gensim.models.word2vec][INFO] - EPOCH 49: training on 2183622 raw words (2186602 effective words) took 0.7s, 2938626 effective words/s
[2023-02-07 14:24:35,201][gensim.models.word2vec][INFO] - EPOCH 50: training on 2183622 raw words (2186602 effective words) took 0.7s, 2947167 effective words/s
[2023-02-07 14:24:35,953][gensim.models.word2vec][INFO] - EPOCH 51: training on 2183622 raw words (2186602 effective words) took 0.8s, 2915330 effective words/s
[2023-02-07 14:24:36,703][gensim.models.word2vec][INFO] - EPOCH 52: training on 2183622 raw words (2186602 effective words) took 0.7s, 2917711 effective words/s
[2023-02-07 14:24:37,451][gensim.models.word2vec][INFO] - EPOCH 53: training on 2183622 raw words (2186602 effective words) took 0.7s, 2929280 effective words/s
[2023-02-07 14:24:38,198][gensim.models.word2vec][INFO] - EPOCH 54: training on 2183622 raw words (2186602 effective words) took 0.7s, 2934874 effective words/s
[2023-02-07 14:24:38,954][gensim.models.word2vec][INFO] - EPOCH 55: training on 2183622 raw words (2186602 effective words) took 0.8s, 2899723 effective words/s
[2023-02-07 14:24:39,704][gensim.models.word2vec][INFO] - EPOCH 56: training on 2183622 raw words (2186602 effective words) took 0.7s, 2922545 effective words/s
[2023-02-07 14:24:40,457][gensim.models.word2vec][INFO] - EPOCH 57: training on 2183622 raw words (2186602 effective words) took 0.8s, 2909003 effective words/s
[2023-02-07 14:24:41,202][gensim.models.word2vec][INFO] - EPOCH 58: training on 2183622 raw words (2186602 effective words) took 0.7s, 2942402 effective words/s
[2023-02-07 14:24:41,946][gensim.models.word2vec][INFO] - EPOCH 59: training on 2183622 raw words (2186602 effective words) took 0.7s, 2949607 effective words/s
[2023-02-07 14:24:42,689][gensim.models.word2vec][INFO] - EPOCH 60: training on 2183622 raw words (2186602 effective words) took 0.7s, 2947085 effective words/s
[2023-02-07 14:24:43,433][gensim.models.word2vec][INFO] - EPOCH 61: training on 2183622 raw words (2186602 effective words) took 0.7s, 2946888 effective words/s
[2023-02-07 14:24:44,178][gensim.models.word2vec][INFO] - EPOCH 62: training on 2183622 raw words (2186602 effective words) took 0.7s, 2940614 effective words/s
[2023-02-07 14:24:44,915][gensim.models.word2vec][INFO] - EPOCH 63: training on 2183622 raw words (2186602 effective words) took 0.7s, 2974154 effective words/s
[2023-02-07 14:24:45,656][gensim.models.word2vec][INFO] - EPOCH 64: training on 2183622 raw words (2186602 effective words) took 0.7s, 2954993 effective words/s
[2023-02-07 14:24:46,407][gensim.models.word2vec][INFO] - EPOCH 65: training on 2183622 raw words (2186602 effective words) took 0.7s, 2919774 effective words/s
[2023-02-07 14:24:47,141][gensim.models.word2vec][INFO] - EPOCH 66: training on 2183622 raw words (2186602 effective words) took 0.7s, 2983096 effective words/s
[2023-02-07 14:24:47,881][gensim.models.word2vec][INFO] - EPOCH 67: training on 2183622 raw words (2186602 effective words) took 0.7s, 2960475 effective words/s
[2023-02-07 14:24:48,616][gensim.models.word2vec][INFO] - EPOCH 68: training on 2183622 raw words (2186602 effective words) took 0.7s, 2983992 effective words/s
[2023-02-07 14:24:49,368][gensim.models.word2vec][INFO] - EPOCH 69: training on 2183622 raw words (2186602 effective words) took 0.8s, 2910968 effective words/s
[2023-02-07 14:24:50,112][gensim.models.word2vec][INFO] - EPOCH 70: training on 2183622 raw words (2186602 effective words) took 0.7s, 2946377 effective words/s
[2023-02-07 14:24:50,853][gensim.models.word2vec][INFO] - EPOCH 71: training on 2183622 raw words (2186602 effective words) took 0.7s, 2954728 effective words/s
[2023-02-07 14:24:51,592][gensim.models.word2vec][INFO] - EPOCH 72: training on 2183622 raw words (2186602 effective words) took 0.7s, 2963299 effective words/s
[2023-02-07 14:24:52,326][gensim.models.word2vec][INFO] - EPOCH 73: training on 2183622 raw words (2186602 effective words) took 0.7s, 2985096 effective words/s
[2023-02-07 14:24:53,072][gensim.models.word2vec][INFO] - EPOCH 74: training on 2183622 raw words (2186602 effective words) took 0.7s, 2938871 effective words/s
[2023-02-07 14:24:53,073][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 163771650 raw words (163995150 effective words) took 56.4s, 2905243 effective words/s', 'datetime': '2023-02-07T14:24:53.073018', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:24:53.073 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:24:56,690][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142337-v7mvv179/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:24:56.690607', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:24:56,691][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:24:56,706][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142337-v7mvv179/files/../tmp/embedding_model.pt
2023-02-07 14:24:56.708 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:24:57.696 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:24:58.094 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:24:58.587 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8285743395539915, 'test_mae': 1.0340715312324693, 'test_r2': 0.13497500712847477}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.9
wandb: percentage 0.09826
wandb:   test_mae 1.03407
wandb:   test_mse 1.82857
wandb:    test_r2 0.13498
wandb: 
wandb: üöÄ View run effortless-sweep-79 at: https://wandb.ai/xiaoqiz/mof2vec/runs/v7mvv179
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_142337-v7mvv179/logs
wandb: Agent Starting Run: bzx1hauh with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 337
wandb: 	model.gensim.alpha: 0.00116136039032028
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.epochs: 57
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.910024418883254
wandb: 	model.gensim.vector_size: 175
wandb: 	model.gensim.window: 1
wandb: 	model.sklearn.learning_rate: 0.40378333928324656
wandb: 	model.sklearn.max_depth: 50
wandb: 	model.sklearn.min_child_weight: 0.03199638441803634
wandb: 	model.sklearn.n_estimators: 1857
wandb: 	model.sklearn.num_leaves: 413
wandb: 	model.sklearn.reg_alpha: 0.03062269947776361
wandb: 	model.sklearn.reg_lambda: 0.8386747822194929
wandb: 	model.sklearn.subsample: 0.2907728739481223
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142511-bzx1hauh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-sweep-80
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/bzx1hauh
2023-02-07 14:25:19.011 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 14:25:19.011 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 337 for sweep.
2023-02-07 14:25:19.012 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.00116136039032028 for sweep.
2023-02-07 14:25:19.012 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 14:25:19.012 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 57 for sweep.
2023-02-07 14:25:19.012 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 14:25:19.013 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.910024418883254 for sweep.
2023-02-07 14:25:19.013 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 175 for sweep.
2023-02-07 14:25:19.013 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 1 for sweep.
2023-02-07 14:25:19.013 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.40378333928324656 for sweep.
2023-02-07 14:25:19.013 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 50 for sweep.
2023-02-07 14:25:19.015 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03199638441803634 for sweep.
2023-02-07 14:25:19.015 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1857 for sweep.
2023-02-07 14:25:19.015 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 413 for sweep.
2023-02-07 14:25:19.015 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.03062269947776361 for sweep.
2023-02-07 14:25:19.016 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.8386747822194929 for sweep.
2023-02-07 14:25:19.016 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2907728739481223 for sweep.
2023-02-07 14:25:19.016 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:25:19.022 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142511-bzx1hauh/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 337, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 175, 'window': 1, 'min_count': 1, 'dm': 1, 'sample': 0.910024418883254, 'workers': 4, 'alpha': 0.00116136039032028, 'epochs': 57}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1857, 'max_depth': 50, 'num_leaves': 413, 'reg_alpha': 0.03062269947776361, 'reg_lambda': 0.8386747822194929, 'subsample': 0.2907728739481223, 'min_child_weight': 0.03199638441803634, 'n_jobs': 4, 'learning_rate': 0.40378333928324656}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 26/3257 [00:00<00:12, 253.46it/s]  2%|‚ñè         | 53/3257 [00:00<00:12, 255.56it/s]  3%|‚ñé         | 84/3257 [00:00<00:11, 277.29it/s]  3%|‚ñé         | 112/3257 [00:00<00:11, 265.82it/s]  4%|‚ñç         | 144/3257 [00:00<00:10, 284.50it/s]  5%|‚ñå         | 173/3257 [00:00<00:11, 273.91it/s]  6%|‚ñå         | 203/3257 [00:00<00:10, 279.43it/s]  7%|‚ñã         | 237/3257 [00:00<00:10, 295.56it/s]  8%|‚ñä         | 267/3257 [00:00<00:10, 290.03it/s]  9%|‚ñâ         | 301/3257 [00:01<00:09, 304.35it/s] 10%|‚ñà         | 332/3257 [00:01<00:09, 303.91it/s] 11%|‚ñà         | 363/3257 [00:01<00:09, 296.13it/s] 12%|‚ñà‚ñè        | 393/3257 [00:01<00:10, 281.04it/s] 13%|‚ñà‚ñé        | 423/3257 [00:01<00:09, 283.81it/s] 14%|‚ñà‚ñç        | 452/3257 [00:01<00:10, 257.12it/s] 15%|‚ñà‚ñç        | 480/3257 [00:01<00:10, 260.18it/s] 16%|‚ñà‚ñå        | 512/3257 [00:01<00:09, 274.68it/s] 17%|‚ñà‚ñã        | 540/3257 [00:01<00:10, 269.75it/s] 17%|‚ñà‚ñã        | 568/3257 [00:02<00:10, 255.01it/s] 18%|‚ñà‚ñä        | 594/3257 [00:02<00:10, 251.59it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:02<00:10, 250.89it/s] 20%|‚ñà‚ñâ        | 647/3257 [00:02<00:10, 255.13it/s] 21%|‚ñà‚ñà        | 673/3257 [00:02<00:10, 255.81it/s] 21%|‚ñà‚ñà‚ñè       | 699/3257 [00:02<00:10, 251.03it/s] 22%|‚ñà‚ñà‚ñè       | 726/3257 [00:02<00:10, 253.04it/s] 23%|‚ñà‚ñà‚ñé       | 752/3257 [00:02<00:09, 253.21it/s] 24%|‚ñà‚ñà‚ñç       | 778/3257 [00:03<00:13, 177.75it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:03<00:12, 196.84it/s] 26%|‚ñà‚ñà‚ñå       | 832/3257 [00:03<00:11, 210.47it/s] 26%|‚ñà‚ñà‚ñã       | 856/3257 [00:03<00:11, 216.81it/s] 27%|‚ñà‚ñà‚ñã       | 881/3257 [00:03<00:10, 225.29it/s] 28%|‚ñà‚ñà‚ñä       | 908/3257 [00:03<00:09, 237.39it/s] 29%|‚ñà‚ñà‚ñä       | 933/3257 [00:03<00:09, 235.14it/s] 29%|‚ñà‚ñà‚ñâ       | 959/3257 [00:03<00:09, 239.91it/s] 30%|‚ñà‚ñà‚ñà       | 984/3257 [00:03<00:09, 236.44it/s] 31%|‚ñà‚ñà‚ñà       | 1008/3257 [00:03<00:09, 226.29it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1031/3257 [00:04<00:10, 221.68it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1054/3257 [00:04<00:10, 217.12it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1079/3257 [00:04<00:09, 225.72it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1102/3257 [00:04<00:09, 218.71it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1125/3257 [00:04<00:09, 221.39it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1148/3257 [00:04<00:09, 217.78it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1175/3257 [00:04<00:09, 227.71it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1198/3257 [00:04<00:09, 221.40it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1221/3257 [00:04<00:09, 223.28it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:05<00:08, 237.79it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1276/3257 [00:05<00:08, 243.74it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1301/3257 [00:05<00:08, 237.25it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:05<00:07, 243.16it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1354/3257 [00:05<00:07, 246.03it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1381/3257 [00:05<00:07, 249.87it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1411/3257 [00:05<00:06, 264.13it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1440/3257 [00:05<00:06, 270.05it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1473/3257 [00:05<00:06, 287.62it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1505/3257 [00:05<00:05, 296.46it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1535/3257 [00:06<00:06, 273.48it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1563/3257 [00:06<00:06, 270.89it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1591/3257 [00:06<00:06, 270.74it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1620/3257 [00:06<00:05, 276.07it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1648/3257 [00:06<00:06, 266.55it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1675/3257 [00:06<00:06, 261.69it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1703/3257 [00:06<00:05, 264.53it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1730/3257 [00:06<00:05, 255.07it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1756/3257 [00:06<00:05, 254.50it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1786/3257 [00:07<00:05, 265.65it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1813/3257 [00:07<00:05, 256.65it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1841/3257 [00:07<00:05, 262.91it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1870/3257 [00:07<00:05, 268.79it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1897/3257 [00:07<00:05, 264.69it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1924/3257 [00:07<00:05, 260.60it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1959/3257 [00:07<00:04, 285.61it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1988/3257 [00:07<00:04, 269.45it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2016/3257 [00:07<00:04, 268.52it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2044/3257 [00:08<00:04, 265.87it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2071/3257 [00:08<00:06, 174.37it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2096/3257 [00:08<00:06, 188.86it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:08<00:05, 197.06it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2143/3257 [00:08<00:05, 197.12it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2170/3257 [00:08<00:05, 214.49it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2198/3257 [00:08<00:04, 231.00it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2223/3257 [00:08<00:04, 235.19it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2248/3257 [00:09<00:04, 233.45it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2273/3257 [00:09<00:04, 230.81it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2301/3257 [00:09<00:03, 243.16it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2334/3257 [00:09<00:03, 267.27it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2364/3257 [00:09<00:03, 276.34it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2394/3257 [00:09<00:03, 282.18it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2423/3257 [00:09<00:03, 267.07it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2451/3257 [00:09<00:03, 249.69it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2482/3257 [00:09<00:02, 264.39it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2514/3257 [00:10<00:02, 279.26it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2544/3257 [00:10<00:02, 282.96it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2573/3257 [00:10<00:02, 267.87it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2601/3257 [00:10<00:02, 266.95it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2636/3257 [00:10<00:02, 288.69it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2666/3257 [00:10<00:02, 273.98it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2694/3257 [00:10<00:02, 271.88it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2722/3257 [00:10<00:02, 254.91it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2754/3257 [00:10<00:01, 271.00it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2782/3257 [00:11<00:01, 262.28it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2810/3257 [00:11<00:01, 265.80it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2837/3257 [00:11<00:01, 251.11it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2868/3257 [00:11<00:01, 266.21it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2895/3257 [00:11<00:01, 254.40it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2923/3257 [00:11<00:01, 261.40it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2950/3257 [00:11<00:01, 250.79it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2977/3257 [00:11<00:01, 255.37it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3008/3257 [00:11<00:00, 270.54it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3037/3257 [00:12<00:00, 273.47it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3069/3257 [00:12<00:00, 284.32it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3099/3257 [00:12<00:00, 287.78it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3130/3257 [00:12<00:00, 293.65it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3160/3257 [00:12<00:00, 281.57it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3189/3257 [00:12<00:00, 277.82it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3217/3257 [00:12<00:00, 272.34it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:12<00:00, 286.11it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:12<00:00, 254.77it/s]
2023-02-07 14:25:32.199 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:25:32,200][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d175,n5,w1,s0.910024,t4>', 'datetime': '2023-02-07T14:25:32.200772', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:25:32,201][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:25:32,201][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:25:32,471][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 14:25:32,472][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:25:32,490][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 6662 unique words (100.00% of original 6662, drops 0)', 'datetime': '2023-02-07T14:25:32.490672', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:25:32,491][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 2911496 word corpus (100.00% of original 2911496, drops 0)', 'datetime': '2023-02-07T14:25:32.491062', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:25:32,514][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 14:25:32,514][gensim.models.word2vec][INFO] - sample=0.910024 downsamples 0 most-common words
[2023-02-07 14:25:32,514][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2911496 word corpus (100.0%% of prior 2911496)', 'datetime': '2023-02-07T14:25:32.514755', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:25:32,554][gensim.models.word2vec][INFO] - estimated required memory for 6662 words and 175 dimensions: 15589100 bytes
[2023-02-07 14:25:32,554][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:25:32,561][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 6662 vocabulary and 175 features, using sg=0 hs=0 sample=0.910024418883254 negative=5 window=1 shrink_windows=True', 'datetime': '2023-02-07T14:25:32.561666', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:25:33,564][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 82.56% examples, 2423799 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:25:33,742][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2914753 effective words) took 1.2s, 2473609 effective words/s
[2023-02-07 14:25:34,744][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 94.69% examples, 2765803 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:25:34,792][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2914753 effective words) took 1.0s, 2782003 effective words/s
[2023-02-07 14:25:35,799][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 98.93% examples, 2871345 words/s, in_qsize 3, out_qsize 1
[2023-02-07 14:25:35,803][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2914753 effective words) took 1.0s, 2886144 effective words/s
[2023-02-07 14:25:36,799][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2914753 effective words) took 1.0s, 2929980 effective words/s
[2023-02-07 14:25:37,782][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2914753 effective words) took 1.0s, 2967601 effective words/s
[2023-02-07 14:25:38,750][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2914753 effective words) took 1.0s, 3016843 effective words/s
[2023-02-07 14:25:39,726][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2914753 effective words) took 1.0s, 2993348 effective words/s
[2023-02-07 14:25:40,691][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2914753 effective words) took 1.0s, 3025949 effective words/s
[2023-02-07 14:25:41,648][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2914753 effective words) took 1.0s, 3047105 effective words/s
[2023-02-07 14:25:42,614][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2914753 effective words) took 1.0s, 3021951 effective words/s
[2023-02-07 14:25:43,567][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2914753 effective words) took 1.0s, 3066196 effective words/s
[2023-02-07 14:25:44,524][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2914753 effective words) took 1.0s, 3050153 effective words/s
[2023-02-07 14:25:45,482][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2914753 effective words) took 1.0s, 3044724 effective words/s
[2023-02-07 14:25:46,448][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2914753 effective words) took 1.0s, 3023089 effective words/s
[2023-02-07 14:25:47,398][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2914753 effective words) took 0.9s, 3073242 effective words/s
[2023-02-07 14:25:48,339][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2914753 effective words) took 0.9s, 3102815 effective words/s
[2023-02-07 14:25:49,289][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2914753 effective words) took 0.9s, 3072878 effective words/s
[2023-02-07 14:25:50,234][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2914753 effective words) took 0.9s, 3089641 effective words/s
[2023-02-07 14:25:51,178][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2914753 effective words) took 0.9s, 3092772 effective words/s
[2023-02-07 14:25:52,122][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2914753 effective words) took 0.9s, 3092591 effective words/s
[2023-02-07 14:25:53,065][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2914753 effective words) took 0.9s, 3094809 effective words/s
[2023-02-07 14:25:54,008][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2914753 effective words) took 0.9s, 3094975 effective words/s
[2023-02-07 14:25:54,951][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2914753 effective words) took 0.9s, 3096356 effective words/s
[2023-02-07 14:25:55,888][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2914753 effective words) took 0.9s, 3115583 effective words/s
[2023-02-07 14:25:56,833][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2914753 effective words) took 0.9s, 3088578 effective words/s
[2023-02-07 14:25:57,787][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2914753 effective words) took 1.0s, 3058717 effective words/s
[2023-02-07 14:25:58,719][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2914753 effective words) took 0.9s, 3132958 effective words/s
[2023-02-07 14:25:59,658][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2914753 effective words) took 0.9s, 3106437 effective words/s
[2023-02-07 14:26:00,614][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2914753 effective words) took 1.0s, 3053959 effective words/s
[2023-02-07 14:26:01,548][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2914753 effective words) took 0.9s, 3125726 effective words/s
[2023-02-07 14:26:02,473][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2914753 effective words) took 0.9s, 3155738 effective words/s
[2023-02-07 14:26:03,401][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2914753 effective words) took 0.9s, 3147577 effective words/s
[2023-02-07 14:26:04,338][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2914753 effective words) took 0.9s, 3112701 effective words/s
[2023-02-07 14:26:05,263][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2914753 effective words) took 0.9s, 3155860 effective words/s
[2023-02-07 14:26:06,200][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2914753 effective words) took 0.9s, 3114885 effective words/s
[2023-02-07 14:26:07,117][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2914753 effective words) took 0.9s, 3182093 effective words/s
[2023-02-07 14:26:08,047][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2914753 effective words) took 0.9s, 3141436 effective words/s
[2023-02-07 14:26:08,976][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2914753 effective words) took 0.9s, 3141520 effective words/s
[2023-02-07 14:26:09,891][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2914753 effective words) took 0.9s, 3191973 effective words/s
[2023-02-07 14:26:10,817][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2914753 effective words) took 0.9s, 3152522 effective words/s
[2023-02-07 14:26:11,752][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2914753 effective words) took 0.9s, 3121114 effective words/s
[2023-02-07 14:26:12,692][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2914753 effective words) took 0.9s, 3104087 effective words/s
[2023-02-07 14:26:13,632][gensim.models.word2vec][INFO] - EPOCH 42: training on 2911496 raw words (2914753 effective words) took 0.9s, 3106680 effective words/s
[2023-02-07 14:26:14,560][gensim.models.word2vec][INFO] - EPOCH 43: training on 2911496 raw words (2914753 effective words) took 0.9s, 3145440 effective words/s
[2023-02-07 14:26:15,487][gensim.models.word2vec][INFO] - EPOCH 44: training on 2911496 raw words (2914753 effective words) took 0.9s, 3147157 effective words/s
[2023-02-07 14:26:16,419][gensim.models.word2vec][INFO] - EPOCH 45: training on 2911496 raw words (2914753 effective words) took 0.9s, 3133047 effective words/s
[2023-02-07 14:26:17,349][gensim.models.word2vec][INFO] - EPOCH 46: training on 2911496 raw words (2914753 effective words) took 0.9s, 3138605 effective words/s
[2023-02-07 14:26:18,265][gensim.models.word2vec][INFO] - EPOCH 47: training on 2911496 raw words (2914753 effective words) took 0.9s, 3188300 effective words/s
[2023-02-07 14:26:19,177][gensim.models.word2vec][INFO] - EPOCH 48: training on 2911496 raw words (2914753 effective words) took 0.9s, 3202476 effective words/s
[2023-02-07 14:26:20,094][gensim.models.word2vec][INFO] - EPOCH 49: training on 2911496 raw words (2914753 effective words) took 0.9s, 3181247 effective words/s
[2023-02-07 14:26:21,026][gensim.models.word2vec][INFO] - EPOCH 50: training on 2911496 raw words (2914753 effective words) took 0.9s, 3131639 effective words/s
[2023-02-07 14:26:21,952][gensim.models.word2vec][INFO] - EPOCH 51: training on 2911496 raw words (2914753 effective words) took 0.9s, 3154678 effective words/s
[2023-02-07 14:26:22,870][gensim.models.word2vec][INFO] - EPOCH 52: training on 2911496 raw words (2914753 effective words) took 0.9s, 3180170 effective words/s
[2023-02-07 14:26:23,798][gensim.models.word2vec][INFO] - EPOCH 53: training on 2911496 raw words (2914753 effective words) took 0.9s, 3145177 effective words/s
[2023-02-07 14:26:24,724][gensim.models.word2vec][INFO] - EPOCH 54: training on 2911496 raw words (2914753 effective words) took 0.9s, 3153776 effective words/s
[2023-02-07 14:26:25,643][gensim.models.word2vec][INFO] - EPOCH 55: training on 2911496 raw words (2914753 effective words) took 0.9s, 3172923 effective words/s
[2023-02-07 14:26:26,567][gensim.models.word2vec][INFO] - EPOCH 56: training on 2911496 raw words (2914753 effective words) took 0.9s, 3161175 effective words/s
[2023-02-07 14:26:26,568][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 165955272 raw words (166140921 effective words) took 54.0s, 3076337 effective words/s', 'datetime': '2023-02-07T14:26:26.568174', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:26:26.568 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:26:31,027][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142511-bzx1hauh/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:26:31.027628', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:26:31,029][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:26:31,057][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142511-bzx1hauh/files/../tmp/embedding_model.pt
2023-02-07 14:26:31.057 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:26:32.454 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:26:32.984 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:26:34.239 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0707193530890224, 'test_mae': 1.0922102711857669, 'test_r2': 0.0204259379021694}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.8
wandb: percentage 0.0
wandb:   test_mae 1.09221
wandb:   test_mse 2.07072
wandb:    test_r2 0.02043
wandb: 
wandb: üöÄ View run wise-sweep-80 at: https://wandb.ai/xiaoqiz/mof2vec/runs/bzx1hauh
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_142511-bzx1hauh/logs
wandb: Agent Starting Run: 27120c3w with config:
wandb: 	data.data.wl_step: 5
wandb: 	data.nn.batch_size: 700
wandb: 	model.gensim.alpha: 0.0004898573686902253
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 50
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.32257482911638946
wandb: 	model.gensim.vector_size: 94
wandb: 	model.gensim.window: 7
wandb: 	model.sklearn.learning_rate: 0.30939678186234576
wandb: 	model.sklearn.max_depth: 17
wandb: 	model.sklearn.min_child_weight: 0.021104050871888193
wandb: 	model.sklearn.n_estimators: 1733
wandb: 	model.sklearn.num_leaves: 360
wandb: 	model.sklearn.reg_alpha: 0.2771389456992643
wandb: 	model.sklearn.reg_lambda: 0.01617099082613825
wandb: 	model.sklearn.subsample: 0.608161459917858
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142646-27120c3w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-81
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/27120c3w
2023-02-07 14:26:54.936 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 5 for sweep.
2023-02-07 14:26:54.937 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 700 for sweep.
2023-02-07 14:26:54.937 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0004898573686902253 for sweep.
2023-02-07 14:26:54.938 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:26:54.938 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 50 for sweep.
2023-02-07 14:26:54.938 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 14:26:54.938 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.32257482911638946 for sweep.
2023-02-07 14:26:54.939 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 94 for sweep.
2023-02-07 14:26:54.939 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 7 for sweep.
2023-02-07 14:26:54.940 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.30939678186234576 for sweep.
2023-02-07 14:26:54.940 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 17 for sweep.
2023-02-07 14:26:54.940 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.021104050871888193 for sweep.
2023-02-07 14:26:54.940 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1733 for sweep.
2023-02-07 14:26:54.940 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 360 for sweep.
2023-02-07 14:26:54.941 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.2771389456992643 for sweep.
2023-02-07 14:26:54.941 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.01617099082613825 for sweep.
2023-02-07 14:26:54.941 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.608161459917858 for sweep.
2023-02-07 14:26:54.941 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:26:54.946 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 5}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142646-27120c3w/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 700, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 94, 'window': 7, 'min_count': 3, 'dm': 0, 'sample': 0.32257482911638946, 'workers': 4, 'alpha': 0.0004898573686902253, 'epochs': 50}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1733, 'max_depth': 17, 'num_leaves': 360, 'reg_alpha': 0.2771389456992643, 'reg_lambda': 0.01617099082613825, 'subsample': 0.608161459917858, 'min_child_weight': 0.021104050871888193, 'n_jobs': 4, 'learning_rate': 0.30939678186234576}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 20/3257 [00:00<00:16, 195.27it/s]  1%|‚ñè         | 41/3257 [00:00<00:16, 198.64it/s]  2%|‚ñè         | 63/3257 [00:00<00:15, 207.49it/s]  3%|‚ñé         | 88/3257 [00:00<00:14, 222.23it/s]  3%|‚ñé         | 111/3257 [00:00<00:15, 198.86it/s]  4%|‚ñç         | 134/3257 [00:00<00:15, 203.83it/s]  5%|‚ñç         | 158/3257 [00:00<00:14, 210.28it/s]  6%|‚ñå         | 180/3257 [00:00<00:15, 204.53it/s]  6%|‚ñå         | 202/3257 [00:00<00:14, 208.53it/s]  7%|‚ñã         | 230/3257 [00:01<00:13, 228.71it/s]  8%|‚ñä         | 254/3257 [00:01<00:13, 219.08it/s]  9%|‚ñä         | 277/3257 [00:01<00:18, 157.18it/s]  9%|‚ñâ         | 300/3257 [00:01<00:17, 171.72it/s] 10%|‚ñâ         | 324/3257 [00:01<00:15, 187.05it/s] 11%|‚ñà         | 345/3257 [00:01<00:15, 187.67it/s] 11%|‚ñà‚ñè        | 367/3257 [00:01<00:14, 194.92it/s] 12%|‚ñà‚ñè        | 388/3257 [00:01<00:15, 185.40it/s] 13%|‚ñà‚ñé        | 412/3257 [00:02<00:14, 198.98it/s] 13%|‚ñà‚ñé        | 433/3257 [00:02<00:15, 178.37it/s] 14%|‚ñà‚ñç        | 455/3257 [00:02<00:14, 188.25it/s] 15%|‚ñà‚ñç        | 477/3257 [00:02<00:14, 196.18it/s] 15%|‚ñà‚ñå        | 499/3257 [00:02<00:13, 202.44it/s] 16%|‚ñà‚ñå        | 522/3257 [00:02<00:13, 207.14it/s] 17%|‚ñà‚ñã        | 545/3257 [00:02<00:12, 212.07it/s] 17%|‚ñà‚ñã        | 567/3257 [00:02<00:13, 197.07it/s] 18%|‚ñà‚ñä        | 588/3257 [00:02<00:13, 193.85it/s] 19%|‚ñà‚ñâ        | 611/3257 [00:03<00:13, 202.79it/s] 19%|‚ñà‚ñâ        | 633/3257 [00:03<00:12, 206.61it/s] 20%|‚ñà‚ñà        | 654/3257 [00:03<00:13, 196.92it/s] 21%|‚ñà‚ñà        | 674/3257 [00:03<00:13, 195.56it/s] 21%|‚ñà‚ñà‚ñè       | 694/3257 [00:03<00:13, 193.34it/s] 22%|‚ñà‚ñà‚ñè       | 717/3257 [00:03<00:12, 199.79it/s] 23%|‚ñà‚ñà‚ñé       | 738/3257 [00:03<00:13, 182.70it/s] 23%|‚ñà‚ñà‚ñé       | 761/3257 [00:03<00:12, 194.63it/s] 24%|‚ñà‚ñà‚ñç       | 781/3257 [00:03<00:13, 189.17it/s] 25%|‚ñà‚ñà‚ñç       | 804/3257 [00:04<00:12, 197.71it/s] 25%|‚ñà‚ñà‚ñå       | 824/3257 [00:04<00:12, 188.74it/s] 26%|‚ñà‚ñà‚ñå       | 844/3257 [00:04<00:13, 181.00it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:04<00:13, 182.69it/s] 27%|‚ñà‚ñà‚ñã       | 883/3257 [00:04<00:12, 183.27it/s] 28%|‚ñà‚ñà‚ñä       | 905/3257 [00:04<00:12, 192.12it/s] 28%|‚ñà‚ñà‚ñä       | 926/3257 [00:04<00:11, 195.74it/s] 29%|‚ñà‚ñà‚ñâ       | 946/3257 [00:04<00:11, 195.08it/s] 30%|‚ñà‚ñà‚ñâ       | 967/3257 [00:04<00:11, 198.55it/s] 30%|‚ñà‚ñà‚ñà       | 987/3257 [00:05<00:11, 195.47it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:05<00:11, 191.97it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:05<00:11, 192.33it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:05<00:11, 188.93it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1068/3257 [00:05<00:11, 193.31it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1088/3257 [00:05<00:11, 194.48it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1109/3257 [00:05<00:10, 198.74it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1129/3257 [00:05<00:11, 191.67it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1149/3257 [00:05<00:11, 188.05it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:06<00:10, 195.90it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:06<00:11, 179.92it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1210/3257 [00:06<00:11, 177.42it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1234/3257 [00:06<00:10, 193.21it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1254/3257 [00:06<00:10, 191.34it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1274/3257 [00:06<00:10, 191.09it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1294/3257 [00:06<00:11, 175.44it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1314/3257 [00:06<00:10, 180.63it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1336/3257 [00:06<00:10, 190.38it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1356/3257 [00:07<00:10, 185.45it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1375/3257 [00:07<00:10, 185.46it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1394/3257 [00:07<00:09, 186.56it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1420/3257 [00:07<00:08, 205.57it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1441/3257 [00:07<00:09, 199.81it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1466/3257 [00:07<00:08, 212.69it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1488/3257 [00:07<00:08, 210.96it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1512/3257 [00:07<00:07, 218.96it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1534/3257 [00:07<00:08, 195.05it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1555/3257 [00:08<00:08, 190.08it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1575/3257 [00:08<00:08, 189.66it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1597/3257 [00:08<00:08, 197.92it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:08<00:08, 201.26it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1640/3257 [00:08<00:12, 129.16it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1658/3257 [00:08<00:11, 138.37it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1676/3257 [00:08<00:10, 146.68it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1697/3257 [00:08<00:09, 161.37it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1717/3257 [00:09<00:09, 170.86it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1736/3257 [00:09<00:09, 167.55it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1758/3257 [00:09<00:08, 181.46it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1779/3257 [00:09<00:07, 187.65it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1801/3257 [00:09<00:07, 195.91it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1822/3257 [00:09<00:07, 197.60it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1843/3257 [00:09<00:07, 194.41it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1866/3257 [00:09<00:06, 203.74it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1887/3257 [00:09<00:06, 198.91it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1910/3257 [00:09<00:06, 205.76it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1932/3257 [00:10<00:06, 209.72it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1961/3257 [00:10<00:05, 230.92it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1985/3257 [00:10<00:05, 216.37it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2008/3257 [00:10<00:05, 218.49it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2033/3257 [00:10<00:05, 222.31it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2056/3257 [00:10<00:05, 204.46it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2078/3257 [00:10<00:05, 203.50it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2099/3257 [00:10<00:05, 199.61it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2120/3257 [00:10<00:05, 199.10it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2141/3257 [00:11<00:05, 196.18it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2161/3257 [00:11<00:05, 194.01it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2182/3257 [00:11<00:05, 194.58it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:11<00:05, 203.37it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2227/3257 [00:11<00:05, 204.01it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2248/3257 [00:11<00:05, 200.12it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2269/3257 [00:11<00:04, 201.59it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2290/3257 [00:11<00:04, 203.41it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2311/3257 [00:11<00:04, 204.20it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2337/3257 [00:12<00:04, 220.38it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2363/3257 [00:12<00:03, 231.77it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2388/3257 [00:12<00:03, 236.16it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2412/3257 [00:12<00:03, 219.87it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2435/3257 [00:12<00:03, 208.67it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2457/3257 [00:12<00:03, 209.16it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2482/3257 [00:12<00:03, 218.91it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2509/3257 [00:12<00:03, 229.90it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2535/3257 [00:12<00:03, 237.65it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2559/3257 [00:13<00:03, 218.84it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2582/3257 [00:13<00:03, 210.73it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2605/3257 [00:13<00:03, 214.08it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2632/3257 [00:13<00:02, 229.28it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2656/3257 [00:13<00:02, 220.89it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2679/3257 [00:13<00:02, 218.19it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2701/3257 [00:13<00:02, 200.18it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2723/3257 [00:13<00:02, 204.97it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2751/3257 [00:13<00:02, 220.23it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2774/3257 [00:14<00:02, 208.75it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2802/3257 [00:14<00:02, 225.95it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2825/3257 [00:14<00:02, 209.41it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2847/3257 [00:14<00:01, 209.89it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2875/3257 [00:14<00:01, 228.65it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2899/3257 [00:14<00:01, 214.53it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2925/3257 [00:14<00:01, 226.40it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2949/3257 [00:14<00:01, 205.60it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2971/3257 [00:14<00:01, 208.93it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2993/3257 [00:15<00:01, 200.63it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3017/3257 [00:15<00:01, 209.25it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3043/3257 [00:15<00:00, 221.84it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3069/3257 [00:15<00:00, 230.51it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3093/3257 [00:15<00:00, 226.30it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3120/3257 [00:15<00:00, 236.67it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3144/3257 [00:15<00:00, 133.53it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3165/3257 [00:16<00:00, 146.41it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3185/3257 [00:16<00:00, 156.00it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3207/3257 [00:16<00:00, 167.23it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3227/3257 [00:16<00:00, 174.15it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:16<00:00, 186.11it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 196.78it/s]
2023-02-07 14:27:12.045 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:27:12,046][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d94,n5,mc3,s0.322575,t4>', 'datetime': '2023-02-07T14:27:12.046475', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:27:12,046][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:27:12,046][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:27:12,453][gensim.models.doc2vec][INFO] - collected 21699 word types and 3257 unique tags from a corpus of 3257 examples and 4367244 words
[2023-02-07 14:27:12,454][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:27:12,498][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 15507 unique words (71.46% of original 21699, drops 6192)', 'datetime': '2023-02-07T14:27:12.498181', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:27:12,498][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 4358064 word corpus (99.79% of original 4367244, drops 9180)', 'datetime': '2023-02-07T14:27:12.498578', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:27:12,553][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 21699 items
[2023-02-07 14:27:12,554][gensim.models.word2vec][INFO] - sample=0.322575 downsamples 0 most-common words
[2023-02-07 14:27:12,554][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4358064 word corpus (100.0%% of prior 4358064)', 'datetime': '2023-02-07T14:27:12.554848', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:27:12,646][gensim.models.word2vec][INFO] - estimated required memory for 15507 words and 94 dimensions: 21290796 bytes
[2023-02-07 14:27:12,646][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:27:12,655][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 15507 vocabulary and 94 features, using sg=1 hs=0 sample=0.32257482911638946 negative=5 window=7 shrink_windows=True', 'datetime': '2023-02-07T14:27:12.655663', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:27:13,662][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 56.34% examples, 2513043 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:14,393][gensim.models.word2vec][INFO] - EPOCH 0: training on 4367244 raw words (4359560 effective words) took 1.7s, 2513590 effective words/s
[2023-02-07 14:27:15,397][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 54.77% examples, 2439364 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:16,138][gensim.models.word2vec][INFO] - EPOCH 1: training on 4367244 raw words (4359560 effective words) took 1.7s, 2500367 effective words/s
[2023-02-07 14:27:17,142][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 60.02% examples, 2657129 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:17,769][gensim.models.word2vec][INFO] - EPOCH 2: training on 4367244 raw words (4359560 effective words) took 1.6s, 2675238 effective words/s
[2023-02-07 14:27:18,773][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 60.67% examples, 2683165 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:19,390][gensim.models.word2vec][INFO] - EPOCH 3: training on 4367244 raw words (4359560 effective words) took 1.6s, 2693356 effective words/s
[2023-02-07 14:27:20,395][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 59.96% examples, 2651110 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:27:21,030][gensim.models.word2vec][INFO] - EPOCH 4: training on 4367244 raw words (4359560 effective words) took 1.6s, 2660083 effective words/s
[2023-02-07 14:27:22,033][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 60.30% examples, 2668357 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:22,657][gensim.models.word2vec][INFO] - EPOCH 5: training on 4367244 raw words (4359560 effective words) took 1.6s, 2681699 effective words/s
[2023-02-07 14:27:23,666][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 59.96% examples, 2642912 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:24,287][gensim.models.word2vec][INFO] - EPOCH 6: training on 4367244 raw words (4359560 effective words) took 1.6s, 2677982 effective words/s
[2023-02-07 14:27:25,293][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 60.02% examples, 2648668 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:25,977][gensim.models.word2vec][INFO] - EPOCH 7: training on 4367244 raw words (4359560 effective words) took 1.7s, 2580213 effective words/s
[2023-02-07 14:27:26,983][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 53.85% examples, 2401536 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:27,788][gensim.models.word2vec][INFO] - EPOCH 8: training on 4367244 raw words (4359560 effective words) took 1.8s, 2410258 effective words/s
[2023-02-07 14:27:28,791][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 53.91% examples, 2406184 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:29,591][gensim.models.word2vec][INFO] - EPOCH 9: training on 4367244 raw words (4359560 effective words) took 1.8s, 2421076 effective words/s
[2023-02-07 14:27:30,593][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 53.21% examples, 2380821 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:31,414][gensim.models.word2vec][INFO] - EPOCH 10: training on 4367244 raw words (4359560 effective words) took 1.8s, 2392528 effective words/s
[2023-02-07 14:27:32,421][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 53.85% examples, 2397899 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:33,219][gensim.models.word2vec][INFO] - EPOCH 11: training on 4367244 raw words (4359560 effective words) took 1.8s, 2417305 effective words/s
[2023-02-07 14:27:34,226][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 60.02% examples, 2648104 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:34,825][gensim.models.word2vec][INFO] - EPOCH 12: training on 4367244 raw words (4359560 effective words) took 1.6s, 2717791 effective words/s
[2023-02-07 14:27:35,830][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 62.57% examples, 2760410 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:36,395][gensim.models.word2vec][INFO] - EPOCH 13: training on 4367244 raw words (4359560 effective words) took 1.6s, 2780218 effective words/s
[2023-02-07 14:27:37,397][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 61.96% examples, 2742057 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:37,988][gensim.models.word2vec][INFO] - EPOCH 14: training on 4367244 raw words (4359560 effective words) took 1.6s, 2738567 effective words/s
[2023-02-07 14:27:38,991][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 60.67% examples, 2685317 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:39,617][gensim.models.word2vec][INFO] - EPOCH 15: training on 4367244 raw words (4359560 effective words) took 1.6s, 2679017 effective words/s
[2023-02-07 14:27:40,619][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 60.30% examples, 2670745 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:41,239][gensim.models.word2vec][INFO] - EPOCH 16: training on 4367244 raw words (4359560 effective words) took 1.6s, 2690743 effective words/s
[2023-02-07 14:27:42,241][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 61.50% examples, 2722607 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:42,849][gensim.models.word2vec][INFO] - EPOCH 17: training on 4367244 raw words (4359560 effective words) took 1.6s, 2710298 effective words/s
[2023-02-07 14:27:43,852][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 60.42% examples, 2674933 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:44,475][gensim.models.word2vec][INFO] - EPOCH 18: training on 4367244 raw words (4359560 effective words) took 1.6s, 2682699 effective words/s
[2023-02-07 14:27:45,478][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 60.67% examples, 2686090 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:46,103][gensim.models.word2vec][INFO] - EPOCH 19: training on 4367244 raw words (4359560 effective words) took 1.6s, 2679577 effective words/s
[2023-02-07 14:27:47,108][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 60.02% examples, 2655921 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:47,733][gensim.models.word2vec][INFO] - EPOCH 20: training on 4367244 raw words (4359560 effective words) took 1.6s, 2678618 effective words/s
[2023-02-07 14:27:48,742][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 60.02% examples, 2643222 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:49,370][gensim.models.word2vec][INFO] - EPOCH 21: training on 4367244 raw words (4359560 effective words) took 1.6s, 2664196 effective words/s
[2023-02-07 14:27:50,374][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 59.96% examples, 2655336 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:50,995][gensim.models.word2vec][INFO] - EPOCH 22: training on 4367244 raw words (4359560 effective words) took 1.6s, 2684986 effective words/s
[2023-02-07 14:27:51,997][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 61.50% examples, 2723865 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:52,598][gensim.models.word2vec][INFO] - EPOCH 23: training on 4367244 raw words (4359560 effective words) took 1.6s, 2722896 effective words/s
[2023-02-07 14:27:53,607][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 60.82% examples, 2683912 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:54,218][gensim.models.word2vec][INFO] - EPOCH 24: training on 4367244 raw words (4359560 effective words) took 1.6s, 2695788 effective words/s
[2023-02-07 14:27:55,223][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 60.82% examples, 2689992 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:55,830][gensim.models.word2vec][INFO] - EPOCH 25: training on 4367244 raw words (4359560 effective words) took 1.6s, 2708461 effective words/s
[2023-02-07 14:27:56,834][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 60.67% examples, 2684106 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:57,442][gensim.models.word2vec][INFO] - EPOCH 26: training on 4367244 raw words (4359560 effective words) took 1.6s, 2707671 effective words/s
[2023-02-07 14:27:58,445][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 60.82% examples, 2696335 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:27:59,047][gensim.models.word2vec][INFO] - EPOCH 27: training on 4367244 raw words (4359560 effective words) took 1.6s, 2720518 effective words/s
[2023-02-07 14:28:00,057][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 61.68% examples, 2708830 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:00,650][gensim.models.word2vec][INFO] - EPOCH 28: training on 4367244 raw words (4359560 effective words) took 1.6s, 2722104 effective words/s
[2023-02-07 14:28:01,654][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 60.82% examples, 2688648 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:02,262][gensim.models.word2vec][INFO] - EPOCH 29: training on 4367244 raw words (4359560 effective words) took 1.6s, 2707092 effective words/s
[2023-02-07 14:28:03,263][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 58.03% examples, 2588859 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:03,940][gensim.models.word2vec][INFO] - EPOCH 30: training on 4367244 raw words (4359560 effective words) took 1.7s, 2600672 effective words/s
[2023-02-07 14:28:04,947][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 61.68% examples, 2718378 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:05,548][gensim.models.word2vec][INFO] - EPOCH 31: training on 4367244 raw words (4359560 effective words) took 1.6s, 2712479 effective words/s
[2023-02-07 14:28:06,552][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 60.42% examples, 2672940 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:07,179][gensim.models.word2vec][INFO] - EPOCH 32: training on 4367244 raw words (4359560 effective words) took 1.6s, 2675552 effective words/s
[2023-02-07 14:28:08,181][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 60.42% examples, 2678553 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:08,801][gensim.models.word2vec][INFO] - EPOCH 33: training on 4367244 raw words (4359560 effective words) took 1.6s, 2691049 effective words/s
[2023-02-07 14:28:09,802][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 59.38% examples, 2643235 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:10,445][gensim.models.word2vec][INFO] - EPOCH 34: training on 4367244 raw words (4359560 effective words) took 1.6s, 2653193 effective words/s
[2023-02-07 14:28:11,451][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 58.86% examples, 2615259 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:12,099][gensim.models.word2vec][INFO] - EPOCH 35: training on 4367244 raw words (4359560 effective words) took 1.7s, 2637986 effective words/s
[2023-02-07 14:28:13,107][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 60.02% examples, 2646064 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:13,735][gensim.models.word2vec][INFO] - EPOCH 36: training on 4367244 raw words (4359560 effective words) took 1.6s, 2668309 effective words/s
[2023-02-07 14:28:14,739][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 58.86% examples, 2618071 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:15,384][gensim.models.word2vec][INFO] - EPOCH 37: training on 4367244 raw words (4359560 effective words) took 1.6s, 2645357 effective words/s
[2023-02-07 14:28:16,393][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 60.82% examples, 2676889 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:16,996][gensim.models.word2vec][INFO] - EPOCH 38: training on 4367244 raw words (4359560 effective words) took 1.6s, 2705859 effective words/s
[2023-02-07 14:28:18,006][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 60.82% examples, 2671621 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:18,616][gensim.models.word2vec][INFO] - EPOCH 39: training on 4367244 raw words (4359560 effective words) took 1.6s, 2693557 effective words/s
[2023-02-07 14:28:19,626][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 60.02% examples, 2642359 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:20,258][gensim.models.word2vec][INFO] - EPOCH 40: training on 4367244 raw words (4359560 effective words) took 1.6s, 2657774 effective words/s
[2023-02-07 14:28:21,266][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 59.96% examples, 2647101 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:21,888][gensim.models.word2vec][INFO] - EPOCH 41: training on 4367244 raw words (4359560 effective words) took 1.6s, 2677407 effective words/s
[2023-02-07 14:28:22,890][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 60.42% examples, 2678826 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:23,512][gensim.models.word2vec][INFO] - EPOCH 42: training on 4367244 raw words (4359560 effective words) took 1.6s, 2687821 effective words/s
[2023-02-07 14:28:24,516][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 60.82% examples, 2687228 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:25,119][gensim.models.word2vec][INFO] - EPOCH 43: training on 4367244 raw words (4359560 effective words) took 1.6s, 2713737 effective words/s
[2023-02-07 14:28:26,130][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 60.82% examples, 2671656 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:26,745][gensim.models.word2vec][INFO] - EPOCH 44: training on 4367244 raw words (4359560 effective words) took 1.6s, 2684868 effective words/s
[2023-02-07 14:28:27,752][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 60.02% examples, 2647322 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:28,394][gensim.models.word2vec][INFO] - EPOCH 45: training on 4367244 raw words (4359560 effective words) took 1.6s, 2646713 effective words/s
[2023-02-07 14:28:29,397][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 58.86% examples, 2620032 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:30,046][gensim.models.word2vec][INFO] - EPOCH 46: training on 4367244 raw words (4359560 effective words) took 1.7s, 2640592 effective words/s
[2023-02-07 14:28:31,048][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 59.38% examples, 2641682 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:31,683][gensim.models.word2vec][INFO] - EPOCH 47: training on 4367244 raw words (4359560 effective words) took 1.6s, 2664810 effective words/s
[2023-02-07 14:28:32,688][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 59.96% examples, 2654667 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:33,325][gensim.models.word2vec][INFO] - EPOCH 48: training on 4367244 raw words (4359560 effective words) took 1.6s, 2657450 effective words/s
[2023-02-07 14:28:34,329][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 59.38% examples, 2638597 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:28:34,975][gensim.models.word2vec][INFO] - EPOCH 49: training on 4367244 raw words (4359560 effective words) took 1.6s, 2645195 effective words/s
[2023-02-07 14:28:34,975][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 218362200 raw words (217978000 effective words) took 82.3s, 2647989 effective words/s', 'datetime': '2023-02-07T14:28:34.975794', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:28:34.976 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:28:41,578][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142646-27120c3w/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:28:41.577999', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:28:41,579][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:28:41,619][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142646-27120c3w/files/../tmp/embedding_model.pt
2023-02-07 14:28:41.620 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:28:42.772 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:28:43.204 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:28:43.884 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.004528438118786, 'test_mae': 1.0396569423960305, 'test_r2': 0.05173819823075643}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.77
wandb: percentage 0.28536
wandb:   test_mae 1.03966
wandb:   test_mse 2.00453
wandb:    test_r2 0.05174
wandb: 
wandb: üöÄ View run charmed-sweep-81 at: https://wandb.ai/xiaoqiz/mof2vec/runs/27120c3w
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_142646-27120c3w/logs
wandb: Agent Starting Run: d37owek5 with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 920
wandb: 	model.gensim.alpha: 0.00048058974745824657
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 45
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.2065501508624689
wandb: 	model.gensim.vector_size: 162
wandb: 	model.gensim.window: 16
wandb: 	model.sklearn.learning_rate: 0.02875573529247948
wandb: 	model.sklearn.max_depth: 20
wandb: 	model.sklearn.min_child_weight: 0.01471197729582682
wandb: 	model.sklearn.n_estimators: 2103
wandb: 	model.sklearn.num_leaves: 431
wandb: 	model.sklearn.reg_alpha: 0.16302878930435172
wandb: 	model.sklearn.reg_lambda: 0.02987521661917712
wandb: 	model.sklearn.subsample: 0.3073924152839574
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142856-d37owek5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-82
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/d37owek5
2023-02-07 14:29:04.586 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 14:29:04.586 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 920 for sweep.
2023-02-07 14:29:04.586 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.00048058974745824657 for sweep.
2023-02-07 14:29:04.587 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:29:04.587 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 45 for sweep.
2023-02-07 14:29:04.587 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 14:29:04.587 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.2065501508624689 for sweep.
2023-02-07 14:29:04.588 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 162 for sweep.
2023-02-07 14:29:04.588 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 16 for sweep.
2023-02-07 14:29:04.588 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.02875573529247948 for sweep.
2023-02-07 14:29:04.588 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 20 for sweep.
2023-02-07 14:29:04.589 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.01471197729582682 for sweep.
2023-02-07 14:29:04.589 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2103 for sweep.
2023-02-07 14:29:04.589 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 431 for sweep.
2023-02-07 14:29:04.589 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.16302878930435172 for sweep.
2023-02-07 14:29:04.590 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.02987521661917712 for sweep.
2023-02-07 14:29:04.590 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3073924152839574 for sweep.
2023-02-07 14:29:04.590 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:29:04.596 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142856-d37owek5/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 920, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 162, 'window': 16, 'min_count': 6, 'dm': 0, 'sample': 0.2065501508624689, 'workers': 4, 'alpha': 0.00048058974745824657, 'epochs': 45}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2103, 'max_depth': 20, 'num_leaves': 431, 'reg_alpha': 0.16302878930435172, 'reg_lambda': 0.02987521661917712, 'subsample': 0.3073924152839574, 'min_child_weight': 0.01471197729582682, 'n_jobs': 4, 'learning_rate': 0.02875573529247948}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 29/3257 [00:00<00:11, 282.74it/s]  2%|‚ñè         | 58/3257 [00:00<00:11, 281.08it/s]  3%|‚ñé         | 90/3257 [00:00<00:10, 292.36it/s]  4%|‚ñé         | 120/3257 [00:00<00:11, 283.00it/s]  5%|‚ñç         | 151/3257 [00:00<00:10, 289.53it/s]  6%|‚ñå         | 180/3257 [00:00<00:10, 285.24it/s]  7%|‚ñã         | 215/3257 [00:00<00:09, 305.61it/s]  8%|‚ñä         | 249/3257 [00:00<00:09, 313.42it/s]  9%|‚ñä         | 284/3257 [00:00<00:09, 322.47it/s] 10%|‚ñâ         | 317/3257 [00:01<00:09, 317.91it/s] 11%|‚ñà         | 349/3257 [00:01<00:09, 313.98it/s] 12%|‚ñà‚ñè        | 381/3257 [00:01<00:09, 309.11it/s] 13%|‚ñà‚ñé        | 413/3257 [00:01<00:09, 311.52it/s] 14%|‚ñà‚ñé        | 445/3257 [00:01<00:09, 285.03it/s] 15%|‚ñà‚ñç        | 477/3257 [00:01<00:09, 293.28it/s] 16%|‚ñà‚ñå        | 510/3257 [00:01<00:09, 301.00it/s] 17%|‚ñà‚ñã        | 542/3257 [00:01<00:08, 303.90it/s] 18%|‚ñà‚ñä        | 573/3257 [00:01<00:09, 276.62it/s] 19%|‚ñà‚ñä        | 605/3257 [00:02<00:09, 288.15it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:02<00:08, 294.93it/s] 20%|‚ñà‚ñà        | 667/3257 [00:02<00:09, 282.89it/s] 21%|‚ñà‚ñà‚ñè       | 696/3257 [00:02<00:09, 280.83it/s] 22%|‚ñà‚ñà‚ñè       | 726/3257 [00:02<00:08, 284.37it/s] 23%|‚ñà‚ñà‚ñé       | 755/3257 [00:02<00:08, 284.28it/s] 24%|‚ñà‚ñà‚ñç       | 784/3257 [00:02<00:08, 283.92it/s] 25%|‚ñà‚ñà‚ñå       | 815/3257 [00:02<00:08, 290.94it/s] 26%|‚ñà‚ñà‚ñå       | 845/3257 [00:02<00:08, 276.88it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:02<00:08, 279.25it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:03<00:08, 286.75it/s] 29%|‚ñà‚ñà‚ñä       | 935/3257 [00:03<00:08, 284.90it/s] 30%|‚ñà‚ñà‚ñâ       | 967/3257 [00:03<00:07, 290.94it/s] 31%|‚ñà‚ñà‚ñà       | 997/3257 [00:03<00:07, 286.47it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:03<00:07, 280.20it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1056/3257 [00:03<00:07, 277.92it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1086/3257 [00:03<00:07, 284.08it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1116/3257 [00:03<00:07, 287.64it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1145/3257 [00:03<00:07, 276.06it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1173/3257 [00:04<00:10, 192.31it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1196/3257 [00:04<00:10, 196.67it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1223/3257 [00:04<00:09, 213.86it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1255/3257 [00:04<00:08, 237.37it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1281/3257 [00:04<00:08, 240.54it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1309/3257 [00:04<00:07, 248.62it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1342/3257 [00:04<00:07, 267.38it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:04<00:07, 264.77it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1399/3257 [00:05<00:06, 271.47it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:05<00:06, 285.51it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1466/3257 [00:05<00:05, 300.09it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1501/3257 [00:05<00:05, 312.14it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1533/3257 [00:05<00:05, 293.06it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1563/3257 [00:05<00:05, 288.49it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1593/3257 [00:05<00:05, 289.51it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1624/3257 [00:05<00:05, 293.92it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1654/3257 [00:05<00:05, 283.72it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1683/3257 [00:06<00:05, 277.66it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1714/3257 [00:06<00:05, 285.53it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1743/3257 [00:06<00:05, 265.00it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1776/3257 [00:06<00:05, 279.62it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1806/3257 [00:06<00:05, 283.74it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1835/3257 [00:06<00:05, 281.09it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1868/3257 [00:06<00:04, 294.74it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1898/3257 [00:06<00:04, 290.47it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1928/3257 [00:06<00:04, 290.19it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1966/3257 [00:06<00:04, 315.67it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1998/3257 [00:07<00:04, 312.70it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2031/3257 [00:07<00:03, 316.65it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2063/3257 [00:07<00:04, 289.94it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2095/3257 [00:07<00:03, 296.47it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2126/3257 [00:07<00:03, 284.11it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2155/3257 [00:07<00:03, 283.47it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2186/3257 [00:07<00:03, 288.12it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2217/3257 [00:07<00:03, 291.67it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2247/3257 [00:07<00:03, 288.51it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2276/3257 [00:08<00:03, 281.75it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2310/3257 [00:08<00:03, 296.48it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2340/3257 [00:08<00:04, 205.45it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2373/3257 [00:08<00:03, 231.65it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2406/3257 [00:08<00:03, 252.56it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2435/3257 [00:08<00:03, 260.88it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2467/3257 [00:08<00:02, 276.10it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2502/3257 [00:08<00:02, 294.61it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2537/3257 [00:09<00:02, 307.03it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2569/3257 [00:09<00:02, 290.45it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2599/3257 [00:09<00:02, 291.65it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2637/3257 [00:09<00:01, 315.09it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:09<00:01, 304.70it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2701/3257 [00:09<00:01, 288.91it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:09<00:01, 293.06it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2766/3257 [00:09<00:01, 305.94it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2797/3257 [00:09<00:01, 306.37it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2828/3257 [00:10<00:01, 296.14it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2862/3257 [00:10<00:01, 307.24it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2894/3257 [00:10<00:01, 307.01it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:10<00:01, 311.81it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2960/3257 [00:10<00:00, 299.22it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2991/3257 [00:10<00:00, 296.42it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3024/3257 [00:10<00:00, 305.16it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3061/3257 [00:10<00:00, 322.58it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3094/3257 [00:10<00:00, 321.64it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3129/3257 [00:10<00:00, 327.47it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3162/3257 [00:11<00:00, 318.93it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3194/3257 [00:11<00:00, 311.19it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3226/3257 [00:11<00:00, 300.41it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 286.36it/s]
2023-02-07 14:29:16.318 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:29:16,319][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d162,n5,mc6,s0.20655,t4>', 'datetime': '2023-02-07T14:29:16.319340', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:29:16,319][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:29:16,319][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:29:16,571][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 14:29:16,571][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:29:16,581][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 3569 unique words (53.57% of original 6662, drops 3093)', 'datetime': '2023-02-07T14:29:16.581435', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:29:16,581][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 2903654 word corpus (99.73% of original 2911496, drops 7842)', 'datetime': '2023-02-07T14:29:16.581775', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:29:16,594][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 14:29:16,594][gensim.models.word2vec][INFO] - sample=0.20655 downsamples 0 most-common words
[2023-02-07 14:29:16,595][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2903654 word corpus (100.0%% of prior 2903654)', 'datetime': '2023-02-07T14:29:16.595051', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:29:16,615][gensim.models.word2vec][INFO] - estimated required memory for 3569 words and 162 dimensions: 9171860 bytes
[2023-02-07 14:29:16,616][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:29:16,620][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 3569 vocabulary and 162 features, using sg=1 hs=0 sample=0.2065501508624689 negative=5 window=16 shrink_windows=True', 'datetime': '2023-02-07T14:29:16.620252', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:29:17,611][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2906911 effective words) took 1.0s, 2939733 effective words/s
[2023-02-07 14:29:18,595][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2906911 effective words) took 1.0s, 2956791 effective words/s
[2023-02-07 14:29:19,586][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2906911 effective words) took 1.0s, 2938938 effective words/s
[2023-02-07 14:29:20,575][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2906911 effective words) took 1.0s, 2943381 effective words/s
[2023-02-07 14:29:21,570][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2906911 effective words) took 1.0s, 2927593 effective words/s
[2023-02-07 14:29:22,563][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2906911 effective words) took 1.0s, 2931638 effective words/s
[2023-02-07 14:29:23,564][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 100.00% examples, 2906305 words/s, in_qsize 0, out_qsize 1
[2023-02-07 14:29:23,565][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2906911 effective words) took 1.0s, 2905348 effective words/s
[2023-02-07 14:29:24,563][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2906911 effective words) took 1.0s, 2915429 effective words/s
[2023-02-07 14:29:25,567][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 98.93% examples, 2874362 words/s, in_qsize 3, out_qsize 1
[2023-02-07 14:29:25,570][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2906911 effective words) took 1.0s, 2890413 effective words/s
[2023-02-07 14:29:26,572][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 99.82% examples, 2897743 words/s, in_qsize 1, out_qsize 1
[2023-02-07 14:29:26,572][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2906911 effective words) took 1.0s, 2903668 effective words/s
[2023-02-07 14:29:27,565][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2906911 effective words) took 1.0s, 2932088 effective words/s
[2023-02-07 14:29:28,552][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2906911 effective words) took 1.0s, 2948826 effective words/s
[2023-02-07 14:29:29,542][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2906911 effective words) took 1.0s, 2942055 effective words/s
[2023-02-07 14:29:30,537][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2906911 effective words) took 1.0s, 2928479 effective words/s
[2023-02-07 14:29:31,525][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2906911 effective words) took 1.0s, 2947123 effective words/s
[2023-02-07 14:29:32,499][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2906911 effective words) took 1.0s, 2990451 effective words/s
[2023-02-07 14:29:33,477][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2906911 effective words) took 1.0s, 2976653 effective words/s
[2023-02-07 14:29:34,482][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 96.16% examples, 2787540 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:34,518][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2906911 effective words) took 1.0s, 2797095 effective words/s
[2023-02-07 14:29:35,526][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 96.16% examples, 2780537 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:35,559][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2906911 effective words) took 1.0s, 2796986 effective words/s
[2023-02-07 14:29:36,561][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 96.16% examples, 2796813 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:36,596][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2906911 effective words) took 1.0s, 2808498 effective words/s
[2023-02-07 14:29:37,598][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 97.08% examples, 2823357 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:37,625][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2906911 effective words) took 1.0s, 2828406 effective words/s
[2023-02-07 14:29:38,627][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 95.89% examples, 2784251 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:38,668][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2906911 effective words) took 1.0s, 2789588 effective words/s
[2023-02-07 14:29:39,672][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 94.69% examples, 2751641 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:39,721][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2906911 effective words) took 1.1s, 2764324 effective words/s
[2023-02-07 14:29:40,726][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 97.61% examples, 2835055 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:40,743][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2906911 effective words) took 1.0s, 2848072 effective words/s
[2023-02-07 14:29:41,745][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 95.89% examples, 2786672 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:41,785][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2906911 effective words) took 1.0s, 2795259 effective words/s
[2023-02-07 14:29:42,788][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 96.47% examples, 2802719 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:42,818][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2906911 effective words) took 1.0s, 2817999 effective words/s
[2023-02-07 14:29:43,825][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 97.39% examples, 2816377 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:43,844][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2906911 effective words) took 1.0s, 2834482 effective words/s
[2023-02-07 14:29:44,847][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 97.08% examples, 2821460 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:44,872][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2906911 effective words) took 1.0s, 2833708 effective words/s
[2023-02-07 14:29:45,875][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 97.08% examples, 2820920 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:45,900][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2906911 effective words) took 1.0s, 2831381 effective words/s
[2023-02-07 14:29:46,904][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 97.39% examples, 2828024 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:46,924][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2906911 effective words) took 1.0s, 2842998 effective words/s
[2023-02-07 14:29:47,926][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 97.08% examples, 2822669 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:47,951][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2906911 effective words) took 1.0s, 2835360 effective words/s
[2023-02-07 14:29:48,955][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 97.08% examples, 2818833 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:48,981][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2906911 effective words) took 1.0s, 2825382 effective words/s
[2023-02-07 14:29:49,986][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 97.39% examples, 2826555 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:50,005][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2906911 effective words) took 1.0s, 2844362 effective words/s
[2023-02-07 14:29:51,008][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 98.13% examples, 2852357 words/s, in_qsize 6, out_qsize 0
[2023-02-07 14:29:51,020][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2906911 effective words) took 1.0s, 2869867 effective words/s
[2023-02-07 14:29:52,022][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 96.47% examples, 2802995 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:52,055][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2906911 effective words) took 1.0s, 2812503 effective words/s
[2023-02-07 14:29:53,057][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 98.43% examples, 2862550 words/s, in_qsize 5, out_qsize 0
[2023-02-07 14:29:53,070][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2906911 effective words) took 1.0s, 2868871 effective words/s
[2023-02-07 14:29:54,072][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 97.08% examples, 2824055 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:54,100][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2906911 effective words) took 1.0s, 2827549 effective words/s
[2023-02-07 14:29:55,102][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 97.08% examples, 2823307 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:55,129][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2906911 effective words) took 1.0s, 2829431 effective words/s
[2023-02-07 14:29:56,131][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 96.71% examples, 2814659 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:56,158][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2906911 effective words) took 1.0s, 2828286 effective words/s
[2023-02-07 14:29:57,163][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 96.71% examples, 2806261 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:57,189][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2906911 effective words) took 1.0s, 2823954 effective words/s
[2023-02-07 14:29:58,191][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 98.62% examples, 2867853 words/s, in_qsize 4, out_qsize 0
[2023-02-07 14:29:58,203][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2906911 effective words) took 1.0s, 2871713 effective words/s
[2023-02-07 14:29:59,205][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 97.08% examples, 2822609 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:29:59,230][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2906911 effective words) took 1.0s, 2833734 effective words/s
[2023-02-07 14:30:00,232][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 98.13% examples, 2851163 words/s, in_qsize 6, out_qsize 0
[2023-02-07 14:30:00,246][gensim.models.word2vec][INFO] - EPOCH 42: training on 2911496 raw words (2906911 effective words) took 1.0s, 2863646 effective words/s
[2023-02-07 14:30:01,249][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 97.61% examples, 2841084 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:30:01,267][gensim.models.word2vec][INFO] - EPOCH 43: training on 2911496 raw words (2906911 effective words) took 1.0s, 2850943 effective words/s
[2023-02-07 14:30:02,269][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 97.08% examples, 2824066 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:30:02,296][gensim.models.word2vec][INFO] - EPOCH 44: training on 2911496 raw words (2906911 effective words) took 1.0s, 2830659 effective words/s
[2023-02-07 14:30:02,296][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 131017320 raw words (130810995 effective words) took 45.7s, 2863882 effective words/s', 'datetime': '2023-02-07T14:30:02.296698', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:30:02.296 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:30:05,210][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142856-d37owek5/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:30:05.210332', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:30:05,211][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:30:05,232][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_142856-d37owek5/files/../tmp/embedding_model.pt
2023-02-07 14:30:05.232 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:30:06.607 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:30:07.118 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:30:08.260 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0363993456017844, 'test_mae': 1.0478409830869342, 'test_r2': 0.036661353433178}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.73
wandb: percentage 0.46427
wandb:   test_mae 1.04784
wandb:   test_mse 2.0364
wandb:    test_r2 0.03666
wandb: 
wandb: üöÄ View run rare-sweep-82 at: https://wandb.ai/xiaoqiz/mof2vec/runs/d37owek5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_142856-d37owek5/logs
wandb: Agent Starting Run: wdxrgrv8 with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 550
wandb: 	model.gensim.alpha: 0.00212436569463137
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 40
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.9580575047758324
wandb: 	model.gensim.vector_size: 126
wandb: 	model.gensim.window: 9
wandb: 	model.sklearn.learning_rate: 0.09745110463928554
wandb: 	model.sklearn.max_depth: 43
wandb: 	model.sklearn.min_child_weight: 0.032286000287799536
wandb: 	model.sklearn.n_estimators: 4063
wandb: 	model.sklearn.num_leaves: 413
wandb: 	model.sklearn.reg_alpha: 0.019392417756431416
wandb: 	model.sklearn.reg_lambda: 0.9598889840249524
wandb: 	model.sklearn.subsample: 0.26906864735310076
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143021-wdxrgrv8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-83
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/wdxrgrv8
2023-02-07 14:30:36.959 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 14:30:36.960 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 550 for sweep.
2023-02-07 14:30:36.960 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.00212436569463137 for sweep.
2023-02-07 14:30:36.960 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:30:36.960 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 40 for sweep.
2023-02-07 14:30:36.961 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 14:30:36.961 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.9580575047758324 for sweep.
2023-02-07 14:30:36.961 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 126 for sweep.
2023-02-07 14:30:36.961 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 9 for sweep.
2023-02-07 14:30:36.961 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.09745110463928554 for sweep.
2023-02-07 14:30:36.962 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 43 for sweep.
2023-02-07 14:30:36.962 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.032286000287799536 for sweep.
2023-02-07 14:30:36.962 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4063 for sweep.
2023-02-07 14:30:36.962 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 413 for sweep.
2023-02-07 14:30:36.962 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.019392417756431416 for sweep.
2023-02-07 14:30:36.963 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.9598889840249524 for sweep.
2023-02-07 14:30:36.963 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.26906864735310076 for sweep.
2023-02-07 14:30:36.963 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:30:37.002 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143021-wdxrgrv8/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 550, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 126, 'window': 9, 'min_count': 4, 'dm': 0, 'sample': 0.9580575047758324, 'workers': 4, 'alpha': 0.00212436569463137, 'epochs': 40}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4063, 'max_depth': 43, 'num_leaves': 413, 'reg_alpha': 0.019392417756431416, 'reg_lambda': 0.9598889840249524, 'subsample': 0.26906864735310076, 'min_child_weight': 0.032286000287799536, 'n_jobs': 4, 'learning_rate': 0.09745110463928554}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 29/3257 [00:00<00:11, 280.36it/s]  2%|‚ñè         | 58/3257 [00:00<00:12, 260.94it/s]  3%|‚ñé         | 88/3257 [00:00<00:11, 276.51it/s]  4%|‚ñé         | 116/3257 [00:00<00:12, 261.60it/s]  5%|‚ñç         | 150/3257 [00:00<00:10, 286.48it/s]  5%|‚ñå         | 179/3257 [00:00<00:11, 277.42it/s]  7%|‚ñã         | 212/3257 [00:00<00:10, 292.78it/s]  8%|‚ñä         | 246/3257 [00:00<00:09, 306.66it/s]  9%|‚ñä         | 277/3257 [00:00<00:09, 302.77it/s]  9%|‚ñâ         | 308/3257 [00:01<00:09, 303.72it/s] 10%|‚ñà         | 339/3257 [00:01<00:09, 305.10it/s] 11%|‚ñà‚ñè        | 371/3257 [00:01<00:09, 309.30it/s] 12%|‚ñà‚ñè        | 402/3257 [00:01<00:09, 294.56it/s] 13%|‚ñà‚ñé        | 432/3257 [00:01<00:10, 275.49it/s] 14%|‚ñà‚ñç        | 461/3257 [00:01<00:10, 279.01it/s] 15%|‚ñà‚ñå        | 491/3257 [00:01<00:09, 283.21it/s] 16%|‚ñà‚ñå        | 524/3257 [00:01<00:09, 293.02it/s] 17%|‚ñà‚ñã        | 554/3257 [00:01<00:09, 294.10it/s] 18%|‚ñà‚ñä        | 584/3257 [00:02<00:13, 197.42it/s] 19%|‚ñà‚ñâ        | 619/3257 [00:02<00:11, 229.43it/s] 20%|‚ñà‚ñâ        | 647/3257 [00:02<00:11, 235.42it/s] 21%|‚ñà‚ñà        | 675/3257 [00:02<00:10, 246.01it/s] 22%|‚ñà‚ñà‚ñè       | 703/3257 [00:02<00:10, 253.99it/s] 22%|‚ñà‚ñà‚ñè       | 731/3257 [00:02<00:09, 255.49it/s] 23%|‚ñà‚ñà‚ñé       | 758/3257 [00:02<00:09, 257.63it/s] 24%|‚ñà‚ñà‚ñç       | 786/3257 [00:02<00:09, 262.79it/s] 25%|‚ñà‚ñà‚ñå       | 816/3257 [00:03<00:08, 272.09it/s] 26%|‚ñà‚ñà‚ñå       | 844/3257 [00:03<00:09, 257.93it/s] 27%|‚ñà‚ñà‚ñã       | 872/3257 [00:03<00:09, 263.58it/s] 28%|‚ñà‚ñà‚ñä       | 902/3257 [00:03<00:08, 271.56it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:03<00:08, 274.57it/s] 29%|‚ñà‚ñà‚ñâ       | 960/3257 [00:03<00:08, 278.43it/s] 30%|‚ñà‚ñà‚ñà       | 988/3257 [00:03<00:08, 270.37it/s] 31%|‚ñà‚ñà‚ñà       | 1016/3257 [00:03<00:08, 265.40it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1043/3257 [00:03<00:08, 258.50it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1075/3257 [00:03<00:07, 275.35it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1103/3257 [00:04<00:07, 271.16it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1131/3257 [00:04<00:07, 268.33it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1161/3257 [00:04<00:07, 277.05it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1189/3257 [00:04<00:08, 257.42it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1216/3257 [00:04<00:07, 256.79it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1246/3257 [00:04<00:07, 267.06it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:04<00:07, 264.96it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1302/3257 [00:04<00:07, 258.21it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1331/3257 [00:04<00:07, 266.39it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1360/3257 [00:05<00:06, 271.32it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1388/3257 [00:05<00:07, 264.80it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:05<00:06, 284.65it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1456/3257 [00:05<00:06, 296.98it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1487/3257 [00:05<00:05, 298.51it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:05<00:05, 300.89it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1550/3257 [00:05<00:05, 286.48it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1579/3257 [00:05<00:05, 282.55it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1613/3257 [00:05<00:05, 298.32it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1643/3257 [00:06<00:05, 289.45it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1673/3257 [00:06<00:05, 283.42it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1703/3257 [00:06<00:05, 286.85it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1732/3257 [00:06<00:07, 192.53it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1763/3257 [00:06<00:06, 215.15it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1795/3257 [00:06<00:06, 238.46it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1823/3257 [00:06<00:05, 248.25it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1852/3257 [00:06<00:05, 257.61it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1884/3257 [00:07<00:05, 273.02it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:07<00:04, 277.91it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1951/3257 [00:07<00:04, 297.06it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1982/3257 [00:07<00:04, 295.97it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2013/3257 [00:07<00:04, 296.30it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2044/3257 [00:07<00:04, 291.06it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2074/3257 [00:07<00:04, 280.59it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2103/3257 [00:07<00:04, 275.50it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2131/3257 [00:07<00:04, 270.04it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2160/3257 [00:07<00:04, 272.76it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2189/3257 [00:08<00:03, 276.50it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2218/3257 [00:08<00:03, 279.26it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2246/3257 [00:08<00:03, 274.59it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2274/3257 [00:08<00:03, 267.24it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2306/3257 [00:08<00:03, 280.34it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2341/3257 [00:08<00:03, 298.86it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2371/3257 [00:08<00:02, 297.88it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2403/3257 [00:08<00:02, 303.11it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2434/3257 [00:08<00:02, 280.87it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2465/3257 [00:09<00:02, 288.58it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2498/3257 [00:09<00:02, 298.45it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2533/3257 [00:09<00:02, 313.19it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2565/3257 [00:09<00:02, 294.06it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2595/3257 [00:09<00:02, 284.06it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2632/3257 [00:09<00:02, 306.44it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2664/3257 [00:09<00:02, 293.26it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2694/3257 [00:09<00:01, 292.35it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2724/3257 [00:09<00:01, 277.08it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2758/3257 [00:10<00:01, 293.15it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2789/3257 [00:10<00:01, 297.87it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2820/3257 [00:10<00:01, 291.78it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2850/3257 [00:10<00:01, 289.72it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:10<00:01, 308.26it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2917/3257 [00:10<00:01, 296.01it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2947/3257 [00:10<00:01, 278.73it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2976/3257 [00:10<00:01, 270.09it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3004/3257 [00:10<00:00, 272.48it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3036/3257 [00:11<00:00, 285.45it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3071/3257 [00:11<00:00, 301.96it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3104/3257 [00:11<00:00, 308.81it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3136/3257 [00:11<00:00, 306.21it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3167/3257 [00:11<00:00, 296.51it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3197/3257 [00:11<00:00, 187.82it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3225/3257 [00:11<00:00, 206.29it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 272.98it/s]
2023-02-07 14:30:49.294 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:30:49,295][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d126,n5,mc4,s0.958058,t4>', 'datetime': '2023-02-07T14:30:49.295423', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:30:49,295][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:30:49,295][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:30:49,550][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 14:30:49,550][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:30:49,563][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 4674 unique words (70.16% of original 6662, drops 1988)', 'datetime': '2023-02-07T14:30:49.563910', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:30:49,564][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 2908210 word corpus (99.89% of original 2911496, drops 3286)', 'datetime': '2023-02-07T14:30:49.564281', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:30:49,582][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 14:30:49,582][gensim.models.word2vec][INFO] - sample=0.958058 downsamples 0 most-common words
[2023-02-07 14:30:49,583][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2908210 word corpus (100.0%% of prior 2908210)', 'datetime': '2023-02-07T14:30:49.583017', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:30:49,611][gensim.models.word2vec][INFO] - estimated required memory for 4674 words and 126 dimensions: 9341320 bytes
[2023-02-07 14:30:49,612][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:30:49,616][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 4674 vocabulary and 126 features, using sg=1 hs=0 sample=0.9580575047758324 negative=5 window=9 shrink_windows=True', 'datetime': '2023-02-07T14:30:49.616143', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:30:50,617][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 87.10% examples, 2560047 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:30:50,747][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2911467 effective words) took 1.1s, 2577206 effective words/s
[2023-02-07 14:30:51,752][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 93.00% examples, 2718094 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:30:51,815][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2911467 effective words) took 1.1s, 2730295 effective words/s
[2023-02-07 14:30:52,819][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 94.69% examples, 2755880 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:30:52,868][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2911467 effective words) took 1.1s, 2769335 effective words/s
[2023-02-07 14:30:53,869][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 95.46% examples, 2782837 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:30:53,910][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2911467 effective words) took 1.0s, 2797119 effective words/s
[2023-02-07 14:30:54,913][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 96.47% examples, 2806807 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:30:54,944][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2911467 effective words) took 1.0s, 2818535 effective words/s
[2023-02-07 14:30:55,946][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 95.12% examples, 2771566 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:30:55,991][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2911467 effective words) took 1.0s, 2786341 effective words/s
[2023-02-07 14:30:56,995][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 95.03% examples, 2765160 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:30:57,042][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2911467 effective words) took 1.0s, 2772916 effective words/s
[2023-02-07 14:30:58,050][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 93.49% examples, 2718348 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:30:58,108][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2911467 effective words) took 1.1s, 2735783 effective words/s
[2023-02-07 14:30:59,113][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 97.08% examples, 2820668 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:30:59,138][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2911467 effective words) took 1.0s, 2830784 effective words/s
[2023-02-07 14:31:00,145][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 97.39% examples, 2823742 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:31:00,164][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2911467 effective words) took 1.0s, 2840210 effective words/s
[2023-02-07 14:31:01,166][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 97.61% examples, 2845990 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:31:01,182][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2911467 effective words) took 1.0s, 2863531 effective words/s
[2023-02-07 14:31:02,184][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 97.91% examples, 2856385 words/s, in_qsize 6, out_qsize 0
[2023-02-07 14:31:02,198][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2911467 effective words) took 1.0s, 2870225 effective words/s
[2023-02-07 14:31:03,200][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 99.36% examples, 2891884 words/s, in_qsize 2, out_qsize 1
[2023-02-07 14:31:03,201][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2911467 effective words) took 1.0s, 2906843 effective words/s
[2023-02-07 14:31:04,202][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 100.00% examples, 2909906 words/s, in_qsize 0, out_qsize 1
[2023-02-07 14:31:04,203][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2911467 effective words) took 1.0s, 2908840 effective words/s
[2023-02-07 14:31:05,205][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 97.91% examples, 2854195 words/s, in_qsize 6, out_qsize 0
[2023-02-07 14:31:05,219][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2911467 effective words) took 1.0s, 2867793 effective words/s
[2023-02-07 14:31:06,221][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 98.43% examples, 2865499 words/s, in_qsize 5, out_qsize 0
[2023-02-07 14:31:06,236][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2911467 effective words) took 1.0s, 2867186 effective words/s
[2023-02-07 14:31:07,237][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 97.61% examples, 2848616 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:31:07,255][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2911467 effective words) took 1.0s, 2860515 effective words/s
[2023-02-07 14:31:08,259][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 97.61% examples, 2840813 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:31:08,275][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2911467 effective words) took 1.0s, 2857940 effective words/s
[2023-02-07 14:31:09,276][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 96.47% examples, 2810314 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:31:09,310][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2911467 effective words) took 1.0s, 2816077 effective words/s
[2023-02-07 14:31:10,313][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 97.61% examples, 2845184 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:31:10,330][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2911467 effective words) took 1.0s, 2857592 effective words/s
[2023-02-07 14:31:11,332][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 100.00% examples, 2910975 words/s, in_qsize 0, out_qsize 1
[2023-02-07 14:31:11,333][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2911467 effective words) took 1.0s, 2907684 effective words/s
[2023-02-07 14:31:12,323][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2911467 effective words) took 1.0s, 2943617 effective words/s
[2023-02-07 14:31:13,318][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2911467 effective words) took 1.0s, 2929455 effective words/s
[2023-02-07 14:31:14,313][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2911467 effective words) took 1.0s, 2930167 effective words/s
[2023-02-07 14:31:15,308][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2911467 effective words) took 1.0s, 2928810 effective words/s
[2023-02-07 14:31:16,313][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 98.93% examples, 2874792 words/s, in_qsize 3, out_qsize 1
[2023-02-07 14:31:16,317][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2911467 effective words) took 1.0s, 2890124 effective words/s
[2023-02-07 14:31:17,319][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 99.82% examples, 2900556 words/s, in_qsize 1, out_qsize 1
[2023-02-07 14:31:17,320][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2911467 effective words) took 1.0s, 2906730 effective words/s
[2023-02-07 14:31:18,320][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2911467 effective words) took 1.0s, 2914662 effective words/s
[2023-02-07 14:31:19,321][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2911467 effective words) took 1.0s, 2912349 effective words/s
[2023-02-07 14:31:20,312][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2911467 effective words) took 1.0s, 2940151 effective words/s
[2023-02-07 14:31:21,317][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 98.93% examples, 2876818 words/s, in_qsize 3, out_qsize 1
[2023-02-07 14:31:21,321][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2911467 effective words) took 1.0s, 2891317 effective words/s
[2023-02-07 14:31:22,323][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 99.36% examples, 2889763 words/s, in_qsize 2, out_qsize 1
[2023-02-07 14:31:22,324][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2911467 effective words) took 1.0s, 2903765 effective words/s
[2023-02-07 14:31:23,328][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 98.62% examples, 2868993 words/s, in_qsize 4, out_qsize 0
[2023-02-07 14:31:23,337][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2911467 effective words) took 1.0s, 2877672 effective words/s
[2023-02-07 14:31:24,329][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2911467 effective words) took 1.0s, 2939361 effective words/s
[2023-02-07 14:31:25,300][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2911467 effective words) took 1.0s, 3002028 effective words/s
[2023-02-07 14:31:26,269][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2911467 effective words) took 1.0s, 3009917 effective words/s
[2023-02-07 14:31:27,237][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2911467 effective words) took 1.0s, 3011015 effective words/s
[2023-02-07 14:31:28,202][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2911467 effective words) took 1.0s, 3020649 effective words/s
[2023-02-07 14:31:29,186][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2911467 effective words) took 1.0s, 2964250 effective words/s
[2023-02-07 14:31:30,165][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2911467 effective words) took 1.0s, 2976609 effective words/s
[2023-02-07 14:31:30,165][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 116459840 raw words (116458680 effective words) took 40.5s, 2872021 effective words/s', 'datetime': '2023-02-07T14:31:30.165891', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:31:30.166 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:31:33,363][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143021-wdxrgrv8/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:31:33.363372', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:31:33,364][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:31:33,387][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143021-wdxrgrv8/files/../tmp/embedding_model.pt
2023-02-07 14:31:33.387 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:31:34.591 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:31:35.041 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:31:35.899 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.022604501416405, 'test_mae': 1.0428482072165042, 'test_r2': 0.04318713952510811}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.029 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: | 0.032 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: / 0.032 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: - 0.032 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: \ 0.032 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: | 0.032 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: / 0.032 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: - 0.032 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: \ 0.032 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: | 0.032 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: / 0.032 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: - 0.032 MB of 0.034 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.89
wandb: percentage 0.29841
wandb:   test_mae 1.04285
wandb:   test_mse 2.0226
wandb:    test_r2 0.04319
wandb: 
wandb: üöÄ View run snowy-sweep-83 at: https://wandb.ai/xiaoqiz/mof2vec/runs/wdxrgrv8
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_143021-wdxrgrv8/logs
wandb: Agent Starting Run: aa5v5nne with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 476
wandb: 	model.gensim.alpha: 0.0019297704550856583
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 76
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.4126083577398292
wandb: 	model.gensim.vector_size: 10
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.2518019153194701
wandb: 	model.sklearn.max_depth: 10
wandb: 	model.sklearn.min_child_weight: 0.07259509068697612
wandb: 	model.sklearn.n_estimators: 3289
wandb: 	model.sklearn.num_leaves: 469
wandb: 	model.sklearn.reg_alpha: 0.2431826943962311
wandb: 	model.sklearn.reg_lambda: 0.2922838328391486
wandb: 	model.sklearn.subsample: 0.440598093058574
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143155-aa5v5nne
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-84
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/aa5v5nne
2023-02-07 14:32:05.127 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 14:32:05.127 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 476 for sweep.
2023-02-07 14:32:05.128 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0019297704550856583 for sweep.
2023-02-07 14:32:05.128 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:32:05.128 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 76 for sweep.
2023-02-07 14:32:05.128 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 14:32:05.129 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4126083577398292 for sweep.
2023-02-07 14:32:05.129 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 10 for sweep.
2023-02-07 14:32:05.129 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 14:32:05.129 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.2518019153194701 for sweep.
2023-02-07 14:32:05.129 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 10 for sweep.
2023-02-07 14:32:05.130 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.07259509068697612 for sweep.
2023-02-07 14:32:05.130 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3289 for sweep.
2023-02-07 14:32:05.130 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 469 for sweep.
2023-02-07 14:32:05.132 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.2431826943962311 for sweep.
2023-02-07 14:32:05.132 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.2922838328391486 for sweep.
2023-02-07 14:32:05.132 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.440598093058574 for sweep.
2023-02-07 14:32:05.132 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:32:05.138 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143155-aa5v5nne/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 476, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 10, 'window': 3, 'min_count': 2, 'dm': 0, 'sample': 0.4126083577398292, 'workers': 4, 'alpha': 0.0019297704550856583, 'epochs': 76}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3289, 'max_depth': 10, 'num_leaves': 469, 'reg_alpha': 0.2431826943962311, 'reg_lambda': 0.2922838328391486, 'subsample': 0.440598093058574, 'min_child_weight': 0.07259509068697612, 'n_jobs': 4, 'learning_rate': 0.2518019153194701}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 30/3257 [00:00<00:11, 291.90it/s]  2%|‚ñè         | 61/3257 [00:00<00:10, 300.31it/s]  3%|‚ñé         | 95/3257 [00:00<00:09, 316.43it/s]  4%|‚ñç         | 127/3257 [00:00<00:09, 313.18it/s]  5%|‚ñç         | 162/3257 [00:00<00:09, 321.73it/s]  6%|‚ñå         | 195/3257 [00:00<00:09, 315.82it/s]  7%|‚ñã         | 228/3257 [00:00<00:09, 320.17it/s]  8%|‚ñä         | 261/3257 [00:00<00:09, 307.27it/s]  9%|‚ñâ         | 296/3257 [00:00<00:09, 317.94it/s] 10%|‚ñà         | 328/3257 [00:01<00:09, 316.27it/s] 11%|‚ñà         | 360/3257 [00:01<00:09, 310.30it/s] 12%|‚ñà‚ñè        | 392/3257 [00:01<00:09, 300.13it/s] 13%|‚ñà‚ñé        | 425/3257 [00:01<00:09, 300.23it/s] 14%|‚ñà‚ñç        | 456/3257 [00:01<00:09, 298.84it/s] 15%|‚ñà‚ñå        | 490/3257 [00:01<00:08, 309.13it/s] 16%|‚ñà‚ñå        | 526/3257 [00:01<00:08, 319.17it/s] 17%|‚ñà‚ñã        | 559/3257 [00:01<00:08, 318.16it/s] 18%|‚ñà‚ñä        | 591/3257 [00:01<00:08, 308.91it/s] 19%|‚ñà‚ñâ        | 622/3257 [00:02<00:08, 306.74it/s] 20%|‚ñà‚ñà        | 653/3257 [00:02<00:08, 307.64it/s] 21%|‚ñà‚ñà        | 684/3257 [00:02<00:08, 303.55it/s] 22%|‚ñà‚ñà‚ñè       | 720/3257 [00:02<00:08, 316.05it/s] 23%|‚ñà‚ñà‚ñé       | 752/3257 [00:02<00:07, 314.99it/s] 24%|‚ñà‚ñà‚ñç       | 784/3257 [00:02<00:07, 314.72it/s] 25%|‚ñà‚ñà‚ñå       | 819/3257 [00:02<00:07, 323.82it/s] 26%|‚ñà‚ñà‚ñå       | 852/3257 [00:02<00:07, 314.34it/s] 27%|‚ñà‚ñà‚ñã       | 884/3257 [00:02<00:07, 307.88it/s] 28%|‚ñà‚ñà‚ñä       | 915/3257 [00:02<00:07, 303.09it/s] 29%|‚ñà‚ñà‚ñâ       | 951/3257 [00:03<00:07, 317.94it/s] 30%|‚ñà‚ñà‚ñà       | 983/3257 [00:03<00:07, 317.62it/s] 31%|‚ñà‚ñà‚ñà       | 1015/3257 [00:03<00:07, 311.24it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1047/3257 [00:03<00:07, 303.95it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1081/3257 [00:03<00:06, 311.94it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1116/3257 [00:03<00:06, 321.82it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1149/3257 [00:03<00:06, 313.07it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1182/3257 [00:03<00:06, 316.60it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1214/3257 [00:03<00:06, 303.77it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:04<00:06, 316.37it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1281/3257 [00:04<00:06, 312.98it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1313/3257 [00:04<00:08, 220.57it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:04<00:07, 249.71it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1382/3257 [00:04<00:07, 265.34it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1420/3257 [00:04<00:06, 291.97it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1455/3257 [00:04<00:05, 306.49it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1491/3257 [00:04<00:05, 320.56it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1525/3257 [00:04<00:05, 315.24it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1558/3257 [00:05<00:05, 311.55it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1593/3257 [00:05<00:05, 320.77it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1630/3257 [00:05<00:04, 334.77it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1664/3257 [00:05<00:05, 315.74it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1697/3257 [00:05<00:04, 315.51it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1729/3257 [00:05<00:04, 313.26it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1762/3257 [00:05<00:04, 317.73it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1797/3257 [00:05<00:04, 323.85it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1830/3257 [00:05<00:04, 319.28it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1868/3257 [00:06<00:04, 334.14it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1902/3257 [00:06<00:04, 333.67it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1937/3257 [00:06<00:03, 337.76it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1977/3257 [00:06<00:03, 349.65it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2012/3257 [00:06<00:03, 347.86it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2047/3257 [00:06<00:03, 340.53it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2082/3257 [00:06<00:03, 332.98it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2117/3257 [00:06<00:03, 335.25it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2151/3257 [00:06<00:03, 320.94it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2185/3257 [00:06<00:03, 325.30it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2220/3257 [00:07<00:03, 331.14it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2254/3257 [00:07<00:03, 327.82it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2287/3257 [00:07<00:03, 320.71it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2322/3257 [00:07<00:02, 327.13it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2362/3257 [00:07<00:02, 345.72it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2401/3257 [00:07<00:02, 357.08it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2437/3257 [00:07<00:02, 337.59it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2475/3257 [00:07<00:02, 346.18it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2514/3257 [00:07<00:02, 357.32it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2551/3257 [00:08<00:01, 357.93it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2587/3257 [00:08<00:02, 229.72it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2630/3257 [00:08<00:02, 271.30it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2663/3257 [00:08<00:02, 282.29it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2696/3257 [00:08<00:01, 293.95it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2729/3257 [00:08<00:01, 301.55it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:08<00:01, 318.25it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2807/3257 [00:08<00:01, 335.24it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2842/3257 [00:09<00:01, 335.53it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:09<00:01, 362.66it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2924/3257 [00:09<00:00, 360.22it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2961/3257 [00:09<00:00, 347.83it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2997/3257 [00:09<00:00, 351.01it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3037/3257 [00:09<00:00, 363.12it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:09<00:00, 377.61it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3120/3257 [00:09<00:00, 386.49it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3159/3257 [00:09<00:00, 368.67it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3197/3257 [00:10<00:00, 352.88it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3233/3257 [00:10<00:00, 345.07it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:10<00:00, 318.47it/s]
2023-02-07 14:32:15.698 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:32:15,699][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d10,n5,mc2,s0.412608,t4>', 'datetime': '2023-02-07T14:32:15.698977', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:32:15,699][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:32:15,699][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:32:15,893][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 14:32:15,894][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:32:15,900][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 2542 unique words (90.17% of original 2819, drops 277)', 'datetime': '2023-02-07T14:32:15.900562', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:32:15,900][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 2183345 word corpus (99.99% of original 2183622, drops 277)', 'datetime': '2023-02-07T14:32:15.900914', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:32:15,909][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 14:32:15,910][gensim.models.word2vec][INFO] - sample=0.412608 downsamples 0 most-common words
[2023-02-07 14:32:15,910][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2183345 word corpus (100.0%% of prior 2183345)', 'datetime': '2023-02-07T14:32:15.910197', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:32:15,925][gensim.models.word2vec][INFO] - estimated required memory for 2542 words and 10 dimensions: 2256040 bytes
[2023-02-07 14:32:15,925][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:32:15,926][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2542 vocabulary and 10 features, using sg=1 hs=0 sample=0.4126083577398292 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T14:32:15.926248', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:32:16,426][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2186602 effective words) took 0.5s, 4396874 effective words/s
[2023-02-07 14:32:16,883][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2186602 effective words) took 0.5s, 4807047 effective words/s
[2023-02-07 14:32:17,347][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2186602 effective words) took 0.5s, 4727383 effective words/s
[2023-02-07 14:32:17,811][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2186602 effective words) took 0.5s, 4737998 effective words/s
[2023-02-07 14:32:18,274][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2186602 effective words) took 0.5s, 4734629 effective words/s
[2023-02-07 14:32:18,734][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2186602 effective words) took 0.5s, 4765856 effective words/s
[2023-02-07 14:32:19,197][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2186602 effective words) took 0.5s, 4734360 effective words/s
[2023-02-07 14:32:19,658][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2186602 effective words) took 0.5s, 4753122 effective words/s
[2023-02-07 14:32:20,114][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2186602 effective words) took 0.5s, 4814310 effective words/s
[2023-02-07 14:32:20,573][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2186602 effective words) took 0.5s, 4781477 effective words/s
[2023-02-07 14:32:21,034][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2186602 effective words) took 0.5s, 4755438 effective words/s
[2023-02-07 14:32:21,494][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2186602 effective words) took 0.5s, 4768413 effective words/s
[2023-02-07 14:32:21,954][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2186602 effective words) took 0.5s, 4760324 effective words/s
[2023-02-07 14:32:22,410][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2186602 effective words) took 0.5s, 4815464 effective words/s
[2023-02-07 14:32:22,873][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2186602 effective words) took 0.5s, 4729376 effective words/s
[2023-02-07 14:32:23,347][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2186602 effective words) took 0.5s, 4634527 effective words/s
[2023-02-07 14:32:23,818][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2186602 effective words) took 0.5s, 4658815 effective words/s
[2023-02-07 14:32:24,294][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2186602 effective words) took 0.5s, 4605604 effective words/s
[2023-02-07 14:32:24,762][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2186602 effective words) took 0.5s, 4680769 effective words/s
[2023-02-07 14:32:25,230][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2186602 effective words) took 0.5s, 4686989 effective words/s
[2023-02-07 14:32:25,691][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2186602 effective words) took 0.5s, 4759361 effective words/s
[2023-02-07 14:32:26,154][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2186602 effective words) took 0.5s, 4727101 effective words/s
[2023-02-07 14:32:26,618][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2186602 effective words) took 0.5s, 4730185 effective words/s
[2023-02-07 14:32:27,081][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2186602 effective words) took 0.5s, 4741556 effective words/s
[2023-02-07 14:32:27,543][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2186602 effective words) took 0.5s, 4741546 effective words/s
[2023-02-07 14:32:28,004][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2186602 effective words) took 0.5s, 4759881 effective words/s
[2023-02-07 14:32:28,465][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2186602 effective words) took 0.5s, 4767446 effective words/s
[2023-02-07 14:32:28,929][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2186602 effective words) took 0.5s, 4745683 effective words/s
[2023-02-07 14:32:29,394][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2186602 effective words) took 0.5s, 4715629 effective words/s
[2023-02-07 14:32:29,865][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2186602 effective words) took 0.5s, 4652234 effective words/s
[2023-02-07 14:32:30,333][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2186602 effective words) took 0.5s, 4693189 effective words/s
[2023-02-07 14:32:30,800][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2186602 effective words) took 0.5s, 4701356 effective words/s
[2023-02-07 14:32:31,258][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2186602 effective words) took 0.5s, 4784468 effective words/s
[2023-02-07 14:32:31,721][gensim.models.word2vec][INFO] - EPOCH 33: training on 2183622 raw words (2186602 effective words) took 0.5s, 4738993 effective words/s
[2023-02-07 14:32:32,181][gensim.models.word2vec][INFO] - EPOCH 34: training on 2183622 raw words (2186602 effective words) took 0.5s, 4769714 effective words/s
[2023-02-07 14:32:32,641][gensim.models.word2vec][INFO] - EPOCH 35: training on 2183622 raw words (2186602 effective words) took 0.5s, 4759737 effective words/s
[2023-02-07 14:32:33,099][gensim.models.word2vec][INFO] - EPOCH 36: training on 2183622 raw words (2186602 effective words) took 0.5s, 4798521 effective words/s
[2023-02-07 14:32:33,560][gensim.models.word2vec][INFO] - EPOCH 37: training on 2183622 raw words (2186602 effective words) took 0.5s, 4749579 effective words/s
[2023-02-07 14:32:34,022][gensim.models.word2vec][INFO] - EPOCH 38: training on 2183622 raw words (2186602 effective words) took 0.5s, 4754590 effective words/s
[2023-02-07 14:32:34,484][gensim.models.word2vec][INFO] - EPOCH 39: training on 2183622 raw words (2186602 effective words) took 0.5s, 4741674 effective words/s
[2023-02-07 14:32:34,952][gensim.models.word2vec][INFO] - EPOCH 40: training on 2183622 raw words (2186602 effective words) took 0.5s, 4688579 effective words/s
[2023-02-07 14:32:35,415][gensim.models.word2vec][INFO] - EPOCH 41: training on 2183622 raw words (2186602 effective words) took 0.5s, 4735751 effective words/s
[2023-02-07 14:32:35,875][gensim.models.word2vec][INFO] - EPOCH 42: training on 2183622 raw words (2186602 effective words) took 0.5s, 4766718 effective words/s
[2023-02-07 14:32:36,337][gensim.models.word2vec][INFO] - EPOCH 43: training on 2183622 raw words (2186602 effective words) took 0.5s, 4739947 effective words/s
[2023-02-07 14:32:36,796][gensim.models.word2vec][INFO] - EPOCH 44: training on 2183622 raw words (2186602 effective words) took 0.5s, 4784969 effective words/s
[2023-02-07 14:32:37,254][gensim.models.word2vec][INFO] - EPOCH 45: training on 2183622 raw words (2186602 effective words) took 0.5s, 4792760 effective words/s
[2023-02-07 14:32:37,710][gensim.models.word2vec][INFO] - EPOCH 46: training on 2183622 raw words (2186602 effective words) took 0.5s, 4808861 effective words/s
[2023-02-07 14:32:38,169][gensim.models.word2vec][INFO] - EPOCH 47: training on 2183622 raw words (2186602 effective words) took 0.5s, 4776391 effective words/s
[2023-02-07 14:32:38,633][gensim.models.word2vec][INFO] - EPOCH 48: training on 2183622 raw words (2186602 effective words) took 0.5s, 4733308 effective words/s
[2023-02-07 14:32:39,095][gensim.models.word2vec][INFO] - EPOCH 49: training on 2183622 raw words (2186602 effective words) took 0.5s, 4746887 effective words/s
[2023-02-07 14:32:39,558][gensim.models.word2vec][INFO] - EPOCH 50: training on 2183622 raw words (2186602 effective words) took 0.5s, 4753253 effective words/s
[2023-02-07 14:32:40,020][gensim.models.word2vec][INFO] - EPOCH 51: training on 2183622 raw words (2186602 effective words) took 0.5s, 4742959 effective words/s
[2023-02-07 14:32:40,480][gensim.models.word2vec][INFO] - EPOCH 52: training on 2183622 raw words (2186602 effective words) took 0.5s, 4772173 effective words/s
[2023-02-07 14:32:40,935][gensim.models.word2vec][INFO] - EPOCH 53: training on 2183622 raw words (2186602 effective words) took 0.5s, 4820084 effective words/s
[2023-02-07 14:32:41,392][gensim.models.word2vec][INFO] - EPOCH 54: training on 2183622 raw words (2186602 effective words) took 0.5s, 4798642 effective words/s
[2023-02-07 14:32:41,853][gensim.models.word2vec][INFO] - EPOCH 55: training on 2183622 raw words (2186602 effective words) took 0.5s, 4752635 effective words/s
[2023-02-07 14:32:42,326][gensim.models.word2vec][INFO] - EPOCH 56: training on 2183622 raw words (2186602 effective words) took 0.5s, 4631740 effective words/s
[2023-02-07 14:32:42,802][gensim.models.word2vec][INFO] - EPOCH 57: training on 2183622 raw words (2186602 effective words) took 0.5s, 4616441 effective words/s
[2023-02-07 14:32:43,276][gensim.models.word2vec][INFO] - EPOCH 58: training on 2183622 raw words (2186602 effective words) took 0.5s, 4622687 effective words/s
[2023-02-07 14:32:43,750][gensim.models.word2vec][INFO] - EPOCH 59: training on 2183622 raw words (2186602 effective words) took 0.5s, 4632064 effective words/s
[2023-02-07 14:32:44,223][gensim.models.word2vec][INFO] - EPOCH 60: training on 2183622 raw words (2186602 effective words) took 0.5s, 4630706 effective words/s
[2023-02-07 14:32:44,700][gensim.models.word2vec][INFO] - EPOCH 61: training on 2183622 raw words (2186602 effective words) took 0.5s, 4605890 effective words/s
[2023-02-07 14:32:45,173][gensim.models.word2vec][INFO] - EPOCH 62: training on 2183622 raw words (2186602 effective words) took 0.5s, 4629697 effective words/s
[2023-02-07 14:32:45,650][gensim.models.word2vec][INFO] - EPOCH 63: training on 2183622 raw words (2186602 effective words) took 0.5s, 4596845 effective words/s
[2023-02-07 14:32:46,128][gensim.models.word2vec][INFO] - EPOCH 64: training on 2183622 raw words (2186602 effective words) took 0.5s, 4586023 effective words/s
[2023-02-07 14:32:46,603][gensim.models.word2vec][INFO] - EPOCH 65: training on 2183622 raw words (2186602 effective words) took 0.5s, 4616754 effective words/s
[2023-02-07 14:32:47,081][gensim.models.word2vec][INFO] - EPOCH 66: training on 2183622 raw words (2186602 effective words) took 0.5s, 4591409 effective words/s
[2023-02-07 14:32:47,555][gensim.models.word2vec][INFO] - EPOCH 67: training on 2183622 raw words (2186602 effective words) took 0.5s, 4623761 effective words/s
[2023-02-07 14:32:48,029][gensim.models.word2vec][INFO] - EPOCH 68: training on 2183622 raw words (2186602 effective words) took 0.5s, 4621355 effective words/s
[2023-02-07 14:32:48,501][gensim.models.word2vec][INFO] - EPOCH 69: training on 2183622 raw words (2186602 effective words) took 0.5s, 4644491 effective words/s
[2023-02-07 14:32:48,978][gensim.models.word2vec][INFO] - EPOCH 70: training on 2183622 raw words (2186602 effective words) took 0.5s, 4603511 effective words/s
[2023-02-07 14:32:49,453][gensim.models.word2vec][INFO] - EPOCH 71: training on 2183622 raw words (2186602 effective words) took 0.5s, 4617986 effective words/s
[2023-02-07 14:32:49,928][gensim.models.word2vec][INFO] - EPOCH 72: training on 2183622 raw words (2186602 effective words) took 0.5s, 4614773 effective words/s
[2023-02-07 14:32:50,407][gensim.models.word2vec][INFO] - EPOCH 73: training on 2183622 raw words (2186602 effective words) took 0.5s, 4573841 effective words/s
[2023-02-07 14:32:50,889][gensim.models.word2vec][INFO] - EPOCH 74: training on 2183622 raw words (2186602 effective words) took 0.5s, 4549590 effective words/s
[2023-02-07 14:32:51,370][gensim.models.word2vec][INFO] - EPOCH 75: training on 2183622 raw words (2186602 effective words) took 0.5s, 4560124 effective words/s
[2023-02-07 14:32:51,371][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 165955272 raw words (166181752 effective words) took 35.4s, 4688547 effective words/s', 'datetime': '2023-02-07T14:32:51.371788', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:32:51.372 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:32:54,029][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143155-aa5v5nne/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:32:54.029199', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:32:54,030][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:32:54,037][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143155-aa5v5nne/files/../tmp/embedding_model.pt
2023-02-07 14:32:54.038 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:32:54.868 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:32:55.216 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:32:55.381 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.0251240780928206, 'test_mae': 1.0854720969347318, 'test_r2': 0.041995229111945864}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.58
wandb: percentage 0.09826
wandb:   test_mae 1.08547
wandb:   test_mse 2.02512
wandb:    test_r2 0.042
wandb: 
wandb: üöÄ View run glorious-sweep-84 at: https://wandb.ai/xiaoqiz/mof2vec/runs/aa5v5nne
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_143155-aa5v5nne/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: hh3115uq with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 353
wandb: 	model.gensim.alpha: 0.0008897909922865865
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 70
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.9448213723167148
wandb: 	model.gensim.vector_size: 176
wandb: 	model.gensim.window: 4
wandb: 	model.sklearn.learning_rate: 0.07059132866032118
wandb: 	model.sklearn.max_depth: 8
wandb: 	model.sklearn.min_child_weight: 0.009622798840175644
wandb: 	model.sklearn.n_estimators: 1754
wandb: 	model.sklearn.num_leaves: 452
wandb: 	model.sklearn.reg_alpha: 0.041984414745584744
wandb: 	model.sklearn.reg_lambda: 0.014321018669861052
wandb: 	model.sklearn.subsample: 0.5885606960530889
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143338-hh3115uq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-sweep-85
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/hh3115uq
2023-02-07 14:33:47.243 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 14:33:47.243 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 353 for sweep.
2023-02-07 14:33:47.244 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0008897909922865865 for sweep.
2023-02-07 14:33:47.244 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:33:47.244 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 70 for sweep.
2023-02-07 14:33:47.245 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 14:33:47.245 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.9448213723167148 for sweep.
2023-02-07 14:33:47.245 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 176 for sweep.
2023-02-07 14:33:47.245 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 4 for sweep.
2023-02-07 14:33:47.245 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.07059132866032118 for sweep.
2023-02-07 14:33:47.246 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 8 for sweep.
2023-02-07 14:33:47.246 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.009622798840175644 for sweep.
2023-02-07 14:33:47.246 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1754 for sweep.
2023-02-07 14:33:47.246 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 452 for sweep.
2023-02-07 14:33:47.247 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.041984414745584744 for sweep.
2023-02-07 14:33:47.247 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.014321018669861052 for sweep.
2023-02-07 14:33:47.247 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5885606960530889 for sweep.
2023-02-07 14:33:47.247 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:33:47.255 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143338-hh3115uq/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 353, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 176, 'window': 4, 'min_count': 1, 'dm': 0, 'sample': 0.9448213723167148, 'workers': 4, 'alpha': 0.0008897909922865865, 'epochs': 70}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1754, 'max_depth': 8, 'num_leaves': 452, 'reg_alpha': 0.041984414745584744, 'reg_lambda': 0.014321018669861052, 'subsample': 0.5885606960530889, 'min_child_weight': 0.009622798840175644, 'n_jobs': 4, 'learning_rate': 0.07059132866032118}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 34/3257 [00:00<00:09, 336.43it/s]  2%|‚ñè         | 71/3257 [00:00<00:09, 352.30it/s]  3%|‚ñé         | 107/3257 [00:00<00:08, 351.65it/s]  5%|‚ñç         | 149/3257 [00:00<00:08, 374.72it/s]  6%|‚ñå         | 187/3257 [00:00<00:08, 376.43it/s]  7%|‚ñã         | 230/3257 [00:00<00:07, 393.66it/s]  8%|‚ñä         | 270/3257 [00:00<00:07, 388.52it/s] 10%|‚ñâ         | 313/3257 [00:00<00:07, 398.92it/s] 11%|‚ñà         | 356/3257 [00:00<00:07, 406.75it/s] 12%|‚ñà‚ñè        | 397/3257 [00:01<00:07, 391.16it/s] 13%|‚ñà‚ñé        | 437/3257 [00:01<00:07, 361.70it/s] 15%|‚ñà‚ñç        | 476/3257 [00:01<00:07, 368.83it/s] 16%|‚ñà‚ñå        | 517/3257 [00:01<00:07, 377.40it/s] 17%|‚ñà‚ñã        | 556/3257 [00:01<00:07, 378.43it/s] 18%|‚ñà‚ñä        | 595/3257 [00:01<00:07, 361.69it/s] 19%|‚ñà‚ñâ        | 632/3257 [00:01<00:09, 265.12it/s] 20%|‚ñà‚ñà        | 665/3257 [00:01<00:09, 278.53it/s] 22%|‚ñà‚ñà‚ñè       | 703/3257 [00:02<00:08, 302.90it/s] 23%|‚ñà‚ñà‚ñé       | 737/3257 [00:02<00:08, 311.67it/s] 24%|‚ñà‚ñà‚ñé       | 773/3257 [00:02<00:07, 322.28it/s] 25%|‚ñà‚ñà‚ñç       | 807/3257 [00:02<00:07, 326.87it/s] 26%|‚ñà‚ñà‚ñå       | 841/3257 [00:02<00:07, 329.10it/s] 27%|‚ñà‚ñà‚ñã       | 877/3257 [00:02<00:07, 334.80it/s] 28%|‚ñà‚ñà‚ñä       | 915/3257 [00:02<00:06, 345.80it/s] 29%|‚ñà‚ñà‚ñâ       | 956/3257 [00:02<00:06, 362.55it/s] 30%|‚ñà‚ñà‚ñà       | 993/3257 [00:02<00:06, 361.05it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1030/3257 [00:02<00:06, 351.57it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1066/3257 [00:03<00:06, 344.72it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1101/3257 [00:03<00:06, 343.41it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1138/3257 [00:03<00:06, 350.18it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1175/3257 [00:03<00:05, 351.62it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1211/3257 [00:03<00:06, 335.52it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:03<00:05, 347.72it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1284/3257 [00:03<00:05, 345.97it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1322/3257 [00:03<00:05, 353.87it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1360/3257 [00:03<00:05, 359.34it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1397/3257 [00:03<00:05, 357.47it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1436/3257 [00:04<00:04, 366.61it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1473/3257 [00:04<00:04, 362.91it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1511/3257 [00:04<00:04, 366.87it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1548/3257 [00:04<00:04, 350.50it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1586/3257 [00:04<00:04, 357.12it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1623/3257 [00:04<00:04, 358.44it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1659/3257 [00:04<00:04, 356.42it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1695/3257 [00:04<00:04, 355.71it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1731/3257 [00:04<00:04, 349.39it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1770/3257 [00:05<00:04, 360.16it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1807/3257 [00:05<00:04, 359.24it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1846/3257 [00:05<00:03, 366.88it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1886/3257 [00:05<00:03, 375.19it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1924/3257 [00:05<00:05, 264.61it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1969/3257 [00:05<00:04, 305.58it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:05<00:03, 315.15it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2040/3257 [00:05<00:03, 324.09it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2075/3257 [00:05<00:03, 328.44it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2113/3257 [00:06<00:03, 341.43it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2149/3257 [00:06<00:03, 336.25it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2190/3257 [00:06<00:03, 355.15it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2228/3257 [00:06<00:02, 360.66it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2265/3257 [00:06<00:02, 362.28it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2303/3257 [00:06<00:02, 365.22it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2350/3257 [00:06<00:02, 394.64it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2394/3257 [00:06<00:02, 405.89it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2435/3257 [00:06<00:02, 390.62it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2476/3257 [00:07<00:01, 393.51it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2519/3257 [00:07<00:01, 402.54it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2560/3257 [00:07<00:01, 381.35it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2599/3257 [00:07<00:01, 363.66it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:07<00:01, 375.97it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2678/3257 [00:07<00:01, 365.98it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2715/3257 [00:07<00:01, 351.81it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2759/3257 [00:07<00:01, 376.23it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2800/3257 [00:07<00:01, 385.27it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2839/3257 [00:08<00:01, 374.06it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2886/3257 [00:08<00:00, 394.96it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2926/3257 [00:08<00:00, 393.38it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2966/3257 [00:08<00:00, 380.81it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3005/3257 [00:08<00:00, 382.20it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3045/3257 [00:08<00:00, 385.75it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3084/3257 [00:08<00:00, 385.56it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3126/3257 [00:08<00:00, 395.34it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3166/3257 [00:08<00:00, 370.93it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3204/3257 [00:08<00:00, 360.96it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3241/3257 [00:09<00:00, 356.78it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 356.92it/s]
2023-02-07 14:33:56.575 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:33:56,576][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d176,n5,s0.944821,t4>', 'datetime': '2023-02-07T14:33:56.576895', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:33:56,578][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:33:56,578][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:33:56,714][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 14:33:56,714][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:33:56,717][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 924 unique words (100.00% of original 924, drops 0)', 'datetime': '2023-02-07T14:33:56.717022', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:33:56,717][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 1455748 word corpus (100.00% of original 1455748, drops 0)', 'datetime': '2023-02-07T14:33:56.717211', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:33:56,721][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 14:33:56,721][gensim.models.word2vec][INFO] - sample=0.944821 downsamples 0 most-common words
[2023-02-07 14:33:56,721][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455748 word corpus (100.0%% of prior 1455748)', 'datetime': '2023-02-07T14:33:56.721740', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:33:56,727][gensim.models.word2vec][INFO] - estimated required memory for 924 words and 176 dimensions: 4707320 bytes
[2023-02-07 14:33:56,727][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:33:56,730][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 924 vocabulary and 176 features, using sg=1 hs=0 sample=0.9448213723167148 negative=5 window=4 shrink_windows=True', 'datetime': '2023-02-07T14:33:56.730496', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:33:57,522][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1459005 effective words) took 0.8s, 1848300 effective words/s
[2023-02-07 14:33:58,336][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1459005 effective words) took 0.8s, 1796288 effective words/s
[2023-02-07 14:33:59,114][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1459005 effective words) took 0.8s, 1882134 effective words/s
[2023-02-07 14:33:59,904][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1459005 effective words) took 0.8s, 1853767 effective words/s
[2023-02-07 14:34:00,686][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1459005 effective words) took 0.8s, 1867794 effective words/s
[2023-02-07 14:34:01,462][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1459005 effective words) took 0.8s, 1885061 effective words/s
[2023-02-07 14:34:02,247][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1459005 effective words) took 0.8s, 1861603 effective words/s
[2023-02-07 14:34:03,029][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1459005 effective words) took 0.8s, 1869512 effective words/s
[2023-02-07 14:34:03,816][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1459005 effective words) took 0.8s, 1858552 effective words/s
[2023-02-07 14:34:04,594][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1459005 effective words) took 0.8s, 1881686 effective words/s
[2023-02-07 14:34:05,378][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1459005 effective words) took 0.8s, 1864250 effective words/s
[2023-02-07 14:34:06,164][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1459005 effective words) took 0.8s, 1862144 effective words/s
[2023-02-07 14:34:06,944][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1459005 effective words) took 0.8s, 1874860 effective words/s
[2023-02-07 14:34:07,737][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1459005 effective words) took 0.8s, 1842263 effective words/s
[2023-02-07 14:34:08,515][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1459005 effective words) took 0.8s, 1879329 effective words/s
[2023-02-07 14:34:09,293][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1459005 effective words) took 0.8s, 1880826 effective words/s
[2023-02-07 14:34:10,070][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1459005 effective words) took 0.8s, 1881237 effective words/s
[2023-02-07 14:34:10,844][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1459005 effective words) took 0.8s, 1891499 effective words/s
[2023-02-07 14:34:11,628][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1459005 effective words) took 0.8s, 1865597 effective words/s
[2023-02-07 14:34:12,415][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1459005 effective words) took 0.8s, 1857470 effective words/s
[2023-02-07 14:34:13,204][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1459005 effective words) took 0.8s, 1853587 effective words/s
[2023-02-07 14:34:13,992][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1459005 effective words) took 0.8s, 1856573 effective words/s
[2023-02-07 14:34:14,773][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1459005 effective words) took 0.8s, 1870802 effective words/s
[2023-02-07 14:34:15,558][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1459005 effective words) took 0.8s, 1861724 effective words/s
[2023-02-07 14:34:16,343][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1459005 effective words) took 0.8s, 1862903 effective words/s
[2023-02-07 14:34:17,141][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1459005 effective words) took 0.8s, 1831319 effective words/s
[2023-02-07 14:34:17,931][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1459005 effective words) took 0.8s, 1852758 effective words/s
[2023-02-07 14:34:18,707][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1459005 effective words) took 0.8s, 1883293 effective words/s
[2023-02-07 14:34:19,493][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1459005 effective words) took 0.8s, 1862106 effective words/s
[2023-02-07 14:34:20,271][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1459005 effective words) took 0.8s, 1880612 effective words/s
[2023-02-07 14:34:21,058][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1459005 effective words) took 0.8s, 1857361 effective words/s
[2023-02-07 14:34:21,840][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1459005 effective words) took 0.8s, 1869987 effective words/s
[2023-02-07 14:34:22,616][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1459005 effective words) took 0.8s, 1883483 effective words/s
[2023-02-07 14:34:23,399][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1459005 effective words) took 0.8s, 1871683 effective words/s
[2023-02-07 14:34:24,176][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1459005 effective words) took 0.8s, 1879787 effective words/s
[2023-02-07 14:34:24,958][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1459005 effective words) took 0.8s, 1869934 effective words/s
[2023-02-07 14:34:25,759][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1459005 effective words) took 0.8s, 1825716 effective words/s
[2023-02-07 14:34:26,545][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1459005 effective words) took 0.8s, 1860707 effective words/s
[2023-02-07 14:34:27,341][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1459005 effective words) took 0.8s, 1835699 effective words/s
[2023-02-07 14:34:28,117][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1459005 effective words) took 0.8s, 1886129 effective words/s
[2023-02-07 14:34:28,905][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1459005 effective words) took 0.8s, 1855935 effective words/s
[2023-02-07 14:34:29,717][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1459005 effective words) took 0.8s, 1799354 effective words/s
[2023-02-07 14:34:30,511][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1459005 effective words) took 0.8s, 1840506 effective words/s
[2023-02-07 14:34:31,296][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1459005 effective words) took 0.8s, 1862960 effective words/s
[2023-02-07 14:34:32,064][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1459005 effective words) took 0.8s, 1904099 effective words/s
[2023-02-07 14:34:32,835][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1459005 effective words) took 0.8s, 1895953 effective words/s
[2023-02-07 14:34:33,614][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1459005 effective words) took 0.8s, 1881690 effective words/s
[2023-02-07 14:34:34,394][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1459005 effective words) took 0.8s, 1874108 effective words/s
[2023-02-07 14:34:35,161][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1459005 effective words) took 0.8s, 1905601 effective words/s
[2023-02-07 14:34:35,946][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1459005 effective words) took 0.8s, 1864508 effective words/s
[2023-02-07 14:34:36,715][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1459005 effective words) took 0.8s, 1900454 effective words/s
[2023-02-07 14:34:37,492][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1459005 effective words) took 0.8s, 1883025 effective words/s
[2023-02-07 14:34:38,269][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1459005 effective words) took 0.8s, 1881627 effective words/s
[2023-02-07 14:34:39,056][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1459005 effective words) took 0.8s, 1857199 effective words/s
[2023-02-07 14:34:39,827][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1459005 effective words) took 0.8s, 1898304 effective words/s
[2023-02-07 14:34:40,596][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1459005 effective words) took 0.8s, 1900973 effective words/s
[2023-02-07 14:34:41,375][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1459005 effective words) took 0.8s, 1877117 effective words/s
[2023-02-07 14:34:42,151][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1459005 effective words) took 0.8s, 1884384 effective words/s
[2023-02-07 14:34:42,920][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1459005 effective words) took 0.8s, 1901201 effective words/s
[2023-02-07 14:34:43,704][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1459005 effective words) took 0.8s, 1866350 effective words/s
[2023-02-07 14:34:44,483][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1459005 effective words) took 0.8s, 1874840 effective words/s
[2023-02-07 14:34:45,265][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1459005 effective words) took 0.8s, 1871846 effective words/s
[2023-02-07 14:34:46,047][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1459005 effective words) took 0.8s, 1868843 effective words/s
[2023-02-07 14:34:46,811][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1459005 effective words) took 0.8s, 1913185 effective words/s
[2023-02-07 14:34:47,578][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1459005 effective words) took 0.8s, 1905756 effective words/s
[2023-02-07 14:34:48,382][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1459005 effective words) took 0.8s, 1818187 effective words/s
[2023-02-07 14:34:49,168][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1459005 effective words) took 0.8s, 1859912 effective words/s
[2023-02-07 14:34:49,969][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1459005 effective words) took 0.8s, 1826613 effective words/s
[2023-02-07 14:34:50,747][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1459005 effective words) took 0.8s, 1880410 effective words/s
[2023-02-07 14:34:51,533][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1459005 effective words) took 0.8s, 1861450 effective words/s
[2023-02-07 14:34:51,533][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 101902360 raw words (102130350 effective words) took 54.8s, 1863586 effective words/s', 'datetime': '2023-02-07T14:34:51.533917', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:34:51.534 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:34:53,746][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143338-hh3115uq/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:34:53.746739', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:34:53,747][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:34:53,756][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143338-hh3115uq/files/../tmp/embedding_model.pt
2023-02-07 14:34:53.756 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:34:55.134 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:34:55.660 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:34:56.895 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8868916764362393, 'test_mae': 1.0122166828258088, 'test_r2': 0.10738741999589085}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.75
wandb: percentage 0.0
wandb:   test_mae 1.01222
wandb:   test_mse 1.88689
wandb:    test_r2 0.10739
wandb: 
wandb: üöÄ View run peach-sweep-85 at: https://wandb.ai/xiaoqiz/mof2vec/runs/hh3115uq
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_143338-hh3115uq/logs
wandb: Agent Starting Run: cwm5rmk9 with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 318
wandb: 	model.gensim.alpha: 0.07501614043963849
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 93
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.9130949017875551
wandb: 	model.gensim.vector_size: 152
wandb: 	model.gensim.window: 6
wandb: 	model.sklearn.learning_rate: 0.005021473815041678
wandb: 	model.sklearn.max_depth: 18
wandb: 	model.sklearn.min_child_weight: 0.013421467862959316
wandb: 	model.sklearn.n_estimators: 2795
wandb: 	model.sklearn.num_leaves: 344
wandb: 	model.sklearn.reg_alpha: 0.07696172592717136
wandb: 	model.sklearn.reg_lambda: 0.1907542037586561
wandb: 	model.sklearn.subsample: 0.35680432198876244
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143539-cwm5rmk9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-86
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/cwm5rmk9
2023-02-07 14:35:47.717 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 14:35:47.718 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 318 for sweep.
2023-02-07 14:35:47.718 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.07501614043963849 for sweep.
2023-02-07 14:35:47.718 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:35:47.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 93 for sweep.
2023-02-07 14:35:47.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 14:35:47.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.9130949017875551 for sweep.
2023-02-07 14:35:47.719 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 152 for sweep.
2023-02-07 14:35:47.720 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 6 for sweep.
2023-02-07 14:35:47.720 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.005021473815041678 for sweep.
2023-02-07 14:35:47.720 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 18 for sweep.
2023-02-07 14:35:47.720 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.013421467862959316 for sweep.
2023-02-07 14:35:47.721 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2795 for sweep.
2023-02-07 14:35:47.721 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 344 for sweep.
2023-02-07 14:35:47.721 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.07696172592717136 for sweep.
2023-02-07 14:35:47.721 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.1907542037586561 for sweep.
2023-02-07 14:35:47.722 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.35680432198876244 for sweep.
2023-02-07 14:35:47.722 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:35:47.727 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143539-cwm5rmk9/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 318, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 152, 'window': 6, 'min_count': 2, 'dm': 0, 'sample': 0.9130949017875551, 'workers': 4, 'alpha': 0.07501614043963849, 'epochs': 93}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2795, 'max_depth': 18, 'num_leaves': 344, 'reg_alpha': 0.07696172592717136, 'reg_lambda': 0.1907542037586561, 'subsample': 0.35680432198876244, 'min_child_weight': 0.013421467862959316, 'n_jobs': 4, 'learning_rate': 0.005021473815041678}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 35/3257 [00:00<00:09, 349.39it/s]  2%|‚ñè         | 73/3257 [00:00<00:08, 366.31it/s]  3%|‚ñé         | 110/3257 [00:00<00:08, 367.58it/s]  5%|‚ñç         | 153/3257 [00:00<00:07, 390.79it/s]  6%|‚ñå         | 193/3257 [00:00<00:07, 386.35it/s]  7%|‚ñã         | 232/3257 [00:00<00:11, 268.78it/s]  8%|‚ñä         | 272/3257 [00:00<00:09, 300.79it/s] 10%|‚ñâ         | 314/3257 [00:00<00:08, 331.04it/s] 11%|‚ñà         | 356/3257 [00:01<00:08, 354.77it/s] 12%|‚ñà‚ñè        | 395/3257 [00:01<00:08, 349.61it/s] 13%|‚ñà‚ñé        | 432/3257 [00:01<00:08, 346.82it/s] 14%|‚ñà‚ñç        | 468/3257 [00:01<00:08, 343.96it/s] 15%|‚ñà‚ñå        | 504/3257 [00:01<00:08, 341.21it/s] 17%|‚ñà‚ñã        | 542/3257 [00:01<00:07, 351.69it/s] 18%|‚ñà‚ñä        | 578/3257 [00:01<00:08, 329.28it/s] 19%|‚ñà‚ñâ        | 619/3257 [00:01<00:07, 351.18it/s] 20%|‚ñà‚ñà        | 655/3257 [00:01<00:07, 340.68it/s] 21%|‚ñà‚ñà        | 692/3257 [00:02<00:07, 348.29it/s] 22%|‚ñà‚ñà‚ñè       | 730/3257 [00:02<00:07, 354.23it/s] 24%|‚ñà‚ñà‚ñé       | 770/3257 [00:02<00:06, 362.66it/s] 25%|‚ñà‚ñà‚ñç       | 807/3257 [00:02<00:06, 352.70it/s] 26%|‚ñà‚ñà‚ñå       | 843/3257 [00:02<00:06, 350.99it/s] 27%|‚ñà‚ñà‚ñã       | 879/3257 [00:02<00:06, 350.98it/s] 28%|‚ñà‚ñà‚ñä       | 917/3257 [00:02<00:06, 357.63it/s] 29%|‚ñà‚ñà‚ñâ       | 957/3257 [00:02<00:06, 369.91it/s] 31%|‚ñà‚ñà‚ñà       | 995/3257 [00:02<00:06, 363.22it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1032/3257 [00:02<00:06, 361.85it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1069/3257 [00:03<00:06, 364.18it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1106/3257 [00:03<00:05, 363.88it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1143/3257 [00:03<00:05, 359.74it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1180/3257 [00:03<00:05, 353.32it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1216/3257 [00:03<00:06, 334.95it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1251/3257 [00:03<00:05, 338.85it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1286/3257 [00:03<00:05, 329.91it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1323/3257 [00:03<00:05, 339.74it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1358/3257 [00:03<00:05, 341.18it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1393/3257 [00:04<00:05, 339.03it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1428/3257 [00:04<00:05, 341.17it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1463/3257 [00:04<00:05, 337.22it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1503/3257 [00:04<00:04, 355.31it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1539/3257 [00:04<00:05, 336.83it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1574/3257 [00:04<00:04, 340.34it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1612/3257 [00:04<00:04, 351.59it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1648/3257 [00:04<00:06, 242.57it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1681/3257 [00:05<00:06, 261.46it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:05<00:05, 278.16it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1746/3257 [00:05<00:05, 281.77it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1784/3257 [00:05<00:04, 305.99it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1818/3257 [00:05<00:04, 314.76it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1855/3257 [00:05<00:04, 330.11it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1894/3257 [00:05<00:03, 346.51it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1935/3257 [00:05<00:03, 364.10it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1978/3257 [00:05<00:03, 382.46it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2018/3257 [00:05<00:03, 385.80it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2057/3257 [00:06<00:03, 366.44it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2095/3257 [00:06<00:03, 369.46it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2133/3257 [00:06<00:03, 360.04it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2170/3257 [00:06<00:03, 356.63it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2206/3257 [00:06<00:02, 350.34it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2242/3257 [00:06<00:03, 328.43it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2276/3257 [00:06<00:03, 319.07it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2319/3257 [00:06<00:02, 348.83it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2363/3257 [00:06<00:02, 374.30it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:07<00:02, 378.03it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2444/3257 [00:07<00:02, 374.45it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2485/3257 [00:07<00:02, 381.57it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2530/3257 [00:07<00:01, 400.26it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2571/3257 [00:07<00:01, 379.20it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2611/3257 [00:07<00:01, 384.56it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2651/3257 [00:07<00:01, 387.66it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2691/3257 [00:07<00:01, 390.62it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2731/3257 [00:07<00:01, 378.05it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2769/3257 [00:07<00:01, 375.99it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2810/3257 [00:08<00:01, 384.77it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2849/3257 [00:08<00:01, 378.33it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2893/3257 [00:08<00:00, 388.97it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2932/3257 [00:08<00:00, 384.72it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2971/3257 [00:08<00:00, 370.58it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3010/3257 [00:08<00:00, 375.81it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:08<00:00, 267.64it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3090/3257 [00:08<00:00, 298.75it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3132/3257 [00:09<00:00, 328.07it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3170/3257 [00:09<00:00, 340.50it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3208/3257 [00:09<00:00, 349.73it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3249/3257 [00:09<00:00, 365.35it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 347.29it/s]
2023-02-07 14:35:57.302 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:35:57,303][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d152,n5,mc2,s0.913095,t4>', 'datetime': '2023-02-07T14:35:57.303908', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:35:57,305][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:35:57,305][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:35:57,443][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 14:35:57,443][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:35:57,446][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 881 unique words (95.35% of original 924, drops 43)', 'datetime': '2023-02-07T14:35:57.446058', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:35:57,447][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 1455705 word corpus (100.00% of original 1455748, drops 43)', 'datetime': '2023-02-07T14:35:57.447381', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:35:57,450][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 14:35:57,450][gensim.models.word2vec][INFO] - sample=0.913095 downsamples 0 most-common words
[2023-02-07 14:35:57,450][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455705 word corpus (100.0%% of prior 1455705)', 'datetime': '2023-02-07T14:35:57.450959', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:35:57,456][gensim.models.word2vec][INFO] - estimated required memory for 881 words and 152 dimensions: 4143452 bytes
[2023-02-07 14:35:57,456][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:35:57,459][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 881 vocabulary and 152 features, using sg=1 hs=0 sample=0.9130949017875551 negative=5 window=6 shrink_windows=True', 'datetime': '2023-02-07T14:35:57.459213', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:35:57,867][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458962 effective words) took 0.4s, 3593690 effective words/s
[2023-02-07 14:35:58,239][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458962 effective words) took 0.4s, 3937686 effective words/s
[2023-02-07 14:35:58,612][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458962 effective words) took 0.4s, 3926171 effective words/s
[2023-02-07 14:35:58,984][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458962 effective words) took 0.4s, 3936041 effective words/s
[2023-02-07 14:35:59,354][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458962 effective words) took 0.4s, 3960954 effective words/s
[2023-02-07 14:35:59,725][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458962 effective words) took 0.4s, 3946841 effective words/s
[2023-02-07 14:36:00,095][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458962 effective words) took 0.4s, 3955878 effective words/s
[2023-02-07 14:36:00,465][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458962 effective words) took 0.4s, 3961126 effective words/s
[2023-02-07 14:36:00,837][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458962 effective words) took 0.4s, 3939382 effective words/s
[2023-02-07 14:36:01,207][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458962 effective words) took 0.4s, 3967736 effective words/s
[2023-02-07 14:36:01,572][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458962 effective words) took 0.4s, 4013885 effective words/s
[2023-02-07 14:36:01,941][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458962 effective words) took 0.4s, 3974509 effective words/s
[2023-02-07 14:36:02,309][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458962 effective words) took 0.4s, 3971790 effective words/s
[2023-02-07 14:36:02,673][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458962 effective words) took 0.4s, 4024042 effective words/s
[2023-02-07 14:36:03,050][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458962 effective words) took 0.4s, 3889703 effective words/s
[2023-02-07 14:36:03,425][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458962 effective words) took 0.4s, 3900955 effective words/s
[2023-02-07 14:36:03,801][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458962 effective words) took 0.4s, 3918312 effective words/s
[2023-02-07 14:36:04,173][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458962 effective words) took 0.4s, 3936782 effective words/s
[2023-02-07 14:36:04,547][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458962 effective words) took 0.4s, 3915341 effective words/s
[2023-02-07 14:36:04,919][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458962 effective words) took 0.4s, 3932822 effective words/s
[2023-02-07 14:36:05,293][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458962 effective words) took 0.4s, 3914679 effective words/s
[2023-02-07 14:36:05,667][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458962 effective words) took 0.4s, 3920289 effective words/s
[2023-02-07 14:36:06,040][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458962 effective words) took 0.4s, 3926671 effective words/s
[2023-02-07 14:36:06,414][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458962 effective words) took 0.4s, 3914203 effective words/s
[2023-02-07 14:36:06,787][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458962 effective words) took 0.4s, 3925726 effective words/s
[2023-02-07 14:36:07,156][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458962 effective words) took 0.4s, 3969063 effective words/s
[2023-02-07 14:36:07,528][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458962 effective words) took 0.4s, 3934588 effective words/s
[2023-02-07 14:36:07,904][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458962 effective words) took 0.4s, 3899673 effective words/s
[2023-02-07 14:36:08,288][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458962 effective words) took 0.4s, 3809034 effective words/s
[2023-02-07 14:36:08,668][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458962 effective words) took 0.4s, 3855736 effective words/s
[2023-02-07 14:36:09,044][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458962 effective words) took 0.4s, 3898527 effective words/s
[2023-02-07 14:36:09,423][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458962 effective words) took 0.4s, 3867615 effective words/s
[2023-02-07 14:36:09,803][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458962 effective words) took 0.4s, 3853626 effective words/s
[2023-02-07 14:36:10,183][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458962 effective words) took 0.4s, 3853015 effective words/s
[2023-02-07 14:36:10,565][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458962 effective words) took 0.4s, 3829376 effective words/s
[2023-02-07 14:36:10,947][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458962 effective words) took 0.4s, 3842410 effective words/s
[2023-02-07 14:36:11,323][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458962 effective words) took 0.4s, 3889602 effective words/s
[2023-02-07 14:36:11,702][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458962 effective words) took 0.4s, 3870630 effective words/s
[2023-02-07 14:36:12,083][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458962 effective words) took 0.4s, 3840077 effective words/s
[2023-02-07 14:36:12,466][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458962 effective words) took 0.4s, 3821915 effective words/s
[2023-02-07 14:36:12,847][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458962 effective words) took 0.4s, 3842304 effective words/s
[2023-02-07 14:36:13,232][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458962 effective words) took 0.4s, 3809334 effective words/s
[2023-02-07 14:36:13,619][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458962 effective words) took 0.4s, 3784266 effective words/s
[2023-02-07 14:36:14,006][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458962 effective words) took 0.4s, 3792088 effective words/s
[2023-02-07 14:36:14,393][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458962 effective words) took 0.4s, 3780895 effective words/s
[2023-02-07 14:36:14,781][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458962 effective words) took 0.4s, 3777511 effective words/s
[2023-02-07 14:36:15,164][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458962 effective words) took 0.4s, 3828488 effective words/s
[2023-02-07 14:36:15,551][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458962 effective words) took 0.4s, 3782327 effective words/s
[2023-02-07 14:36:15,937][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458962 effective words) took 0.4s, 3796749 effective words/s
[2023-02-07 14:36:16,325][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458962 effective words) took 0.4s, 3776284 effective words/s
[2023-02-07 14:36:16,713][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458962 effective words) took 0.4s, 3776134 effective words/s
[2023-02-07 14:36:17,102][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458962 effective words) took 0.4s, 3759198 effective words/s
[2023-02-07 14:36:17,494][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458962 effective words) took 0.4s, 3739976 effective words/s
[2023-02-07 14:36:17,883][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458962 effective words) took 0.4s, 3757904 effective words/s
[2023-02-07 14:36:18,272][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458962 effective words) took 0.4s, 3769522 effective words/s
[2023-02-07 14:36:18,665][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458962 effective words) took 0.4s, 3729002 effective words/s
[2023-02-07 14:36:19,056][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1458962 effective words) took 0.4s, 3740370 effective words/s
[2023-02-07 14:36:19,446][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1458962 effective words) took 0.4s, 3757948 effective words/s
[2023-02-07 14:36:19,836][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1458962 effective words) took 0.4s, 3757703 effective words/s
[2023-02-07 14:36:20,227][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1458962 effective words) took 0.4s, 3737486 effective words/s
[2023-02-07 14:36:20,615][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1458962 effective words) took 0.4s, 3778207 effective words/s
[2023-02-07 14:36:20,996][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1458962 effective words) took 0.4s, 3842305 effective words/s
[2023-02-07 14:36:21,375][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1458962 effective words) took 0.4s, 3861639 effective words/s
[2023-02-07 14:36:21,762][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1458962 effective words) took 0.4s, 3780378 effective words/s
[2023-02-07 14:36:22,149][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1458962 effective words) took 0.4s, 3785156 effective words/s
[2023-02-07 14:36:22,535][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1458962 effective words) took 0.4s, 3797648 effective words/s
[2023-02-07 14:36:22,919][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1458962 effective words) took 0.4s, 3826427 effective words/s
[2023-02-07 14:36:23,302][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1458962 effective words) took 0.4s, 3815355 effective words/s
[2023-02-07 14:36:23,697][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1458962 effective words) took 0.4s, 3713110 effective words/s
[2023-02-07 14:36:24,089][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1458962 effective words) took 0.4s, 3736398 effective words/s
[2023-02-07 14:36:24,491][gensim.models.word2vec][INFO] - EPOCH 70: training on 1455748 raw words (1458962 effective words) took 0.4s, 3647604 effective words/s
[2023-02-07 14:36:24,890][gensim.models.word2vec][INFO] - EPOCH 71: training on 1455748 raw words (1458962 effective words) took 0.4s, 3669712 effective words/s
[2023-02-07 14:36:25,289][gensim.models.word2vec][INFO] - EPOCH 72: training on 1455748 raw words (1458962 effective words) took 0.4s, 3661378 effective words/s
[2023-02-07 14:36:25,685][gensim.models.word2vec][INFO] - EPOCH 73: training on 1455748 raw words (1458962 effective words) took 0.4s, 3702315 effective words/s
[2023-02-07 14:36:26,083][gensim.models.word2vec][INFO] - EPOCH 74: training on 1455748 raw words (1458962 effective words) took 0.4s, 3672522 effective words/s
[2023-02-07 14:36:26,487][gensim.models.word2vec][INFO] - EPOCH 75: training on 1455748 raw words (1458962 effective words) took 0.4s, 3632071 effective words/s
[2023-02-07 14:36:26,890][gensim.models.word2vec][INFO] - EPOCH 76: training on 1455748 raw words (1458962 effective words) took 0.4s, 3625706 effective words/s
[2023-02-07 14:36:27,299][gensim.models.word2vec][INFO] - EPOCH 77: training on 1455748 raw words (1458962 effective words) took 0.4s, 3580494 effective words/s
[2023-02-07 14:36:27,712][gensim.models.word2vec][INFO] - EPOCH 78: training on 1455748 raw words (1458962 effective words) took 0.4s, 3546716 effective words/s
[2023-02-07 14:36:28,130][gensim.models.word2vec][INFO] - EPOCH 79: training on 1455748 raw words (1458962 effective words) took 0.4s, 3506544 effective words/s
[2023-02-07 14:36:28,544][gensim.models.word2vec][INFO] - EPOCH 80: training on 1455748 raw words (1458962 effective words) took 0.4s, 3539152 effective words/s
[2023-02-07 14:36:28,960][gensim.models.word2vec][INFO] - EPOCH 81: training on 1455748 raw words (1458962 effective words) took 0.4s, 3517826 effective words/s
[2023-02-07 14:36:29,376][gensim.models.word2vec][INFO] - EPOCH 82: training on 1455748 raw words (1458962 effective words) took 0.4s, 3522334 effective words/s
[2023-02-07 14:36:29,793][gensim.models.word2vec][INFO] - EPOCH 83: training on 1455748 raw words (1458962 effective words) took 0.4s, 3518627 effective words/s
[2023-02-07 14:36:30,214][gensim.models.word2vec][INFO] - EPOCH 84: training on 1455748 raw words (1458962 effective words) took 0.4s, 3478673 effective words/s
[2023-02-07 14:36:30,631][gensim.models.word2vec][INFO] - EPOCH 85: training on 1455748 raw words (1458962 effective words) took 0.4s, 3506824 effective words/s
[2023-02-07 14:36:31,053][gensim.models.word2vec][INFO] - EPOCH 86: training on 1455748 raw words (1458962 effective words) took 0.4s, 3487820 effective words/s
[2023-02-07 14:36:31,470][gensim.models.word2vec][INFO] - EPOCH 87: training on 1455748 raw words (1458962 effective words) took 0.4s, 3505084 effective words/s
[2023-02-07 14:36:31,891][gensim.models.word2vec][INFO] - EPOCH 88: training on 1455748 raw words (1458962 effective words) took 0.4s, 3478171 effective words/s
[2023-02-07 14:36:32,311][gensim.models.word2vec][INFO] - EPOCH 89: training on 1455748 raw words (1458962 effective words) took 0.4s, 3488258 effective words/s
[2023-02-07 14:36:32,729][gensim.models.word2vec][INFO] - EPOCH 90: training on 1455748 raw words (1458962 effective words) took 0.4s, 3503397 effective words/s
[2023-02-07 14:36:33,148][gensim.models.word2vec][INFO] - EPOCH 91: training on 1455748 raw words (1458962 effective words) took 0.4s, 3493315 effective words/s
[2023-02-07 14:36:33,571][gensim.models.word2vec][INFO] - EPOCH 92: training on 1455748 raw words (1458962 effective words) took 0.4s, 3465532 effective words/s
[2023-02-07 14:36:33,572][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 135384564 raw words (135683466 effective words) took 36.1s, 3757218 effective words/s', 'datetime': '2023-02-07T14:36:33.572214', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:36:33.572 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:36:36,037][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143539-cwm5rmk9/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:36:36.037767', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:36:36,038][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:36:36,044][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143539-cwm5rmk9/files/../tmp/embedding_model.pt
2023-02-07 14:36:36.044 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:36:37.308 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:36:37.812 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:36:38.907 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.041451403661865, 'test_mae': 1.083504910815073, 'test_r2': 0.03427142790875359}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.51
wandb: percentage 0.04654
wandb:   test_mae 1.0835
wandb:   test_mse 2.04145
wandb:    test_r2 0.03427
wandb: 
wandb: üöÄ View run earnest-sweep-86 at: https://wandb.ai/xiaoqiz/mof2vec/runs/cwm5rmk9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_143539-cwm5rmk9/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 6g30229m with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 983
wandb: 	model.gensim.alpha: 0.0007711191331645028
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.epochs: 71
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.20978969353901888
wandb: 	model.gensim.vector_size: 27
wandb: 	model.gensim.window: 1
wandb: 	model.sklearn.learning_rate: 0.1731226076648943
wandb: 	model.sklearn.max_depth: 44
wandb: 	model.sklearn.min_child_weight: 0.007041438149725006
wandb: 	model.sklearn.n_estimators: 4753
wandb: 	model.sklearn.num_leaves: 331
wandb: 	model.sklearn.reg_alpha: 0.021630922638399712
wandb: 	model.sklearn.reg_lambda: 0.00825178310836562
wandb: 	model.sklearn.subsample: 0.22740751837669543
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143723-6g30229m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-87
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/6g30229m
2023-02-07 14:37:30.579 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 14:37:30.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 983 for sweep.
2023-02-07 14:37:30.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0007711191331645028 for sweep.
2023-02-07 14:37:30.580 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 14:37:30.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 71 for sweep.
2023-02-07 14:37:30.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 14:37:30.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.20978969353901888 for sweep.
2023-02-07 14:37:30.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 27 for sweep.
2023-02-07 14:37:30.581 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 1 for sweep.
2023-02-07 14:37:30.582 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.1731226076648943 for sweep.
2023-02-07 14:37:30.582 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 44 for sweep.
2023-02-07 14:37:30.582 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.007041438149725006 for sweep.
2023-02-07 14:37:30.583 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4753 for sweep.
2023-02-07 14:37:30.583 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 331 for sweep.
2023-02-07 14:37:30.583 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.021630922638399712 for sweep.
2023-02-07 14:37:30.583 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.00825178310836562 for sweep.
2023-02-07 14:37:30.583 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.22740751837669543 for sweep.
2023-02-07 14:37:30.584 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:37:30.588 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143723-6g30229m/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 983, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 27, 'window': 1, 'min_count': 2, 'dm': 1, 'sample': 0.20978969353901888, 'workers': 4, 'alpha': 0.0007711191331645028, 'epochs': 71}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4753, 'max_depth': 44, 'num_leaves': 331, 'reg_alpha': 0.021630922638399712, 'reg_lambda': 0.00825178310836562, 'subsample': 0.22740751837669543, 'min_child_weight': 0.007041438149725006, 'n_jobs': 4, 'learning_rate': 0.1731226076648943}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|‚ñè         | 41/3257 [00:00<00:07, 409.28it/s]  3%|‚ñé         | 85/3257 [00:00<00:07, 425.26it/s]  4%|‚ñç         | 128/3257 [00:00<00:07, 412.45it/s]  5%|‚ñå         | 170/3257 [00:00<00:07, 411.70it/s]  7%|‚ñã         | 212/3257 [00:00<00:07, 410.53it/s]  8%|‚ñä         | 258/3257 [00:00<00:07, 426.29it/s]  9%|‚ñâ         | 304/3257 [00:00<00:06, 435.84it/s] 11%|‚ñà         | 348/3257 [00:00<00:06, 436.95it/s] 12%|‚ñà‚ñè        | 392/3257 [00:00<00:06, 430.22it/s] 13%|‚ñà‚ñé        | 436/3257 [00:01<00:06, 416.22it/s] 15%|‚ñà‚ñç        | 480/3257 [00:01<00:06, 422.01it/s] 16%|‚ñà‚ñå        | 524/3257 [00:01<00:06, 424.37it/s] 17%|‚ñà‚ñã        | 567/3257 [00:01<00:06, 411.49it/s] 19%|‚ñà‚ñä        | 609/3257 [00:01<00:06, 412.98it/s] 20%|‚ñà‚ñâ        | 651/3257 [00:01<00:06, 407.83it/s] 21%|‚ñà‚ñà        | 692/3257 [00:01<00:06, 388.84it/s] 22%|‚ñà‚ñà‚ñè       | 732/3257 [00:01<00:06, 375.82it/s] 24%|‚ñà‚ñà‚ñç       | 774/3257 [00:01<00:06, 387.49it/s] 25%|‚ñà‚ñà‚ñå       | 818/3257 [00:01<00:06, 398.76it/s] 26%|‚ñà‚ñà‚ñã       | 859/3257 [00:02<00:06, 394.76it/s] 28%|‚ñà‚ñà‚ñä       | 902/3257 [00:02<00:05, 402.82it/s] 29%|‚ñà‚ñà‚ñâ       | 944/3257 [00:02<00:05, 406.61it/s] 30%|‚ñà‚ñà‚ñà       | 988/3257 [00:02<00:05, 412.87it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1030/3257 [00:02<00:05, 410.75it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1073/3257 [00:02<00:05, 414.80it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1115/3257 [00:02<00:05, 414.88it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1157/3257 [00:02<00:07, 289.87it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1194/3257 [00:03<00:06, 306.57it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1235/3257 [00:03<00:06, 331.62it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:03<00:05, 347.06it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1315/3257 [00:03<00:05, 359.94it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1357/3257 [00:03<00:05, 376.46it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1399/3257 [00:03<00:04, 388.12it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1446/3257 [00:03<00:04, 409.38it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1494/3257 [00:03<00:04, 429.14it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1538/3257 [00:03<00:04, 424.24it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1581/3257 [00:03<00:04, 417.90it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1627/3257 [00:04<00:03, 429.94it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1671/3257 [00:04<00:03, 410.80it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1714/3257 [00:04<00:03, 413.97it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1756/3257 [00:04<00:03, 405.27it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1802/3257 [00:04<00:03, 420.49it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1845/3257 [00:04<00:03, 417.00it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1888/3257 [00:04<00:03, 418.13it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1930/3257 [00:04<00:03, 411.40it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1978/3257 [00:04<00:02, 427.91it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2021/3257 [00:05<00:02, 426.14it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2064/3257 [00:05<00:02, 405.89it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2107/3257 [00:05<00:02, 410.81it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2149/3257 [00:05<00:02, 399.56it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2193/3257 [00:05<00:02, 408.83it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:05<00:02, 414.14it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2278/3257 [00:05<00:02, 401.54it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2327/3257 [00:05<00:02, 425.05it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2374/3257 [00:05<00:02, 437.06it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2419/3257 [00:05<00:01, 437.82it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2463/3257 [00:06<00:01, 429.67it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2512/3257 [00:06<00:01, 445.64it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2557/3257 [00:06<00:02, 315.75it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2598/3257 [00:06<00:01, 335.46it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2646/3257 [00:06<00:01, 370.34it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2691/3257 [00:06<00:01, 389.00it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2733/3257 [00:06<00:01, 388.77it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2775/3257 [00:06<00:01, 397.14it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2822/3257 [00:07<00:01, 415.67it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2869/3257 [00:07<00:00, 430.82it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2914/3257 [00:07<00:00, 434.51it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2959/3257 [00:07<00:00, 420.94it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3004/3257 [00:07<00:00, 425.93it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3049/3257 [00:07<00:00, 432.33it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3097/3257 [00:07<00:00, 445.84it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3143/3257 [00:07<00:00, 445.38it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3188/3257 [00:07<00:00, 437.18it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3234/3257 [00:07<00:00, 442.52it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 405.66it/s]
2023-02-07 14:37:38.786 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:37:38,787][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d27,n5,w1,mc2,s0.20979,t4>', 'datetime': '2023-02-07T14:37:38.787414', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:37:38,787][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:37:38,787][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:37:38,918][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 14:37:38,919][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:37:38,921][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 881 unique words (95.35% of original 924, drops 43)', 'datetime': '2023-02-07T14:37:38.921213', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:37:38,921][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 1455705 word corpus (100.00% of original 1455748, drops 43)', 'datetime': '2023-02-07T14:37:38.921392', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:37:38,924][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 14:37:38,925][gensim.models.word2vec][INFO] - sample=0.20979 downsamples 0 most-common words
[2023-02-07 14:37:38,925][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455705 word corpus (100.0%% of prior 1455705)', 'datetime': '2023-02-07T14:37:38.925693', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:37:38,931][gensim.models.word2vec][INFO] - estimated required memory for 881 words and 27 dimensions: 1633952 bytes
[2023-02-07 14:37:38,931][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:37:38,932][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 881 vocabulary and 27 features, using sg=0 hs=0 sample=0.20978969353901888 negative=5 window=1 shrink_windows=True', 'datetime': '2023-02-07T14:37:38.932243', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:37:39,378][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458962 effective words) took 0.4s, 3278349 effective words/s
[2023-02-07 14:37:39,778][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458962 effective words) took 0.4s, 3661926 effective words/s
[2023-02-07 14:37:40,179][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458962 effective words) took 0.4s, 3647696 effective words/s
[2023-02-07 14:37:40,577][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458962 effective words) took 0.4s, 3673885 effective words/s
[2023-02-07 14:37:40,968][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458962 effective words) took 0.4s, 3751787 effective words/s
[2023-02-07 14:37:41,351][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458962 effective words) took 0.4s, 3821101 effective words/s
[2023-02-07 14:37:41,732][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458962 effective words) took 0.4s, 3840037 effective words/s
[2023-02-07 14:37:42,110][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458962 effective words) took 0.4s, 3874819 effective words/s
[2023-02-07 14:37:42,490][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458962 effective words) took 0.4s, 3856962 effective words/s
[2023-02-07 14:37:42,873][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458962 effective words) took 0.4s, 3821850 effective words/s
[2023-02-07 14:37:43,253][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458962 effective words) took 0.4s, 3852129 effective words/s
[2023-02-07 14:37:43,635][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458962 effective words) took 0.4s, 3834103 effective words/s
[2023-02-07 14:37:44,018][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458962 effective words) took 0.4s, 3812729 effective words/s
[2023-02-07 14:37:44,399][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458962 effective words) took 0.4s, 3843555 effective words/s
[2023-02-07 14:37:44,775][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458962 effective words) took 0.4s, 3893059 effective words/s
[2023-02-07 14:37:45,150][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458962 effective words) took 0.4s, 3904641 effective words/s
[2023-02-07 14:37:45,523][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458962 effective words) took 0.4s, 3919490 effective words/s
[2023-02-07 14:37:45,897][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458962 effective words) took 0.4s, 3912000 effective words/s
[2023-02-07 14:37:46,271][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458962 effective words) took 0.4s, 3923490 effective words/s
[2023-02-07 14:37:46,659][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458962 effective words) took 0.4s, 3781361 effective words/s
[2023-02-07 14:37:47,043][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458962 effective words) took 0.4s, 3815128 effective words/s
[2023-02-07 14:37:47,423][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458962 effective words) took 0.4s, 3854115 effective words/s
[2023-02-07 14:37:47,804][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458962 effective words) took 0.4s, 3842728 effective words/s
[2023-02-07 14:37:48,182][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458962 effective words) took 0.4s, 3867233 effective words/s
[2023-02-07 14:37:48,566][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458962 effective words) took 0.4s, 3816110 effective words/s
[2023-02-07 14:37:48,944][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458962 effective words) took 0.4s, 3869055 effective words/s
[2023-02-07 14:37:49,318][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458962 effective words) took 0.4s, 3914004 effective words/s
[2023-02-07 14:37:49,694][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458962 effective words) took 0.4s, 3894622 effective words/s
[2023-02-07 14:37:50,064][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458962 effective words) took 0.4s, 3954242 effective words/s
[2023-02-07 14:37:50,437][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458962 effective words) took 0.4s, 3922059 effective words/s
[2023-02-07 14:37:50,807][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458962 effective words) took 0.4s, 3958353 effective words/s
[2023-02-07 14:37:51,174][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458962 effective words) took 0.4s, 3991525 effective words/s
[2023-02-07 14:37:51,546][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458962 effective words) took 0.4s, 3927863 effective words/s
[2023-02-07 14:37:51,922][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458962 effective words) took 0.4s, 3894154 effective words/s
[2023-02-07 14:37:52,300][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458962 effective words) took 0.4s, 3878894 effective words/s
[2023-02-07 14:37:52,668][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458962 effective words) took 0.4s, 3972895 effective words/s
[2023-02-07 14:37:53,034][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458962 effective words) took 0.4s, 3993694 effective words/s
[2023-02-07 14:37:53,403][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458962 effective words) took 0.4s, 3966617 effective words/s
[2023-02-07 14:37:53,775][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458962 effective words) took 0.4s, 3943084 effective words/s
[2023-02-07 14:37:54,144][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458962 effective words) took 0.4s, 3969519 effective words/s
[2023-02-07 14:37:54,514][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458962 effective words) took 0.4s, 3953090 effective words/s
[2023-02-07 14:37:54,885][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458962 effective words) took 0.4s, 3948355 effective words/s
[2023-02-07 14:37:55,257][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458962 effective words) took 0.4s, 3926691 effective words/s
[2023-02-07 14:37:55,630][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458962 effective words) took 0.4s, 3929450 effective words/s
[2023-02-07 14:37:56,001][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458962 effective words) took 0.4s, 3953546 effective words/s
[2023-02-07 14:37:56,374][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458962 effective words) took 0.4s, 3925459 effective words/s
[2023-02-07 14:37:56,744][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458962 effective words) took 0.4s, 3959263 effective words/s
[2023-02-07 14:37:57,112][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458962 effective words) took 0.4s, 3969542 effective words/s
[2023-02-07 14:37:57,484][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458962 effective words) took 0.4s, 3940549 effective words/s
[2023-02-07 14:37:57,851][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458962 effective words) took 0.4s, 3993086 effective words/s
[2023-02-07 14:37:58,226][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458962 effective words) took 0.4s, 3912739 effective words/s
[2023-02-07 14:37:58,595][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458962 effective words) took 0.4s, 3963748 effective words/s
[2023-02-07 14:37:58,965][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458962 effective words) took 0.4s, 3954286 effective words/s
[2023-02-07 14:37:59,335][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458962 effective words) took 0.4s, 3959701 effective words/s
[2023-02-07 14:37:59,705][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458962 effective words) took 0.4s, 3957365 effective words/s
[2023-02-07 14:38:00,077][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458962 effective words) took 0.4s, 3942748 effective words/s
[2023-02-07 14:38:00,442][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1458962 effective words) took 0.4s, 4009850 effective words/s
[2023-02-07 14:38:00,813][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1458962 effective words) took 0.4s, 3939118 effective words/s
[2023-02-07 14:38:01,191][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1458962 effective words) took 0.4s, 3883055 effective words/s
[2023-02-07 14:38:01,561][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1458962 effective words) took 0.4s, 3956686 effective words/s
[2023-02-07 14:38:01,930][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1458962 effective words) took 0.4s, 3964328 effective words/s
[2023-02-07 14:38:02,298][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1458962 effective words) took 0.4s, 3976666 effective words/s
[2023-02-07 14:38:02,668][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1458962 effective words) took 0.4s, 3956874 effective words/s
[2023-02-07 14:38:03,035][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1458962 effective words) took 0.4s, 3979543 effective words/s
[2023-02-07 14:38:03,402][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1458962 effective words) took 0.4s, 3997233 effective words/s
[2023-02-07 14:38:03,764][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1458962 effective words) took 0.4s, 4040447 effective words/s
[2023-02-07 14:38:04,132][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1458962 effective words) took 0.4s, 3983792 effective words/s
[2023-02-07 14:38:04,498][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1458962 effective words) took 0.4s, 3995831 effective words/s
[2023-02-07 14:38:04,870][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1458962 effective words) took 0.4s, 3934791 effective words/s
[2023-02-07 14:38:05,238][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1458962 effective words) took 0.4s, 3983095 effective words/s
[2023-02-07 14:38:05,606][gensim.models.word2vec][INFO] - EPOCH 70: training on 1455748 raw words (1458962 effective words) took 0.4s, 3970588 effective words/s
[2023-02-07 14:38:05,608][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 103358108 raw words (103586302 effective words) took 26.7s, 3883172 effective words/s', 'datetime': '2023-02-07T14:38:05.608120', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:38:05.608 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:38:07,701][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143723-6g30229m/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:38:07.701857', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:38:07,702][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:38:07,706][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143723-6g30229m/files/../tmp/embedding_model.pt
2023-02-07 14:38:07.706 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:38:08.524 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:38:08.861 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:38:09.133 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.3187993776787117, 'test_mae': 1.1358142619598315, 'test_r2': -0.09693074640665977}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.65
wandb: percentage 0.04654
wandb:   test_mae 1.13581
wandb:   test_mse 2.3188
wandb:    test_r2 -0.09693
wandb: 
wandb: üöÄ View run whole-sweep-87 at: https://wandb.ai/xiaoqiz/mof2vec/runs/6g30229m
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_143723-6g30229m/logs
wandb: Agent Starting Run: gkeajiik with config:
wandb: 	data.data.wl_step: 4
wandb: 	data.nn.batch_size: 552
wandb: 	model.gensim.alpha: 0.0032540906583621715
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 92
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.4276556717698496
wandb: 	model.gensim.vector_size: 79
wandb: 	model.gensim.window: 2
wandb: 	model.sklearn.learning_rate: 0.05930944415735338
wandb: 	model.sklearn.max_depth: 81
wandb: 	model.sklearn.min_child_weight: 0.03238004745269453
wandb: 	model.sklearn.n_estimators: 3860
wandb: 	model.sklearn.num_leaves: 385
wandb: 	model.sklearn.reg_alpha: 0.005809911155089707
wandb: 	model.sklearn.reg_lambda: 0.7984883151066008
wandb: 	model.sklearn.subsample: 0.5963113185902722
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143822-gkeajiik
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-sweep-88
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/gkeajiik
2023-02-07 14:38:30.036 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 4 for sweep.
2023-02-07 14:38:30.036 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 552 for sweep.
2023-02-07 14:38:30.037 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0032540906583621715 for sweep.
2023-02-07 14:38:30.037 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:38:30.038 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 92 for sweep.
2023-02-07 14:38:30.038 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 14:38:30.038 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.4276556717698496 for sweep.
2023-02-07 14:38:30.038 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 79 for sweep.
2023-02-07 14:38:30.038 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 2 for sweep.
2023-02-07 14:38:30.039 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.05930944415735338 for sweep.
2023-02-07 14:38:30.039 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 81 for sweep.
2023-02-07 14:38:30.039 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.03238004745269453 for sweep.
2023-02-07 14:38:30.039 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3860 for sweep.
2023-02-07 14:38:30.040 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 385 for sweep.
2023-02-07 14:38:30.040 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.005809911155089707 for sweep.
2023-02-07 14:38:30.040 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.7984883151066008 for sweep.
2023-02-07 14:38:30.040 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5963113185902722 for sweep.
2023-02-07 14:38:30.040 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:38:30.046 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 4}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143822-gkeajiik/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 552, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 79, 'window': 2, 'min_count': 1, 'dm': 0, 'sample': 0.4276556717698496, 'workers': 4, 'alpha': 0.0032540906583621715, 'epochs': 92}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3860, 'max_depth': 81, 'num_leaves': 385, 'reg_alpha': 0.005809911155089707, 'reg_lambda': 0.7984883151066008, 'subsample': 0.5963113185902722, 'min_child_weight': 0.03238004745269453, 'n_jobs': 4, 'learning_rate': 0.05930944415735338}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 26/3257 [00:00<00:12, 253.42it/s]  2%|‚ñè         | 52/3257 [00:00<00:12, 255.35it/s]  2%|‚ñè         | 80/3257 [00:00<00:12, 264.27it/s]  3%|‚ñé         | 107/3257 [00:00<00:12, 251.25it/s]  4%|‚ñç         | 135/3257 [00:00<00:12, 258.52it/s]  5%|‚ñå         | 163/3257 [00:00<00:11, 260.74it/s]  6%|‚ñå         | 193/3257 [00:00<00:11, 270.70it/s]  7%|‚ñã         | 225/3257 [00:00<00:10, 284.85it/s]  8%|‚ñä         | 254/3257 [00:00<00:10, 285.56it/s]  9%|‚ñâ         | 287/3257 [00:01<00:10, 294.64it/s] 10%|‚ñâ         | 317/3257 [00:01<00:10, 289.62it/s] 11%|‚ñà         | 346/3257 [00:01<00:10, 288.63it/s] 12%|‚ñà‚ñè        | 375/3257 [00:01<00:10, 283.90it/s] 12%|‚ñà‚ñè        | 404/3257 [00:01<00:10, 273.36it/s] 13%|‚ñà‚ñé        | 432/3257 [00:01<00:11, 256.11it/s] 14%|‚ñà‚ñç        | 459/3257 [00:01<00:10, 257.52it/s] 15%|‚ñà‚ñç        | 488/3257 [00:01<00:10, 264.78it/s] 16%|‚ñà‚ñå        | 519/3257 [00:01<00:09, 276.70it/s] 17%|‚ñà‚ñã        | 547/3257 [00:02<00:10, 268.67it/s] 18%|‚ñà‚ñä        | 575/3257 [00:02<00:10, 252.38it/s] 18%|‚ñà‚ñä        | 601/3257 [00:02<00:13, 192.23it/s] 19%|‚ñà‚ñâ        | 630/3257 [00:02<00:12, 213.66it/s] 20%|‚ñà‚ñà        | 655/3257 [00:02<00:11, 219.44it/s] 21%|‚ñà‚ñà        | 683/3257 [00:02<00:11, 225.48it/s] 22%|‚ñà‚ñà‚ñè       | 712/3257 [00:02<00:10, 241.74it/s] 23%|‚ñà‚ñà‚ñé       | 738/3257 [00:02<00:10, 233.97it/s] 24%|‚ñà‚ñà‚ñé       | 769/3257 [00:03<00:09, 253.01it/s] 24%|‚ñà‚ñà‚ñç       | 796/3257 [00:03<00:09, 249.88it/s] 25%|‚ñà‚ñà‚ñå       | 822/3257 [00:03<00:09, 250.99it/s] 26%|‚ñà‚ñà‚ñå       | 848/3257 [00:03<00:10, 239.18it/s] 27%|‚ñà‚ñà‚ñã       | 874/3257 [00:03<00:09, 243.07it/s] 28%|‚ñà‚ñà‚ñä       | 899/3257 [00:03<00:09, 244.26it/s] 28%|‚ñà‚ñà‚ñä       | 927/3257 [00:03<00:09, 252.84it/s] 29%|‚ñà‚ñà‚ñâ       | 953/3257 [00:03<00:09, 254.73it/s] 30%|‚ñà‚ñà‚ñà       | 979/3257 [00:03<00:08, 254.76it/s] 31%|‚ñà‚ñà‚ñà       | 1006/3257 [00:03<00:08, 257.76it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1032/3257 [00:04<00:08, 247.99it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1057/3257 [00:04<00:09, 240.58it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1083/3257 [00:04<00:08, 244.07it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1111/3257 [00:04<00:08, 254.14it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1137/3257 [00:04<00:08, 249.42it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1164/3257 [00:04<00:08, 254.18it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1190/3257 [00:04<00:08, 235.26it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1215/3257 [00:04<00:08, 238.51it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1245/3257 [00:04<00:07, 253.42it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1272/3257 [00:05<00:07, 257.33it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1298/3257 [00:05<00:07, 246.46it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1328/3257 [00:05<00:07, 255.08it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1354/3257 [00:05<00:07, 253.67it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1381/3257 [00:05<00:07, 252.06it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1410/3257 [00:05<00:07, 262.63it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1438/3257 [00:05<00:06, 266.04it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1471/3257 [00:05<00:06, 283.57it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1503/3257 [00:05<00:06, 291.27it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1533/3257 [00:06<00:06, 269.26it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1561/3257 [00:06<00:06, 261.80it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1588/3257 [00:06<00:06, 261.59it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1618/3257 [00:06<00:06, 270.62it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1646/3257 [00:06<00:06, 262.07it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1673/3257 [00:06<00:06, 252.01it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1700/3257 [00:06<00:06, 255.02it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:06<00:06, 249.27it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1754/3257 [00:06<00:06, 247.59it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1784/3257 [00:06<00:05, 260.62it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1811/3257 [00:07<00:05, 252.51it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1838/3257 [00:07<00:05, 256.27it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1868/3257 [00:07<00:05, 268.44it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1895/3257 [00:07<00:07, 177.14it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1922/3257 [00:07<00:06, 196.14it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1957/3257 [00:07<00:05, 230.11it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1984/3257 [00:07<00:05, 231.79it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2014/3257 [00:08<00:05, 245.89it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2041/3257 [00:08<00:04, 250.39it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2068/3257 [00:08<00:04, 246.28it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2096/3257 [00:08<00:04, 251.45it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2122/3257 [00:08<00:04, 242.18it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2147/3257 [00:08<00:04, 239.18it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2177/3257 [00:08<00:04, 255.36it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2204/3257 [00:08<00:04, 256.21it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2230/3257 [00:08<00:04, 253.70it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2256/3257 [00:08<00:03, 252.39it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2282/3257 [00:09<00:03, 247.96it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2309/3257 [00:09<00:03, 253.73it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2343/3257 [00:09<00:03, 277.73it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2373/3257 [00:09<00:03, 282.95it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2405/3257 [00:09<00:02, 285.47it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2434/3257 [00:09<00:02, 274.92it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2462/3257 [00:09<00:02, 276.25it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2492/3257 [00:09<00:02, 281.67it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2523/3257 [00:09<00:02, 288.36it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2552/3257 [00:10<00:02, 287.45it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2581/3257 [00:10<00:02, 265.00it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2611/3257 [00:10<00:02, 272.76it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2641/3257 [00:10<00:02, 280.00it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2670/3257 [00:10<00:02, 271.84it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2698/3257 [00:10<00:02, 255.42it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2724/3257 [00:10<00:02, 253.01it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:10<00:01, 266.92it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2783/3257 [00:10<00:01, 266.43it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2812/3257 [00:11<00:01, 272.17it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2840/3257 [00:11<00:01, 252.99it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2872/3257 [00:11<00:01, 267.93it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2900/3257 [00:11<00:01, 259.26it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2928/3257 [00:11<00:01, 263.77it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2955/3257 [00:11<00:01, 254.33it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2981/3257 [00:11<00:01, 250.02it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:11<00:00, 261.06it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3041/3257 [00:11<00:00, 267.79it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3072/3257 [00:11<00:00, 278.64it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3100/3257 [00:12<00:00, 276.53it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3130/3257 [00:12<00:00, 281.83it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3159/3257 [00:12<00:00, 265.83it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3186/3257 [00:12<00:00, 259.94it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3213/3257 [00:12<00:00, 255.99it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:12<00:00, 268.69it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:12<00:00, 256.69it/s]
2023-02-07 14:38:43.198 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:38:43,199][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d79,n5,s0.427656,t4>', 'datetime': '2023-02-07T14:38:43.199837', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:38:43,200][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:38:43,200][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:38:43,510][gensim.models.doc2vec][INFO] - collected 13061 word types and 3257 unique tags from a corpus of 3257 examples and 3639370 words
[2023-02-07 14:38:43,510][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:38:43,541][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 13061 unique words (100.00% of original 13061, drops 0)', 'datetime': '2023-02-07T14:38:43.541083', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:38:43,541][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 3639370 word corpus (100.00% of original 3639370, drops 0)', 'datetime': '2023-02-07T14:38:43.541448', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:38:43,583][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 13061 items
[2023-02-07 14:38:43,583][gensim.models.word2vec][INFO] - sample=0.427656 downsamples 0 most-common words
[2023-02-07 14:38:43,584][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 3639370 word corpus (100.0%% of prior 3639370)', 'datetime': '2023-02-07T14:38:43.584066', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:38:43,657][gensim.models.word2vec][INFO] - estimated required memory for 13061 words and 79 dimensions: 16465664 bytes
[2023-02-07 14:38:43,657][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:38:43,662][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 13061 vocabulary and 79 features, using sg=1 hs=0 sample=0.4276556717698496 negative=5 window=2 shrink_windows=True', 'datetime': '2023-02-07T14:38:43.662704', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:38:44,671][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 86.37% examples, 3150482 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:38:44,820][gensim.models.word2vec][INFO] - EPOCH 0: training on 3639370 raw words (3642627 effective words) took 1.2s, 3153216 effective words/s
[2023-02-07 14:38:45,824][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 91.03% examples, 3330610 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:38:45,912][gensim.models.word2vec][INFO] - EPOCH 1: training on 3639370 raw words (3642627 effective words) took 1.1s, 3340396 effective words/s
[2023-02-07 14:38:46,919][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 91.65% examples, 3346258 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:38:46,999][gensim.models.word2vec][INFO] - EPOCH 2: training on 3639370 raw words (3642627 effective words) took 1.1s, 3355381 effective words/s
[2023-02-07 14:38:48,003][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 90.33% examples, 3301677 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:38:48,105][gensim.models.word2vec][INFO] - EPOCH 3: training on 3639370 raw words (3642627 effective words) took 1.1s, 3298893 effective words/s
[2023-02-07 14:38:49,108][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 90.42% examples, 3315662 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:38:49,205][gensim.models.word2vec][INFO] - EPOCH 4: training on 3639370 raw words (3642627 effective words) took 1.1s, 3317244 effective words/s
[2023-02-07 14:38:50,213][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 91.65% examples, 3344595 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:38:50,292][gensim.models.word2vec][INFO] - EPOCH 5: training on 3639370 raw words (3642627 effective words) took 1.1s, 3357136 effective words/s
[2023-02-07 14:38:51,296][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 90.67% examples, 3318117 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:38:51,385][gensim.models.word2vec][INFO] - EPOCH 6: training on 3639370 raw words (3642627 effective words) took 1.1s, 3336023 effective words/s
[2023-02-07 14:38:52,391][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 91.65% examples, 3352146 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:38:52,471][gensim.models.word2vec][INFO] - EPOCH 7: training on 3639370 raw words (3642627 effective words) took 1.1s, 3360352 effective words/s
[2023-02-07 14:38:53,476][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 91.65% examples, 3352412 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:38:53,554][gensim.models.word2vec][INFO] - EPOCH 8: training on 3639370 raw words (3642627 effective words) took 1.1s, 3368411 effective words/s
[2023-02-07 14:38:54,558][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 93.09% examples, 3400186 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:38:54,624][gensim.models.word2vec][INFO] - EPOCH 9: training on 3639370 raw words (3642627 effective words) took 1.1s, 3406791 effective words/s
[2023-02-07 14:38:55,627][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 92.54% examples, 3392465 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:38:55,703][gensim.models.word2vec][INFO] - EPOCH 10: training on 3639370 raw words (3642627 effective words) took 1.1s, 3384618 effective words/s
[2023-02-07 14:38:56,706][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 92.75% examples, 3396825 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:38:56,772][gensim.models.word2vec][INFO] - EPOCH 11: training on 3639370 raw words (3642627 effective words) took 1.1s, 3410611 effective words/s
[2023-02-07 14:38:57,778][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 94.11% examples, 3422713 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:38:57,836][gensim.models.word2vec][INFO] - EPOCH 12: training on 3639370 raw words (3642627 effective words) took 1.1s, 3429113 effective words/s
[2023-02-07 14:38:58,838][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 91.03% examples, 3335832 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:38:58,928][gensim.models.word2vec][INFO] - EPOCH 13: training on 3639370 raw words (3642627 effective words) took 1.1s, 3343203 effective words/s
[2023-02-07 14:38:59,929][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 95.30% examples, 3476469 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:38:59,975][gensim.models.word2vec][INFO] - EPOCH 14: training on 3639370 raw words (3642627 effective words) took 1.0s, 3484730 effective words/s
[2023-02-07 14:39:00,977][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 93.40% examples, 3418627 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:01,040][gensim.models.word2vec][INFO] - EPOCH 15: training on 3639370 raw words (3642627 effective words) took 1.1s, 3425449 effective words/s
[2023-02-07 14:39:02,044][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 96.41% examples, 3506995 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:02,076][gensim.models.word2vec][INFO] - EPOCH 16: training on 3639370 raw words (3642627 effective words) took 1.0s, 3522403 effective words/s
[2023-02-07 14:39:03,081][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 97.24% examples, 3536709 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:03,103][gensim.models.word2vec][INFO] - EPOCH 17: training on 3639370 raw words (3642627 effective words) took 1.0s, 3547494 effective words/s
[2023-02-07 14:39:04,108][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 96.41% examples, 3503191 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:04,141][gensim.models.word2vec][INFO] - EPOCH 18: training on 3639370 raw words (3642627 effective words) took 1.0s, 3513159 effective words/s
[2023-02-07 14:39:05,143][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 96.41% examples, 3515158 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:05,177][gensim.models.word2vec][INFO] - EPOCH 19: training on 3639370 raw words (3642627 effective words) took 1.0s, 3522901 effective words/s
[2023-02-07 14:39:06,182][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 96.41% examples, 3501857 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:06,215][gensim.models.word2vec][INFO] - EPOCH 20: training on 3639370 raw words (3642627 effective words) took 1.0s, 3513278 effective words/s
[2023-02-07 14:39:07,217][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 97.54% examples, 3558218 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:07,239][gensim.models.word2vec][INFO] - EPOCH 21: training on 3639370 raw words (3642627 effective words) took 1.0s, 3562676 effective words/s
[2023-02-07 14:39:08,244][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 97.45% examples, 3536857 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:08,267][gensim.models.word2vec][INFO] - EPOCH 22: training on 3639370 raw words (3642627 effective words) took 1.0s, 3547235 effective words/s
[2023-02-07 14:39:09,268][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 97.85% examples, 3569721 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:09,286][gensim.models.word2vec][INFO] - EPOCH 23: training on 3639370 raw words (3642627 effective words) took 1.0s, 3577988 effective words/s
[2023-02-07 14:39:10,289][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 97.85% examples, 3563463 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:10,306][gensim.models.word2vec][INFO] - EPOCH 24: training on 3639370 raw words (3642627 effective words) took 1.0s, 3573963 effective words/s
[2023-02-07 14:39:11,308][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 98.62% examples, 3595591 words/s, in_qsize 5, out_qsize 0
[2023-02-07 14:39:11,320][gensim.models.word2vec][INFO] - EPOCH 25: training on 3639370 raw words (3642627 effective words) took 1.0s, 3598709 effective words/s
[2023-02-07 14:39:12,322][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 99.85% examples, 3630544 words/s, in_qsize 1, out_qsize 1
[2023-02-07 14:39:12,324][gensim.models.word2vec][INFO] - EPOCH 26: training on 3639370 raw words (3642627 effective words) took 1.0s, 3630405 effective words/s
[2023-02-07 14:39:13,326][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 97.85% examples, 3569701 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:13,344][gensim.models.word2vec][INFO] - EPOCH 27: training on 3639370 raw words (3642627 effective words) took 1.0s, 3575686 effective words/s
[2023-02-07 14:39:14,352][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 97.45% examples, 3529821 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:14,375][gensim.models.word2vec][INFO] - EPOCH 28: training on 3639370 raw words (3642627 effective words) took 1.0s, 3537585 effective words/s
[2023-02-07 14:39:15,378][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 96.68% examples, 3518485 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:39:15,408][gensim.models.word2vec][INFO] - EPOCH 29: training on 3639370 raw words (3642627 effective words) took 1.0s, 3530927 effective words/s
[2023-02-07 14:39:16,416][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 97.24% examples, 3529534 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:16,438][gensim.models.word2vec][INFO] - EPOCH 30: training on 3639370 raw words (3642627 effective words) took 1.0s, 3542070 effective words/s
[2023-02-07 14:39:17,439][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 96.68% examples, 3524130 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:39:17,468][gensim.models.word2vec][INFO] - EPOCH 31: training on 3639370 raw words (3642627 effective words) took 1.0s, 3540985 effective words/s
[2023-02-07 14:39:18,474][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 97.45% examples, 3535276 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:18,497][gensim.models.word2vec][INFO] - EPOCH 32: training on 3639370 raw words (3642627 effective words) took 1.0s, 3544481 effective words/s
[2023-02-07 14:39:19,500][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 97.88% examples, 3565937 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:19,517][gensim.models.word2vec][INFO] - EPOCH 33: training on 3639370 raw words (3642627 effective words) took 1.0s, 3575886 effective words/s
[2023-02-07 14:39:20,521][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 97.24% examples, 3542489 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:20,543][gensim.models.word2vec][INFO] - EPOCH 34: training on 3639370 raw words (3642627 effective words) took 1.0s, 3554723 effective words/s
[2023-02-07 14:39:21,548][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 97.45% examples, 3538886 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:21,572][gensim.models.word2vec][INFO] - EPOCH 35: training on 3639370 raw words (3642627 effective words) took 1.0s, 3542398 effective words/s
[2023-02-07 14:39:22,576][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 98.62% examples, 3590064 words/s, in_qsize 5, out_qsize 0
[2023-02-07 14:39:22,587][gensim.models.word2vec][INFO] - EPOCH 36: training on 3639370 raw words (3642627 effective words) took 1.0s, 3597182 effective words/s
[2023-02-07 14:39:23,590][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 98.43% examples, 3585585 words/s, in_qsize 6, out_qsize 0
[2023-02-07 14:39:23,602][gensim.models.word2vec][INFO] - EPOCH 37: training on 3639370 raw words (3642627 effective words) took 1.0s, 3595489 effective words/s
[2023-02-07 14:39:24,607][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 97.24% examples, 3538164 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:24,631][gensim.models.word2vec][INFO] - EPOCH 38: training on 3639370 raw words (3642627 effective words) took 1.0s, 3545564 effective words/s
[2023-02-07 14:39:25,637][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 98.80% examples, 3588485 words/s, in_qsize 4, out_qsize 0
[2023-02-07 14:39:25,645][gensim.models.word2vec][INFO] - EPOCH 39: training on 3639370 raw words (3642627 effective words) took 1.0s, 3597269 effective words/s
[2023-02-07 14:39:26,650][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 97.85% examples, 3560580 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:26,667][gensim.models.word2vec][INFO] - EPOCH 40: training on 3639370 raw words (3642627 effective words) took 1.0s, 3570346 effective words/s
[2023-02-07 14:39:27,670][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 98.62% examples, 3592900 words/s, in_qsize 5, out_qsize 0
[2023-02-07 14:39:27,682][gensim.models.word2vec][INFO] - EPOCH 41: training on 3639370 raw words (3642627 effective words) took 1.0s, 3593583 effective words/s
[2023-02-07 14:39:28,684][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 99.48% examples, 3619672 words/s, in_qsize 2, out_qsize 1
[2023-02-07 14:39:28,688][gensim.models.word2vec][INFO] - EPOCH 42: training on 3639370 raw words (3642627 effective words) took 1.0s, 3626170 effective words/s
[2023-02-07 14:39:29,691][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 99.17% examples, 3611773 words/s, in_qsize 3, out_qsize 1
[2023-02-07 14:39:29,695][gensim.models.word2vec][INFO] - EPOCH 43: training on 3639370 raw words (3642627 effective words) took 1.0s, 3622592 effective words/s
[2023-02-07 14:39:30,699][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 97.85% examples, 3562263 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:30,717][gensim.models.word2vec][INFO] - EPOCH 44: training on 3639370 raw words (3642627 effective words) took 1.0s, 3570960 effective words/s
[2023-02-07 14:39:31,719][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 98.62% examples, 3592716 words/s, in_qsize 5, out_qsize 0
[2023-02-07 14:39:31,732][gensim.models.word2vec][INFO] - EPOCH 45: training on 3639370 raw words (3642627 effective words) took 1.0s, 3592493 effective words/s
[2023-02-07 14:39:32,733][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 98.19% examples, 3578376 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:39:32,750][gensim.models.word2vec][INFO] - EPOCH 46: training on 3639370 raw words (3642627 effective words) took 1.0s, 3581820 effective words/s
[2023-02-07 14:39:33,750][gensim.models.word2vec][INFO] - EPOCH 47: training on 3639370 raw words (3642627 effective words) took 1.0s, 3646046 effective words/s
[2023-02-07 14:39:34,752][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 99.48% examples, 3621191 words/s, in_qsize 2, out_qsize 1
[2023-02-07 14:39:34,756][gensim.models.word2vec][INFO] - EPOCH 48: training on 3639370 raw words (3642627 effective words) took 1.0s, 3626439 effective words/s
[2023-02-07 14:39:35,758][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 99.85% examples, 3628033 words/s, in_qsize 1, out_qsize 1
[2023-02-07 14:39:35,760][gensim.models.word2vec][INFO] - EPOCH 49: training on 3639370 raw words (3642627 effective words) took 1.0s, 3630400 effective words/s
[2023-02-07 14:39:36,759][gensim.models.word2vec][INFO] - EPOCH 50: training on 3639370 raw words (3642627 effective words) took 1.0s, 3652993 effective words/s
[2023-02-07 14:39:37,754][gensim.models.word2vec][INFO] - EPOCH 51: training on 3639370 raw words (3642627 effective words) took 1.0s, 3663104 effective words/s
[2023-02-07 14:39:38,756][gensim.models.word2vec][INFO] - EPOCH 52: training on 3639370 raw words (3642627 effective words) took 1.0s, 3644798 effective words/s
[2023-02-07 14:39:39,757][gensim.models.word2vec][INFO] - EPOCH 53: training on 3639370 raw words (3642627 effective words) took 1.0s, 3644369 effective words/s
[2023-02-07 14:39:40,759][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 98.43% examples, 3586818 words/s, in_qsize 6, out_qsize 0
[2023-02-07 14:39:40,772][gensim.models.word2vec][INFO] - EPOCH 54: training on 3639370 raw words (3642627 effective words) took 1.0s, 3593400 effective words/s
[2023-02-07 14:39:41,775][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 99.48% examples, 3616593 words/s, in_qsize 2, out_qsize 1
[2023-02-07 14:39:41,779][gensim.models.word2vec][INFO] - EPOCH 55: training on 3639370 raw words (3642627 effective words) took 1.0s, 3623049 effective words/s
[2023-02-07 14:39:42,769][gensim.models.word2vec][INFO] - EPOCH 56: training on 3639370 raw words (3642627 effective words) took 1.0s, 3685548 effective words/s
[2023-02-07 14:39:43,769][gensim.models.word2vec][INFO] - EPOCH 57: training on 3639370 raw words (3642627 effective words) took 1.0s, 3647641 effective words/s
[2023-02-07 14:39:44,769][gensim.models.word2vec][INFO] - EPOCH 58: training on 3639370 raw words (3642627 effective words) took 1.0s, 3647726 effective words/s
[2023-02-07 14:39:45,773][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 100.00% examples, 3633092 words/s, in_qsize 0, out_qsize 1
[2023-02-07 14:39:45,774][gensim.models.word2vec][INFO] - EPOCH 59: training on 3639370 raw words (3642627 effective words) took 1.0s, 3631691 effective words/s
[2023-02-07 14:39:46,763][gensim.models.word2vec][INFO] - EPOCH 60: training on 3639370 raw words (3642627 effective words) took 1.0s, 3684031 effective words/s
[2023-02-07 14:39:47,766][gensim.models.word2vec][INFO] - EPOCH 61 - PROGRESS: at 98.80% examples, 3601216 words/s, in_qsize 4, out_qsize 0
[2023-02-07 14:39:47,774][gensim.models.word2vec][INFO] - EPOCH 61: training on 3639370 raw words (3642627 effective words) took 1.0s, 3611011 effective words/s
[2023-02-07 14:39:48,770][gensim.models.word2vec][INFO] - EPOCH 62: training on 3639370 raw words (3642627 effective words) took 1.0s, 3657889 effective words/s
[2023-02-07 14:39:49,772][gensim.models.word2vec][INFO] - EPOCH 63 - PROGRESS: at 99.54% examples, 3624856 words/s, in_qsize 2, out_qsize 1
[2023-02-07 14:39:49,775][gensim.models.word2vec][INFO] - EPOCH 63: training on 3639370 raw words (3642627 effective words) took 1.0s, 3629884 effective words/s
[2023-02-07 14:39:50,777][gensim.models.word2vec][INFO] - EPOCH 64 - PROGRESS: at 100.00% examples, 3638694 words/s, in_qsize 0, out_qsize 1
[2023-02-07 14:39:50,778][gensim.models.word2vec][INFO] - EPOCH 64: training on 3639370 raw words (3642627 effective words) took 1.0s, 3637237 effective words/s
[2023-02-07 14:39:51,781][gensim.models.word2vec][INFO] - EPOCH 65 - PROGRESS: at 99.17% examples, 3609030 words/s, in_qsize 3, out_qsize 1
[2023-02-07 14:39:51,787][gensim.models.word2vec][INFO] - EPOCH 65: training on 3639370 raw words (3642627 effective words) took 1.0s, 3613527 effective words/s
[2023-02-07 14:39:52,791][gensim.models.word2vec][INFO] - EPOCH 66 - PROGRESS: at 100.00% examples, 3631113 words/s, in_qsize 0, out_qsize 1
[2023-02-07 14:39:52,792][gensim.models.word2vec][INFO] - EPOCH 66: training on 3639370 raw words (3642627 effective words) took 1.0s, 3629723 effective words/s
[2023-02-07 14:39:53,788][gensim.models.word2vec][INFO] - EPOCH 67: training on 3639370 raw words (3642627 effective words) took 1.0s, 3658001 effective words/s
[2023-02-07 14:39:54,785][gensim.models.word2vec][INFO] - EPOCH 68: training on 3639370 raw words (3642627 effective words) took 1.0s, 3658297 effective words/s
[2023-02-07 14:39:55,774][gensim.models.word2vec][INFO] - EPOCH 69: training on 3639370 raw words (3642627 effective words) took 1.0s, 3689964 effective words/s
[2023-02-07 14:39:56,776][gensim.models.word2vec][INFO] - EPOCH 70 - PROGRESS: at 98.80% examples, 3602795 words/s, in_qsize 4, out_qsize 0
[2023-02-07 14:39:56,783][gensim.models.word2vec][INFO] - EPOCH 70: training on 3639370 raw words (3642627 effective words) took 1.0s, 3612148 effective words/s
[2023-02-07 14:39:57,772][gensim.models.word2vec][INFO] - EPOCH 71: training on 3639370 raw words (3642627 effective words) took 1.0s, 3689084 effective words/s
[2023-02-07 14:39:58,759][gensim.models.word2vec][INFO] - EPOCH 72: training on 3639370 raw words (3642627 effective words) took 1.0s, 3695491 effective words/s
[2023-02-07 14:39:59,755][gensim.models.word2vec][INFO] - EPOCH 73: training on 3639370 raw words (3642627 effective words) took 1.0s, 3660885 effective words/s
[2023-02-07 14:40:00,757][gensim.models.word2vec][INFO] - EPOCH 74 - PROGRESS: at 99.48% examples, 3620042 words/s, in_qsize 2, out_qsize 1
[2023-02-07 14:40:00,761][gensim.models.word2vec][INFO] - EPOCH 74: training on 3639370 raw words (3642627 effective words) took 1.0s, 3624410 effective words/s
[2023-02-07 14:40:01,763][gensim.models.word2vec][INFO] - EPOCH 75: training on 3639370 raw words (3642627 effective words) took 1.0s, 3642797 effective words/s
[2023-02-07 14:40:02,767][gensim.models.word2vec][INFO] - EPOCH 76 - PROGRESS: at 100.00% examples, 3631152 words/s, in_qsize 0, out_qsize 1
[2023-02-07 14:40:02,768][gensim.models.word2vec][INFO] - EPOCH 76: training on 3639370 raw words (3642627 effective words) took 1.0s, 3629759 effective words/s
[2023-02-07 14:40:03,768][gensim.models.word2vec][INFO] - EPOCH 77: training on 3639370 raw words (3642627 effective words) took 1.0s, 3645390 effective words/s
[2023-02-07 14:40:04,760][gensim.models.word2vec][INFO] - EPOCH 78: training on 3639370 raw words (3642627 effective words) took 1.0s, 3680082 effective words/s
[2023-02-07 14:40:05,740][gensim.models.word2vec][INFO] - EPOCH 79: training on 3639370 raw words (3642627 effective words) took 1.0s, 3720420 effective words/s
[2023-02-07 14:40:06,724][gensim.models.word2vec][INFO] - EPOCH 80: training on 3639370 raw words (3642627 effective words) took 1.0s, 3706303 effective words/s
[2023-02-07 14:40:07,712][gensim.models.word2vec][INFO] - EPOCH 81: training on 3639370 raw words (3642627 effective words) took 1.0s, 3696723 effective words/s
[2023-02-07 14:40:08,704][gensim.models.word2vec][INFO] - EPOCH 82: training on 3639370 raw words (3642627 effective words) took 1.0s, 3677490 effective words/s
[2023-02-07 14:40:09,702][gensim.models.word2vec][INFO] - EPOCH 83: training on 3639370 raw words (3642627 effective words) took 1.0s, 3656356 effective words/s
[2023-02-07 14:40:10,695][gensim.models.word2vec][INFO] - EPOCH 84: training on 3639370 raw words (3642627 effective words) took 1.0s, 3673199 effective words/s
[2023-02-07 14:40:11,681][gensim.models.word2vec][INFO] - EPOCH 85: training on 3639370 raw words (3642627 effective words) took 1.0s, 3697153 effective words/s
[2023-02-07 14:40:12,662][gensim.models.word2vec][INFO] - EPOCH 86: training on 3639370 raw words (3642627 effective words) took 1.0s, 3719382 effective words/s
[2023-02-07 14:40:13,636][gensim.models.word2vec][INFO] - EPOCH 87: training on 3639370 raw words (3642627 effective words) took 1.0s, 3746968 effective words/s
[2023-02-07 14:40:14,619][gensim.models.word2vec][INFO] - EPOCH 88: training on 3639370 raw words (3642627 effective words) took 1.0s, 3710509 effective words/s
[2023-02-07 14:40:15,600][gensim.models.word2vec][INFO] - EPOCH 89: training on 3639370 raw words (3642627 effective words) took 1.0s, 3716326 effective words/s
[2023-02-07 14:40:16,576][gensim.models.word2vec][INFO] - EPOCH 90: training on 3639370 raw words (3642627 effective words) took 1.0s, 3739040 effective words/s
[2023-02-07 14:40:17,559][gensim.models.word2vec][INFO] - EPOCH 91: training on 3639370 raw words (3642627 effective words) took 1.0s, 3713036 effective words/s
[2023-02-07 14:40:17,559][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 334822040 raw words (335121684 effective words) took 93.9s, 3569088 effective words/s', 'datetime': '2023-02-07T14:40:17.559589', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:40:17.559 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:40:24,602][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143822-gkeajiik/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:40:24.602589', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:40:24,603][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:40:24,635][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_143822-gkeajiik/files/../tmp/embedding_model.pt
2023-02-07 14:40:24.636 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:40:25.918 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:40:26.337 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:40:26.933 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8708309299415302, 'test_mae': 1.0409445481637687, 'test_r2': 0.1149851133581884}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.97
wandb: percentage 0.0
wandb:   test_mae 1.04094
wandb:   test_mse 1.87083
wandb:    test_r2 0.11499
wandb: 
wandb: üöÄ View run clear-sweep-88 at: https://wandb.ai/xiaoqiz/mof2vec/runs/gkeajiik
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_143822-gkeajiik/logs
wandb: Agent Starting Run: m4745wue with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 989
wandb: 	model.gensim.alpha: 0.000571054074247893
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 22
wandb: 	model.gensim.min_count: 7
wandb: 	model.gensim.sample: 0.8937883767869217
wandb: 	model.gensim.vector_size: 213
wandb: 	model.gensim.window: 1
wandb: 	model.sklearn.learning_rate: 0.0559261100677791
wandb: 	model.sklearn.max_depth: 15
wandb: 	model.sklearn.min_child_weight: 0.004337801671783322
wandb: 	model.sklearn.n_estimators: 990
wandb: 	model.sklearn.num_leaves: 335
wandb: 	model.sklearn.reg_alpha: 0.019226831571796055
wandb: 	model.sklearn.reg_lambda: 0.10476719605391516
wandb: 	model.sklearn.subsample: 0.6828788904045543
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144038-m4745wue
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-89
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/m4745wue
2023-02-07 14:40:45.527 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 14:40:45.528 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 989 for sweep.
2023-02-07 14:40:45.528 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.000571054074247893 for sweep.
2023-02-07 14:40:45.528 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:40:45.528 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 22 for sweep.
2023-02-07 14:40:45.529 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 7 for sweep.
2023-02-07 14:40:45.529 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.8937883767869217 for sweep.
2023-02-07 14:40:45.529 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 213 for sweep.
2023-02-07 14:40:45.529 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 1 for sweep.
2023-02-07 14:40:45.530 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.0559261100677791 for sweep.
2023-02-07 14:40:45.530 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 15 for sweep.
2023-02-07 14:40:45.530 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.004337801671783322 for sweep.
2023-02-07 14:40:45.530 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 990 for sweep.
2023-02-07 14:40:45.530 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 335 for sweep.
2023-02-07 14:40:45.531 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.019226831571796055 for sweep.
2023-02-07 14:40:45.531 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.10476719605391516 for sweep.
2023-02-07 14:40:45.531 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.6828788904045543 for sweep.
2023-02-07 14:40:45.531 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:40:45.538 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144038-m4745wue/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 989, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 213, 'window': 1, 'min_count': 7, 'dm': 0, 'sample': 0.8937883767869217, 'workers': 4, 'alpha': 0.000571054074247893, 'epochs': 22}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 990, 'max_depth': 15, 'num_leaves': 335, 'reg_alpha': 0.019226831571796055, 'reg_lambda': 0.10476719605391516, 'subsample': 0.6828788904045543, 'min_child_weight': 0.004337801671783322, 'n_jobs': 4, 'learning_rate': 0.0559261100677791}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 34/3257 [00:00<00:09, 336.16it/s]  2%|‚ñè         | 71/3257 [00:00<00:09, 351.44it/s]  3%|‚ñé         | 107/3257 [00:00<00:08, 352.54it/s]  5%|‚ñç         | 147/3257 [00:00<00:08, 368.60it/s]  6%|‚ñå         | 184/3257 [00:00<00:08, 358.82it/s]  7%|‚ñã         | 227/3257 [00:00<00:07, 380.43it/s]  8%|‚ñä         | 266/3257 [00:00<00:07, 374.39it/s]  9%|‚ñâ         | 307/3257 [00:00<00:07, 385.21it/s] 11%|‚ñà         | 346/3257 [00:00<00:07, 380.16it/s] 12%|‚ñà‚ñè        | 385/3257 [00:01<00:07, 376.52it/s] 13%|‚ñà‚ñé        | 423/3257 [00:01<00:07, 373.57it/s] 14%|‚ñà‚ñç        | 461/3257 [00:01<00:07, 352.65it/s] 15%|‚ñà‚ñå        | 500/3257 [00:01<00:07, 360.89it/s] 17%|‚ñà‚ñã        | 538/3257 [00:01<00:07, 366.08it/s] 18%|‚ñà‚ñä        | 575/3257 [00:01<00:07, 346.75it/s] 19%|‚ñà‚ñâ        | 616/3257 [00:01<00:07, 363.60it/s] 20%|‚ñà‚ñà        | 653/3257 [00:01<00:07, 350.39it/s] 21%|‚ñà‚ñà        | 689/3257 [00:01<00:07, 344.11it/s] 22%|‚ñà‚ñà‚ñè       | 724/3257 [00:02<00:07, 341.47it/s] 23%|‚ñà‚ñà‚ñé       | 762/3257 [00:02<00:07, 350.72it/s] 25%|‚ñà‚ñà‚ñç       | 799/3257 [00:02<00:06, 352.55it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:02<00:06, 350.31it/s] 27%|‚ñà‚ñà‚ñã       | 871/3257 [00:02<00:06, 340.90it/s] 28%|‚ñà‚ñà‚ñä       | 906/3257 [00:02<00:06, 343.28it/s] 29%|‚ñà‚ñà‚ñâ       | 941/3257 [00:02<00:06, 342.57it/s] 30%|‚ñà‚ñà‚ñâ       | 977/3257 [00:02<00:06, 347.48it/s] 31%|‚ñà‚ñà‚ñà       | 1012/3257 [00:02<00:06, 336.25it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1046/3257 [00:02<00:06, 333.31it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1081/3257 [00:03<00:06, 336.94it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1116/3257 [00:03<00:06, 340.38it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1151/3257 [00:03<00:06, 320.26it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1184/3257 [00:03<00:06, 312.46it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1216/3257 [00:03<00:06, 298.33it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:03<00:06, 304.74it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1280/3257 [00:03<00:09, 212.80it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1314/3257 [00:03<00:08, 240.44it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1350/3257 [00:04<00:07, 266.36it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1384/3257 [00:04<00:06, 283.58it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:04<00:05, 309.26it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1462/3257 [00:04<00:05, 330.65it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1501/3257 [00:04<00:05, 344.78it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1537/3257 [00:04<00:05, 330.39it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1572/3257 [00:04<00:05, 335.34it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1608/3257 [00:04<00:04, 341.52it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1643/3257 [00:04<00:04, 325.02it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1677/3257 [00:05<00:05, 313.26it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1709/3257 [00:05<00:04, 312.89it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1741/3257 [00:05<00:05, 297.40it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1777/3257 [00:05<00:04, 312.63it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1810/3257 [00:05<00:04, 316.67it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1846/3257 [00:05<00:04, 327.85it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1882/3257 [00:05<00:04, 336.93it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:05<00:04, 331.97it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1957/3257 [00:05<00:03, 352.38it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1993/3257 [00:05<00:03, 351.09it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2030/3257 [00:06<00:03, 355.21it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2066/3257 [00:06<00:03, 334.82it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2100/3257 [00:06<00:03, 329.61it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2134/3257 [00:06<00:03, 318.45it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2167/3257 [00:06<00:03, 311.52it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2199/3257 [00:06<00:03, 307.72it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2231/3257 [00:06<00:03, 310.18it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2264/3257 [00:06<00:03, 315.10it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2299/3257 [00:06<00:02, 324.02it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2343/3257 [00:07<00:02, 357.12it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2385/3257 [00:07<00:02, 375.14it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2423/3257 [00:07<00:02, 325.28it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2457/3257 [00:07<00:02, 296.97it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2494/3257 [00:07<00:02, 315.55it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2535/3257 [00:07<00:02, 338.37it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2570/3257 [00:07<00:02, 327.43it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2608/3257 [00:07<00:01, 341.58it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2647/3257 [00:07<00:01, 355.12it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2685/3257 [00:08<00:01, 360.54it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2722/3257 [00:08<00:01, 334.53it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2762/3257 [00:08<00:01, 351.34it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2798/3257 [00:08<00:01, 244.85it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2829/3257 [00:08<00:01, 257.02it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2866/3257 [00:08<00:01, 282.12it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2899/3257 [00:08<00:01, 294.07it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2934/3257 [00:08<00:01, 307.04it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2968/3257 [00:09<00:00, 315.33it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3003/3257 [00:09<00:00, 324.10it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3041/3257 [00:09<00:00, 338.80it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3083/3257 [00:09<00:00, 360.46it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3124/3257 [00:09<00:00, 374.71it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3162/3257 [00:09<00:00, 366.67it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3200/3257 [00:09<00:00, 363.59it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3238/3257 [00:09<00:00, 367.12it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 331.08it/s]
2023-02-07 14:40:55.652 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:40:55,653][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d213,n5,mc7,s0.893788,t4>', 'datetime': '2023-02-07T14:40:55.653075', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:40:55,653][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:40:55,653][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:40:55,846][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 14:40:55,847][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:40:55,851][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 retains 1451 unique words (51.47% of original 2819, drops 1368)', 'datetime': '2023-02-07T14:40:55.851109', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:40:55,851][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 leaves 2179471 word corpus (99.81% of original 2183622, drops 4151)', 'datetime': '2023-02-07T14:40:55.851340', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:40:55,856][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 14:40:55,856][gensim.models.word2vec][INFO] - sample=0.893788 downsamples 0 most-common words
[2023-02-07 14:40:55,856][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2179471 word corpus (100.0%% of prior 2179471)', 'datetime': '2023-02-07T14:40:55.856490', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:40:55,865][gensim.models.word2vec][INFO] - estimated required memory for 1451 words and 213 dimensions: 6624368 bytes
[2023-02-07 14:40:55,865][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:40:55,869][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 1451 vocabulary and 213 features, using sg=1 hs=0 sample=0.8937883767869217 negative=5 window=1 shrink_windows=True', 'datetime': '2023-02-07T14:40:55.869152', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:40:56,871][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 97.91% examples, 2139819 words/s, in_qsize 5, out_qsize 0
[2023-02-07 14:40:56,884][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2182728 effective words) took 1.0s, 2153410 effective words/s
[2023-02-07 14:40:57,887][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 95.89% examples, 2091939 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:40:57,920][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2182728 effective words) took 1.0s, 2111357 effective words/s
[2023-02-07 14:40:58,926][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 95.33% examples, 2072347 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:40:58,968][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2182728 effective words) took 1.0s, 2083533 effective words/s
[2023-02-07 14:40:59,972][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 95.33% examples, 2080496 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:00,012][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2182728 effective words) took 1.0s, 2095400 effective words/s
[2023-02-07 14:41:01,015][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 93.74% examples, 2055174 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:01,069][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2182728 effective words) took 1.1s, 2070553 effective words/s
[2023-02-07 14:41:02,071][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 94.75% examples, 2072611 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:02,116][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2182728 effective words) took 1.0s, 2086498 effective words/s
[2023-02-07 14:41:03,127][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 95.89% examples, 2074845 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:41:03,160][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2182728 effective words) took 1.0s, 2094003 effective words/s
[2023-02-07 14:41:04,173][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 93.74% examples, 2032890 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:41:04,226][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2182728 effective words) took 1.1s, 2050396 effective words/s
[2023-02-07 14:41:05,235][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 92.63% examples, 2020563 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:05,299][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2182728 effective words) took 1.1s, 2037100 effective words/s
[2023-02-07 14:41:06,308][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 93.74% examples, 2039888 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:06,363][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2182728 effective words) took 1.1s, 2053597 effective words/s
[2023-02-07 14:41:07,365][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 91.65% examples, 2015630 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:07,443][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2182728 effective words) took 1.1s, 2024244 effective words/s
[2023-02-07 14:41:08,450][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 92.20% examples, 2016025 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:08,517][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2182728 effective words) took 1.1s, 2035674 effective words/s
[2023-02-07 14:41:09,522][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 95.89% examples, 2087263 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:09,557][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2182728 effective words) took 1.0s, 2102486 effective words/s
[2023-02-07 14:41:10,562][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 96.28% examples, 2096566 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:10,592][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2182728 effective words) took 1.0s, 2112264 effective words/s
[2023-02-07 14:41:11,599][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 94.38% examples, 2053494 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:11,650][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2182728 effective words) took 1.1s, 2066153 effective words/s
[2023-02-07 14:41:12,654][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 93.74% examples, 2049671 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:12,710][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2182728 effective words) took 1.1s, 2063017 effective words/s
[2023-02-07 14:41:13,716][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 95.33% examples, 2074174 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:13,758][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2182728 effective words) took 1.0s, 2086381 effective words/s
[2023-02-07 14:41:14,766][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 95.89% examples, 2078843 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:41:14,801][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2182728 effective words) took 1.0s, 2094061 effective words/s
[2023-02-07 14:41:15,808][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 94.38% examples, 2055338 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:15,857][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2182728 effective words) took 1.1s, 2071514 effective words/s
[2023-02-07 14:41:16,863][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 95.33% examples, 2073685 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:16,906][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2182728 effective words) took 1.0s, 2083303 effective words/s
[2023-02-07 14:41:17,914][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 94.75% examples, 2061300 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:17,958][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2182728 effective words) took 1.1s, 2078251 effective words/s
[2023-02-07 14:41:18,961][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 93.74% examples, 2051352 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:41:19,020][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2182728 effective words) took 1.1s, 2057692 effective words/s
[2023-02-07 14:41:19,021][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 48039684 raw words (48020016 effective words) took 23.2s, 2074152 effective words/s', 'datetime': '2023-02-07T14:41:19.021080', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:41:19.021 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:41:20,326][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144038-m4745wue/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:41:20.326072', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:41:20,326][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:41:20,347][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144038-m4745wue/files/../tmp/embedding_model.pt
2023-02-07 14:41:20.347 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:41:21.797 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:41:22.355 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:41:23.720 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.936519079863418, 'test_mae': 1.0778799761040994, 'test_r2': 0.0839106909577384}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.34
wandb: percentage 0.48528
wandb:   test_mae 1.07788
wandb:   test_mse 1.93652
wandb:    test_r2 0.08391
wandb: 
wandb: üöÄ View run playful-sweep-89 at: https://wandb.ai/xiaoqiz/mof2vec/runs/m4745wue
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_144038-m4745wue/logs
wandb: Agent Starting Run: 7m3qiioa with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 804
wandb: 	model.gensim.alpha: 0.0018488288544875344
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 99
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.7410047677250045
wandb: 	model.gensim.vector_size: 110
wandb: 	model.gensim.window: 8
wandb: 	model.sklearn.learning_rate: 0.29687264499427524
wandb: 	model.sklearn.max_depth: 97
wandb: 	model.sklearn.min_child_weight: 0.01358461907546495
wandb: 	model.sklearn.n_estimators: 1834
wandb: 	model.sklearn.num_leaves: 471
wandb: 	model.sklearn.reg_alpha: 0.013460423176472456
wandb: 	model.sklearn.reg_lambda: 0.1807821584213514
wandb: 	model.sklearn.subsample: 0.2653713413884547
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144136-7m3qiioa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-90
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/7m3qiioa
2023-02-07 14:41:43.679 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 14:41:43.680 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 804 for sweep.
2023-02-07 14:41:43.680 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0018488288544875344 for sweep.
2023-02-07 14:41:43.680 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:41:43.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 99 for sweep.
2023-02-07 14:41:43.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 14:41:43.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7410047677250045 for sweep.
2023-02-07 14:41:43.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 110 for sweep.
2023-02-07 14:41:43.681 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 8 for sweep.
2023-02-07 14:41:43.682 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.29687264499427524 for sweep.
2023-02-07 14:41:43.682 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 97 for sweep.
2023-02-07 14:41:43.682 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.01358461907546495 for sweep.
2023-02-07 14:41:43.682 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1834 for sweep.
2023-02-07 14:41:43.683 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 471 for sweep.
2023-02-07 14:41:43.683 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.013460423176472456 for sweep.
2023-02-07 14:41:43.683 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.1807821584213514 for sweep.
2023-02-07 14:41:43.683 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2653713413884547 for sweep.
2023-02-07 14:41:43.683 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:41:43.689 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144136-7m3qiioa/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 804, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 110, 'window': 8, 'min_count': 2, 'dm': 0, 'sample': 0.7410047677250045, 'workers': 4, 'alpha': 0.0018488288544875344, 'epochs': 99}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1834, 'max_depth': 97, 'num_leaves': 471, 'reg_alpha': 0.013460423176472456, 'reg_lambda': 0.1807821584213514, 'subsample': 0.2653713413884547, 'min_child_weight': 0.01358461907546495, 'n_jobs': 4, 'learning_rate': 0.29687264499427524}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 40/3257 [00:00<00:08, 399.82it/s]  3%|‚ñé         | 82/3257 [00:00<00:07, 410.50it/s]  4%|‚ñç         | 124/3257 [00:00<00:07, 405.31it/s]  5%|‚ñå         | 168/3257 [00:00<00:07, 418.30it/s]  7%|‚ñã         | 213/3257 [00:00<00:07, 428.93it/s]  8%|‚ñä         | 259/3257 [00:00<00:06, 434.48it/s]  9%|‚ñâ         | 305/3257 [00:00<00:06, 442.70it/s] 11%|‚ñà         | 350/3257 [00:00<00:06, 438.34it/s] 12%|‚ñà‚ñè        | 394/3257 [00:00<00:06, 420.42it/s] 13%|‚ñà‚ñé        | 437/3257 [00:01<00:06, 405.18it/s] 15%|‚ñà‚ñç        | 480/3257 [00:01<00:06, 409.43it/s] 16%|‚ñà‚ñå        | 526/3257 [00:01<00:06, 421.90it/s] 17%|‚ñà‚ñã        | 569/3257 [00:01<00:06, 414.70it/s] 19%|‚ñà‚ñâ        | 613/3257 [00:01<00:06, 421.03it/s] 20%|‚ñà‚ñà        | 656/3257 [00:01<00:06, 410.08it/s] 21%|‚ñà‚ñà‚ñè       | 698/3257 [00:01<00:06, 408.88it/s] 23%|‚ñà‚ñà‚ñé       | 739/3257 [00:01<00:06, 403.13it/s] 24%|‚ñà‚ñà‚ñç       | 782/3257 [00:01<00:06, 409.36it/s] 25%|‚ñà‚ñà‚ñå       | 824/3257 [00:01<00:05, 411.80it/s] 27%|‚ñà‚ñà‚ñã       | 866/3257 [00:02<00:05, 400.74it/s] 28%|‚ñà‚ñà‚ñä       | 909/3257 [00:02<00:05, 408.80it/s] 29%|‚ñà‚ñà‚ñâ       | 952/3257 [00:02<00:05, 413.54it/s] 31%|‚ñà‚ñà‚ñà       | 994/3257 [00:02<00:07, 288.60it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1033/3257 [00:02<00:07, 309.73it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1075/3257 [00:02<00:06, 335.60it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1115/3257 [00:02<00:06, 351.70it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1154/3257 [00:02<00:05, 360.37it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1193/3257 [00:03<00:05, 361.80it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1234/3257 [00:03<00:05, 373.92it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:03<00:05, 380.00it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1314/3257 [00:03<00:05, 379.46it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1357/3257 [00:03<00:04, 393.60it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1398/3257 [00:03<00:04, 396.58it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1443/3257 [00:03<00:04, 409.67it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1490/3257 [00:03<00:04, 425.75it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1533/3257 [00:03<00:04, 415.67it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1575/3257 [00:03<00:04, 409.57it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1620/3257 [00:04<00:03, 419.53it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1663/3257 [00:04<00:03, 407.96it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1704/3257 [00:04<00:03, 408.46it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1745/3257 [00:04<00:03, 396.58it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1790/3257 [00:04<00:03, 411.45it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1832/3257 [00:04<00:03, 402.72it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1877/3257 [00:04<00:03, 415.20it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1919/3257 [00:04<00:03, 412.46it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1969/3257 [00:04<00:02, 437.37it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2013/3257 [00:05<00:02, 429.78it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2057/3257 [00:05<00:02, 416.02it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2099/3257 [00:05<00:02, 412.71it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2141/3257 [00:05<00:02, 410.44it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2183/3257 [00:05<00:02, 410.15it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2226/3257 [00:05<00:02, 414.50it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2268/3257 [00:05<00:02, 410.04it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2310/3257 [00:05<00:02, 403.60it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2360/3257 [00:05<00:02, 429.05it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2403/3257 [00:06<00:02, 309.67it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2442/3257 [00:06<00:02, 326.84it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2488/3257 [00:06<00:02, 359.37it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2537/3257 [00:06<00:01, 392.87it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2580/3257 [00:06<00:01, 387.06it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2629/3257 [00:06<00:01, 414.97it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2673/3257 [00:06<00:01, 414.68it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2716/3257 [00:06<00:01, 399.40it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2764/3257 [00:06<00:01, 420.49it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2808/3257 [00:07<00:01, 424.55it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2852/3257 [00:07<00:00, 423.14it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2897/3257 [00:07<00:00, 430.54it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2941/3257 [00:07<00:00, 432.10it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2985/3257 [00:07<00:00, 416.74it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3030/3257 [00:07<00:00, 424.44it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3080/3257 [00:07<00:00, 443.60it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3125/3257 [00:07<00:00, 444.52it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3170/3257 [00:07<00:00, 430.37it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3214/3257 [00:07<00:00, 427.10it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 403.18it/s]
2023-02-07 14:41:51.938 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:41:51,939][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d110,n5,mc2,s0.741005,t4>', 'datetime': '2023-02-07T14:41:51.939531', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:41:51,939][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:41:51,941][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:41:52,076][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 14:41:52,077][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:41:52,079][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 881 unique words (95.35% of original 924, drops 43)', 'datetime': '2023-02-07T14:41:52.079307', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:41:52,079][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 1455705 word corpus (100.00% of original 1455748, drops 43)', 'datetime': '2023-02-07T14:41:52.079487', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:41:52,082][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 14:41:52,083][gensim.models.word2vec][INFO] - sample=0.741005 downsamples 0 most-common words
[2023-02-07 14:41:52,083][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455705 word corpus (100.0%% of prior 1455705)', 'datetime': '2023-02-07T14:41:52.083747', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:41:52,089][gensim.models.word2vec][INFO] - estimated required memory for 881 words and 110 dimensions: 3300260 bytes
[2023-02-07 14:41:52,089][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:41:52,091][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 881 vocabulary and 110 features, using sg=1 hs=0 sample=0.7410047677250045 negative=5 window=8 shrink_windows=True', 'datetime': '2023-02-07T14:41:52.091410', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:41:52,676][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458962 effective words) took 0.6s, 2499197 effective words/s
[2023-02-07 14:41:53,186][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458962 effective words) took 0.5s, 2867744 effective words/s
[2023-02-07 14:41:53,698][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458962 effective words) took 0.5s, 2860432 effective words/s
[2023-02-07 14:41:54,193][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458962 effective words) took 0.5s, 2954404 effective words/s
[2023-02-07 14:41:54,724][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458962 effective words) took 0.5s, 2756365 effective words/s
[2023-02-07 14:41:55,254][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458962 effective words) took 0.5s, 2758705 effective words/s
[2023-02-07 14:41:55,782][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458962 effective words) took 0.5s, 2771580 effective words/s
[2023-02-07 14:41:56,315][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458962 effective words) took 0.5s, 2742862 effective words/s
[2023-02-07 14:41:56,838][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458962 effective words) took 0.5s, 2795598 effective words/s
[2023-02-07 14:41:57,358][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458962 effective words) took 0.5s, 2815648 effective words/s
[2023-02-07 14:41:57,882][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458962 effective words) took 0.5s, 2787717 effective words/s
[2023-02-07 14:41:58,405][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458962 effective words) took 0.5s, 2806392 effective words/s
[2023-02-07 14:41:58,935][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458962 effective words) took 0.5s, 2758589 effective words/s
[2023-02-07 14:41:59,471][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458962 effective words) took 0.5s, 2725307 effective words/s
[2023-02-07 14:42:00,004][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458962 effective words) took 0.5s, 2745169 effective words/s
[2023-02-07 14:42:00,541][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458962 effective words) took 0.5s, 2722961 effective words/s
[2023-02-07 14:42:01,076][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458962 effective words) took 0.5s, 2733632 effective words/s
[2023-02-07 14:42:01,602][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458962 effective words) took 0.5s, 2780054 effective words/s
[2023-02-07 14:42:02,140][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458962 effective words) took 0.5s, 2724786 effective words/s
[2023-02-07 14:42:02,671][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458962 effective words) took 0.5s, 2755269 effective words/s
[2023-02-07 14:42:03,199][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458962 effective words) took 0.5s, 2768885 effective words/s
[2023-02-07 14:42:03,726][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458962 effective words) took 0.5s, 2776823 effective words/s
[2023-02-07 14:42:04,261][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458962 effective words) took 0.5s, 2734878 effective words/s
[2023-02-07 14:42:04,794][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458962 effective words) took 0.5s, 2743437 effective words/s
[2023-02-07 14:42:05,312][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458962 effective words) took 0.5s, 2821703 effective words/s
[2023-02-07 14:42:05,832][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458962 effective words) took 0.5s, 2816117 effective words/s
[2023-02-07 14:42:06,351][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458962 effective words) took 0.5s, 2816225 effective words/s
[2023-02-07 14:42:06,873][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458962 effective words) took 0.5s, 2801800 effective words/s
[2023-02-07 14:42:07,399][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458962 effective words) took 0.5s, 2786030 effective words/s
[2023-02-07 14:42:07,932][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458962 effective words) took 0.5s, 2745117 effective words/s
[2023-02-07 14:42:08,475][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458962 effective words) took 0.5s, 2692326 effective words/s
[2023-02-07 14:42:09,007][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458962 effective words) took 0.5s, 2749709 effective words/s
[2023-02-07 14:42:09,545][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458962 effective words) took 0.5s, 2722312 effective words/s
[2023-02-07 14:42:10,074][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458962 effective words) took 0.5s, 2763941 effective words/s
[2023-02-07 14:42:10,587][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458962 effective words) took 0.5s, 2851680 effective words/s
[2023-02-07 14:42:11,105][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458962 effective words) took 0.5s, 2824306 effective words/s
[2023-02-07 14:42:11,623][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458962 effective words) took 0.5s, 2828087 effective words/s
[2023-02-07 14:42:12,138][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458962 effective words) took 0.5s, 2840554 effective words/s
[2023-02-07 14:42:12,653][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458962 effective words) took 0.5s, 2844804 effective words/s
[2023-02-07 14:42:13,174][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458962 effective words) took 0.5s, 2807234 effective words/s
[2023-02-07 14:42:13,696][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458962 effective words) took 0.5s, 2799317 effective words/s
[2023-02-07 14:42:14,225][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458962 effective words) took 0.5s, 2765082 effective words/s
[2023-02-07 14:42:14,747][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458962 effective words) took 0.5s, 2804805 effective words/s
[2023-02-07 14:42:15,267][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458962 effective words) took 0.5s, 2816171 effective words/s
[2023-02-07 14:42:15,792][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458962 effective words) took 0.5s, 2790363 effective words/s
[2023-02-07 14:42:16,317][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458962 effective words) took 0.5s, 2787936 effective words/s
[2023-02-07 14:42:16,835][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458962 effective words) took 0.5s, 2821220 effective words/s
[2023-02-07 14:42:17,354][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458962 effective words) took 0.5s, 2820982 effective words/s
[2023-02-07 14:42:17,875][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458962 effective words) took 0.5s, 2806263 effective words/s
[2023-02-07 14:42:18,402][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458962 effective words) took 0.5s, 2778146 effective words/s
[2023-02-07 14:42:18,934][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458962 effective words) took 0.5s, 2750204 effective words/s
[2023-02-07 14:42:19,473][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458962 effective words) took 0.5s, 2711001 effective words/s
[2023-02-07 14:42:20,000][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458962 effective words) took 0.5s, 2774113 effective words/s
[2023-02-07 14:42:20,518][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458962 effective words) took 0.5s, 2826025 effective words/s
[2023-02-07 14:42:21,045][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458962 effective words) took 0.5s, 2775988 effective words/s
[2023-02-07 14:42:21,573][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458962 effective words) took 0.5s, 2768961 effective words/s
[2023-02-07 14:42:22,106][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1458962 effective words) took 0.5s, 2747973 effective words/s
[2023-02-07 14:42:22,630][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1458962 effective words) took 0.5s, 2793153 effective words/s
[2023-02-07 14:42:23,162][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1458962 effective words) took 0.5s, 2745479 effective words/s
[2023-02-07 14:42:23,688][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1458962 effective words) took 0.5s, 2786148 effective words/s
[2023-02-07 14:42:24,201][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1458962 effective words) took 0.5s, 2850190 effective words/s
[2023-02-07 14:42:24,720][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1458962 effective words) took 0.5s, 2815392 effective words/s
[2023-02-07 14:42:25,247][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1458962 effective words) took 0.5s, 2782995 effective words/s
[2023-02-07 14:42:25,770][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1458962 effective words) took 0.5s, 2799922 effective words/s
[2023-02-07 14:42:26,293][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1458962 effective words) took 0.5s, 2794670 effective words/s
[2023-02-07 14:42:26,820][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1458962 effective words) took 0.5s, 2775844 effective words/s
[2023-02-07 14:42:27,341][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1458962 effective words) took 0.5s, 2804603 effective words/s
[2023-02-07 14:42:27,869][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1458962 effective words) took 0.5s, 2772466 effective words/s
[2023-02-07 14:42:28,394][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1458962 effective words) took 0.5s, 2787384 effective words/s
[2023-02-07 14:42:28,927][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1458962 effective words) took 0.5s, 2747428 effective words/s
[2023-02-07 14:42:29,455][gensim.models.word2vec][INFO] - EPOCH 70: training on 1455748 raw words (1458962 effective words) took 0.5s, 2768937 effective words/s
[2023-02-07 14:42:29,972][gensim.models.word2vec][INFO] - EPOCH 71: training on 1455748 raw words (1458962 effective words) took 0.5s, 2831836 effective words/s
[2023-02-07 14:42:30,485][gensim.models.word2vec][INFO] - EPOCH 72: training on 1455748 raw words (1458962 effective words) took 0.5s, 2852246 effective words/s
[2023-02-07 14:42:30,996][gensim.models.word2vec][INFO] - EPOCH 73: training on 1455748 raw words (1458962 effective words) took 0.5s, 2862041 effective words/s
[2023-02-07 14:42:31,499][gensim.models.word2vec][INFO] - EPOCH 74: training on 1455748 raw words (1458962 effective words) took 0.5s, 2906080 effective words/s
[2023-02-07 14:42:32,010][gensim.models.word2vec][INFO] - EPOCH 75: training on 1455748 raw words (1458962 effective words) took 0.5s, 2863311 effective words/s
[2023-02-07 14:42:32,519][gensim.models.word2vec][INFO] - EPOCH 76: training on 1455748 raw words (1458962 effective words) took 0.5s, 2872679 effective words/s
[2023-02-07 14:42:33,031][gensim.models.word2vec][INFO] - EPOCH 77: training on 1455748 raw words (1458962 effective words) took 0.5s, 2862487 effective words/s
[2023-02-07 14:42:33,541][gensim.models.word2vec][INFO] - EPOCH 78: training on 1455748 raw words (1458962 effective words) took 0.5s, 2868301 effective words/s
[2023-02-07 14:42:34,054][gensim.models.word2vec][INFO] - EPOCH 79: training on 1455748 raw words (1458962 effective words) took 0.5s, 2850727 effective words/s
[2023-02-07 14:42:34,560][gensim.models.word2vec][INFO] - EPOCH 80: training on 1455748 raw words (1458962 effective words) took 0.5s, 2888968 effective words/s
[2023-02-07 14:42:35,060][gensim.models.word2vec][INFO] - EPOCH 81: training on 1455748 raw words (1458962 effective words) took 0.5s, 2928265 effective words/s
[2023-02-07 14:42:35,563][gensim.models.word2vec][INFO] - EPOCH 82: training on 1455748 raw words (1458962 effective words) took 0.5s, 2909828 effective words/s
[2023-02-07 14:42:36,062][gensim.models.word2vec][INFO] - EPOCH 83: training on 1455748 raw words (1458962 effective words) took 0.5s, 2933590 effective words/s
[2023-02-07 14:42:36,564][gensim.models.word2vec][INFO] - EPOCH 84: training on 1455748 raw words (1458962 effective words) took 0.5s, 2912443 effective words/s
[2023-02-07 14:42:37,061][gensim.models.word2vec][INFO] - EPOCH 85: training on 1455748 raw words (1458962 effective words) took 0.5s, 2944030 effective words/s
[2023-02-07 14:42:37,572][gensim.models.word2vec][INFO] - EPOCH 86: training on 1455748 raw words (1458962 effective words) took 0.5s, 2859591 effective words/s
[2023-02-07 14:42:38,077][gensim.models.word2vec][INFO] - EPOCH 87: training on 1455748 raw words (1458962 effective words) took 0.5s, 2904637 effective words/s
[2023-02-07 14:42:38,583][gensim.models.word2vec][INFO] - EPOCH 88: training on 1455748 raw words (1458962 effective words) took 0.5s, 2894107 effective words/s
[2023-02-07 14:42:39,086][gensim.models.word2vec][INFO] - EPOCH 89: training on 1455748 raw words (1458962 effective words) took 0.5s, 2904840 effective words/s
[2023-02-07 14:42:39,594][gensim.models.word2vec][INFO] - EPOCH 90: training on 1455748 raw words (1458962 effective words) took 0.5s, 2884164 effective words/s
[2023-02-07 14:42:40,096][gensim.models.word2vec][INFO] - EPOCH 91: training on 1455748 raw words (1458962 effective words) took 0.5s, 2911303 effective words/s
[2023-02-07 14:42:40,601][gensim.models.word2vec][INFO] - EPOCH 92: training on 1455748 raw words (1458962 effective words) took 0.5s, 2894477 effective words/s
[2023-02-07 14:42:41,113][gensim.models.word2vec][INFO] - EPOCH 93: training on 1455748 raw words (1458962 effective words) took 0.5s, 2861542 effective words/s
[2023-02-07 14:42:41,624][gensim.models.word2vec][INFO] - EPOCH 94: training on 1455748 raw words (1458962 effective words) took 0.5s, 2861727 effective words/s
[2023-02-07 14:42:42,130][gensim.models.word2vec][INFO] - EPOCH 95: training on 1455748 raw words (1458962 effective words) took 0.5s, 2897135 effective words/s
[2023-02-07 14:42:42,630][gensim.models.word2vec][INFO] - EPOCH 96: training on 1455748 raw words (1458962 effective words) took 0.5s, 2927855 effective words/s
[2023-02-07 14:42:43,138][gensim.models.word2vec][INFO] - EPOCH 97: training on 1455748 raw words (1458962 effective words) took 0.5s, 2874420 effective words/s
[2023-02-07 14:42:43,641][gensim.models.word2vec][INFO] - EPOCH 98: training on 1455748 raw words (1458962 effective words) took 0.5s, 2913057 effective words/s
[2023-02-07 14:42:43,641][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 144119052 raw words (144437238 effective words) took 51.5s, 2801888 effective words/s', 'datetime': '2023-02-07T14:42:43.641592', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:42:43.641 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:42:46,023][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144136-7m3qiioa/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:42:46.023188', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:42:46,023][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:42:46,032][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144136-7m3qiioa/files/../tmp/embedding_model.pt
2023-02-07 14:42:46.032 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:42:47.132 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:42:47.554 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:42:48.313 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.6223086519506982, 'test_mae': 0.9733282094421379, 'test_r2': 0.23255100996804146}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.75
wandb: percentage 0.04654
wandb:   test_mae 0.97333
wandb:   test_mse 1.62231
wandb:    test_r2 0.23255
wandb: 
wandb: üöÄ View run rich-sweep-90 at: https://wandb.ai/xiaoqiz/mof2vec/runs/7m3qiioa
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_144136-7m3qiioa/logs
wandb: Agent Starting Run: hsptt5e9 with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 925
wandb: 	model.gensim.alpha: 0.0003565036689356116
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 26
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.6237679217976073
wandb: 	model.gensim.vector_size: 151
wandb: 	model.gensim.window: 10
wandb: 	model.sklearn.learning_rate: 0.8088971343889123
wandb: 	model.sklearn.max_depth: 80
wandb: 	model.sklearn.min_child_weight: 0.004578471799473966
wandb: 	model.sklearn.n_estimators: 1140
wandb: 	model.sklearn.num_leaves: 25
wandb: 	model.sklearn.reg_alpha: 0.017034561295477955
wandb: 	model.sklearn.reg_lambda: 0.06381699202670411
wandb: 	model.sklearn.subsample: 0.20806654079034315
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144311-hsptt5e9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-sweep-91
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/hsptt5e9
2023-02-07 14:43:19.106 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 14:43:19.107 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 925 for sweep.
2023-02-07 14:43:19.107 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0003565036689356116 for sweep.
2023-02-07 14:43:19.107 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:43:19.107 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 26 for sweep.
2023-02-07 14:43:19.108 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 14:43:19.108 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6237679217976073 for sweep.
2023-02-07 14:43:19.108 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 151 for sweep.
2023-02-07 14:43:19.108 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 10 for sweep.
2023-02-07 14:43:19.108 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.8088971343889123 for sweep.
2023-02-07 14:43:19.109 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 80 for sweep.
2023-02-07 14:43:19.109 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.004578471799473966 for sweep.
2023-02-07 14:43:19.109 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 1140 for sweep.
2023-02-07 14:43:19.109 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 25 for sweep.
2023-02-07 14:43:19.110 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.017034561295477955 for sweep.
2023-02-07 14:43:19.110 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.06381699202670411 for sweep.
2023-02-07 14:43:19.110 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.20806654079034315 for sweep.
2023-02-07 14:43:19.111 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:43:19.116 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144311-hsptt5e9/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 925, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 151, 'window': 10, 'min_count': 3, 'dm': 0, 'sample': 0.6237679217976073, 'workers': 4, 'alpha': 0.0003565036689356116, 'epochs': 26}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 1140, 'max_depth': 80, 'num_leaves': 25, 'reg_alpha': 0.017034561295477955, 'reg_lambda': 0.06381699202670411, 'subsample': 0.20806654079034315, 'min_child_weight': 0.004578471799473966, 'n_jobs': 4, 'learning_rate': 0.8088971343889123}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 35/3257 [00:00<00:09, 343.98it/s]  2%|‚ñè         | 71/3257 [00:00<00:09, 351.40it/s]  3%|‚ñé         | 107/3257 [00:00<00:09, 342.40it/s]  4%|‚ñç         | 145/3257 [00:00<00:08, 353.83it/s]  6%|‚ñå         | 181/3257 [00:00<00:08, 351.87it/s]  7%|‚ñã         | 220/3257 [00:00<00:08, 363.57it/s]  8%|‚ñä         | 259/3257 [00:00<00:08, 366.15it/s]  9%|‚ñâ         | 300/3257 [00:00<00:07, 376.29it/s] 10%|‚ñà         | 339/3257 [00:00<00:07, 378.61it/s] 12%|‚ñà‚ñè        | 377/3257 [00:01<00:07, 370.59it/s] 13%|‚ñà‚ñé        | 415/3257 [00:01<00:07, 366.84it/s] 14%|‚ñà‚ñç        | 452/3257 [00:01<00:08, 345.33it/s] 15%|‚ñà‚ñå        | 490/3257 [00:01<00:07, 353.45it/s] 16%|‚ñà‚ñå        | 527/3257 [00:01<00:07, 358.18it/s] 17%|‚ñà‚ñã        | 563/3257 [00:01<00:07, 355.38it/s] 18%|‚ñà‚ñä        | 599/3257 [00:01<00:10, 250.85it/s] 20%|‚ñà‚ñâ        | 637/3257 [00:01<00:09, 279.87it/s] 21%|‚ñà‚ñà        | 672/3257 [00:02<00:08, 295.85it/s] 22%|‚ñà‚ñà‚ñè       | 707/3257 [00:02<00:08, 308.66it/s] 23%|‚ñà‚ñà‚ñé       | 741/3257 [00:02<00:07, 315.66it/s] 24%|‚ñà‚ñà‚ñç       | 780/3257 [00:02<00:07, 332.38it/s] 25%|‚ñà‚ñà‚ñå       | 818/3257 [00:02<00:07, 342.39it/s] 26%|‚ñà‚ñà‚ñå       | 854/3257 [00:02<00:07, 341.77it/s] 27%|‚ñà‚ñà‚ñã       | 891/3257 [00:02<00:06, 347.96it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:02<00:06, 359.46it/s] 30%|‚ñà‚ñà‚ñâ       | 969/3257 [00:02<00:06, 365.12it/s] 31%|‚ñà‚ñà‚ñà       | 1007/3257 [00:02<00:06, 360.04it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1044/3257 [00:03<00:06, 357.14it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1081/3257 [00:03<00:06, 358.75it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1118/3257 [00:03<00:05, 359.74it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1155/3257 [00:03<00:05, 357.60it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:03<00:05, 346.85it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1229/3257 [00:03<00:05, 355.91it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1267/3257 [00:03<00:05, 361.47it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1304/3257 [00:03<00:05, 341.22it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1344/3257 [00:03<00:05, 356.86it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1380/3257 [00:04<00:05, 354.49it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1419/3257 [00:04<00:05, 363.40it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1460/3257 [00:04<00:04, 374.28it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1502/3257 [00:04<00:04, 385.07it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1541/3257 [00:04<00:04, 368.61it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1579/3257 [00:04<00:04, 362.48it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1619/3257 [00:04<00:04, 369.30it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1657/3257 [00:04<00:04, 354.99it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1693/3257 [00:04<00:04, 352.58it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1729/3257 [00:04<00:04, 349.52it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1767/3257 [00:05<00:04, 357.16it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1805/3257 [00:05<00:03, 363.39it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1842/3257 [00:05<00:03, 363.23it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1881/3257 [00:05<00:03, 371.00it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1919/3257 [00:05<00:03, 366.24it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1965/3257 [00:05<00:03, 392.35it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2005/3257 [00:05<00:04, 273.83it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2044/3257 [00:05<00:04, 299.34it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2079/3257 [00:06<00:03, 309.59it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2116/3257 [00:06<00:03, 323.55it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2151/3257 [00:06<00:03, 322.78it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2189/3257 [00:06<00:03, 337.57it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2227/3257 [00:06<00:02, 347.54it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2263/3257 [00:06<00:02, 350.68it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2300/3257 [00:06<00:02, 355.98it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2347/3257 [00:06<00:02, 388.93it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2388/3257 [00:06<00:02, 394.41it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2428/3257 [00:06<00:02, 387.25it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2467/3257 [00:07<00:02, 385.03it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2510/3257 [00:07<00:01, 397.68it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2550/3257 [00:07<00:01, 387.53it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2589/3257 [00:07<00:01, 368.33it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2634/3257 [00:07<00:01, 390.65it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2674/3257 [00:07<00:01, 384.45it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2713/3257 [00:07<00:01, 374.57it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2756/3257 [00:07<00:01, 385.46it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2797/3257 [00:07<00:01, 390.80it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2837/3257 [00:08<00:01, 256.73it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2882/3257 [00:08<00:01, 296.81it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2919/3257 [00:08<00:01, 313.00it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2955/3257 [00:08<00:00, 323.53it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2993/3257 [00:08<00:00, 338.24it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3037/3257 [00:08<00:00, 364.09it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3079/3257 [00:08<00:00, 379.56it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3122/3257 [00:08<00:00, 392.10it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3163/3257 [00:09<00:00, 384.56it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3203/3257 [00:09<00:00, 386.32it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:09<00:00, 389.68it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 352.22it/s]
2023-02-07 14:43:28.541 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:43:28,542][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d151,n5,mc3,s0.623768,t4>', 'datetime': '2023-02-07T14:43:28.542452', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:43:28,542][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:43:28,543][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:43:28,681][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 14:43:28,681][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:43:28,683][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 768 unique words (83.12% of original 924, drops 156)', 'datetime': '2023-02-07T14:43:28.683836', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:43:28,684][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 1455479 word corpus (99.98% of original 1455748, drops 269)', 'datetime': '2023-02-07T14:43:28.684028', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:43:28,686][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 14:43:28,686][gensim.models.word2vec][INFO] - sample=0.623768 downsamples 0 most-common words
[2023-02-07 14:43:28,687][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455479 word corpus (100.0%% of prior 1455479)', 'datetime': '2023-02-07T14:43:28.686992', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:43:28,693][gensim.models.word2vec][INFO] - estimated required memory for 768 words and 151 dimensions: 3930372 bytes
[2023-02-07 14:43:28,693][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:43:28,696][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 768 vocabulary and 151 features, using sg=1 hs=0 sample=0.6237679217976073 negative=5 window=10 shrink_windows=True', 'datetime': '2023-02-07T14:43:28.696159', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:43:29,464][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458736 effective words) took 0.8s, 1902924 effective words/s
[2023-02-07 14:43:30,225][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458736 effective words) took 0.8s, 1919978 effective words/s
[2023-02-07 14:43:30,997][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458736 effective words) took 0.8s, 1893918 effective words/s
[2023-02-07 14:43:31,760][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458736 effective words) took 0.8s, 1917382 effective words/s
[2023-02-07 14:43:32,520][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458736 effective words) took 0.8s, 1922831 effective words/s
[2023-02-07 14:43:33,282][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458736 effective words) took 0.8s, 1919134 effective words/s
[2023-02-07 14:43:34,040][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458736 effective words) took 0.8s, 1929578 effective words/s
[2023-02-07 14:43:34,815][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458736 effective words) took 0.8s, 1888093 effective words/s
[2023-02-07 14:43:35,578][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458736 effective words) took 0.8s, 1917895 effective words/s
[2023-02-07 14:43:36,341][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458736 effective words) took 0.8s, 1915037 effective words/s
[2023-02-07 14:43:37,105][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458736 effective words) took 0.8s, 1915909 effective words/s
[2023-02-07 14:43:37,868][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458736 effective words) took 0.8s, 1914531 effective words/s
[2023-02-07 14:43:38,634][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458736 effective words) took 0.8s, 1908553 effective words/s
[2023-02-07 14:43:39,398][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458736 effective words) took 0.8s, 1914765 effective words/s
[2023-02-07 14:43:40,162][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458736 effective words) took 0.8s, 1913528 effective words/s
[2023-02-07 14:43:40,930][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458736 effective words) took 0.8s, 1905465 effective words/s
[2023-02-07 14:43:41,697][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458736 effective words) took 0.8s, 1908129 effective words/s
[2023-02-07 14:43:42,463][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458736 effective words) took 0.8s, 1905688 effective words/s
[2023-02-07 14:43:43,229][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458736 effective words) took 0.8s, 1908926 effective words/s
[2023-02-07 14:43:43,997][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458736 effective words) took 0.8s, 1902826 effective words/s
[2023-02-07 14:43:44,709][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458736 effective words) took 0.7s, 2052774 effective words/s
[2023-02-07 14:43:45,417][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458736 effective words) took 0.7s, 2066880 effective words/s
[2023-02-07 14:43:46,125][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458736 effective words) took 0.7s, 2063636 effective words/s
[2023-02-07 14:43:46,843][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458736 effective words) took 0.7s, 2036802 effective words/s
[2023-02-07 14:43:47,552][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458736 effective words) took 0.7s, 2060342 effective words/s
[2023-02-07 14:43:48,264][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458736 effective words) took 0.7s, 2055720 effective words/s
[2023-02-07 14:43:48,264][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 37849448 raw words (37927136 effective words) took 19.6s, 1938188 effective words/s', 'datetime': '2023-02-07T14:43:48.264775', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:43:48.264 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:43:49,192][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144311-hsptt5e9/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:43:49.192802', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:43:49,194][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:43:49,202][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144311-hsptt5e9/files/../tmp/embedding_model.pt
2023-02-07 14:43:49.202 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:43:50.402 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:43:50.870 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:43:51.852 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.19414992748021, 'test_mae': 1.1303781039807146, 'test_r2': -0.03796410368558867}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.028 MB uploaded (0.000 MB deduped)wandb: / 0.008 MB of 0.028 MB uploaded (0.000 MB deduped)wandb: - 0.008 MB of 0.028 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.028 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.028 MB uploaded (0.000 MB deduped)wandb: / 0.008 MB of 0.028 MB uploaded (0.000 MB deduped)wandb: - 0.008 MB of 0.028 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.23
wandb: percentage 0.16883
wandb:   test_mae 1.13038
wandb:   test_mse 2.19415
wandb:    test_r2 -0.03796
wandb: 
wandb: üöÄ View run blooming-sweep-91 at: https://wandb.ai/xiaoqiz/mof2vec/runs/hsptt5e9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_144311-hsptt5e9/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: h5el8epc with config:
wandb: 	data.data.wl_step: 6
wandb: 	data.nn.batch_size: 545
wandb: 	model.gensim.alpha: 0.0008784807359641297
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 98
wandb: 	model.gensim.min_count: 1
wandb: 	model.gensim.sample: 0.35295299048871204
wandb: 	model.gensim.vector_size: 192
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.036865971915965585
wandb: 	model.sklearn.max_depth: 15
wandb: 	model.sklearn.min_child_weight: 0.09512577930956728
wandb: 	model.sklearn.n_estimators: 2120
wandb: 	model.sklearn.num_leaves: 311
wandb: 	model.sklearn.reg_alpha: 0.04542996274784121
wandb: 	model.sklearn.reg_lambda: 0.32018128205183066
wandb: 	model.sklearn.subsample: 0.5799861009675347
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144416-h5el8epc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-92
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/h5el8epc
2023-02-07 14:44:23.641 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 6 for sweep.
2023-02-07 14:44:23.642 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 545 for sweep.
2023-02-07 14:44:23.642 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0008784807359641297 for sweep.
2023-02-07 14:44:23.642 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:44:23.643 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 98 for sweep.
2023-02-07 14:44:23.643 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 1 for sweep.
2023-02-07 14:44:23.644 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.35295299048871204 for sweep.
2023-02-07 14:44:23.644 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 192 for sweep.
2023-02-07 14:44:23.644 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 14:44:23.644 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.036865971915965585 for sweep.
2023-02-07 14:44:23.644 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 15 for sweep.
2023-02-07 14:44:23.645 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.09512577930956728 for sweep.
2023-02-07 14:44:23.645 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2120 for sweep.
2023-02-07 14:44:23.645 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 311 for sweep.
2023-02-07 14:44:23.645 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.04542996274784121 for sweep.
2023-02-07 14:44:23.646 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.32018128205183066 for sweep.
2023-02-07 14:44:23.646 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5799861009675347 for sweep.
2023-02-07 14:44:23.646 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:44:23.652 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 6}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144416-h5el8epc/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 545, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 192, 'window': 3, 'min_count': 1, 'dm': 0, 'sample': 0.35295299048871204, 'workers': 4, 'alpha': 0.0008784807359641297, 'epochs': 98}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2120, 'max_depth': 15, 'num_leaves': 311, 'reg_alpha': 0.04542996274784121, 'reg_lambda': 0.32018128205183066, 'subsample': 0.5799861009675347, 'min_child_weight': 0.09512577930956728, 'n_jobs': 4, 'learning_rate': 0.036865971915965585}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 18/3257 [00:00<00:18, 176.17it/s]  1%|          | 39/3257 [00:00<00:16, 193.33it/s]  2%|‚ñè         | 59/3257 [00:00<00:17, 183.42it/s]  2%|‚ñè         | 78/3257 [00:00<00:24, 129.74it/s]  3%|‚ñé         | 97/3257 [00:00<00:21, 145.26it/s]  4%|‚ñé         | 116/3257 [00:00<00:20, 154.63it/s]  4%|‚ñç         | 138/3257 [00:00<00:18, 172.12it/s]  5%|‚ñç         | 159/3257 [00:00<00:16, 182.68it/s]  5%|‚ñå         | 179/3257 [00:01<00:16, 185.49it/s]  6%|‚ñå         | 201/3257 [00:01<00:15, 191.47it/s]  7%|‚ñã         | 229/3257 [00:01<00:14, 216.02it/s]  8%|‚ñä         | 252/3257 [00:01<00:14, 210.80it/s]  8%|‚ñä         | 274/3257 [00:01<00:14, 211.37it/s]  9%|‚ñâ         | 299/3257 [00:01<00:13, 221.29it/s] 10%|‚ñâ         | 322/3257 [00:01<00:13, 222.86it/s] 11%|‚ñà         | 345/3257 [00:01<00:13, 213.65it/s] 11%|‚ñà‚ñè        | 367/3257 [00:01<00:13, 214.78it/s] 12%|‚ñà‚ñè        | 389/3257 [00:02<00:14, 199.66it/s] 13%|‚ñà‚ñé        | 412/3257 [00:02<00:13, 207.77it/s] 13%|‚ñà‚ñé        | 434/3257 [00:02<00:15, 181.24it/s] 14%|‚ñà‚ñç        | 458/3257 [00:02<00:14, 193.59it/s] 15%|‚ñà‚ñç        | 479/3257 [00:02<00:14, 195.42it/s] 15%|‚ñà‚ñå        | 504/3257 [00:02<00:13, 206.99it/s] 16%|‚ñà‚ñå        | 526/3257 [00:02<00:13, 207.90it/s] 17%|‚ñà‚ñã        | 548/3257 [00:02<00:12, 210.44it/s] 18%|‚ñà‚ñä        | 570/3257 [00:02<00:13, 201.82it/s] 18%|‚ñà‚ñä        | 591/3257 [00:03<00:13, 196.43it/s] 19%|‚ñà‚ñâ        | 616/3257 [00:03<00:12, 209.98it/s] 20%|‚ñà‚ñâ        | 638/3257 [00:03<00:12, 208.07it/s] 20%|‚ñà‚ñà        | 659/3257 [00:03<00:13, 191.77it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:13, 195.26it/s] 22%|‚ñà‚ñà‚ñè       | 706/3257 [00:03<00:12, 202.37it/s] 22%|‚ñà‚ñà‚ñè       | 727/3257 [00:03<00:12, 199.58it/s] 23%|‚ñà‚ñà‚ñé       | 748/3257 [00:03<00:12, 197.66it/s] 24%|‚ñà‚ñà‚ñé       | 771/3257 [00:03<00:12, 205.06it/s] 24%|‚ñà‚ñà‚ñç       | 792/3257 [00:04<00:12, 199.06it/s] 25%|‚ñà‚ñà‚ñç       | 814/3257 [00:04<00:12, 202.47it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:04<00:12, 198.39it/s] 26%|‚ñà‚ñà‚ñã       | 855/3257 [00:04<00:12, 187.68it/s] 27%|‚ñà‚ñà‚ñã       | 875/3257 [00:04<00:12, 189.74it/s] 27%|‚ñà‚ñà‚ñã       | 895/3257 [00:04<00:12, 191.59it/s] 28%|‚ñà‚ñà‚ñä       | 915/3257 [00:04<00:12, 193.49it/s] 29%|‚ñà‚ñà‚ñä       | 936/3257 [00:04<00:11, 197.68it/s] 29%|‚ñà‚ñà‚ñâ       | 959/3257 [00:04<00:11, 204.49it/s] 30%|‚ñà‚ñà‚ñà       | 980/3257 [00:04<00:11, 200.31it/s] 31%|‚ñà‚ñà‚ñà       | 1001/3257 [00:05<00:11, 199.89it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1022/3257 [00:05<00:11, 200.25it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1043/3257 [00:05<00:12, 182.98it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1065/3257 [00:05<00:11, 191.96it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1085/3257 [00:05<00:11, 193.18it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1105/3257 [00:05<00:11, 193.63it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1125/3257 [00:05<00:11, 193.51it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1145/3257 [00:05<00:11, 187.22it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:05<00:10, 197.45it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1190/3257 [00:06<00:11, 178.55it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1209/3257 [00:06<00:11, 177.84it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1233/3257 [00:06<00:10, 192.56it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1253/3257 [00:06<00:10, 192.44it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1274/3257 [00:06<00:10, 197.01it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1294/3257 [00:06<00:10, 183.32it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1315/3257 [00:06<00:10, 190.60it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1337/3257 [00:06<00:09, 198.22it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1358/3257 [00:06<00:09, 192.03it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1378/3257 [00:07<00:09, 192.67it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1398/3257 [00:07<00:14, 132.11it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1423/3257 [00:07<00:11, 155.25it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1446/3257 [00:07<00:10, 171.44it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1472/3257 [00:07<00:09, 191.10it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1496/3257 [00:07<00:08, 201.36it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:07<00:08, 200.93it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1541/3257 [00:08<00:08, 195.99it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1562/3257 [00:08<00:08, 193.38it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1582/3257 [00:08<00:08, 192.35it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1605/3257 [00:08<00:08, 202.68it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1629/3257 [00:08<00:07, 211.39it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1651/3257 [00:08<00:08, 194.60it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1671/3257 [00:08<00:08, 188.92it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1691/3257 [00:08<00:08, 190.81it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:08<00:07, 198.40it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1735/3257 [00:09<00:08, 182.58it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1759/3257 [00:09<00:07, 196.74it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1780/3257 [00:09<00:07, 199.64it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1803/3257 [00:09<00:07, 204.61it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1824/3257 [00:09<00:07, 203.13it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1845/3257 [00:09<00:06, 202.50it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:09<00:06, 205.83it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1891/3257 [00:09<00:06, 206.46it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1916/3257 [00:09<00:06, 210.05it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1941/3257 [00:09<00:05, 219.61it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1968/3257 [00:10<00:05, 232.96it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1992/3257 [00:10<00:05, 222.17it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2015/3257 [00:10<00:05, 216.59it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2038/3257 [00:10<00:05, 220.14it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2061/3257 [00:10<00:06, 193.41it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2084/3257 [00:10<00:05, 202.67it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2105/3257 [00:10<00:05, 201.16it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2126/3257 [00:10<00:05, 192.44it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2146/3257 [00:11<00:05, 185.71it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2169/3257 [00:11<00:05, 194.75it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2189/3257 [00:11<00:05, 194.94it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2209/3257 [00:11<00:05, 192.93it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2232/3257 [00:11<00:05, 202.37it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2253/3257 [00:11<00:05, 196.75it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2273/3257 [00:11<00:05, 183.47it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2298/3257 [00:11<00:04, 201.51it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2321/3257 [00:11<00:04, 208.44it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2347/3257 [00:11<00:04, 222.85it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2370/3257 [00:12<00:03, 224.65it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2396/3257 [00:12<00:03, 231.96it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2420/3257 [00:12<00:03, 217.81it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2443/3257 [00:12<00:03, 205.29it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2468/3257 [00:12<00:03, 216.70it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2490/3257 [00:12<00:03, 216.91it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2516/3257 [00:12<00:03, 229.04it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2540/3257 [00:12<00:03, 226.34it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2563/3257 [00:12<00:03, 217.83it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2585/3257 [00:13<00:03, 204.81it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2611/3257 [00:13<00:02, 218.15it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2637/3257 [00:13<00:02, 229.42it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2661/3257 [00:13<00:02, 210.27it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2686/3257 [00:13<00:02, 219.44it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2709/3257 [00:13<00:02, 194.48it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2732/3257 [00:13<00:02, 199.81it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2757/3257 [00:13<00:02, 211.69it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2779/3257 [00:13<00:02, 208.33it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:14<00:02, 220.72it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2829/3257 [00:14<00:02, 202.55it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2853/3257 [00:14<00:01, 210.05it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2882/3257 [00:14<00:01, 231.30it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2906/3257 [00:14<00:02, 127.94it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2929/3257 [00:14<00:02, 145.17it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2949/3257 [00:15<00:02, 152.66it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2972/3257 [00:15<00:01, 168.70it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2993/3257 [00:15<00:01, 172.26it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3016/3257 [00:15<00:01, 184.42it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3040/3257 [00:15<00:01, 197.48it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3066/3257 [00:15<00:00, 212.01it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3089/3257 [00:15<00:00, 213.02it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3116/3257 [00:15<00:00, 226.81it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3140/3257 [00:15<00:00, 216.69it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3163/3257 [00:16<00:00, 210.82it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3185/3257 [00:16<00:00, 203.74it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3208/3257 [00:16<00:00, 208.50it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3230/3257 [00:16<00:00, 210.50it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3253/3257 [00:16<00:00, 212.78it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:16<00:00, 197.59it/s]
2023-02-07 14:44:40.806 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:44:40,807][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d192,n5,s0.352953,t4>', 'datetime': '2023-02-07T14:44:40.807865', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:44:40,808][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:44:40,808][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:44:41,264][gensim.models.doc2vec][INFO] - collected 31803 word types and 3257 unique tags from a corpus of 3257 examples and 5095118 words
[2023-02-07 14:44:41,264][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:44:41,343][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 31803 unique words (100.00% of original 31803, drops 0)', 'datetime': '2023-02-07T14:44:41.343745', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:44:41,344][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 5095118 word corpus (100.00% of original 5095118, drops 0)', 'datetime': '2023-02-07T14:44:41.344095', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:44:41,451][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 31803 items
[2023-02-07 14:44:41,452][gensim.models.word2vec][INFO] - sample=0.352953 downsamples 0 most-common words
[2023-02-07 14:44:41,452][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 5095118 word corpus (100.0%% of prior 5095118)', 'datetime': '2023-02-07T14:44:41.452372', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:44:41,640][gensim.models.word2vec][INFO] - estimated required memory for 31803 words and 192 dimensions: 67903684 bytes
[2023-02-07 14:44:41,640][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:44:41,667][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 31803 vocabulary and 192 features, using sg=1 hs=0 sample=0.35295299048871204 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T14:44:41.667948', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:44:42,674][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 51.21% examples, 2652715 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:44:43,572][gensim.models.word2vec][INFO] - EPOCH 0: training on 5095118 raw words (5086629 effective words) took 1.9s, 2674123 effective words/s
[2023-02-07 14:44:44,575][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 50.84% examples, 2634104 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:44:45,483][gensim.models.word2vec][INFO] - EPOCH 1: training on 5095118 raw words (5086629 effective words) took 1.9s, 2664335 effective words/s
[2023-02-07 14:44:46,487][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 51.21% examples, 2655600 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:44:47,392][gensim.models.word2vec][INFO] - EPOCH 2: training on 5095118 raw words (5086629 effective words) took 1.9s, 2665991 effective words/s
[2023-02-07 14:44:48,395][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 50.97% examples, 2644927 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:44:49,316][gensim.models.word2vec][INFO] - EPOCH 3: training on 5095118 raw words (5086629 effective words) took 1.9s, 2647624 effective words/s
[2023-02-07 14:44:50,324][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 50.75% examples, 2612490 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:44:51,258][gensim.models.word2vec][INFO] - EPOCH 4: training on 5095118 raw words (5086629 effective words) took 1.9s, 2621080 effective words/s
[2023-02-07 14:44:52,267][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 50.14% examples, 2574671 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:44:53,204][gensim.models.word2vec][INFO] - EPOCH 5: training on 5095118 raw words (5086629 effective words) took 1.9s, 2615918 effective words/s
[2023-02-07 14:44:54,206][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 50.14% examples, 2594018 words/s, in_qsize 8, out_qsize 3
[2023-02-07 14:44:55,137][gensim.models.word2vec][INFO] - EPOCH 6: training on 5095118 raw words (5086629 effective words) took 1.9s, 2632647 effective words/s
[2023-02-07 14:44:56,146][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 50.63% examples, 2613692 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:44:57,072][gensim.models.word2vec][INFO] - EPOCH 7: training on 5095118 raw words (5086629 effective words) took 1.9s, 2631562 effective words/s
[2023-02-07 14:44:58,075][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 50.84% examples, 2634619 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:44:59,000][gensim.models.word2vec][INFO] - EPOCH 8: training on 5095118 raw words (5086629 effective words) took 1.9s, 2640032 effective words/s
[2023-02-07 14:45:00,007][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 50.14% examples, 2579099 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:00,961][gensim.models.word2vec][INFO] - EPOCH 9: training on 5095118 raw words (5086629 effective words) took 2.0s, 2595111 effective words/s
[2023-02-07 14:45:01,963][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 50.38% examples, 2611172 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:02,898][gensim.models.word2vec][INFO] - EPOCH 10: training on 5095118 raw words (5086629 effective words) took 1.9s, 2628501 effective words/s
[2023-02-07 14:45:03,900][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 50.17% examples, 2603568 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:04,837][gensim.models.word2vec][INFO] - EPOCH 11: training on 5095118 raw words (5086629 effective words) took 1.9s, 2624805 effective words/s
[2023-02-07 14:45:05,842][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 50.63% examples, 2623045 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:06,768][gensim.models.word2vec][INFO] - EPOCH 12: training on 5095118 raw words (5086629 effective words) took 1.9s, 2636411 effective words/s
[2023-02-07 14:45:07,773][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 50.14% examples, 2585748 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:08,709][gensim.models.word2vec][INFO] - EPOCH 13: training on 5095118 raw words (5086629 effective words) took 1.9s, 2623120 effective words/s
[2023-02-07 14:45:09,714][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 51.21% examples, 2654217 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:10,629][gensim.models.word2vec][INFO] - EPOCH 14: training on 5095118 raw words (5086629 effective words) took 1.9s, 2651517 effective words/s
[2023-02-07 14:45:11,631][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 49.83% examples, 2583335 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:12,591][gensim.models.word2vec][INFO] - EPOCH 15: training on 5095118 raw words (5086629 effective words) took 2.0s, 2593605 effective words/s
[2023-02-07 14:45:13,602][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 50.63% examples, 2606594 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:14,523][gensim.models.word2vec][INFO] - EPOCH 16: training on 5095118 raw words (5086629 effective words) took 1.9s, 2635165 effective words/s
[2023-02-07 14:45:15,526][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 50.84% examples, 2633824 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:16,442][gensim.models.word2vec][INFO] - EPOCH 17: training on 5095118 raw words (5086629 effective words) took 1.9s, 2653035 effective words/s
[2023-02-07 14:45:17,450][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 51.21% examples, 2644394 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:18,361][gensim.models.word2vec][INFO] - EPOCH 18: training on 5095118 raw words (5086629 effective words) took 1.9s, 2652442 effective words/s
[2023-02-07 14:45:19,364][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 51.21% examples, 2659908 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:20,276][gensim.models.word2vec][INFO] - EPOCH 19: training on 5095118 raw words (5086629 effective words) took 1.9s, 2657929 effective words/s
[2023-02-07 14:45:21,284][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 50.75% examples, 2610596 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:45:22,207][gensim.models.word2vec][INFO] - EPOCH 20: training on 5095118 raw words (5086629 effective words) took 1.9s, 2636656 effective words/s
[2023-02-07 14:45:23,211][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 50.75% examples, 2622560 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:45:24,131][gensim.models.word2vec][INFO] - EPOCH 21: training on 5095118 raw words (5086629 effective words) took 1.9s, 2645683 effective words/s
[2023-02-07 14:45:25,135][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 52.04% examples, 2706221 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:26,006][gensim.models.word2vec][INFO] - EPOCH 22: training on 5095118 raw words (5086629 effective words) took 1.9s, 2715256 effective words/s
[2023-02-07 14:45:27,008][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 50.54% examples, 2621350 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:27,939][gensim.models.word2vec][INFO] - EPOCH 23: training on 5095118 raw words (5086629 effective words) took 1.9s, 2634338 effective words/s
[2023-02-07 14:45:28,945][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 51.89% examples, 2687242 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:29,820][gensim.models.word2vec][INFO] - EPOCH 24: training on 5095118 raw words (5086629 effective words) took 1.9s, 2705212 effective words/s
[2023-02-07 14:45:30,822][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 50.84% examples, 2637432 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:31,741][gensim.models.word2vec][INFO] - EPOCH 25: training on 5095118 raw words (5086629 effective words) took 1.9s, 2650458 effective words/s
[2023-02-07 14:45:32,746][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 51.89% examples, 2693156 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:33,611][gensim.models.word2vec][INFO] - EPOCH 26: training on 5095118 raw words (5086629 effective words) took 1.9s, 2722231 effective words/s
[2023-02-07 14:45:34,617][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 53.30% examples, 2771820 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:35,447][gensim.models.word2vec][INFO] - EPOCH 27: training on 5095118 raw words (5086629 effective words) took 1.8s, 2772587 effective words/s
[2023-02-07 14:45:36,453][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 52.47% examples, 2719105 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:45:37,317][gensim.models.word2vec][INFO] - EPOCH 28: training on 5095118 raw words (5086629 effective words) took 1.9s, 2722484 effective words/s
[2023-02-07 14:45:38,321][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 52.87% examples, 2740196 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:39,201][gensim.models.word2vec][INFO] - EPOCH 29: training on 5095118 raw words (5086629 effective words) took 1.9s, 2702086 effective words/s
[2023-02-07 14:45:40,203][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 50.97% examples, 2644518 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:41,122][gensim.models.word2vec][INFO] - EPOCH 30: training on 5095118 raw words (5086629 effective words) took 1.9s, 2650110 effective words/s
[2023-02-07 14:45:42,129][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 50.75% examples, 2613038 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:43,052][gensim.models.word2vec][INFO] - EPOCH 31: training on 5095118 raw words (5086629 effective words) took 1.9s, 2637521 effective words/s
[2023-02-07 14:45:44,056][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 50.84% examples, 2632513 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:44,981][gensim.models.word2vec][INFO] - EPOCH 32: training on 5095118 raw words (5086629 effective words) took 1.9s, 2639454 effective words/s
[2023-02-07 14:45:45,984][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 51.06% examples, 2650484 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:46,899][gensim.models.word2vec][INFO] - EPOCH 33: training on 5095118 raw words (5086629 effective words) took 1.9s, 2653334 effective words/s
[2023-02-07 14:45:47,908][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 51.21% examples, 2643558 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:48,817][gensim.models.word2vec][INFO] - EPOCH 34: training on 5095118 raw words (5086629 effective words) took 1.9s, 2655346 effective words/s
[2023-02-07 14:45:49,819][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 50.63% examples, 2626483 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:50,733][gensim.models.word2vec][INFO] - EPOCH 35: training on 5095118 raw words (5086629 effective words) took 1.9s, 2655497 effective words/s
[2023-02-07 14:45:51,736][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 51.37% examples, 2669546 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:45:52,625][gensim.models.word2vec][INFO] - EPOCH 36: training on 5095118 raw words (5086629 effective words) took 1.9s, 2691436 effective words/s
[2023-02-07 14:45:53,634][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 51.21% examples, 2643150 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:54,526][gensim.models.word2vec][INFO] - EPOCH 37: training on 5095118 raw words (5086629 effective words) took 1.9s, 2677455 effective words/s
[2023-02-07 14:45:55,529][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 51.70% examples, 2690796 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:45:56,425][gensim.models.word2vec][INFO] - EPOCH 38: training on 5095118 raw words (5086629 effective words) took 1.9s, 2681613 effective words/s
[2023-02-07 14:45:57,428][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 51.40% examples, 2669441 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:45:58,332][gensim.models.word2vec][INFO] - EPOCH 39: training on 5095118 raw words (5086629 effective words) took 1.9s, 2669608 effective words/s
[2023-02-07 14:45:59,336][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 50.14% examples, 2588903 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:00,273][gensim.models.word2vec][INFO] - EPOCH 40: training on 5095118 raw words (5086629 effective words) took 1.9s, 2622909 effective words/s
[2023-02-07 14:46:01,280][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 51.21% examples, 2647525 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:02,181][gensim.models.word2vec][INFO] - EPOCH 41: training on 5095118 raw words (5086629 effective words) took 1.9s, 2667684 effective words/s
[2023-02-07 14:46:03,184][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 51.06% examples, 2652360 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:04,097][gensim.models.word2vec][INFO] - EPOCH 42: training on 5095118 raw words (5086629 effective words) took 1.9s, 2657506 effective words/s
[2023-02-07 14:46:05,101][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 51.21% examples, 2656934 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:46:05,998][gensim.models.word2vec][INFO] - EPOCH 43: training on 5095118 raw words (5086629 effective words) took 1.9s, 2677904 effective words/s
[2023-02-07 14:46:07,006][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 51.89% examples, 2685902 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:07,896][gensim.models.word2vec][INFO] - EPOCH 44: training on 5095118 raw words (5086629 effective words) took 1.9s, 2682489 effective words/s
[2023-02-07 14:46:08,901][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 51.21% examples, 2655219 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:09,803][gensim.models.word2vec][INFO] - EPOCH 45: training on 5095118 raw words (5086629 effective words) took 1.9s, 2669052 effective words/s
[2023-02-07 14:46:10,815][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 51.89% examples, 2675795 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:11,700][gensim.models.word2vec][INFO] - EPOCH 46: training on 5095118 raw words (5086629 effective words) took 1.9s, 2683849 effective words/s
[2023-02-07 14:46:12,704][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 51.21% examples, 2656992 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:46:13,600][gensim.models.word2vec][INFO] - EPOCH 47: training on 5095118 raw words (5086629 effective words) took 1.9s, 2680236 effective words/s
[2023-02-07 14:46:14,602][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 49.86% examples, 2594068 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:15,558][gensim.models.word2vec][INFO] - EPOCH 48: training on 5095118 raw words (5086629 effective words) took 2.0s, 2599316 effective words/s
[2023-02-07 14:46:16,562][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 51.21% examples, 2655347 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:17,455][gensim.models.word2vec][INFO] - EPOCH 49: training on 5095118 raw words (5086629 effective words) took 1.9s, 2683612 effective words/s
[2023-02-07 14:46:18,457][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 49.83% examples, 2583872 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:19,421][gensim.models.word2vec][INFO] - EPOCH 50: training on 5095118 raw words (5086629 effective words) took 2.0s, 2589319 effective words/s
[2023-02-07 14:46:20,423][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 51.70% examples, 2691297 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:21,319][gensim.models.word2vec][INFO] - EPOCH 51: training on 5095118 raw words (5086629 effective words) took 1.9s, 2681446 effective words/s
[2023-02-07 14:46:22,331][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 49.86% examples, 2572549 words/s, in_qsize 7, out_qsize 1
[2023-02-07 14:46:23,287][gensim.models.word2vec][INFO] - EPOCH 52: training on 5095118 raw words (5086629 effective words) took 2.0s, 2588130 effective words/s
[2023-02-07 14:46:24,294][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 48.36% examples, 2505232 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:25,294][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 98.65% examples, 2508117 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:25,310][gensim.models.word2vec][INFO] - EPOCH 53: training on 5095118 raw words (5086629 effective words) took 2.0s, 2517096 effective words/s
[2023-02-07 14:46:26,312][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 49.00% examples, 2545687 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:27,309][gensim.models.word2vec][INFO] - EPOCH 54: training on 5095118 raw words (5086629 effective words) took 2.0s, 2546882 effective words/s
[2023-02-07 14:46:28,311][gensim.models.word2vec][INFO] - EPOCH 55 - PROGRESS: at 49.25% examples, 2554240 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:29,279][gensim.models.word2vec][INFO] - EPOCH 55: training on 5095118 raw words (5086629 effective words) took 2.0s, 2583446 effective words/s
[2023-02-07 14:46:30,281][gensim.models.word2vec][INFO] - EPOCH 56 - PROGRESS: at 51.40% examples, 2672841 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:31,178][gensim.models.word2vec][INFO] - EPOCH 56: training on 5095118 raw words (5086629 effective words) took 1.9s, 2681329 effective words/s
[2023-02-07 14:46:32,184][gensim.models.word2vec][INFO] - EPOCH 57 - PROGRESS: at 50.75% examples, 2616113 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:33,111][gensim.models.word2vec][INFO] - EPOCH 57: training on 5095118 raw words (5086629 effective words) took 1.9s, 2634345 effective words/s
[2023-02-07 14:46:34,118][gensim.models.word2vec][INFO] - EPOCH 58 - PROGRESS: at 52.63% examples, 2719766 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:34,982][gensim.models.word2vec][INFO] - EPOCH 58: training on 5095118 raw words (5086629 effective words) took 1.9s, 2720243 effective words/s
[2023-02-07 14:46:35,989][gensim.models.word2vec][INFO] - EPOCH 59 - PROGRESS: at 51.95% examples, 2686973 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:36,871][gensim.models.word2vec][INFO] - EPOCH 59: training on 5095118 raw words (5086629 effective words) took 1.9s, 2694393 effective words/s
[2023-02-07 14:46:37,881][gensim.models.word2vec][INFO] - EPOCH 60 - PROGRESS: at 51.89% examples, 2680312 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:38,756][gensim.models.word2vec][INFO] - EPOCH 60: training on 5095118 raw words (5086629 effective words) took 1.9s, 2700701 effective words/s
[2023-02-07 14:46:39,759][gensim.models.word2vec][INFO] - EPOCH 61 - PROGRESS: at 50.75% examples, 2625751 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:40,681][gensim.models.word2vec][INFO] - EPOCH 61: training on 5095118 raw words (5086629 effective words) took 1.9s, 2644994 effective words/s
[2023-02-07 14:46:41,683][gensim.models.word2vec][INFO] - EPOCH 62 - PROGRESS: at 52.47% examples, 2728966 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:42,549][gensim.models.word2vec][INFO] - EPOCH 62: training on 5095118 raw words (5086629 effective words) took 1.9s, 2725539 effective words/s
[2023-02-07 14:46:43,551][gensim.models.word2vec][INFO] - EPOCH 63 - PROGRESS: at 50.84% examples, 2636945 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:44,474][gensim.models.word2vec][INFO] - EPOCH 63: training on 5095118 raw words (5086629 effective words) took 1.9s, 2644021 effective words/s
[2023-02-07 14:46:45,481][gensim.models.word2vec][INFO] - EPOCH 64 - PROGRESS: at 51.89% examples, 2687242 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:46:46,362][gensim.models.word2vec][INFO] - EPOCH 64: training on 5095118 raw words (5086629 effective words) took 1.9s, 2697678 effective words/s
[2023-02-07 14:46:47,363][gensim.models.word2vec][INFO] - EPOCH 65 - PROGRESS: at 51.55% examples, 2681724 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:48,252][gensim.models.word2vec][INFO] - EPOCH 65: training on 5095118 raw words (5086629 effective words) took 1.9s, 2693788 effective words/s
[2023-02-07 14:46:49,254][gensim.models.word2vec][INFO] - EPOCH 66 - PROGRESS: at 52.63% examples, 2734783 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:50,114][gensim.models.word2vec][INFO] - EPOCH 66: training on 5095118 raw words (5086629 effective words) took 1.9s, 2734859 effective words/s
[2023-02-07 14:46:51,116][gensim.models.word2vec][INFO] - EPOCH 67 - PROGRESS: at 51.55% examples, 2680058 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:52,009][gensim.models.word2vec][INFO] - EPOCH 67: training on 5095118 raw words (5086629 effective words) took 1.9s, 2686046 effective words/s
[2023-02-07 14:46:53,011][gensim.models.word2vec][INFO] - EPOCH 68 - PROGRESS: at 52.29% examples, 2719022 words/s, in_qsize 6, out_qsize 0
[2023-02-07 14:46:53,875][gensim.models.word2vec][INFO] - EPOCH 68: training on 5095118 raw words (5086629 effective words) took 1.9s, 2729406 effective words/s
[2023-02-07 14:46:54,876][gensim.models.word2vec][INFO] - EPOCH 69 - PROGRESS: at 51.89% examples, 2700897 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:55,766][gensim.models.word2vec][INFO] - EPOCH 69: training on 5095118 raw words (5086629 effective words) took 1.9s, 2691472 effective words/s
[2023-02-07 14:46:56,770][gensim.models.word2vec][INFO] - EPOCH 70 - PROGRESS: at 51.89% examples, 2694749 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:57,630][gensim.models.word2vec][INFO] - EPOCH 70: training on 5095118 raw words (5086629 effective words) took 1.9s, 2730503 effective words/s
[2023-02-07 14:46:58,632][gensim.models.word2vec][INFO] - EPOCH 71 - PROGRESS: at 52.47% examples, 2728539 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:46:59,485][gensim.models.word2vec][INFO] - EPOCH 71: training on 5095118 raw words (5086629 effective words) took 1.9s, 2744867 effective words/s
[2023-02-07 14:47:00,488][gensim.models.word2vec][INFO] - EPOCH 72 - PROGRESS: at 52.63% examples, 2733942 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:01,340][gensim.models.word2vec][INFO] - EPOCH 72: training on 5095118 raw words (5086629 effective words) took 1.9s, 2744831 effective words/s
[2023-02-07 14:47:02,346][gensim.models.word2vec][INFO] - EPOCH 73 - PROGRESS: at 52.78% examples, 2733491 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:03,196][gensim.models.word2vec][INFO] - EPOCH 73: training on 5095118 raw words (5086629 effective words) took 1.9s, 2743960 effective words/s
[2023-02-07 14:47:04,199][gensim.models.word2vec][INFO] - EPOCH 74 - PROGRESS: at 52.04% examples, 2706734 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:47:05,074][gensim.models.word2vec][INFO] - EPOCH 74: training on 5095118 raw words (5086629 effective words) took 1.9s, 2710043 effective words/s
[2023-02-07 14:47:06,081][gensim.models.word2vec][INFO] - EPOCH 75 - PROGRESS: at 52.63% examples, 2720142 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:06,937][gensim.models.word2vec][INFO] - EPOCH 75: training on 5095118 raw words (5086629 effective words) took 1.9s, 2732713 effective words/s
[2023-02-07 14:47:07,941][gensim.models.word2vec][INFO] - EPOCH 76 - PROGRESS: at 52.47% examples, 2723157 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:08,793][gensim.models.word2vec][INFO] - EPOCH 76: training on 5095118 raw words (5086629 effective words) took 1.9s, 2742141 effective words/s
[2023-02-07 14:47:09,804][gensim.models.word2vec][INFO] - EPOCH 77 - PROGRESS: at 51.70% examples, 2667930 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:47:10,691][gensim.models.word2vec][INFO] - EPOCH 77: training on 5095118 raw words (5086629 effective words) took 1.9s, 2681869 effective words/s
[2023-02-07 14:47:11,700][gensim.models.word2vec][INFO] - EPOCH 78 - PROGRESS: at 52.44% examples, 2709979 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:12,559][gensim.models.word2vec][INFO] - EPOCH 78: training on 5095118 raw words (5086629 effective words) took 1.9s, 2726502 effective words/s
[2023-02-07 14:47:13,563][gensim.models.word2vec][INFO] - EPOCH 79 - PROGRESS: at 51.70% examples, 2689374 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:14,452][gensim.models.word2vec][INFO] - EPOCH 79: training on 5095118 raw words (5086629 effective words) took 1.9s, 2690889 effective words/s
[2023-02-07 14:47:15,454][gensim.models.word2vec][INFO] - EPOCH 80 - PROGRESS: at 51.40% examples, 2672562 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:16,339][gensim.models.word2vec][INFO] - EPOCH 80: training on 5095118 raw words (5086629 effective words) took 1.9s, 2697891 effective words/s
[2023-02-07 14:47:17,344][gensim.models.word2vec][INFO] - EPOCH 81 - PROGRESS: at 51.73% examples, 2683909 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:18,234][gensim.models.word2vec][INFO] - EPOCH 81: training on 5095118 raw words (5086629 effective words) took 1.9s, 2686836 effective words/s
[2023-02-07 14:47:19,240][gensim.models.word2vec][INFO] - EPOCH 82 - PROGRESS: at 53.85% examples, 2799182 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:20,060][gensim.models.word2vec][INFO] - EPOCH 82: training on 5095118 raw words (5086629 effective words) took 1.8s, 2790342 effective words/s
[2023-02-07 14:47:21,065][gensim.models.word2vec][INFO] - EPOCH 83 - PROGRESS: at 53.18% examples, 2762382 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:21,897][gensim.models.word2vec][INFO] - EPOCH 83: training on 5095118 raw words (5086629 effective words) took 1.8s, 2771019 effective words/s
[2023-02-07 14:47:22,898][gensim.models.word2vec][INFO] - EPOCH 84 - PROGRESS: at 53.18% examples, 2772910 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:23,731][gensim.models.word2vec][INFO] - EPOCH 84: training on 5095118 raw words (5086629 effective words) took 1.8s, 2774946 effective words/s
[2023-02-07 14:47:24,737][gensim.models.word2vec][INFO] - EPOCH 85 - PROGRESS: at 51.89% examples, 2690093 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:25,604][gensim.models.word2vec][INFO] - EPOCH 85: training on 5095118 raw words (5086629 effective words) took 1.9s, 2717860 effective words/s
[2023-02-07 14:47:26,609][gensim.models.word2vec][INFO] - EPOCH 86 - PROGRESS: at 52.47% examples, 2720181 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:27,464][gensim.models.word2vec][INFO] - EPOCH 86: training on 5095118 raw words (5086629 effective words) took 1.9s, 2737429 effective words/s
[2023-02-07 14:47:28,465][gensim.models.word2vec][INFO] - EPOCH 87 - PROGRESS: at 52.78% examples, 2744861 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:29,312][gensim.models.word2vec][INFO] - EPOCH 87: training on 5095118 raw words (5086629 effective words) took 1.8s, 2754451 effective words/s
[2023-02-07 14:47:30,315][gensim.models.word2vec][INFO] - EPOCH 88 - PROGRESS: at 52.63% examples, 2729640 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:31,166][gensim.models.word2vec][INFO] - EPOCH 88: training on 5095118 raw words (5086629 effective words) took 1.9s, 2746145 effective words/s
[2023-02-07 14:47:32,168][gensim.models.word2vec][INFO] - EPOCH 89 - PROGRESS: at 51.06% examples, 2653024 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:33,079][gensim.models.word2vec][INFO] - EPOCH 89: training on 5095118 raw words (5086629 effective words) took 1.9s, 2660323 effective words/s
[2023-02-07 14:47:34,081][gensim.models.word2vec][INFO] - EPOCH 90 - PROGRESS: at 51.40% examples, 2671790 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:34,975][gensim.models.word2vec][INFO] - EPOCH 90: training on 5095118 raw words (5086629 effective words) took 1.9s, 2685563 effective words/s
[2023-02-07 14:47:35,981][gensim.models.word2vec][INFO] - EPOCH 91 - PROGRESS: at 51.37% examples, 2660951 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:36,871][gensim.models.word2vec][INFO] - EPOCH 91: training on 5095118 raw words (5086629 effective words) took 1.9s, 2684569 effective words/s
[2023-02-07 14:47:37,875][gensim.models.word2vec][INFO] - EPOCH 92 - PROGRESS: at 51.70% examples, 2685088 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:38,766][gensim.models.word2vec][INFO] - EPOCH 92: training on 5095118 raw words (5086629 effective words) took 1.9s, 2687670 effective words/s
[2023-02-07 14:47:39,771][gensim.models.word2vec][INFO] - EPOCH 93 - PROGRESS: at 51.21% examples, 2653520 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:40,671][gensim.models.word2vec][INFO] - EPOCH 93: training on 5095118 raw words (5086629 effective words) took 1.9s, 2672791 effective words/s
[2023-02-07 14:47:41,673][gensim.models.word2vec][INFO] - EPOCH 94 - PROGRESS: at 51.70% examples, 2689439 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:42,563][gensim.models.word2vec][INFO] - EPOCH 94: training on 5095118 raw words (5086629 effective words) took 1.9s, 2690895 effective words/s
[2023-02-07 14:47:43,565][gensim.models.word2vec][INFO] - EPOCH 95 - PROGRESS: at 51.21% examples, 2660316 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:44,463][gensim.models.word2vec][INFO] - EPOCH 95: training on 5095118 raw words (5086629 effective words) took 1.9s, 2679945 effective words/s
[2023-02-07 14:47:45,465][gensim.models.word2vec][INFO] - EPOCH 96 - PROGRESS: at 52.78% examples, 2742761 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:46,298][gensim.models.word2vec][INFO] - EPOCH 96: training on 5095118 raw words (5086629 effective words) took 1.8s, 2773638 effective words/s
[2023-02-07 14:47:47,300][gensim.models.word2vec][INFO] - EPOCH 97 - PROGRESS: at 51.21% examples, 2661358 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:47:48,203][gensim.models.word2vec][INFO] - EPOCH 97: training on 5095118 raw words (5086629 effective words) took 1.9s, 2672400 effective words/s
[2023-02-07 14:47:48,204][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 499321564 raw words (498489642 effective words) took 186.5s, 2672354 effective words/s', 'datetime': '2023-02-07T14:47:48.204129', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:47:48.204 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:48:01,310][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144416-h5el8epc/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:48:01.310816', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:48:01,311][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:48:01,404][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144416-h5el8epc/files/../tmp/embedding_model.pt
2023-02-07 14:48:01.404 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:48:02.936 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:48:03.459 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:48:04.784 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.9805395782562703, 'test_mae': 1.029757105419698, 'test_r2': 0.06308636323706951}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.96
wandb: percentage 0.0
wandb:   test_mae 1.02976
wandb:   test_mse 1.98054
wandb:    test_r2 0.06309
wandb: 
wandb: üöÄ View run jolly-sweep-92 at: https://wandb.ai/xiaoqiz/mof2vec/runs/h5el8epc
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_144416-h5el8epc/logs
wandb: Agent Starting Run: cz2xhukh with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 538
wandb: 	model.gensim.alpha: 0.045280284497175055
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 21
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.5882480427177886
wandb: 	model.gensim.vector_size: 203
wandb: 	model.gensim.window: 2
wandb: 	model.sklearn.learning_rate: 0.4462384948591397
wandb: 	model.sklearn.max_depth: 39
wandb: 	model.sklearn.min_child_weight: 0.013499612344457028
wandb: 	model.sklearn.n_estimators: 591
wandb: 	model.sklearn.num_leaves: 367
wandb: 	model.sklearn.reg_alpha: 0.9817778939681688
wandb: 	model.sklearn.reg_lambda: 0.4245189743933117
wandb: 	model.sklearn.subsample: 0.3177978974547583
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144816-cz2xhukh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-93
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/cz2xhukh
2023-02-07 14:48:25.824 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 14:48:25.825 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 538 for sweep.
2023-02-07 14:48:25.825 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.045280284497175055 for sweep.
2023-02-07 14:48:25.825 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:48:25.826 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 21 for sweep.
2023-02-07 14:48:25.826 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 14:48:25.826 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.5882480427177886 for sweep.
2023-02-07 14:48:25.826 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 203 for sweep.
2023-02-07 14:48:25.827 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 2 for sweep.
2023-02-07 14:48:25.827 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.4462384948591397 for sweep.
2023-02-07 14:48:25.827 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 39 for sweep.
2023-02-07 14:48:25.827 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.013499612344457028 for sweep.
2023-02-07 14:48:25.828 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 591 for sweep.
2023-02-07 14:48:25.828 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 367 for sweep.
2023-02-07 14:48:25.828 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.9817778939681688 for sweep.
2023-02-07 14:48:25.828 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.4245189743933117 for sweep.
2023-02-07 14:48:25.828 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.3177978974547583 for sweep.
2023-02-07 14:48:25.829 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:48:25.833 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144816-cz2xhukh/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 538, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 203, 'window': 2, 'min_count': 3, 'dm': 0, 'sample': 0.5882480427177886, 'workers': 4, 'alpha': 0.045280284497175055, 'epochs': 21}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 591, 'max_depth': 39, 'num_leaves': 367, 'reg_alpha': 0.9817778939681688, 'reg_lambda': 0.4245189743933117, 'subsample': 0.3177978974547583, 'min_child_weight': 0.013499612344457028, 'n_jobs': 4, 'learning_rate': 0.4462384948591397}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 28/3257 [00:00<00:11, 276.74it/s]  2%|‚ñè         | 56/3257 [00:00<00:11, 273.04it/s]  3%|‚ñé         | 89/3257 [00:00<00:10, 298.28it/s]  4%|‚ñé         | 119/3257 [00:00<00:10, 286.24it/s]  5%|‚ñç         | 151/3257 [00:00<00:10, 296.77it/s]  6%|‚ñå         | 181/3257 [00:00<00:10, 291.07it/s]  7%|‚ñã         | 214/3257 [00:00<00:10, 302.75it/s]  8%|‚ñä         | 247/3257 [00:00<00:09, 310.53it/s]  9%|‚ñä         | 280/3257 [00:00<00:09, 315.96it/s] 10%|‚ñâ         | 312/3257 [00:01<00:09, 309.09it/s] 11%|‚ñà         | 344/3257 [00:01<00:09, 312.06it/s] 12%|‚ñà‚ñè        | 376/3257 [00:01<00:09, 310.31it/s] 13%|‚ñà‚ñé        | 408/3257 [00:01<00:09, 306.03it/s] 13%|‚ñà‚ñé        | 439/3257 [00:01<00:09, 282.58it/s] 14%|‚ñà‚ñç        | 472/3257 [00:01<00:09, 294.47it/s] 15%|‚ñà‚ñå        | 504/3257 [00:01<00:09, 297.51it/s] 16%|‚ñà‚ñã        | 535/3257 [00:01<00:09, 299.92it/s] 17%|‚ñà‚ñã        | 566/3257 [00:01<00:09, 289.73it/s] 18%|‚ñà‚ñä        | 596/3257 [00:02<00:09, 289.79it/s] 19%|‚ñà‚ñâ        | 626/3257 [00:02<00:09, 291.07it/s] 20%|‚ñà‚ñà        | 656/3257 [00:02<00:09, 287.59it/s] 21%|‚ñà‚ñà        | 685/3257 [00:02<00:09, 285.70it/s] 22%|‚ñà‚ñà‚ñè       | 717/3257 [00:02<00:08, 292.82it/s] 23%|‚ñà‚ñà‚ñé       | 747/3257 [00:02<00:08, 282.41it/s] 24%|‚ñà‚ñà‚ñç       | 776/3257 [00:02<00:08, 280.98it/s] 25%|‚ñà‚ñà‚ñç       | 806/3257 [00:02<00:08, 283.85it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:02<00:08, 285.24it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:02<00:08, 278.28it/s] 27%|‚ñà‚ñà‚ñã       | 892/3257 [00:03<00:11, 197.25it/s] 28%|‚ñà‚ñà‚ñä       | 925/3257 [00:03<00:10, 225.81it/s] 29%|‚ñà‚ñà‚ñâ       | 956/3257 [00:03<00:09, 245.13it/s] 30%|‚ñà‚ñà‚ñà       | 985/3257 [00:03<00:08, 255.95it/s] 31%|‚ñà‚ñà‚ñà       | 1013/3257 [00:03<00:08, 260.50it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1041/3257 [00:03<00:08, 261.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1073/3257 [00:03<00:07, 276.40it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1102/3257 [00:03<00:07, 277.21it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1131/3257 [00:04<00:07, 278.41it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1162/3257 [00:04<00:07, 285.49it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1191/3257 [00:04<00:07, 269.66it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1219/3257 [00:04<00:07, 263.34it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1249/3257 [00:04<00:07, 269.60it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1277/3257 [00:04<00:07, 269.42it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1305/3257 [00:04<00:07, 268.96it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1335/3257 [00:04<00:06, 277.62it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1364/3257 [00:04<00:06, 276.24it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1392/3257 [00:04<00:06, 271.15it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1426/3257 [00:05<00:06, 288.70it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1460/3257 [00:05<00:05, 302.13it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1492/3257 [00:05<00:05, 306.85it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1523/3257 [00:05<00:05, 301.38it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1554/3257 [00:05<00:05, 292.18it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1585/3257 [00:05<00:05, 294.85it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1618/3257 [00:05<00:05, 304.63it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1649/3257 [00:05<00:05, 292.78it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1679/3257 [00:05<00:05, 290.41it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1709/3257 [00:06<00:05, 292.78it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1739/3257 [00:06<00:05, 280.51it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1771/3257 [00:06<00:05, 290.66it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1803/3257 [00:06<00:04, 296.61it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1833/3257 [00:06<00:04, 290.13it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1868/3257 [00:06<00:04, 305.84it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1899/3257 [00:06<00:04, 302.63it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1931/3257 [00:06<00:04, 307.14it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1970/3257 [00:06<00:03, 330.39it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2004/3257 [00:06<00:03, 316.12it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2036/3257 [00:07<00:03, 310.38it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2068/3257 [00:07<00:05, 202.99it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2097/3257 [00:07<00:05, 220.75it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2125/3257 [00:07<00:04, 234.03it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2154/3257 [00:07<00:04, 247.28it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2185/3257 [00:07<00:04, 263.12it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2214/3257 [00:07<00:03, 262.49it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2243/3257 [00:08<00:03, 269.83it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2273/3257 [00:08<00:03, 274.11it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2308/3257 [00:08<00:03, 291.67it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2348/3257 [00:08<00:02, 321.52it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2383/3257 [00:08<00:02, 328.07it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2417/3257 [00:08<00:02, 324.38it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2450/3257 [00:08<00:02, 310.12it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2485/3257 [00:08<00:02, 320.21it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2522/3257 [00:08<00:02, 332.84it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2556/3257 [00:08<00:02, 325.73it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2589/3257 [00:09<00:02, 312.00it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2628/3257 [00:09<00:01, 332.47it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2662/3257 [00:09<00:01, 315.53it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2695/3257 [00:09<00:01, 313.70it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2727/3257 [00:09<00:01, 304.20it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2762/3257 [00:09<00:01, 315.02it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2796/3257 [00:09<00:01, 319.30it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2829/3257 [00:09<00:01, 306.27it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2867/3257 [00:09<00:01, 326.45it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2900/3257 [00:10<00:01, 321.49it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2934/3257 [00:10<00:00, 323.98it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2967/3257 [00:10<00:00, 317.27it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2999/3257 [00:10<00:00, 317.36it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3033/3257 [00:10<00:00, 322.84it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3070/3257 [00:10<00:00, 335.99it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3106/3257 [00:10<00:00, 340.96it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3141/3257 [00:10<00:00, 334.59it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3175/3257 [00:10<00:00, 328.96it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3208/3257 [00:10<00:00, 325.23it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3244/3257 [00:11<00:00, 331.21it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 292.67it/s]
2023-02-07 14:48:37.238 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:48:37,239][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d203,n5,mc3,s0.588248,t4>', 'datetime': '2023-02-07T14:48:37.239126', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:48:37,240][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:48:37,240][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:48:37,439][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 14:48:37,439][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:48:37,444][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 2159 unique words (76.59% of original 2819, drops 660)', 'datetime': '2023-02-07T14:48:37.444798', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:48:37,445][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 2182579 word corpus (99.95% of original 2183622, drops 1043)', 'datetime': '2023-02-07T14:48:37.445073', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:48:37,453][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 14:48:37,453][gensim.models.word2vec][INFO] - sample=0.588248 downsamples 0 most-common words
[2023-02-07 14:48:37,453][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2182579 word corpus (100.0%% of prior 2182579)', 'datetime': '2023-02-07T14:48:37.453667', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:48:37,466][gensim.models.word2vec][INFO] - estimated required memory for 2159 words and 203 dimensions: 7881800 bytes
[2023-02-07 14:48:37,467][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:48:37,471][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2159 vocabulary and 203 features, using sg=1 hs=0 sample=0.5882480427177886 negative=5 window=2 shrink_windows=True', 'datetime': '2023-02-07T14:48:37.471535', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:48:38,164][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2185836 effective words) took 0.7s, 3168074 effective words/s
[2023-02-07 14:48:38,802][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2185836 effective words) took 0.6s, 3436649 effective words/s
[2023-02-07 14:48:39,430][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2185836 effective words) took 0.6s, 3485279 effective words/s
[2023-02-07 14:48:40,071][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2185836 effective words) took 0.6s, 3416313 effective words/s
[2023-02-07 14:48:40,693][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2185836 effective words) took 0.6s, 3523878 effective words/s
[2023-02-07 14:48:41,318][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2185836 effective words) took 0.6s, 3508820 effective words/s
[2023-02-07 14:48:41,937][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2185836 effective words) took 0.6s, 3534540 effective words/s
[2023-02-07 14:48:42,560][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2185836 effective words) took 0.6s, 3518779 effective words/s
[2023-02-07 14:48:43,185][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2185836 effective words) took 0.6s, 3508254 effective words/s
[2023-02-07 14:48:43,811][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2185836 effective words) took 0.6s, 3502339 effective words/s
[2023-02-07 14:48:44,425][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2185836 effective words) took 0.6s, 3564709 effective words/s
[2023-02-07 14:48:45,045][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2185836 effective words) took 0.6s, 3536954 effective words/s
[2023-02-07 14:48:45,660][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2185836 effective words) took 0.6s, 3561110 effective words/s
[2023-02-07 14:48:46,273][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2185836 effective words) took 0.6s, 3580767 effective words/s
[2023-02-07 14:48:46,902][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2185836 effective words) took 0.6s, 3486412 effective words/s
[2023-02-07 14:48:47,551][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2185836 effective words) took 0.6s, 3374589 effective words/s
[2023-02-07 14:48:48,196][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2185836 effective words) took 0.6s, 3402631 effective words/s
[2023-02-07 14:48:48,848][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2185836 effective words) took 0.7s, 3361772 effective words/s
[2023-02-07 14:48:49,489][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2185836 effective words) took 0.6s, 3418454 effective words/s
[2023-02-07 14:48:50,151][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2185836 effective words) took 0.7s, 3313873 effective words/s
[2023-02-07 14:48:50,813][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2185836 effective words) took 0.7s, 3307367 effective words/s
[2023-02-07 14:48:50,813][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 45856062 raw words (45902556 effective words) took 13.3s, 3440894 effective words/s', 'datetime': '2023-02-07T14:48:50.813584', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:48:50.813 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:48:51,844][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144816-cz2xhukh/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:48:51.844550', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:48:51,845][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:48:51,865][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144816-cz2xhukh/files/../tmp/embedding_model.pt
2023-02-07 14:48:51.865 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:48:53.222 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:48:53.735 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:48:55.063 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.863021960509374, 'test_mae': 1.050644886217143, 'test_r2': 0.11867922279703824}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.94
wandb: percentage 0.23413
wandb:   test_mae 1.05064
wandb:   test_mse 1.86302
wandb:    test_r2 0.11868
wandb: 
wandb: üöÄ View run legendary-sweep-93 at: https://wandb.ai/xiaoqiz/mof2vec/runs/cz2xhukh
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_144816-cz2xhukh/logs
wandb: Agent Starting Run: vvm3uo8z with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 897
wandb: 	model.gensim.alpha: 0.0020775440916125897
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 68
wandb: 	model.gensim.min_count: 7
wandb: 	model.gensim.sample: 0.8577631168307087
wandb: 	model.gensim.vector_size: 142
wandb: 	model.gensim.window: 13
wandb: 	model.sklearn.learning_rate: 0.06430569663434454
wandb: 	model.sklearn.max_depth: 19
wandb: 	model.sklearn.min_child_weight: 0.025586390473056934
wandb: 	model.sklearn.n_estimators: 3371
wandb: 	model.sklearn.num_leaves: 481
wandb: 	model.sklearn.reg_alpha: 0.2725830389838202
wandb: 	model.sklearn.reg_lambda: 0.005226375132616018
wandb: 	model.sklearn.subsample: 0.230208078733744
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144911-vvm3uo8z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-sweep-94
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/vvm3uo8z
2023-02-07 14:49:19.851 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 14:49:19.852 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 897 for sweep.
2023-02-07 14:49:19.852 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0020775440916125897 for sweep.
2023-02-07 14:49:19.852 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:49:19.852 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 68 for sweep.
2023-02-07 14:49:19.853 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 7 for sweep.
2023-02-07 14:49:19.853 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.8577631168307087 for sweep.
2023-02-07 14:49:19.854 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 142 for sweep.
2023-02-07 14:49:19.854 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 13 for sweep.
2023-02-07 14:49:19.854 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.06430569663434454 for sweep.
2023-02-07 14:49:19.854 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 19 for sweep.
2023-02-07 14:49:19.855 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.025586390473056934 for sweep.
2023-02-07 14:49:19.855 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3371 for sweep.
2023-02-07 14:49:19.855 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 481 for sweep.
2023-02-07 14:49:19.855 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.2725830389838202 for sweep.
2023-02-07 14:49:19.855 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.005226375132616018 for sweep.
2023-02-07 14:49:19.856 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.230208078733744 for sweep.
2023-02-07 14:49:19.856 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:49:19.862 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144911-vvm3uo8z/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 897, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 142, 'window': 13, 'min_count': 7, 'dm': 0, 'sample': 0.8577631168307087, 'workers': 4, 'alpha': 0.0020775440916125897, 'epochs': 68}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3371, 'max_depth': 19, 'num_leaves': 481, 'reg_alpha': 0.2725830389838202, 'reg_lambda': 0.005226375132616018, 'subsample': 0.230208078733744, 'min_child_weight': 0.025586390473056934, 'n_jobs': 4, 'learning_rate': 0.06430569663434454}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 37/3257 [00:00<00:08, 363.85it/s]  2%|‚ñè         | 77/3257 [00:00<00:08, 384.18it/s]  4%|‚ñé         | 116/3257 [00:00<00:08, 385.58it/s]  5%|‚ñç         | 159/3257 [00:00<00:07, 401.51it/s]  6%|‚ñå         | 201/3257 [00:00<00:07, 401.53it/s]  7%|‚ñã         | 242/3257 [00:00<00:10, 291.32it/s]  9%|‚ñâ         | 287/3257 [00:00<00:08, 330.94it/s] 10%|‚ñà         | 332/3257 [00:00<00:08, 361.41it/s] 12%|‚ñà‚ñè        | 375/3257 [00:01<00:07, 377.95it/s] 13%|‚ñà‚ñé        | 417/3257 [00:01<00:07, 389.57it/s] 14%|‚ñà‚ñç        | 458/3257 [00:01<00:07, 381.73it/s] 15%|‚ñà‚ñå        | 502/3257 [00:01<00:06, 397.08it/s] 17%|‚ñà‚ñã        | 546/3257 [00:01<00:06, 405.51it/s] 18%|‚ñà‚ñä        | 588/3257 [00:01<00:06, 398.73it/s] 19%|‚ñà‚ñâ        | 634/3257 [00:01<00:06, 414.05it/s] 21%|‚ñà‚ñà        | 676/3257 [00:01<00:06, 402.43it/s] 22%|‚ñà‚ñà‚ñè       | 718/3257 [00:01<00:06, 405.83it/s] 23%|‚ñà‚ñà‚ñé       | 759/3257 [00:01<00:06, 396.44it/s] 25%|‚ñà‚ñà‚ñç       | 799/3257 [00:02<00:06, 397.10it/s] 26%|‚ñà‚ñà‚ñå       | 839/3257 [00:02<00:06, 393.82it/s] 27%|‚ñà‚ñà‚ñã       | 879/3257 [00:02<00:06, 388.31it/s] 28%|‚ñà‚ñà‚ñä       | 923/3257 [00:02<00:05, 402.30it/s] 30%|‚ñà‚ñà‚ñâ       | 964/3257 [00:02<00:05, 401.01it/s] 31%|‚ñà‚ñà‚ñà       | 1005/3257 [00:02<00:05, 399.82it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1046/3257 [00:02<00:05, 388.83it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1087/3257 [00:02<00:05, 393.74it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1127/3257 [00:02<00:05, 394.73it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1170/3257 [00:03<00:05, 401.74it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1211/3257 [00:03<00:05, 383.68it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1253/3257 [00:03<00:05, 392.79it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1293/3257 [00:03<00:05, 385.61it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1336/3257 [00:03<00:04, 397.49it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1376/3257 [00:03<00:06, 289.77it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1420/3257 [00:03<00:05, 322.95it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1464/3257 [00:03<00:05, 351.00it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1510/3257 [00:03<00:04, 377.74it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1551/3257 [00:04<00:04, 373.33it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1591/3257 [00:04<00:04, 379.86it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:04<00:04, 388.86it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1674/3257 [00:04<00:04, 386.20it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1715/3257 [00:04<00:03, 392.13it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1755/3257 [00:04<00:03, 387.22it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1798/3257 [00:04<00:03, 398.46it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1839/3257 [00:04<00:03, 400.72it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1883/3257 [00:04<00:03, 408.78it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1925/3257 [00:05<00:03, 403.13it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1974/3257 [00:05<00:03, 425.52it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2017/3257 [00:05<00:02, 421.75it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2060/3257 [00:05<00:02, 402.67it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2101/3257 [00:05<00:02, 403.77it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2142/3257 [00:05<00:02, 400.39it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2183/3257 [00:05<00:02, 398.77it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2225/3257 [00:05<00:02, 401.68it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2266/3257 [00:05<00:02, 395.24it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2306/3257 [00:05<00:02, 393.80it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2357/3257 [00:06<00:02, 425.54it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2401/3257 [00:06<00:01, 428.52it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2444/3257 [00:06<00:01, 409.38it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2488/3257 [00:06<00:01, 416.74it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2535/3257 [00:06<00:01, 430.61it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2579/3257 [00:06<00:01, 405.76it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2627/3257 [00:06<00:01, 426.34it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2671/3257 [00:06<00:01, 407.46it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2713/3257 [00:06<00:01, 389.54it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2757/3257 [00:07<00:01, 401.33it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2801/3257 [00:07<00:01, 410.51it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2843/3257 [00:07<00:01, 277.51it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2891/3257 [00:07<00:01, 320.69it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2930/3257 [00:07<00:00, 335.99it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2970/3257 [00:07<00:00, 351.59it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3012/3257 [00:07<00:00, 365.91it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3056/3257 [00:07<00:00, 385.80it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3101/3257 [00:08<00:00, 402.19it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3143/3257 [00:08<00:00, 406.83it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3185/3257 [00:08<00:00, 403.43it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3228/3257 [00:08<00:00, 410.11it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 387.33it/s]
2023-02-07 14:49:28.448 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:49:28,449][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d142,n5,mc7,s0.857763,t4>', 'datetime': '2023-02-07T14:49:28.449285', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:49:28,449][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:49:28,449][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:49:28,584][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 14:49:28,585][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:49:28,586][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 retains 528 unique words (57.14% of original 924, drops 396)', 'datetime': '2023-02-07T14:49:28.586753', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:49:28,586][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=7 leaves 1454417 word corpus (99.91% of original 1455748, drops 1331)', 'datetime': '2023-02-07T14:49:28.586917', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:49:28,588][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 14:49:28,588][gensim.models.word2vec][INFO] - sample=0.857763 downsamples 0 most-common words
[2023-02-07 14:49:28,588][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1454417 word corpus (100.0%% of prior 1454417)', 'datetime': '2023-02-07T14:49:28.588984', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:49:28,593][gensim.models.word2vec][INFO] - estimated required memory for 528 words and 142 dimensions: 3365184 bytes
[2023-02-07 14:49:28,593][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:49:28,596][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 528 vocabulary and 142 features, using sg=1 hs=0 sample=0.8577631168307087 negative=5 window=13 shrink_windows=True', 'datetime': '2023-02-07T14:49:28.596009', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:49:29,285][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1457674 effective words) took 0.7s, 2119832 effective words/s
[2023-02-07 14:49:29,826][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1457674 effective words) took 0.5s, 2708320 effective words/s
[2023-02-07 14:49:30,367][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1457674 effective words) took 0.5s, 2698518 effective words/s
[2023-02-07 14:49:30,907][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1457674 effective words) took 0.5s, 2704923 effective words/s
[2023-02-07 14:49:31,474][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1457674 effective words) took 0.6s, 2577801 effective words/s
[2023-02-07 14:49:32,044][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1457674 effective words) took 0.6s, 2564861 effective words/s
[2023-02-07 14:49:32,612][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1457674 effective words) took 0.6s, 2572611 effective words/s
[2023-02-07 14:49:33,181][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1457674 effective words) took 0.6s, 2567038 effective words/s
[2023-02-07 14:49:33,754][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1457674 effective words) took 0.6s, 2549749 effective words/s
[2023-02-07 14:49:34,326][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1457674 effective words) took 0.6s, 2559320 effective words/s
[2023-02-07 14:49:34,894][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1457674 effective words) took 0.6s, 2572967 effective words/s
[2023-02-07 14:49:35,458][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1457674 effective words) took 0.6s, 2590105 effective words/s
[2023-02-07 14:49:36,024][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1457674 effective words) took 0.6s, 2580615 effective words/s
[2023-02-07 14:49:36,596][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1457674 effective words) took 0.6s, 2557238 effective words/s
[2023-02-07 14:49:37,159][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1457674 effective words) took 0.6s, 2592850 effective words/s
[2023-02-07 14:49:37,730][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1457674 effective words) took 0.6s, 2563332 effective words/s
[2023-02-07 14:49:38,303][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1457674 effective words) took 0.6s, 2546848 effective words/s
[2023-02-07 14:49:38,869][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1457674 effective words) took 0.6s, 2587428 effective words/s
[2023-02-07 14:49:39,436][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1457674 effective words) took 0.6s, 2577009 effective words/s
[2023-02-07 14:49:39,998][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1457674 effective words) took 0.6s, 2598023 effective words/s
[2023-02-07 14:49:40,570][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1457674 effective words) took 0.6s, 2555467 effective words/s
[2023-02-07 14:49:41,143][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1457674 effective words) took 0.6s, 2551060 effective words/s
[2023-02-07 14:49:41,710][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1457674 effective words) took 0.6s, 2580947 effective words/s
[2023-02-07 14:49:42,276][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1457674 effective words) took 0.6s, 2578409 effective words/s
[2023-02-07 14:49:42,841][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1457674 effective words) took 0.6s, 2591596 effective words/s
[2023-02-07 14:49:43,409][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1457674 effective words) took 0.6s, 2575010 effective words/s
[2023-02-07 14:49:43,971][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1457674 effective words) took 0.6s, 2602143 effective words/s
[2023-02-07 14:49:44,539][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1457674 effective words) took 0.6s, 2573437 effective words/s
[2023-02-07 14:49:45,104][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1457674 effective words) took 0.6s, 2583059 effective words/s
[2023-02-07 14:49:45,672][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1457674 effective words) took 0.6s, 2576051 effective words/s
[2023-02-07 14:49:46,247][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1457674 effective words) took 0.6s, 2542266 effective words/s
[2023-02-07 14:49:46,813][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1457674 effective words) took 0.6s, 2580866 effective words/s
[2023-02-07 14:49:47,404][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1457674 effective words) took 0.6s, 2474060 effective words/s
[2023-02-07 14:49:47,974][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1457674 effective words) took 0.6s, 2564138 effective words/s
[2023-02-07 14:49:48,538][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1457674 effective words) took 0.6s, 2588849 effective words/s
[2023-02-07 14:49:49,104][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1457674 effective words) took 0.6s, 2587547 effective words/s
[2023-02-07 14:49:49,676][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1457674 effective words) took 0.6s, 2555626 effective words/s
[2023-02-07 14:49:50,242][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1457674 effective words) took 0.6s, 2580673 effective words/s
[2023-02-07 14:49:50,841][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1457674 effective words) took 0.6s, 2443168 effective words/s
[2023-02-07 14:49:51,414][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1457674 effective words) took 0.6s, 2552019 effective words/s
[2023-02-07 14:49:52,001][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1457674 effective words) took 0.6s, 2487559 effective words/s
[2023-02-07 14:49:52,564][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1457674 effective words) took 0.6s, 2594876 effective words/s
[2023-02-07 14:49:53,163][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1457674 effective words) took 0.6s, 2439469 effective words/s
[2023-02-07 14:49:53,732][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1457674 effective words) took 0.6s, 2569659 effective words/s
[2023-02-07 14:49:54,331][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1457674 effective words) took 0.6s, 2438241 effective words/s
[2023-02-07 14:49:54,903][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1457674 effective words) took 0.6s, 2558942 effective words/s
[2023-02-07 14:49:55,490][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1457674 effective words) took 0.6s, 2493135 effective words/s
[2023-02-07 14:49:56,049][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1457674 effective words) took 0.6s, 2617569 effective words/s
[2023-02-07 14:49:56,614][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1457674 effective words) took 0.6s, 2585326 effective words/s
[2023-02-07 14:49:57,180][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1457674 effective words) took 0.6s, 2580032 effective words/s
[2023-02-07 14:49:57,749][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1457674 effective words) took 0.6s, 2567475 effective words/s
[2023-02-07 14:49:58,319][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1457674 effective words) took 0.6s, 2570282 effective words/s
[2023-02-07 14:49:58,889][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1457674 effective words) took 0.6s, 2563119 effective words/s
[2023-02-07 14:49:59,456][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1457674 effective words) took 0.6s, 2577460 effective words/s
[2023-02-07 14:50:00,023][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1457674 effective words) took 0.6s, 2579377 effective words/s
[2023-02-07 14:50:00,598][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1457674 effective words) took 0.6s, 2541044 effective words/s
[2023-02-07 14:50:01,169][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1457674 effective words) took 0.6s, 2557534 effective words/s
[2023-02-07 14:50:01,736][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1457674 effective words) took 0.6s, 2575584 effective words/s
[2023-02-07 14:50:02,301][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1457674 effective words) took 0.6s, 2586704 effective words/s
[2023-02-07 14:50:02,860][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1457674 effective words) took 0.6s, 2614802 effective words/s
[2023-02-07 14:50:03,424][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1457674 effective words) took 0.6s, 2590096 effective words/s
[2023-02-07 14:50:03,988][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1457674 effective words) took 0.6s, 2591701 effective words/s
[2023-02-07 14:50:04,551][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1457674 effective words) took 0.6s, 2600195 effective words/s
[2023-02-07 14:50:05,106][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1457674 effective words) took 0.6s, 2631282 effective words/s
[2023-02-07 14:50:05,671][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1457674 effective words) took 0.6s, 2585610 effective words/s
[2023-02-07 14:50:06,238][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1457674 effective words) took 0.6s, 2578334 effective words/s
[2023-02-07 14:50:06,803][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1457674 effective words) took 0.6s, 2590254 effective words/s
[2023-02-07 14:50:07,362][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1457674 effective words) took 0.6s, 2613646 effective words/s
[2023-02-07 14:50:07,362][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 98990864 raw words (99121832 effective words) took 38.8s, 2556919 effective words/s', 'datetime': '2023-02-07T14:50:07.362837', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:50:07.363 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:50:09,138][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144911-vvm3uo8z/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:50:09.138597', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:50:09,140][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:50:09,147][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_144911-vvm3uo8z/files/../tmp/embedding_model.pt
2023-02-07 14:50:09.147 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:50:10.400 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:50:10.870 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:50:11.874 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.74585376399169, 'test_mae': 0.9948542503514356, 'test_r2': 0.1741067852237319}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.033 MB of 0.035 MB uploaded (0.000 MB deduped)wandb: | 0.033 MB of 0.035 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.77
wandb: percentage 0.42857
wandb:   test_mae 0.99485
wandb:   test_mse 1.74585
wandb:    test_r2 0.17411
wandb: 
wandb: üöÄ View run clear-sweep-94 at: https://wandb.ai/xiaoqiz/mof2vec/runs/vvm3uo8z
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_144911-vvm3uo8z/logs
wandb: Agent Starting Run: 5kevlqw9 with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 527
wandb: 	model.gensim.alpha: 0.0026751710054549647
wandb: 	model.gensim.dm: 1
wandb: 	model.gensim.epochs: 86
wandb: 	model.gensim.min_count: 2
wandb: 	model.gensim.sample: 0.6586739979056017
wandb: 	model.gensim.vector_size: 102
wandb: 	model.gensim.window: 5
wandb: 	model.sklearn.learning_rate: 0.1123085068647377
wandb: 	model.sklearn.max_depth: 62
wandb: 	model.sklearn.min_child_weight: 0.0434560392867095
wandb: 	model.sklearn.n_estimators: 157
wandb: 	model.sklearn.num_leaves: 446
wandb: 	model.sklearn.reg_alpha: 0.07802428054117556
wandb: 	model.sklearn.reg_lambda: 0.2345345201268367
wandb: 	model.sklearn.subsample: 0.2982782569971155
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145020-5kevlqw9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-95
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/5kevlqw9
2023-02-07 14:50:28.321 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 14:50:28.322 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 527 for sweep.
2023-02-07 14:50:28.322 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0026751710054549647 for sweep.
2023-02-07 14:50:28.322 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 1 for sweep.
2023-02-07 14:50:28.322 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 86 for sweep.
2023-02-07 14:50:28.323 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 2 for sweep.
2023-02-07 14:50:28.323 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6586739979056017 for sweep.
2023-02-07 14:50:28.323 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 102 for sweep.
2023-02-07 14:50:28.323 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 5 for sweep.
2023-02-07 14:50:28.323 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.1123085068647377 for sweep.
2023-02-07 14:50:28.324 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 62 for sweep.
2023-02-07 14:50:28.324 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.0434560392867095 for sweep.
2023-02-07 14:50:28.325 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 157 for sweep.
2023-02-07 14:50:28.325 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 446 for sweep.
2023-02-07 14:50:28.325 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.07802428054117556 for sweep.
2023-02-07 14:50:28.325 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.2345345201268367 for sweep.
2023-02-07 14:50:28.325 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.2982782569971155 for sweep.
2023-02-07 14:50:28.326 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:50:28.331 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145020-5kevlqw9/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 527, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 102, 'window': 5, 'min_count': 2, 'dm': 1, 'sample': 0.6586739979056017, 'workers': 4, 'alpha': 0.0026751710054549647, 'epochs': 86}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 157, 'max_depth': 62, 'num_leaves': 446, 'reg_alpha': 0.07802428054117556, 'reg_lambda': 0.2345345201268367, 'subsample': 0.2982782569971155, 'min_child_weight': 0.0434560392867095, 'n_jobs': 4, 'learning_rate': 0.1123085068647377}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 34/3257 [00:00<00:09, 338.53it/s]  2%|‚ñè         | 74/3257 [00:00<00:08, 372.02it/s]  3%|‚ñé         | 112/3257 [00:00<00:08, 372.55it/s]  5%|‚ñç         | 155/3257 [00:00<00:07, 394.15it/s]  6%|‚ñå         | 195/3257 [00:00<00:07, 392.49it/s]  7%|‚ñã         | 238/3257 [00:00<00:07, 404.09it/s]  9%|‚ñä         | 282/3257 [00:00<00:07, 411.73it/s] 10%|‚ñà         | 326/3257 [00:00<00:06, 420.55it/s] 11%|‚ñà‚ñè        | 369/3257 [00:00<00:06, 414.81it/s] 13%|‚ñà‚ñé        | 411/3257 [00:01<00:07, 406.24it/s] 14%|‚ñà‚ñç        | 452/3257 [00:01<00:07, 384.90it/s] 15%|‚ñà‚ñå        | 494/3257 [00:01<00:07, 394.43it/s] 16%|‚ñà‚ñã        | 536/3257 [00:01<00:06, 400.19it/s] 18%|‚ñà‚ñä        | 577/3257 [00:01<00:06, 383.58it/s] 19%|‚ñà‚ñâ        | 620/3257 [00:01<00:06, 395.17it/s] 20%|‚ñà‚ñà        | 660/3257 [00:01<00:06, 388.71it/s] 21%|‚ñà‚ñà‚ñè       | 700/3257 [00:01<00:06, 385.94it/s] 23%|‚ñà‚ñà‚ñé       | 739/3257 [00:01<00:06, 385.33it/s] 24%|‚ñà‚ñà‚ñç       | 780/3257 [00:01<00:06, 390.27it/s] 25%|‚ñà‚ñà‚ñå       | 821/3257 [00:02<00:06, 394.28it/s] 26%|‚ñà‚ñà‚ñã       | 861/3257 [00:02<00:08, 275.39it/s] 28%|‚ñà‚ñà‚ñä       | 903/3257 [00:02<00:07, 307.27it/s] 29%|‚ñà‚ñà‚ñâ       | 944/3257 [00:02<00:06, 331.53it/s] 30%|‚ñà‚ñà‚ñà       | 986/3257 [00:02<00:06, 350.69it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:02<00:06, 359.65it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:02<00:05, 368.11it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1107/3257 [00:02<00:05, 374.65it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1146/3257 [00:03<00:05, 373.02it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 1185/3257 [00:03<00:05, 376.10it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1224/3257 [00:03<00:05, 369.71it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1267/3257 [00:03<00:05, 385.19it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1306/3257 [00:03<00:05, 372.00it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1348/3257 [00:03<00:04, 383.79it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1387/3257 [00:03<00:05, 369.68it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1432/3257 [00:03<00:04, 388.74it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1476/3257 [00:03<00:04, 402.77it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:04<00:04, 408.67it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1561/3257 [00:04<00:04, 398.70it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1605/3257 [00:04<00:04, 407.11it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1646/3257 [00:04<00:03, 404.97it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1687/3257 [00:04<00:03, 398.16it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1728/3257 [00:04<00:03, 394.84it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1768/3257 [00:04<00:03, 396.27it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1808/3257 [00:04<00:03, 396.46it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1850/3257 [00:04<00:03, 401.01it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1893/3257 [00:04<00:03, 406.67it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1938/3257 [00:05<00:03, 417.17it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1983/3257 [00:05<00:03, 418.65it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2027/3257 [00:05<00:02, 424.69it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2070/3257 [00:05<00:02, 399.41it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2112/3257 [00:05<00:02, 401.36it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2153/3257 [00:05<00:02, 390.14it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2196/3257 [00:05<00:02, 399.10it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2237/3257 [00:05<00:02, 397.16it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2277/3257 [00:06<00:03, 280.42it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2323/3257 [00:06<00:02, 320.12it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2368/3257 [00:06<00:02, 350.06it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2410/3257 [00:06<00:02, 366.39it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2450/3257 [00:06<00:02, 369.55it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2497/3257 [00:06<00:01, 395.58it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2541/3257 [00:06<00:01, 407.32it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2583/3257 [00:06<00:01, 396.85it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2631/3257 [00:06<00:01, 420.22it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2674/3257 [00:06<00:01, 414.45it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2716/3257 [00:07<00:01, 398.78it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:07<00:01, 411.52it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2806/3257 [00:07<00:01, 420.14it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2849/3257 [00:07<00:00, 413.16it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2894/3257 [00:07<00:00, 421.32it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2937/3257 [00:07<00:00, 423.78it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2980/3257 [00:07<00:00, 409.67it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3023/3257 [00:07<00:00, 413.85it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3071/3257 [00:07<00:00, 430.38it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3116/3257 [00:08<00:00, 434.48it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3160/3257 [00:08<00:00, 418.80it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3203/3257 [00:08<00:00, 415.07it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:08<00:00, 418.35it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:08<00:00, 389.27it/s]
2023-02-07 14:50:36.880 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:50:36,881][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d102,n5,w5,mc2,s0.658674,t4>', 'datetime': '2023-02-07T14:50:36.881823', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:50:36,882][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:50:36,882][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:50:37,021][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 14:50:37,022][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:50:37,024][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 retains 881 unique words (95.35% of original 924, drops 43)', 'datetime': '2023-02-07T14:50:37.024488', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:50:37,024][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=2 leaves 1455705 word corpus (100.00% of original 1455748, drops 43)', 'datetime': '2023-02-07T14:50:37.024676', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:50:37,027][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 14:50:37,027][gensim.models.word2vec][INFO] - sample=0.658674 downsamples 0 most-common words
[2023-02-07 14:50:37,028][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455705 word corpus (100.0%% of prior 1455705)', 'datetime': '2023-02-07T14:50:37.027996', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:50:37,033][gensim.models.word2vec][INFO] - estimated required memory for 881 words and 102 dimensions: 3139652 bytes
[2023-02-07 14:50:37,033][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:50:37,035][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 881 vocabulary and 102 features, using sg=0 hs=0 sample=0.6586739979056017 negative=5 window=5 shrink_windows=True', 'datetime': '2023-02-07T14:50:37.035602', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:50:37,640][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458962 effective words) took 0.6s, 2420647 effective words/s
[2023-02-07 14:50:38,212][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458962 effective words) took 0.6s, 2555209 effective words/s
[2023-02-07 14:50:38,793][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458962 effective words) took 0.6s, 2524731 effective words/s
[2023-02-07 14:50:39,368][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458962 effective words) took 0.6s, 2540359 effective words/s
[2023-02-07 14:50:39,932][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458962 effective words) took 0.6s, 2598732 effective words/s
[2023-02-07 14:50:40,498][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458962 effective words) took 0.6s, 2583432 effective words/s
[2023-02-07 14:50:41,060][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458962 effective words) took 0.6s, 2603644 effective words/s
[2023-02-07 14:50:41,623][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458962 effective words) took 0.6s, 2594357 effective words/s
[2023-02-07 14:50:42,184][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458962 effective words) took 0.6s, 2611446 effective words/s
[2023-02-07 14:50:42,740][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458962 effective words) took 0.6s, 2630057 effective words/s
[2023-02-07 14:50:43,305][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458962 effective words) took 0.6s, 2586765 effective words/s
[2023-02-07 14:50:43,872][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458962 effective words) took 0.6s, 2579769 effective words/s
[2023-02-07 14:50:44,435][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458962 effective words) took 0.6s, 2594041 effective words/s
[2023-02-07 14:50:44,994][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458962 effective words) took 0.6s, 2616852 effective words/s
[2023-02-07 14:50:45,560][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458962 effective words) took 0.6s, 2587549 effective words/s
[2023-02-07 14:50:46,125][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458962 effective words) took 0.6s, 2588022 effective words/s
[2023-02-07 14:50:46,691][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458962 effective words) took 0.6s, 2584176 effective words/s
[2023-02-07 14:50:47,253][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458962 effective words) took 0.6s, 2601598 effective words/s
[2023-02-07 14:50:47,806][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458962 effective words) took 0.6s, 2643833 effective words/s
[2023-02-07 14:50:48,366][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458962 effective words) took 0.6s, 2613103 effective words/s
[2023-02-07 14:50:48,930][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458962 effective words) took 0.6s, 2594329 effective words/s
[2023-02-07 14:50:49,497][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458962 effective words) took 0.6s, 2575665 effective words/s
[2023-02-07 14:50:50,058][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458962 effective words) took 0.6s, 2604944 effective words/s
[2023-02-07 14:50:50,626][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458962 effective words) took 0.6s, 2576102 effective words/s
[2023-02-07 14:50:51,187][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458962 effective words) took 0.6s, 2605837 effective words/s
[2023-02-07 14:50:51,763][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458962 effective words) took 0.6s, 2537127 effective words/s
[2023-02-07 14:50:52,329][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458962 effective words) took 0.6s, 2582894 effective words/s
[2023-02-07 14:50:52,907][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458962 effective words) took 0.6s, 2533815 effective words/s
[2023-02-07 14:50:53,468][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458962 effective words) took 0.6s, 2609621 effective words/s
[2023-02-07 14:50:54,034][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458962 effective words) took 0.6s, 2583640 effective words/s
[2023-02-07 14:50:54,606][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458962 effective words) took 0.6s, 2558584 effective words/s
[2023-02-07 14:50:55,183][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458962 effective words) took 0.6s, 2535903 effective words/s
[2023-02-07 14:50:55,745][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458962 effective words) took 0.6s, 2601234 effective words/s
[2023-02-07 14:50:56,313][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458962 effective words) took 0.6s, 2576295 effective words/s
[2023-02-07 14:50:56,879][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458962 effective words) took 0.6s, 2583717 effective words/s
[2023-02-07 14:50:57,444][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458962 effective words) took 0.6s, 2589313 effective words/s
[2023-02-07 14:50:58,015][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458962 effective words) took 0.6s, 2559704 effective words/s
[2023-02-07 14:50:58,575][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458962 effective words) took 0.6s, 2611737 effective words/s
[2023-02-07 14:50:59,140][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458962 effective words) took 0.6s, 2588341 effective words/s
[2023-02-07 14:50:59,710][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458962 effective words) took 0.6s, 2569207 effective words/s
[2023-02-07 14:51:00,284][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458962 effective words) took 0.6s, 2548673 effective words/s
[2023-02-07 14:51:00,850][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458962 effective words) took 0.6s, 2580699 effective words/s
[2023-02-07 14:51:01,418][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458962 effective words) took 0.6s, 2576557 effective words/s
[2023-02-07 14:51:01,980][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458962 effective words) took 0.6s, 2601484 effective words/s
[2023-02-07 14:51:02,541][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458962 effective words) took 0.6s, 2608768 effective words/s
[2023-02-07 14:51:03,111][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458962 effective words) took 0.6s, 2566758 effective words/s
[2023-02-07 14:51:03,674][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458962 effective words) took 0.6s, 2600400 effective words/s
[2023-02-07 14:51:04,240][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458962 effective words) took 0.6s, 2582679 effective words/s
[2023-02-07 14:51:04,803][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458962 effective words) took 0.6s, 2598147 effective words/s
[2023-02-07 14:51:05,368][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458962 effective words) took 0.6s, 2586775 effective words/s
[2023-02-07 14:51:05,938][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458962 effective words) took 0.6s, 2568147 effective words/s
[2023-02-07 14:51:06,499][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458962 effective words) took 0.6s, 2603004 effective words/s
[2023-02-07 14:51:07,062][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458962 effective words) took 0.6s, 2601071 effective words/s
[2023-02-07 14:51:07,635][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458962 effective words) took 0.6s, 2553137 effective words/s
[2023-02-07 14:51:08,209][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458962 effective words) took 0.6s, 2551366 effective words/s
[2023-02-07 14:51:08,772][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458962 effective words) took 0.6s, 2605818 effective words/s
[2023-02-07 14:51:09,334][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1458962 effective words) took 0.6s, 2600960 effective words/s
[2023-02-07 14:51:09,899][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1458962 effective words) took 0.6s, 2592056 effective words/s
[2023-02-07 14:51:10,462][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1458962 effective words) took 0.6s, 2594737 effective words/s
[2023-02-07 14:51:11,028][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1458962 effective words) took 0.6s, 2586115 effective words/s
[2023-02-07 14:51:11,596][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1458962 effective words) took 0.6s, 2574959 effective words/s
[2023-02-07 14:51:12,170][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1458962 effective words) took 0.6s, 2550065 effective words/s
[2023-02-07 14:51:12,730][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1458962 effective words) took 0.6s, 2609640 effective words/s
[2023-02-07 14:51:13,298][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1458962 effective words) took 0.6s, 2577635 effective words/s
[2023-02-07 14:51:13,865][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1458962 effective words) took 0.6s, 2583097 effective words/s
[2023-02-07 14:51:14,434][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1458962 effective words) took 0.6s, 2567991 effective words/s
[2023-02-07 14:51:14,999][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1458962 effective words) took 0.6s, 2588726 effective words/s
[2023-02-07 14:51:15,570][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1458962 effective words) took 0.6s, 2559759 effective words/s
[2023-02-07 14:51:16,139][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1458962 effective words) took 0.6s, 2571174 effective words/s
[2023-02-07 14:51:16,707][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1458962 effective words) took 0.6s, 2579339 effective words/s
[2023-02-07 14:51:17,285][gensim.models.word2vec][INFO] - EPOCH 70: training on 1455748 raw words (1458962 effective words) took 0.6s, 2533682 effective words/s
[2023-02-07 14:51:17,858][gensim.models.word2vec][INFO] - EPOCH 71: training on 1455748 raw words (1458962 effective words) took 0.6s, 2553127 effective words/s
[2023-02-07 14:51:18,430][gensim.models.word2vec][INFO] - EPOCH 72: training on 1455748 raw words (1458962 effective words) took 0.6s, 2557860 effective words/s
[2023-02-07 14:51:19,002][gensim.models.word2vec][INFO] - EPOCH 73: training on 1455748 raw words (1458962 effective words) took 0.6s, 2559807 effective words/s
[2023-02-07 14:51:19,570][gensim.models.word2vec][INFO] - EPOCH 74: training on 1455748 raw words (1458962 effective words) took 0.6s, 2576193 effective words/s
[2023-02-07 14:51:20,136][gensim.models.word2vec][INFO] - EPOCH 75: training on 1455748 raw words (1458962 effective words) took 0.6s, 2583728 effective words/s
[2023-02-07 14:51:20,704][gensim.models.word2vec][INFO] - EPOCH 76: training on 1455748 raw words (1458962 effective words) took 0.6s, 2573433 effective words/s
[2023-02-07 14:51:21,275][gensim.models.word2vec][INFO] - EPOCH 77: training on 1455748 raw words (1458962 effective words) took 0.6s, 2564249 effective words/s
[2023-02-07 14:51:21,840][gensim.models.word2vec][INFO] - EPOCH 78: training on 1455748 raw words (1458962 effective words) took 0.6s, 2587757 effective words/s
[2023-02-07 14:51:22,415][gensim.models.word2vec][INFO] - EPOCH 79: training on 1455748 raw words (1458962 effective words) took 0.6s, 2547012 effective words/s
[2023-02-07 14:51:22,981][gensim.models.word2vec][INFO] - EPOCH 80: training on 1455748 raw words (1458962 effective words) took 0.6s, 2579743 effective words/s
[2023-02-07 14:51:23,547][gensim.models.word2vec][INFO] - EPOCH 81: training on 1455748 raw words (1458962 effective words) took 0.6s, 2589018 effective words/s
[2023-02-07 14:51:24,116][gensim.models.word2vec][INFO] - EPOCH 82: training on 1455748 raw words (1458962 effective words) took 0.6s, 2570964 effective words/s
[2023-02-07 14:51:24,689][gensim.models.word2vec][INFO] - EPOCH 83: training on 1455748 raw words (1458962 effective words) took 0.6s, 2554626 effective words/s
[2023-02-07 14:51:25,262][gensim.models.word2vec][INFO] - EPOCH 84: training on 1455748 raw words (1458962 effective words) took 0.6s, 2555341 effective words/s
[2023-02-07 14:51:25,828][gensim.models.word2vec][INFO] - EPOCH 85: training on 1455748 raw words (1458962 effective words) took 0.6s, 2582268 effective words/s
[2023-02-07 14:51:25,828][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 125194328 raw words (125470732 effective words) took 48.8s, 2571487 effective words/s', 'datetime': '2023-02-07T14:51:25.828897', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:51:25.829 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:51:28,481][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145020-5kevlqw9/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:51:28.481477', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:51:28,482][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:51:28,487][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145020-5kevlqw9/files/../tmp/embedding_model.pt
2023-02-07 14:51:28.487 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:51:29.561 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:51:29.976 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:51:30.720 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.4331701951363125, 'test_mae': 1.1683329804771883, 'test_r2': -0.1510349812829419}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.33
wandb: percentage 0.04654
wandb:   test_mae 1.16833
wandb:   test_mse 2.43317
wandb:    test_r2 -0.15103
wandb: 
wandb: üöÄ View run rural-sweep-95 at: https://wandb.ai/xiaoqiz/mof2vec/runs/5kevlqw9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_145020-5kevlqw9/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: r37oibf1 with config:
wandb: 	data.data.wl_step: 3
wandb: 	data.nn.batch_size: 560
wandb: 	model.gensim.alpha: 0.0012658211074515946
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 71
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.6382591232682984
wandb: 	model.gensim.vector_size: 169
wandb: 	model.gensim.window: 1
wandb: 	model.sklearn.learning_rate: 0.005057049584890376
wandb: 	model.sklearn.max_depth: 21
wandb: 	model.sklearn.min_child_weight: 0.04829238902698819
wandb: 	model.sklearn.n_estimators: 3395
wandb: 	model.sklearn.num_leaves: 429
wandb: 	model.sklearn.reg_alpha: 0.01825994941505035
wandb: 	model.sklearn.reg_lambda: 0.5417160044017716
wandb: 	model.sklearn.subsample: 0.5074134346762718
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145149-r37oibf1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-96
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/r37oibf1
2023-02-07 14:51:56.881 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 3 for sweep.
2023-02-07 14:51:56.882 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 560 for sweep.
2023-02-07 14:51:56.882 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0012658211074515946 for sweep.
2023-02-07 14:51:56.882 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:51:56.882 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 71 for sweep.
2023-02-07 14:51:56.882 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 14:51:56.883 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6382591232682984 for sweep.
2023-02-07 14:51:56.883 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 169 for sweep.
2023-02-07 14:51:56.883 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 1 for sweep.
2023-02-07 14:51:56.883 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.005057049584890376 for sweep.
2023-02-07 14:51:56.884 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 21 for sweep.
2023-02-07 14:51:56.884 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.04829238902698819 for sweep.
2023-02-07 14:51:56.884 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 3395 for sweep.
2023-02-07 14:51:56.884 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 429 for sweep.
2023-02-07 14:51:56.884 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.01825994941505035 for sweep.
2023-02-07 14:51:56.885 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.5417160044017716 for sweep.
2023-02-07 14:51:56.885 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.5074134346762718 for sweep.
2023-02-07 14:51:56.885 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:51:56.890 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 3}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145149-r37oibf1/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 560, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 169, 'window': 1, 'min_count': 5, 'dm': 0, 'sample': 0.6382591232682984, 'workers': 4, 'alpha': 0.0012658211074515946, 'epochs': 71}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 3395, 'max_depth': 21, 'num_leaves': 429, 'reg_alpha': 0.01825994941505035, 'reg_lambda': 0.5417160044017716, 'subsample': 0.5074134346762718, 'min_child_weight': 0.04829238902698819, 'n_jobs': 4, 'learning_rate': 0.005057049584890376}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 31/3257 [00:00<00:10, 307.64it/s]  2%|‚ñè         | 62/3257 [00:00<00:10, 307.95it/s]  3%|‚ñé         | 94/3257 [00:00<00:10, 313.11it/s]  4%|‚ñç         | 126/3257 [00:00<00:10, 306.11it/s]  5%|‚ñç         | 160/3257 [00:00<00:09, 316.68it/s]  6%|‚ñå         | 193/3257 [00:00<00:09, 317.19it/s]  7%|‚ñã         | 231/3257 [00:00<00:09, 333.62it/s]  8%|‚ñä         | 265/3257 [00:00<00:09, 327.38it/s]  9%|‚ñâ         | 303/3257 [00:00<00:08, 340.85it/s] 10%|‚ñà         | 338/3257 [00:01<00:08, 338.11it/s] 11%|‚ñà‚ñè        | 372/3257 [00:01<00:08, 336.68it/s] 12%|‚ñà‚ñè        | 406/3257 [00:01<00:12, 232.74it/s] 13%|‚ñà‚ñé        | 434/3257 [00:01<00:12, 232.76it/s] 14%|‚ñà‚ñç        | 467/3257 [00:01<00:10, 254.14it/s] 15%|‚ñà‚ñå        | 499/3257 [00:01<00:10, 269.92it/s] 16%|‚ñà‚ñã        | 532/3257 [00:01<00:09, 283.14it/s] 17%|‚ñà‚ñã        | 562/3257 [00:01<00:09, 284.12it/s] 18%|‚ñà‚ñä        | 592/3257 [00:02<00:09, 285.57it/s] 19%|‚ñà‚ñâ        | 624/3257 [00:02<00:08, 294.60it/s] 20%|‚ñà‚ñà        | 655/3257 [00:02<00:08, 296.49it/s] 21%|‚ñà‚ñà        | 686/3257 [00:02<00:08, 294.52it/s] 22%|‚ñà‚ñà‚ñè       | 720/3257 [00:02<00:08, 302.91it/s] 23%|‚ñà‚ñà‚ñé       | 751/3257 [00:02<00:08, 295.16it/s] 24%|‚ñà‚ñà‚ñç       | 783/3257 [00:02<00:08, 298.44it/s] 25%|‚ñà‚ñà‚ñå       | 817/3257 [00:02<00:07, 310.18it/s] 26%|‚ñà‚ñà‚ñå       | 849/3257 [00:02<00:08, 291.38it/s] 27%|‚ñà‚ñà‚ñã       | 879/3257 [00:02<00:08, 290.86it/s] 28%|‚ñà‚ñà‚ñä       | 911/3257 [00:03<00:07, 297.50it/s] 29%|‚ñà‚ñà‚ñâ       | 941/3257 [00:03<00:08, 269.17it/s] 30%|‚ñà‚ñà‚ñâ       | 969/3257 [00:03<00:08, 257.05it/s] 31%|‚ñà‚ñà‚ñà       | 996/3257 [00:03<00:09, 239.07it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1021/3257 [00:03<00:09, 239.30it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1046/3257 [00:03<00:09, 226.06it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1069/3257 [00:03<00:09, 222.80it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1092/3257 [00:03<00:09, 223.01it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1119/3257 [00:04<00:09, 232.72it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1143/3257 [00:04<00:09, 233.80it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1171/3257 [00:04<00:08, 245.70it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1196/3257 [00:04<00:08, 240.58it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1222/3257 [00:04<00:08, 245.89it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1252/3257 [00:04<00:07, 260.72it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1279/3257 [00:04<00:07, 259.16it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1306/3257 [00:04<00:07, 262.23it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1335/3257 [00:04<00:07, 269.43it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1363/3257 [00:04<00:06, 270.81it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1391/3257 [00:05<00:06, 267.69it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1426/3257 [00:05<00:06, 291.41it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1459/3257 [00:05<00:05, 302.20it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1490/3257 [00:05<00:05, 302.58it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1521/3257 [00:05<00:05, 301.11it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1552/3257 [00:05<00:05, 292.21it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1583/3257 [00:05<00:05, 296.18it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1617/3257 [00:05<00:05, 306.74it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1648/3257 [00:05<00:05, 296.48it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1678/3257 [00:06<00:07, 208.02it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1710/3257 [00:06<00:06, 232.23it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1737/3257 [00:06<00:06, 234.74it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1771/3257 [00:06<00:05, 258.51it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1804/3257 [00:06<00:05, 275.57it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1834/3257 [00:06<00:05, 273.22it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1869/3257 [00:06<00:04, 287.69it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1899/3257 [00:06<00:04, 291.07it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1929/3257 [00:06<00:04, 289.58it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1969/3257 [00:07<00:04, 319.93it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2002/3257 [00:07<00:04, 313.41it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2034/3257 [00:07<00:03, 312.55it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2066/3257 [00:07<00:04, 288.97it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2096/3257 [00:07<00:04, 278.48it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2125/3257 [00:07<00:04, 263.59it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2152/3257 [00:07<00:04, 259.08it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:07<00:04, 265.70it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2209/3257 [00:07<00:03, 266.98it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2238/3257 [00:08<00:03, 273.00it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2266/3257 [00:08<00:03, 272.09it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2297/3257 [00:08<00:03, 281.98it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2333/3257 [00:08<00:03, 303.22it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2368/3257 [00:08<00:02, 315.28it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2404/3257 [00:08<00:02, 325.91it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2437/3257 [00:08<00:02, 306.27it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2471/3257 [00:08<00:02, 315.10it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2504/3257 [00:08<00:02, 317.42it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2538/3257 [00:09<00:02, 322.61it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2571/3257 [00:09<00:02, 295.62it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2602/3257 [00:09<00:02, 296.06it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2640/3257 [00:09<00:01, 315.38it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2672/3257 [00:09<00:01, 311.68it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2704/3257 [00:09<00:01, 294.97it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2735/3257 [00:09<00:01, 298.80it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2768/3257 [00:09<00:01, 305.11it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2803/3257 [00:09<00:01, 317.83it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2835/3257 [00:10<00:01, 304.03it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2874/3257 [00:10<00:01, 326.52it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2907/3257 [00:10<00:01, 315.21it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2940/3257 [00:10<00:00, 319.26it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2973/3257 [00:10<00:00, 308.68it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3005/3257 [00:10<00:00, 309.35it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3038/3257 [00:10<00:00, 314.17it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3072/3257 [00:10<00:00, 321.18it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3105/3257 [00:10<00:00, 323.10it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3138/3257 [00:10<00:00, 321.18it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3171/3257 [00:11<00:00, 316.18it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3203/3257 [00:11<00:00, 204.03it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3236/3257 [00:11<00:00, 230.33it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:11<00:00, 282.51it/s]
2023-02-07 14:52:08.787 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:52:08,789][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d169,n5,mc5,s0.638259,t4>', 'datetime': '2023-02-07T14:52:08.789021', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:52:08,789][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:52:08,789][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:52:09,043][gensim.models.doc2vec][INFO] - collected 6662 word types and 3257 unique tags from a corpus of 3257 examples and 2911496 words
[2023-02-07 14:52:09,043][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:52:09,053][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 3705 unique words (55.61% of original 6662, drops 2957)', 'datetime': '2023-02-07T14:52:09.053495', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:52:09,053][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 2904334 word corpus (99.75% of original 2911496, drops 7162)', 'datetime': '2023-02-07T14:52:09.053784', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:52:09,065][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 6662 items
[2023-02-07 14:52:09,066][gensim.models.word2vec][INFO] - sample=0.638259 downsamples 0 most-common words
[2023-02-07 14:52:09,066][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2904334 word corpus (100.0%% of prior 2904334)', 'datetime': '2023-02-07T14:52:09.066180', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:52:09,087][gensim.models.word2vec][INFO] - estimated required memory for 3705 words and 169 dimensions: 9714792 bytes
[2023-02-07 14:52:09,087][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:52:09,092][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 3705 vocabulary and 169 features, using sg=1 hs=0 sample=0.6382591232682984 negative=5 window=1 shrink_windows=True', 'datetime': '2023-02-07T14:52:09.092387', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:52:10,099][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 91.86% examples, 2677929 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:52:10,172][gensim.models.word2vec][INFO] - EPOCH 0: training on 2911496 raw words (2907591 effective words) took 1.1s, 2697017 effective words/s
[2023-02-07 14:52:11,176][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 97.39% examples, 2831050 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:52:11,195][gensim.models.word2vec][INFO] - EPOCH 1: training on 2911496 raw words (2907591 effective words) took 1.0s, 2848694 effective words/s
[2023-02-07 14:52:12,189][gensim.models.word2vec][INFO] - EPOCH 2: training on 2911496 raw words (2907591 effective words) took 1.0s, 2929708 effective words/s
[2023-02-07 14:52:13,185][gensim.models.word2vec][INFO] - EPOCH 3: training on 2911496 raw words (2907591 effective words) took 1.0s, 2922357 effective words/s
[2023-02-07 14:52:14,180][gensim.models.word2vec][INFO] - EPOCH 4: training on 2911496 raw words (2907591 effective words) took 1.0s, 2927021 effective words/s
[2023-02-07 14:52:15,178][gensim.models.word2vec][INFO] - EPOCH 5: training on 2911496 raw words (2907591 effective words) took 1.0s, 2918561 effective words/s
[2023-02-07 14:52:16,179][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 99.54% examples, 2896237 words/s, in_qsize 1, out_qsize 1
[2023-02-07 14:52:16,181][gensim.models.word2vec][INFO] - EPOCH 6: training on 2911496 raw words (2907591 effective words) took 1.0s, 2902330 effective words/s
[2023-02-07 14:52:17,186][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 98.93% examples, 2869364 words/s, in_qsize 3, out_qsize 1
[2023-02-07 14:52:17,189][gensim.models.word2vec][INFO] - EPOCH 7: training on 2911496 raw words (2907591 effective words) took 1.0s, 2885760 effective words/s
[2023-02-07 14:52:18,186][gensim.models.word2vec][INFO] - EPOCH 8: training on 2911496 raw words (2907591 effective words) took 1.0s, 2918905 effective words/s
[2023-02-07 14:52:19,173][gensim.models.word2vec][INFO] - EPOCH 9: training on 2911496 raw words (2907591 effective words) took 1.0s, 2950605 effective words/s
[2023-02-07 14:52:20,159][gensim.models.word2vec][INFO] - EPOCH 10: training on 2911496 raw words (2907591 effective words) took 1.0s, 2951684 effective words/s
[2023-02-07 14:52:21,161][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 100.00% examples, 2907106 words/s, in_qsize 0, out_qsize 1
[2023-02-07 14:52:21,161][gensim.models.word2vec][INFO] - EPOCH 11: training on 2911496 raw words (2907591 effective words) took 1.0s, 2906089 effective words/s
[2023-02-07 14:52:22,153][gensim.models.word2vec][INFO] - EPOCH 12: training on 2911496 raw words (2907591 effective words) took 1.0s, 2934965 effective words/s
[2023-02-07 14:52:23,151][gensim.models.word2vec][INFO] - EPOCH 13: training on 2911496 raw words (2907591 effective words) took 1.0s, 2917860 effective words/s
[2023-02-07 14:52:24,151][gensim.models.word2vec][INFO] - EPOCH 14: training on 2911496 raw words (2907591 effective words) took 1.0s, 2911527 effective words/s
[2023-02-07 14:52:25,154][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 96.71% examples, 2810155 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:52:25,181][gensim.models.word2vec][INFO] - EPOCH 15: training on 2911496 raw words (2907591 effective words) took 1.0s, 2825438 effective words/s
[2023-02-07 14:52:26,142][gensim.models.word2vec][INFO] - EPOCH 16: training on 2911496 raw words (2907591 effective words) took 1.0s, 3029568 effective words/s
[2023-02-07 14:52:27,136][gensim.models.word2vec][INFO] - EPOCH 17: training on 2911496 raw words (2907591 effective words) took 1.0s, 2928966 effective words/s
[2023-02-07 14:52:28,103][gensim.models.word2vec][INFO] - EPOCH 18: training on 2911496 raw words (2907591 effective words) took 1.0s, 3009335 effective words/s
[2023-02-07 14:52:29,073][gensim.models.word2vec][INFO] - EPOCH 19: training on 2911496 raw words (2907591 effective words) took 1.0s, 3000665 effective words/s
[2023-02-07 14:52:30,050][gensim.models.word2vec][INFO] - EPOCH 20: training on 2911496 raw words (2907591 effective words) took 1.0s, 2982129 effective words/s
[2023-02-07 14:52:31,022][gensim.models.word2vec][INFO] - EPOCH 21: training on 2911496 raw words (2907591 effective words) took 1.0s, 2994049 effective words/s
[2023-02-07 14:52:31,975][gensim.models.word2vec][INFO] - EPOCH 22: training on 2911496 raw words (2907591 effective words) took 1.0s, 3057079 effective words/s
[2023-02-07 14:52:32,942][gensim.models.word2vec][INFO] - EPOCH 23: training on 2911496 raw words (2907591 effective words) took 1.0s, 3013174 effective words/s
[2023-02-07 14:52:33,896][gensim.models.word2vec][INFO] - EPOCH 24: training on 2911496 raw words (2907591 effective words) took 1.0s, 3051793 effective words/s
[2023-02-07 14:52:34,841][gensim.models.word2vec][INFO] - EPOCH 25: training on 2911496 raw words (2907591 effective words) took 0.9s, 3081688 effective words/s
[2023-02-07 14:52:35,798][gensim.models.word2vec][INFO] - EPOCH 26: training on 2911496 raw words (2907591 effective words) took 1.0s, 3040183 effective words/s
[2023-02-07 14:52:36,761][gensim.models.word2vec][INFO] - EPOCH 27: training on 2911496 raw words (2907591 effective words) took 1.0s, 3022815 effective words/s
[2023-02-07 14:52:37,733][gensim.models.word2vec][INFO] - EPOCH 28: training on 2911496 raw words (2907591 effective words) took 1.0s, 2994912 effective words/s
[2023-02-07 14:52:38,685][gensim.models.word2vec][INFO] - EPOCH 29: training on 2911496 raw words (2907591 effective words) took 1.0s, 3058828 effective words/s
[2023-02-07 14:52:39,650][gensim.models.word2vec][INFO] - EPOCH 30: training on 2911496 raw words (2907591 effective words) took 1.0s, 3018599 effective words/s
[2023-02-07 14:52:40,593][gensim.models.word2vec][INFO] - EPOCH 31: training on 2911496 raw words (2907591 effective words) took 0.9s, 3088466 effective words/s
[2023-02-07 14:52:41,581][gensim.models.word2vec][INFO] - EPOCH 32: training on 2911496 raw words (2907591 effective words) took 1.0s, 2947149 effective words/s
[2023-02-07 14:52:42,575][gensim.models.word2vec][INFO] - EPOCH 33: training on 2911496 raw words (2907591 effective words) took 1.0s, 2928996 effective words/s
[2023-02-07 14:52:43,556][gensim.models.word2vec][INFO] - EPOCH 34: training on 2911496 raw words (2907591 effective words) took 1.0s, 2965440 effective words/s
[2023-02-07 14:52:44,529][gensim.models.word2vec][INFO] - EPOCH 35: training on 2911496 raw words (2907591 effective words) took 1.0s, 2992432 effective words/s
[2023-02-07 14:52:45,510][gensim.models.word2vec][INFO] - EPOCH 36: training on 2911496 raw words (2907591 effective words) took 1.0s, 2968068 effective words/s
[2023-02-07 14:52:46,484][gensim.models.word2vec][INFO] - EPOCH 37: training on 2911496 raw words (2907591 effective words) took 1.0s, 2991558 effective words/s
[2023-02-07 14:52:47,455][gensim.models.word2vec][INFO] - EPOCH 38: training on 2911496 raw words (2907591 effective words) took 1.0s, 2996218 effective words/s
[2023-02-07 14:52:48,353][gensim.models.word2vec][INFO] - EPOCH 39: training on 2911496 raw words (2907591 effective words) took 0.9s, 3242930 effective words/s
[2023-02-07 14:52:49,257][gensim.models.word2vec][INFO] - EPOCH 40: training on 2911496 raw words (2907591 effective words) took 0.9s, 3220251 effective words/s
[2023-02-07 14:52:50,158][gensim.models.word2vec][INFO] - EPOCH 41: training on 2911496 raw words (2907591 effective words) took 0.9s, 3232315 effective words/s
[2023-02-07 14:52:51,065][gensim.models.word2vec][INFO] - EPOCH 42: training on 2911496 raw words (2907591 effective words) took 0.9s, 3211469 effective words/s
[2023-02-07 14:52:51,997][gensim.models.word2vec][INFO] - EPOCH 43: training on 2911496 raw words (2907591 effective words) took 0.9s, 3123142 effective words/s
[2023-02-07 14:52:52,967][gensim.models.word2vec][INFO] - EPOCH 44: training on 2911496 raw words (2907591 effective words) took 1.0s, 3000557 effective words/s
[2023-02-07 14:52:53,847][gensim.models.word2vec][INFO] - EPOCH 45: training on 2911496 raw words (2907591 effective words) took 0.9s, 3309323 effective words/s
[2023-02-07 14:52:54,837][gensim.models.word2vec][INFO] - EPOCH 46: training on 2911496 raw words (2907591 effective words) took 1.0s, 2940333 effective words/s
[2023-02-07 14:52:55,794][gensim.models.word2vec][INFO] - EPOCH 47: training on 2911496 raw words (2907591 effective words) took 1.0s, 3041823 effective words/s
[2023-02-07 14:52:56,798][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 99.82% examples, 2895533 words/s, in_qsize 1, out_qsize 1
[2023-02-07 14:52:56,798][gensim.models.word2vec][INFO] - EPOCH 48: training on 2911496 raw words (2907591 effective words) took 1.0s, 2902595 effective words/s
[2023-02-07 14:52:57,762][gensim.models.word2vec][INFO] - EPOCH 49: training on 2911496 raw words (2907591 effective words) took 1.0s, 3021373 effective words/s
[2023-02-07 14:52:58,743][gensim.models.word2vec][INFO] - EPOCH 50: training on 2911496 raw words (2907591 effective words) took 1.0s, 2967461 effective words/s
[2023-02-07 14:52:59,712][gensim.models.word2vec][INFO] - EPOCH 51: training on 2911496 raw words (2907591 effective words) took 1.0s, 3004273 effective words/s
[2023-02-07 14:53:00,681][gensim.models.word2vec][INFO] - EPOCH 52: training on 2911496 raw words (2907591 effective words) took 1.0s, 3005866 effective words/s
[2023-02-07 14:53:01,645][gensim.models.word2vec][INFO] - EPOCH 53: training on 2911496 raw words (2907591 effective words) took 1.0s, 3020515 effective words/s
[2023-02-07 14:53:02,613][gensim.models.word2vec][INFO] - EPOCH 54: training on 2911496 raw words (2907591 effective words) took 1.0s, 3009574 effective words/s
[2023-02-07 14:53:03,594][gensim.models.word2vec][INFO] - EPOCH 55: training on 2911496 raw words (2907591 effective words) took 1.0s, 2969024 effective words/s
[2023-02-07 14:53:04,572][gensim.models.word2vec][INFO] - EPOCH 56: training on 2911496 raw words (2907591 effective words) took 1.0s, 2975857 effective words/s
[2023-02-07 14:53:05,531][gensim.models.word2vec][INFO] - EPOCH 57: training on 2911496 raw words (2907591 effective words) took 1.0s, 3036374 effective words/s
[2023-02-07 14:53:06,515][gensim.models.word2vec][INFO] - EPOCH 58: training on 2911496 raw words (2907591 effective words) took 1.0s, 2957989 effective words/s
[2023-02-07 14:53:07,485][gensim.models.word2vec][INFO] - EPOCH 59: training on 2911496 raw words (2907591 effective words) took 1.0s, 3001904 effective words/s
[2023-02-07 14:53:08,444][gensim.models.word2vec][INFO] - EPOCH 60: training on 2911496 raw words (2907591 effective words) took 1.0s, 3036296 effective words/s
[2023-02-07 14:53:09,412][gensim.models.word2vec][INFO] - EPOCH 61: training on 2911496 raw words (2907591 effective words) took 1.0s, 3006456 effective words/s
[2023-02-07 14:53:10,395][gensim.models.word2vec][INFO] - EPOCH 62: training on 2911496 raw words (2907591 effective words) took 1.0s, 2964164 effective words/s
[2023-02-07 14:53:11,370][gensim.models.word2vec][INFO] - EPOCH 63: training on 2911496 raw words (2907591 effective words) took 1.0s, 2985330 effective words/s
[2023-02-07 14:53:12,333][gensim.models.word2vec][INFO] - EPOCH 64: training on 2911496 raw words (2907591 effective words) took 1.0s, 3023815 effective words/s
[2023-02-07 14:53:13,312][gensim.models.word2vec][INFO] - EPOCH 65: training on 2911496 raw words (2907591 effective words) took 1.0s, 2982892 effective words/s
[2023-02-07 14:53:14,285][gensim.models.word2vec][INFO] - EPOCH 66: training on 2911496 raw words (2907591 effective words) took 1.0s, 2991071 effective words/s
[2023-02-07 14:53:15,271][gensim.models.word2vec][INFO] - EPOCH 67: training on 2911496 raw words (2907591 effective words) took 1.0s, 2954524 effective words/s
[2023-02-07 14:53:16,249][gensim.models.word2vec][INFO] - EPOCH 68: training on 2911496 raw words (2907591 effective words) took 1.0s, 2975617 effective words/s
[2023-02-07 14:53:17,228][gensim.models.word2vec][INFO] - EPOCH 69: training on 2911496 raw words (2907591 effective words) took 1.0s, 2973282 effective words/s
[2023-02-07 14:53:18,205][gensim.models.word2vec][INFO] - EPOCH 70: training on 2911496 raw words (2907591 effective words) took 1.0s, 2983646 effective words/s
[2023-02-07 14:53:18,205][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 206716216 raw words (206438961 effective words) took 69.1s, 2986980 effective words/s', 'datetime': '2023-02-07T14:53:18.205650', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:53:18.206 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:53:22,512][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145149-r37oibf1/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:53:22.512581', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:53:22,513][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:53:22,535][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145149-r37oibf1/files/../tmp/embedding_model.pt
2023-02-07 14:53:22.535 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:53:23.866 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:53:24.351 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:53:25.500 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8829727968373107, 'test_mae': 1.0459277075058295, 'test_r2': 0.10924128435557245}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.91
wandb: percentage 0.44386
wandb:   test_mae 1.04593
wandb:   test_mse 1.88297
wandb:    test_r2 0.10924
wandb: 
wandb: üöÄ View run winter-sweep-96 at: https://wandb.ai/xiaoqiz/mof2vec/runs/r37oibf1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_145149-r37oibf1/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 6r4p2sv9 with config:
wandb: 	data.data.wl_step: 5
wandb: 	data.nn.batch_size: 794
wandb: 	model.gensim.alpha: 0.004092520447001719
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 55
wandb: 	model.gensim.min_count: 6
wandb: 	model.gensim.sample: 0.9769453618490642
wandb: 	model.gensim.vector_size: 205
wandb: 	model.gensim.window: 1
wandb: 	model.sklearn.learning_rate: 0.03243401484128305
wandb: 	model.sklearn.max_depth: 35
wandb: 	model.sklearn.min_child_weight: 0.014833555667498916
wandb: 	model.sklearn.n_estimators: 2615
wandb: 	model.sklearn.num_leaves: 495
wandb: 	model.sklearn.reg_alpha: 0.014691379606334138
wandb: 	model.sklearn.reg_lambda: 0.17202313342105677
wandb: 	model.sklearn.subsample: 0.24447142374582764
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145349-6r4p2sv9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-97
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/6r4p2sv9
2023-02-07 14:53:57.452 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 5 for sweep.
2023-02-07 14:53:57.452 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 794 for sweep.
2023-02-07 14:53:57.452 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.004092520447001719 for sweep.
2023-02-07 14:53:57.453 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:53:57.453 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 55 for sweep.
2023-02-07 14:53:57.453 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 6 for sweep.
2023-02-07 14:53:57.454 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.9769453618490642 for sweep.
2023-02-07 14:53:57.454 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 205 for sweep.
2023-02-07 14:53:57.454 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 1 for sweep.
2023-02-07 14:53:57.454 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.03243401484128305 for sweep.
2023-02-07 14:53:57.454 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 35 for sweep.
2023-02-07 14:53:57.455 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.014833555667498916 for sweep.
2023-02-07 14:53:57.455 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 2615 for sweep.
2023-02-07 14:53:57.455 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 495 for sweep.
2023-02-07 14:53:57.455 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.014691379606334138 for sweep.
2023-02-07 14:53:57.456 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.17202313342105677 for sweep.
2023-02-07 14:53:57.456 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.24447142374582764 for sweep.
2023-02-07 14:53:57.456 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:53:57.462 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 5}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145349-6r4p2sv9/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 794, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 205, 'window': 1, 'min_count': 6, 'dm': 0, 'sample': 0.9769453618490642, 'workers': 4, 'alpha': 0.004092520447001719, 'epochs': 55}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 2615, 'max_depth': 35, 'num_leaves': 495, 'reg_alpha': 0.014691379606334138, 'reg_lambda': 0.17202313342105677, 'subsample': 0.24447142374582764, 'min_child_weight': 0.014833555667498916, 'n_jobs': 4, 'learning_rate': 0.03243401484128305}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 24/3257 [00:00<00:14, 227.51it/s]  1%|‚ñè         | 48/3257 [00:00<00:13, 232.80it/s]  2%|‚ñè         | 72/3257 [00:00<00:13, 229.08it/s]  3%|‚ñé         | 96/3257 [00:00<00:13, 230.67it/s]  4%|‚ñé         | 120/3257 [00:00<00:13, 225.98it/s]  4%|‚ñç         | 145/3257 [00:00<00:13, 233.00it/s]  5%|‚ñå         | 169/3257 [00:00<00:13, 228.99it/s]  6%|‚ñå         | 194/3257 [00:00<00:13, 234.61it/s]  7%|‚ñã         | 221/3257 [00:00<00:12, 243.65it/s]  8%|‚ñä         | 250/3257 [00:01<00:11, 253.19it/s]  8%|‚ñä         | 276/3257 [00:01<00:11, 252.25it/s]  9%|‚ñâ         | 302/3257 [00:01<00:11, 254.22it/s] 10%|‚ñà         | 328/3257 [00:01<00:11, 250.46it/s] 11%|‚ñà         | 354/3257 [00:01<00:11, 242.11it/s] 12%|‚ñà‚ñè        | 379/3257 [00:01<00:12, 227.86it/s] 12%|‚ñà‚ñè        | 402/3257 [00:01<00:12, 227.46it/s] 13%|‚ñà‚ñé        | 425/3257 [00:01<00:13, 213.25it/s] 14%|‚ñà‚ñé        | 447/3257 [00:01<00:13, 203.40it/s] 15%|‚ñà‚ñç        | 473/3257 [00:02<00:12, 217.32it/s] 15%|‚ñà‚ñå        | 495/3257 [00:02<00:12, 216.34it/s] 16%|‚ñà‚ñå        | 521/3257 [00:02<00:12, 226.98it/s] 17%|‚ñà‚ñã        | 544/3257 [00:02<00:12, 224.66it/s] 17%|‚ñà‚ñã        | 567/3257 [00:02<00:12, 210.20it/s] 18%|‚ñà‚ñä        | 589/3257 [00:02<00:13, 204.45it/s] 19%|‚ñà‚ñâ        | 615/3257 [00:02<00:12, 219.26it/s] 20%|‚ñà‚ñâ        | 638/3257 [00:02<00:11, 221.98it/s] 20%|‚ñà‚ñà        | 661/3257 [00:02<00:12, 203.69it/s] 21%|‚ñà‚ñà        | 683/3257 [00:03<00:12, 207.15it/s] 22%|‚ñà‚ñà‚ñè       | 709/3257 [00:03<00:11, 219.78it/s] 22%|‚ñà‚ñà‚ñè       | 732/3257 [00:03<00:11, 213.89it/s] 23%|‚ñà‚ñà‚ñé       | 754/3257 [00:03<00:12, 207.80it/s] 24%|‚ñà‚ñà‚ñç       | 776/3257 [00:03<00:11, 208.69it/s] 25%|‚ñà‚ñà‚ñç       | 799/3257 [00:03<00:11, 210.93it/s] 25%|‚ñà‚ñà‚ñå       | 821/3257 [00:03<00:11, 207.71it/s] 26%|‚ñà‚ñà‚ñå       | 842/3257 [00:03<00:12, 194.19it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:03<00:11, 200.99it/s] 27%|‚ñà‚ñà‚ñã       | 885/3257 [00:04<00:11, 197.75it/s] 28%|‚ñà‚ñà‚ñä       | 911/3257 [00:04<00:10, 214.12it/s] 29%|‚ñà‚ñà‚ñä       | 933/3257 [00:04<00:11, 209.45it/s] 29%|‚ñà‚ñà‚ñâ       | 958/3257 [00:04<00:10, 219.05it/s] 30%|‚ñà‚ñà‚ñà       | 981/3257 [00:04<00:10, 213.64it/s] 31%|‚ñà‚ñà‚ñà       | 1003/3257 [00:04<00:10, 210.34it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 1025/3257 [00:04<00:10, 208.00it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1046/3257 [00:04<00:11, 195.22it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1067/3257 [00:04<00:11, 199.08it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1088/3257 [00:05<00:10, 200.84it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1111/3257 [00:05<00:10, 206.89it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1132/3257 [00:05<00:10, 196.53it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1153/3257 [00:05<00:10, 199.93it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1175/3257 [00:05<00:10, 204.63it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1196/3257 [00:05<00:15, 129.59it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1214/3257 [00:05<00:14, 139.27it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1241/3257 [00:05<00:12, 166.84it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 1261/3257 [00:06<00:11, 174.17it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1281/3257 [00:06<00:11, 171.40it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1302/3257 [00:06<00:10, 178.81it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1327/3257 [00:06<00:09, 195.52it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1349/3257 [00:06<00:09, 200.64it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1370/3257 [00:06<00:09, 193.93it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1390/3257 [00:06<00:09, 193.09it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1420/3257 [00:06<00:08, 220.66it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1443/3257 [00:06<00:08, 216.01it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1471/3257 [00:07<00:07, 230.79it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1495/3257 [00:07<00:07, 227.01it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:07<00:07, 222.34it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1542/3257 [00:07<00:07, 216.97it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1564/3257 [00:07<00:08, 211.54it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1586/3257 [00:07<00:08, 205.21it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1610/3257 [00:07<00:07, 213.03it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1634/3257 [00:07<00:07, 205.54it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1655/3257 [00:07<00:07, 205.45it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1676/3257 [00:08<00:08, 195.67it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1699/3257 [00:08<00:07, 203.39it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1720/3257 [00:08<00:07, 203.21it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1741/3257 [00:08<00:08, 187.08it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1765/3257 [00:08<00:07, 200.56it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1789/3257 [00:08<00:06, 210.67it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1811/3257 [00:08<00:07, 198.17it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1832/3257 [00:08<00:07, 197.20it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1855/3257 [00:08<00:06, 205.72it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1878/3257 [00:09<00:06, 212.48it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1900/3257 [00:09<00:06, 208.70it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1922/3257 [00:09<00:06, 208.31it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1950/3257 [00:09<00:05, 228.58it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1975/3257 [00:09<00:05, 233.59it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1999/3257 [00:09<00:05, 226.44it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2023/3257 [00:09<00:05, 227.92it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2046/3257 [00:09<00:05, 221.46it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2069/3257 [00:09<00:05, 205.04it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2093/3257 [00:09<00:05, 212.46it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2115/3257 [00:10<00:05, 211.53it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2137/3257 [00:10<00:05, 199.30it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2159/3257 [00:10<00:05, 203.11it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2181/3257 [00:10<00:05, 203.96it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2204/3257 [00:10<00:05, 209.90it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2226/3257 [00:10<00:05, 204.90it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2247/3257 [00:10<00:04, 202.54it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2268/3257 [00:10<00:04, 204.17it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2290/3257 [00:10<00:04, 207.31it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2311/3257 [00:11<00:04, 205.85it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2339/3257 [00:11<00:04, 227.21it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2366/3257 [00:11<00:03, 238.81it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2390/3257 [00:11<00:03, 238.90it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2414/3257 [00:11<00:03, 226.32it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2437/3257 [00:11<00:03, 214.58it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2460/3257 [00:11<00:03, 217.71it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2484/3257 [00:11<00:03, 220.83it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2512/3257 [00:11<00:03, 236.33it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2538/3257 [00:12<00:02, 241.99it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2563/3257 [00:12<00:03, 224.63it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2586/3257 [00:12<00:03, 213.92it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2613/3257 [00:12<00:02, 228.51it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2639/3257 [00:12<00:02, 236.15it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2663/3257 [00:12<00:02, 220.92it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2689/3257 [00:12<00:02, 229.87it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2713/3257 [00:13<00:04, 128.63it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2738/3257 [00:13<00:03, 149.42it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2761/3257 [00:13<00:03, 164.37it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2785/3257 [00:13<00:02, 180.21it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2807/3257 [00:13<00:02, 188.18it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2829/3257 [00:13<00:02, 187.37it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2853/3257 [00:13<00:02, 199.56it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2884/3257 [00:13<00:01, 228.22it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2909/3257 [00:13<00:01, 213.65it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2932/3257 [00:14<00:01, 215.58it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2955/3257 [00:14<00:01, 207.48it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2977/3257 [00:14<00:01, 207.82it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3002/3257 [00:14<00:01, 216.55it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3024/3257 [00:14<00:01, 212.66it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3050/3257 [00:14<00:00, 224.88it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3078/3257 [00:14<00:00, 239.75it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3103/3257 [00:14<00:00, 237.19it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3128/3257 [00:14<00:00, 239.01it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3153/3257 [00:15<00:00, 223.65it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3176/3257 [00:15<00:00, 221.95it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3199/3257 [00:15<00:00, 221.22it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3222/3257 [00:15<00:00, 213.40it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3250/3257 [00:15<00:00, 231.26it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:15<00:00, 209.88it/s]
2023-02-07 14:54:13.527 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:54:13,528][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d205,n5,mc6,s0.976945,t4>', 'datetime': '2023-02-07T14:54:13.528721', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:54:13,529][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:54:13,529][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:54:13,927][gensim.models.doc2vec][INFO] - collected 21699 word types and 3257 unique tags from a corpus of 3257 examples and 4367244 words
[2023-02-07 14:54:13,927][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:54:13,957][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 retains 10832 unique words (49.92% of original 21699, drops 10867)', 'datetime': '2023-02-07T14:54:13.957945', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:54:13,958][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=6 leaves 4339647 word corpus (99.37% of original 4367244, drops 27597)', 'datetime': '2023-02-07T14:54:13.958328', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:54:13,994][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 21699 items
[2023-02-07 14:54:13,995][gensim.models.word2vec][INFO] - sample=0.976945 downsamples 0 most-common words
[2023-02-07 14:54:13,995][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4339647 word corpus (100.0%% of prior 4339647)', 'datetime': '2023-02-07T14:54:13.995644', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:54:14,060][gensim.models.word2vec][INFO] - estimated required memory for 10832 words and 205 dimensions: 26502620 bytes
[2023-02-07 14:54:14,060][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:54:14,075][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 10832 vocabulary and 205 features, using sg=1 hs=0 sample=0.9769453618490642 negative=5 window=1 shrink_windows=True', 'datetime': '2023-02-07T14:54:14.075121', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:54:15,082][gensim.models.word2vec][INFO] - EPOCH 0 - PROGRESS: at 54.77% examples, 2425718 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:15,828][gensim.models.word2vec][INFO] - EPOCH 0: training on 4367244 raw words (4341209 effective words) took 1.8s, 2480232 effective words/s
[2023-02-07 14:54:16,830][gensim.models.word2vec][INFO] - EPOCH 1 - PROGRESS: at 61.99% examples, 2729819 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:17,411][gensim.models.word2vec][INFO] - EPOCH 1: training on 4367244 raw words (4341209 effective words) took 1.6s, 2745831 effective words/s
[2023-02-07 14:54:18,421][gensim.models.word2vec][INFO] - EPOCH 2 - PROGRESS: at 62.57% examples, 2733039 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:18,983][gensim.models.word2vec][INFO] - EPOCH 2: training on 4367244 raw words (4341209 effective words) took 1.6s, 2763888 effective words/s
[2023-02-07 14:54:19,988][gensim.models.word2vec][INFO] - EPOCH 3 - PROGRESS: at 63.19% examples, 2781592 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:20,532][gensim.models.word2vec][INFO] - EPOCH 3: training on 4367244 raw words (4341209 effective words) took 1.5s, 2805405 effective words/s
[2023-02-07 14:54:21,536][gensim.models.word2vec][INFO] - EPOCH 4 - PROGRESS: at 64.32% examples, 2839475 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:22,067][gensim.models.word2vec][INFO] - EPOCH 4: training on 4367244 raw words (4341209 effective words) took 1.5s, 2833239 effective words/s
[2023-02-07 14:54:23,070][gensim.models.word2vec][INFO] - EPOCH 5 - PROGRESS: at 63.56% examples, 2806010 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:23,606][gensim.models.word2vec][INFO] - EPOCH 5: training on 4367244 raw words (4341209 effective words) took 1.5s, 2823532 effective words/s
[2023-02-07 14:54:24,608][gensim.models.word2vec][INFO] - EPOCH 6 - PROGRESS: at 64.69% examples, 2860467 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:25,124][gensim.models.word2vec][INFO] - EPOCH 6: training on 4367244 raw words (4341209 effective words) took 1.5s, 2862824 effective words/s
[2023-02-07 14:54:26,129][gensim.models.word2vec][INFO] - EPOCH 7 - PROGRESS: at 65.09% examples, 2877389 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:26,631][gensim.models.word2vec][INFO] - EPOCH 7: training on 4367244 raw words (4341209 effective words) took 1.5s, 2886611 effective words/s
[2023-02-07 14:54:27,634][gensim.models.word2vec][INFO] - EPOCH 8 - PROGRESS: at 65.09% examples, 2879015 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:28,121][gensim.models.word2vec][INFO] - EPOCH 8: training on 4367244 raw words (4341209 effective words) took 1.5s, 2915560 effective words/s
[2023-02-07 14:54:29,124][gensim.models.word2vec][INFO] - EPOCH 9 - PROGRESS: at 66.38% examples, 2942024 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:29,596][gensim.models.word2vec][INFO] - EPOCH 9: training on 4367244 raw words (4341209 effective words) took 1.5s, 2945716 effective words/s
[2023-02-07 14:54:30,600][gensim.models.word2vec][INFO] - EPOCH 10 - PROGRESS: at 65.37% examples, 2895725 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:31,093][gensim.models.word2vec][INFO] - EPOCH 10: training on 4367244 raw words (4341209 effective words) took 1.5s, 2905878 effective words/s
[2023-02-07 14:54:32,100][gensim.models.word2vec][INFO] - EPOCH 11 - PROGRESS: at 65.27% examples, 2881423 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:32,595][gensim.models.word2vec][INFO] - EPOCH 11: training on 4367244 raw words (4341209 effective words) took 1.5s, 2895994 effective words/s
[2023-02-07 14:54:33,596][gensim.models.word2vec][INFO] - EPOCH 12 - PROGRESS: at 67.18% examples, 2978908 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:34,044][gensim.models.word2vec][INFO] - EPOCH 12: training on 4367244 raw words (4341209 effective words) took 1.4s, 2997732 effective words/s
[2023-02-07 14:54:35,049][gensim.models.word2vec][INFO] - EPOCH 13 - PROGRESS: at 67.79% examples, 2995286 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:35,489][gensim.models.word2vec][INFO] - EPOCH 13: training on 4367244 raw words (4341209 effective words) took 1.4s, 3007504 effective words/s
[2023-02-07 14:54:36,491][gensim.models.word2vec][INFO] - EPOCH 14 - PROGRESS: at 68.62% examples, 3039229 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:36,914][gensim.models.word2vec][INFO] - EPOCH 14: training on 4367244 raw words (4341209 effective words) took 1.4s, 3050047 effective words/s
[2023-02-07 14:54:37,920][gensim.models.word2vec][INFO] - EPOCH 15 - PROGRESS: at 67.06% examples, 2954999 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:38,380][gensim.models.word2vec][INFO] - EPOCH 15: training on 4367244 raw words (4341209 effective words) took 1.5s, 2963505 effective words/s
[2023-02-07 14:54:39,383][gensim.models.word2vec][INFO] - EPOCH 16 - PROGRESS: at 67.92% examples, 3011582 words/s, in_qsize 8, out_qsize 0
[2023-02-07 14:54:39,824][gensim.models.word2vec][INFO] - EPOCH 16: training on 4367244 raw words (4341209 effective words) took 1.4s, 3010143 effective words/s
[2023-02-07 14:54:40,828][gensim.models.word2vec][INFO] - EPOCH 17 - PROGRESS: at 67.06% examples, 2962175 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:41,279][gensim.models.word2vec][INFO] - EPOCH 17: training on 4367244 raw words (4341209 effective words) took 1.5s, 2986337 effective words/s
[2023-02-07 14:54:42,280][gensim.models.word2vec][INFO] - EPOCH 18 - PROGRESS: at 69.36% examples, 3078376 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:42,694][gensim.models.word2vec][INFO] - EPOCH 18: training on 4367244 raw words (4341209 effective words) took 1.4s, 3071072 effective words/s
[2023-02-07 14:54:43,696][gensim.models.word2vec][INFO] - EPOCH 19 - PROGRESS: at 68.62% examples, 3038742 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:44,123][gensim.models.word2vec][INFO] - EPOCH 19: training on 4367244 raw words (4341209 effective words) took 1.4s, 3041888 effective words/s
[2023-02-07 14:54:45,125][gensim.models.word2vec][INFO] - EPOCH 20 - PROGRESS: at 69.36% examples, 3077322 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:45,533][gensim.models.word2vec][INFO] - EPOCH 20: training on 4367244 raw words (4341209 effective words) took 1.4s, 3081842 effective words/s
[2023-02-07 14:54:46,535][gensim.models.word2vec][INFO] - EPOCH 21 - PROGRESS: at 68.38% examples, 3030729 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:46,972][gensim.models.word2vec][INFO] - EPOCH 21: training on 4367244 raw words (4341209 effective words) took 1.4s, 3020403 effective words/s
[2023-02-07 14:54:47,975][gensim.models.word2vec][INFO] - EPOCH 22 - PROGRESS: at 67.79% examples, 3002265 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:48,408][gensim.models.word2vec][INFO] - EPOCH 22: training on 4367244 raw words (4341209 effective words) took 1.4s, 3025564 effective words/s
[2023-02-07 14:54:49,410][gensim.models.word2vec][INFO] - EPOCH 23 - PROGRESS: at 69.17% examples, 3069054 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:49,822][gensim.models.word2vec][INFO] - EPOCH 23: training on 4367244 raw words (4341209 effective words) took 1.4s, 3074801 effective words/s
[2023-02-07 14:54:50,825][gensim.models.word2vec][INFO] - EPOCH 24 - PROGRESS: at 69.70% examples, 3093365 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:51,227][gensim.models.word2vec][INFO] - EPOCH 24: training on 4367244 raw words (4341209 effective words) took 1.4s, 3091602 effective words/s
[2023-02-07 14:54:52,230][gensim.models.word2vec][INFO] - EPOCH 25 - PROGRESS: at 68.62% examples, 3038253 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:52,661][gensim.models.word2vec][INFO] - EPOCH 25: training on 4367244 raw words (4341209 effective words) took 1.4s, 3031456 effective words/s
[2023-02-07 14:54:53,669][gensim.models.word2vec][INFO] - EPOCH 26 - PROGRESS: at 68.62% examples, 3025366 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:54,097][gensim.models.word2vec][INFO] - EPOCH 26: training on 4367244 raw words (4341209 effective words) took 1.4s, 3029345 effective words/s
[2023-02-07 14:54:55,101][gensim.models.word2vec][INFO] - EPOCH 27 - PROGRESS: at 67.64% examples, 2988608 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:55,539][gensim.models.word2vec][INFO] - EPOCH 27: training on 4367244 raw words (4341209 effective words) took 1.4s, 3012470 effective words/s
[2023-02-07 14:54:56,541][gensim.models.word2vec][INFO] - EPOCH 28 - PROGRESS: at 69.70% examples, 3095795 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:56,944][gensim.models.word2vec][INFO] - EPOCH 28: training on 4367244 raw words (4341209 effective words) took 1.4s, 3093064 effective words/s
[2023-02-07 14:54:57,948][gensim.models.word2vec][INFO] - EPOCH 29 - PROGRESS: at 69.82% examples, 3099765 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:58,345][gensim.models.word2vec][INFO] - EPOCH 29: training on 4367244 raw words (4341209 effective words) took 1.4s, 3102725 effective words/s
[2023-02-07 14:54:59,348][gensim.models.word2vec][INFO] - EPOCH 30 - PROGRESS: at 68.90% examples, 3056729 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:54:59,769][gensim.models.word2vec][INFO] - EPOCH 30: training on 4367244 raw words (4341209 effective words) took 1.4s, 3052772 effective words/s
[2023-02-07 14:55:00,773][gensim.models.word2vec][INFO] - EPOCH 31 - PROGRESS: at 69.17% examples, 3060804 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:01,189][gensim.models.word2vec][INFO] - EPOCH 31: training on 4367244 raw words (4341209 effective words) took 1.4s, 3057782 effective words/s
[2023-02-07 14:55:02,192][gensim.models.word2vec][INFO] - EPOCH 32 - PROGRESS: at 68.90% examples, 3055318 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:02,600][gensim.models.word2vec][INFO] - EPOCH 32: training on 4367244 raw words (4341209 effective words) took 1.4s, 3079363 effective words/s
[2023-02-07 14:55:03,607][gensim.models.word2vec][INFO] - EPOCH 33 - PROGRESS: at 70.68% examples, 3118842 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:03,987][gensim.models.word2vec][INFO] - EPOCH 33: training on 4367244 raw words (4341209 effective words) took 1.4s, 3134846 effective words/s
[2023-02-07 14:55:04,991][gensim.models.word2vec][INFO] - EPOCH 34 - PROGRESS: at 70.80% examples, 3136654 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:05,373][gensim.models.word2vec][INFO] - EPOCH 34: training on 4367244 raw words (4341209 effective words) took 1.4s, 3135891 effective words/s
[2023-02-07 14:55:06,378][gensim.models.word2vec][INFO] - EPOCH 35 - PROGRESS: at 70.22% examples, 3108126 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:06,767][gensim.models.word2vec][INFO] - EPOCH 35: training on 4367244 raw words (4341209 effective words) took 1.4s, 3118764 effective words/s
[2023-02-07 14:55:07,769][gensim.models.word2vec][INFO] - EPOCH 36 - PROGRESS: at 69.70% examples, 3095471 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:08,170][gensim.models.word2vec][INFO] - EPOCH 36: training on 4367244 raw words (4341209 effective words) took 1.4s, 3096070 effective words/s
[2023-02-07 14:55:09,174][gensim.models.word2vec][INFO] - EPOCH 37 - PROGRESS: at 69.70% examples, 3092152 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:09,569][gensim.models.word2vec][INFO] - EPOCH 37: training on 4367244 raw words (4341209 effective words) took 1.4s, 3107216 effective words/s
[2023-02-07 14:55:10,572][gensim.models.word2vec][INFO] - EPOCH 38 - PROGRESS: at 71.05% examples, 3150473 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:10,945][gensim.models.word2vec][INFO] - EPOCH 38: training on 4367244 raw words (4341209 effective words) took 1.4s, 3160643 effective words/s
[2023-02-07 14:55:11,953][gensim.models.word2vec][INFO] - EPOCH 39 - PROGRESS: at 70.22% examples, 3096897 words/s, in_qsize 6, out_qsize 1
[2023-02-07 14:55:12,346][gensim.models.word2vec][INFO] - EPOCH 39: training on 4367244 raw words (4341209 effective words) took 1.4s, 3102610 effective words/s
[2023-02-07 14:55:13,349][gensim.models.word2vec][INFO] - EPOCH 40 - PROGRESS: at 71.05% examples, 3150031 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:13,730][gensim.models.word2vec][INFO] - EPOCH 40: training on 4367244 raw words (4341209 effective words) took 1.4s, 3139685 effective words/s
[2023-02-07 14:55:14,735][gensim.models.word2vec][INFO] - EPOCH 41 - PROGRESS: at 69.82% examples, 3100656 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:15,141][gensim.models.word2vec][INFO] - EPOCH 41: training on 4367244 raw words (4341209 effective words) took 1.4s, 3083037 effective words/s
[2023-02-07 14:55:16,145][gensim.models.word2vec][INFO] - EPOCH 42 - PROGRESS: at 69.82% examples, 3096453 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:16,530][gensim.models.word2vec][INFO] - EPOCH 42: training on 4367244 raw words (4341209 effective words) took 1.4s, 3126967 effective words/s
[2023-02-07 14:55:17,537][gensim.models.word2vec][INFO] - EPOCH 43 - PROGRESS: at 72.34% examples, 3171723 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:17,900][gensim.models.word2vec][INFO] - EPOCH 43: training on 4367244 raw words (4341209 effective words) took 1.4s, 3171174 effective words/s
[2023-02-07 14:55:18,906][gensim.models.word2vec][INFO] - EPOCH 44 - PROGRESS: at 71.66% examples, 3161388 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:19,273][gensim.models.word2vec][INFO] - EPOCH 44: training on 4367244 raw words (4341209 effective words) took 1.4s, 3167411 effective words/s
[2023-02-07 14:55:20,277][gensim.models.word2vec][INFO] - EPOCH 45 - PROGRESS: at 69.70% examples, 3091845 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:20,682][gensim.models.word2vec][INFO] - EPOCH 45: training on 4367244 raw words (4341209 effective words) took 1.4s, 3085634 effective words/s
[2023-02-07 14:55:21,685][gensim.models.word2vec][INFO] - EPOCH 46 - PROGRESS: at 70.22% examples, 3110033 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:22,078][gensim.models.word2vec][INFO] - EPOCH 46: training on 4367244 raw words (4341209 effective words) took 1.4s, 3112850 effective words/s
[2023-02-07 14:55:23,080][gensim.models.word2vec][INFO] - EPOCH 47 - PROGRESS: at 69.17% examples, 3067404 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:23,489][gensim.models.word2vec][INFO] - EPOCH 47: training on 4367244 raw words (4341209 effective words) took 1.4s, 3079829 effective words/s
[2023-02-07 14:55:24,492][gensim.models.word2vec][INFO] - EPOCH 48 - PROGRESS: at 70.83% examples, 3141323 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:24,866][gensim.models.word2vec][INFO] - EPOCH 48: training on 4367244 raw words (4341209 effective words) took 1.4s, 3156913 effective words/s
[2023-02-07 14:55:25,868][gensim.models.word2vec][INFO] - EPOCH 49 - PROGRESS: at 70.83% examples, 3142525 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:26,256][gensim.models.word2vec][INFO] - EPOCH 49: training on 4367244 raw words (4341209 effective words) took 1.4s, 3124642 effective words/s
[2023-02-07 14:55:27,260][gensim.models.word2vec][INFO] - EPOCH 50 - PROGRESS: at 71.38% examples, 3154997 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:27,632][gensim.models.word2vec][INFO] - EPOCH 50: training on 4367244 raw words (4341209 effective words) took 1.4s, 3159379 effective words/s
[2023-02-07 14:55:28,634][gensim.models.word2vec][INFO] - EPOCH 51 - PROGRESS: at 70.68% examples, 3132444 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:29,019][gensim.models.word2vec][INFO] - EPOCH 51: training on 4367244 raw words (4341209 effective words) took 1.4s, 3132242 effective words/s
[2023-02-07 14:55:30,021][gensim.models.word2vec][INFO] - EPOCH 52 - PROGRESS: at 69.54% examples, 3086169 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:30,423][gensim.models.word2vec][INFO] - EPOCH 52: training on 4367244 raw words (4341209 effective words) took 1.4s, 3095958 effective words/s
[2023-02-07 14:55:31,426][gensim.models.word2vec][INFO] - EPOCH 53 - PROGRESS: at 70.59% examples, 3134524 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:31,804][gensim.models.word2vec][INFO] - EPOCH 53: training on 4367244 raw words (4341209 effective words) took 1.4s, 3148887 effective words/s
[2023-02-07 14:55:32,811][gensim.models.word2vec][INFO] - EPOCH 54 - PROGRESS: at 71.63% examples, 3155181 words/s, in_qsize 7, out_qsize 0
[2023-02-07 14:55:33,173][gensim.models.word2vec][INFO] - EPOCH 54: training on 4367244 raw words (4341209 effective words) took 1.4s, 3176516 effective words/s
[2023-02-07 14:55:33,173][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 240198420 raw words (238766495 effective words) took 79.1s, 3018613 effective words/s', 'datetime': '2023-02-07T14:55:33.173642', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:55:33.173 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:55:39,442][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145349-6r4p2sv9/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:55:39.442721', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:55:39,443][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:55:39,490][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145349-6r4p2sv9/files/../tmp/embedding_model.pt
2023-02-07 14:55:39.491 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:55:41.013 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:55:41.563 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:55:43.003 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8562378785663143, 'test_mae': 1.0308459702010717, 'test_r2': 0.12188849917563294}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.94
wandb: percentage 0.50081
wandb:   test_mae 1.03085
wandb:   test_mse 1.85624
wandb:    test_r2 0.12189
wandb: 
wandb: üöÄ View run kind-sweep-97 at: https://wandb.ai/xiaoqiz/mof2vec/runs/6r4p2sv9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_145349-6r4p2sv9/logs
wandb: Agent Starting Run: 4d498094 with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 732
wandb: 	model.gensim.alpha: 0.001168273338321576
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 82
wandb: 	model.gensim.min_count: 4
wandb: 	model.gensim.sample: 0.9272914405444412
wandb: 	model.gensim.vector_size: 218
wandb: 	model.gensim.window: 3
wandb: 	model.sklearn.learning_rate: 0.4724392969289591
wandb: 	model.sklearn.max_depth: 10
wandb: 	model.sklearn.min_child_weight: 0.04184245903379421
wandb: 	model.sklearn.n_estimators: 4679
wandb: 	model.sklearn.num_leaves: 398
wandb: 	model.sklearn.reg_alpha: 0.22676697494375425
wandb: 	model.sklearn.reg_lambda: 0.5286317801808018
wandb: 	model.sklearn.subsample: 0.26601703389217124
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145555-4d498094
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-98
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/4d498094
2023-02-07 14:56:03.480 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 14:56:03.481 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 732 for sweep.
2023-02-07 14:56:03.481 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.001168273338321576 for sweep.
2023-02-07 14:56:03.481 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:56:03.482 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 82 for sweep.
2023-02-07 14:56:03.482 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 4 for sweep.
2023-02-07 14:56:03.482 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.9272914405444412 for sweep.
2023-02-07 14:56:03.482 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 218 for sweep.
2023-02-07 14:56:03.483 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 3 for sweep.
2023-02-07 14:56:03.483 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.4724392969289591 for sweep.
2023-02-07 14:56:03.483 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 10 for sweep.
2023-02-07 14:56:03.483 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.04184245903379421 for sweep.
2023-02-07 14:56:03.483 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 4679 for sweep.
2023-02-07 14:56:03.484 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 398 for sweep.
2023-02-07 14:56:03.484 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.22676697494375425 for sweep.
2023-02-07 14:56:03.484 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.5286317801808018 for sweep.
2023-02-07 14:56:03.484 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.26601703389217124 for sweep.
2023-02-07 14:56:03.484 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:56:03.491 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145555-4d498094/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 732, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 218, 'window': 3, 'min_count': 4, 'dm': 0, 'sample': 0.9272914405444412, 'workers': 4, 'alpha': 0.001168273338321576, 'epochs': 82}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 4679, 'max_depth': 10, 'num_leaves': 398, 'reg_alpha': 0.22676697494375425, 'reg_lambda': 0.5286317801808018, 'subsample': 0.26601703389217124, 'min_child_weight': 0.04184245903379421, 'n_jobs': 4, 'learning_rate': 0.4724392969289591}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 40/3257 [00:00<00:08, 396.49it/s]  2%|‚ñè         | 81/3257 [00:00<00:07, 403.40it/s]  4%|‚ñé         | 122/3257 [00:00<00:07, 399.07it/s]  5%|‚ñå         | 165/3257 [00:00<00:07, 410.00it/s]  6%|‚ñã         | 211/3257 [00:00<00:07, 424.36it/s]  8%|‚ñä         | 258/3257 [00:00<00:06, 437.40it/s]  9%|‚ñâ         | 304/3257 [00:00<00:06, 444.32it/s] 11%|‚ñà         | 349/3257 [00:00<00:06, 441.97it/s] 12%|‚ñà‚ñè        | 394/3257 [00:00<00:06, 430.51it/s] 13%|‚ñà‚ñé        | 438/3257 [00:01<00:06, 410.50it/s] 15%|‚ñà‚ñç        | 481/3257 [00:01<00:06, 413.53it/s] 16%|‚ñà‚ñå        | 526/3257 [00:01<00:06, 420.15it/s] 17%|‚ñà‚ñã        | 569/3257 [00:01<00:06, 415.03it/s] 19%|‚ñà‚ñâ        | 611/3257 [00:01<00:06, 413.50it/s] 20%|‚ñà‚ñà        | 653/3257 [00:01<00:06, 414.66it/s] 21%|‚ñà‚ñà‚ñè       | 695/3257 [00:01<00:06, 406.52it/s] 23%|‚ñà‚ñà‚ñé       | 736/3257 [00:01<00:08, 294.00it/s] 24%|‚ñà‚ñà‚ñç       | 778/3257 [00:01<00:07, 322.73it/s] 25%|‚ñà‚ñà‚ñå       | 821/3257 [00:02<00:07, 346.59it/s] 26%|‚ñà‚ñà‚ñã       | 859/3257 [00:02<00:06, 353.65it/s] 28%|‚ñà‚ñà‚ñä       | 902/3257 [00:02<00:06, 373.69it/s] 29%|‚ñà‚ñà‚ñâ       | 945/3257 [00:02<00:05, 387.70it/s] 30%|‚ñà‚ñà‚ñà       | 986/3257 [00:02<00:05, 392.95it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:02<00:05, 395.22it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1068/3257 [00:02<00:05, 398.23it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1109/3257 [00:02<00:05, 400.28it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 1150/3257 [00:02<00:05, 397.39it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1190/3257 [00:03<00:05, 393.11it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1234/3257 [00:03<00:04, 406.25it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1275/3257 [00:03<00:04, 406.84it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 1317/3257 [00:03<00:04, 408.85it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1361/3257 [00:03<00:04, 417.83it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1403/3257 [00:03<00:04, 415.69it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1453/3257 [00:03<00:04, 437.84it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1503/3257 [00:03<00:03, 455.46it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1549/3257 [00:03<00:03, 433.92it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1593/3257 [00:03<00:03, 432.98it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1637/3257 [00:04<00:03, 425.73it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1680/3257 [00:04<00:03, 416.67it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1723/3257 [00:04<00:03, 419.36it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1766/3257 [00:04<00:03, 410.15it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1808/3257 [00:04<00:03, 411.01it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1852/3257 [00:04<00:03, 418.53it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1894/3257 [00:04<00:03, 418.39it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1941/3257 [00:04<00:03, 432.29it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1986/3257 [00:04<00:02, 435.13it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2030/3257 [00:05<00:03, 310.88it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2067/3257 [00:05<00:03, 320.09it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2109/3257 [00:05<00:03, 343.45it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2147/3257 [00:05<00:03, 347.99it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2193/3257 [00:05<00:02, 376.83it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2236/3257 [00:05<00:02, 387.81it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2277/3257 [00:05<00:02, 385.08it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2327/3257 [00:05<00:02, 416.29it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2374/3257 [00:05<00:02, 429.81it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2419/3257 [00:06<00:01, 433.36it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2463/3257 [00:06<00:01, 430.24it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2509/3257 [00:06<00:01, 438.19it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2554/3257 [00:06<00:01, 436.40it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2598/3257 [00:06<00:01, 425.79it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2646/3257 [00:06<00:01, 441.08it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2691/3257 [00:06<00:01, 438.07it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2735/3257 [00:06<00:01, 422.26it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2778/3257 [00:06<00:01, 422.52it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2822/3257 [00:06<00:01, 425.79it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2868/3257 [00:07<00:00, 434.91it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2912/3257 [00:07<00:00, 434.50it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2956/3257 [00:07<00:00, 417.52it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3002/3257 [00:07<00:00, 426.55it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3047/3257 [00:07<00:00, 432.14it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3092/3257 [00:07<00:00, 436.83it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3140/3257 [00:07<00:00, 444.96it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3185/3257 [00:07<00:00, 430.10it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3229/3257 [00:07<00:00, 432.84it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:07<00:00, 407.87it/s]
2023-02-07 14:56:11.645 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:56:11,646][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d218,n5,mc4,s0.927291,t4>', 'datetime': '2023-02-07T14:56:11.646740', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:56:11,647][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:56:11,647][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:56:11,776][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 14:56:11,776][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:56:11,778][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 retains 742 unique words (80.30% of original 924, drops 182)', 'datetime': '2023-02-07T14:56:11.778664', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:56:11,778][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=4 leaves 1455401 word corpus (99.98% of original 1455748, drops 347)', 'datetime': '2023-02-07T14:56:11.778839', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:56:11,781][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 14:56:11,782][gensim.models.word2vec][INFO] - sample=0.927291 downsamples 0 most-common words
[2023-02-07 14:56:11,782][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1455401 word corpus (100.0%% of prior 1455401)', 'datetime': '2023-02-07T14:56:11.782380', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:56:11,787][gensim.models.word2vec][INFO] - estimated required memory for 742 words and 218 dimensions: 5156552 bytes
[2023-02-07 14:56:11,787][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:56:11,791][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 742 vocabulary and 218 features, using sg=1 hs=0 sample=0.9272914405444412 negative=5 window=3 shrink_windows=True', 'datetime': '2023-02-07T14:56:11.791047', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:56:12,653][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458658 effective words) took 0.9s, 1694231 effective words/s
[2023-02-07 14:56:13,491][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458658 effective words) took 0.8s, 1743472 effective words/s
[2023-02-07 14:56:14,365][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458658 effective words) took 0.9s, 1671622 effective words/s
[2023-02-07 14:56:15,228][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458658 effective words) took 0.9s, 1693123 effective words/s
[2023-02-07 14:56:16,127][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458658 effective words) took 0.9s, 1627040 effective words/s
[2023-02-07 14:56:17,008][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458658 effective words) took 0.9s, 1658050 effective words/s
[2023-02-07 14:56:17,902][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458658 effective words) took 0.9s, 1635562 effective words/s
[2023-02-07 14:56:18,803][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458658 effective words) took 0.9s, 1621348 effective words/s
[2023-02-07 14:56:19,703][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458658 effective words) took 0.9s, 1622500 effective words/s
[2023-02-07 14:56:20,596][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458658 effective words) took 0.9s, 1636024 effective words/s
[2023-02-07 14:56:21,488][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458658 effective words) took 0.9s, 1638294 effective words/s
[2023-02-07 14:56:22,375][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458658 effective words) took 0.9s, 1649433 effective words/s
[2023-02-07 14:56:23,273][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458658 effective words) took 0.9s, 1625911 effective words/s
[2023-02-07 14:56:24,175][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458658 effective words) took 0.9s, 1619842 effective words/s
[2023-02-07 14:56:25,065][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458658 effective words) took 0.9s, 1643133 effective words/s
[2023-02-07 14:56:25,981][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458658 effective words) took 0.9s, 1595583 effective words/s
[2023-02-07 14:56:26,878][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458658 effective words) took 0.9s, 1628057 effective words/s
[2023-02-07 14:56:27,782][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458658 effective words) took 0.9s, 1617249 effective words/s
[2023-02-07 14:56:28,691][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458658 effective words) took 0.9s, 1608277 effective words/s
[2023-02-07 14:56:29,585][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458658 effective words) took 0.9s, 1634630 effective words/s
[2023-02-07 14:56:30,474][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458658 effective words) took 0.9s, 1644744 effective words/s
[2023-02-07 14:56:31,362][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458658 effective words) took 0.9s, 1644158 effective words/s
[2023-02-07 14:56:32,263][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458658 effective words) took 0.9s, 1621884 effective words/s
[2023-02-07 14:56:33,148][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458658 effective words) took 0.9s, 1652450 effective words/s
[2023-02-07 14:56:34,038][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458658 effective words) took 0.9s, 1641770 effective words/s
[2023-02-07 14:56:34,934][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458658 effective words) took 0.9s, 1631377 effective words/s
[2023-02-07 14:56:35,827][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458658 effective words) took 0.9s, 1636654 effective words/s
[2023-02-07 14:56:36,717][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458658 effective words) took 0.9s, 1642421 effective words/s
[2023-02-07 14:56:37,614][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458658 effective words) took 0.9s, 1628074 effective words/s
[2023-02-07 14:56:38,510][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458658 effective words) took 0.9s, 1631780 effective words/s
[2023-02-07 14:56:39,397][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458658 effective words) took 0.9s, 1645181 effective words/s
[2023-02-07 14:56:40,294][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458658 effective words) took 0.9s, 1628727 effective words/s
[2023-02-07 14:56:41,177][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458658 effective words) took 0.9s, 1654414 effective words/s
[2023-02-07 14:56:42,066][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458658 effective words) took 0.9s, 1644148 effective words/s
[2023-02-07 14:56:42,956][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458658 effective words) took 0.9s, 1642008 effective words/s
[2023-02-07 14:56:43,842][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458658 effective words) took 0.9s, 1649143 effective words/s
[2023-02-07 14:56:44,739][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458658 effective words) took 0.9s, 1629946 effective words/s
[2023-02-07 14:56:45,631][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458658 effective words) took 0.9s, 1637190 effective words/s
[2023-02-07 14:56:46,524][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458658 effective words) took 0.9s, 1636911 effective words/s
[2023-02-07 14:56:47,420][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458658 effective words) took 0.9s, 1629951 effective words/s
[2023-02-07 14:56:48,322][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458658 effective words) took 0.9s, 1620410 effective words/s
[2023-02-07 14:56:49,220][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458658 effective words) took 0.9s, 1625463 effective words/s
[2023-02-07 14:56:50,123][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458658 effective words) took 0.9s, 1617348 effective words/s
[2023-02-07 14:56:51,038][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458658 effective words) took 0.9s, 1598950 effective words/s
[2023-02-07 14:56:51,945][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458658 effective words) took 0.9s, 1611589 effective words/s
[2023-02-07 14:56:52,852][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458658 effective words) took 0.9s, 1612470 effective words/s
[2023-02-07 14:56:53,745][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458658 effective words) took 0.9s, 1635506 effective words/s
[2023-02-07 14:56:54,635][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458658 effective words) took 0.9s, 1642127 effective words/s
[2023-02-07 14:56:55,518][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458658 effective words) took 0.9s, 1654691 effective words/s
[2023-02-07 14:56:56,424][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458658 effective words) took 0.9s, 1612801 effective words/s
[2023-02-07 14:56:57,310][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458658 effective words) took 0.9s, 1647759 effective words/s
[2023-02-07 14:56:58,204][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458658 effective words) took 0.9s, 1634216 effective words/s
[2023-02-07 14:56:59,099][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458658 effective words) took 0.9s, 1634387 effective words/s
[2023-02-07 14:56:59,976][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458658 effective words) took 0.9s, 1666239 effective words/s
[2023-02-07 14:57:00,859][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458658 effective words) took 0.9s, 1654662 effective words/s
[2023-02-07 14:57:01,742][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458658 effective words) took 0.9s, 1654271 effective words/s
[2023-02-07 14:57:02,611][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1458658 effective words) took 0.9s, 1682329 effective words/s
[2023-02-07 14:57:03,484][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1458658 effective words) took 0.9s, 1674953 effective words/s
[2023-02-07 14:57:04,354][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1458658 effective words) took 0.9s, 1681041 effective words/s
[2023-02-07 14:57:05,232][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1458658 effective words) took 0.9s, 1665749 effective words/s
[2023-02-07 14:57:06,112][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1458658 effective words) took 0.9s, 1658991 effective words/s
[2023-02-07 14:57:06,982][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1458658 effective words) took 0.9s, 1679296 effective words/s
[2023-02-07 14:57:07,861][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1458658 effective words) took 0.9s, 1663762 effective words/s
[2023-02-07 14:57:08,734][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1458658 effective words) took 0.9s, 1674131 effective words/s
[2023-02-07 14:57:09,616][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1458658 effective words) took 0.9s, 1655226 effective words/s
[2023-02-07 14:57:10,493][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1458658 effective words) took 0.9s, 1666516 effective words/s
[2023-02-07 14:57:11,381][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1458658 effective words) took 0.9s, 1646047 effective words/s
[2023-02-07 14:57:12,263][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1458658 effective words) took 0.9s, 1656738 effective words/s
[2023-02-07 14:57:13,143][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1458658 effective words) took 0.9s, 1659835 effective words/s
[2023-02-07 14:57:14,008][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1458658 effective words) took 0.9s, 1691227 effective words/s
[2023-02-07 14:57:14,879][gensim.models.word2vec][INFO] - EPOCH 70: training on 1455748 raw words (1458658 effective words) took 0.9s, 1675510 effective words/s
[2023-02-07 14:57:15,760][gensim.models.word2vec][INFO] - EPOCH 71: training on 1455748 raw words (1458658 effective words) took 0.9s, 1660494 effective words/s
[2023-02-07 14:57:16,637][gensim.models.word2vec][INFO] - EPOCH 72: training on 1455748 raw words (1458658 effective words) took 0.9s, 1664034 effective words/s
[2023-02-07 14:57:17,510][gensim.models.word2vec][INFO] - EPOCH 73: training on 1455748 raw words (1458658 effective words) took 0.9s, 1673590 effective words/s
[2023-02-07 14:57:18,382][gensim.models.word2vec][INFO] - EPOCH 74: training on 1455748 raw words (1458658 effective words) took 0.9s, 1677804 effective words/s
[2023-02-07 14:57:19,245][gensim.models.word2vec][INFO] - EPOCH 75: training on 1455748 raw words (1458658 effective words) took 0.9s, 1692805 effective words/s
[2023-02-07 14:57:20,153][gensim.models.word2vec][INFO] - EPOCH 76: training on 1455748 raw words (1458658 effective words) took 0.9s, 1610166 effective words/s
[2023-02-07 14:57:21,031][gensim.models.word2vec][INFO] - EPOCH 77: training on 1455748 raw words (1458658 effective words) took 0.9s, 1664747 effective words/s
[2023-02-07 14:57:21,968][gensim.models.word2vec][INFO] - EPOCH 78: training on 1455748 raw words (1458658 effective words) took 0.9s, 1558396 effective words/s
[2023-02-07 14:57:22,889][gensim.models.word2vec][INFO] - EPOCH 79: training on 1455748 raw words (1458658 effective words) took 0.9s, 1586577 effective words/s
[2023-02-07 14:57:23,846][gensim.models.word2vec][INFO] - EPOCH 80: training on 1455748 raw words (1458658 effective words) took 1.0s, 1526758 effective words/s
[2023-02-07 14:57:24,767][gensim.models.word2vec][INFO] - EPOCH 81: training on 1455748 raw words (1458658 effective words) took 0.9s, 1587595 effective words/s
[2023-02-07 14:57:24,768][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 119371336 raw words (119609956 effective words) took 73.0s, 1639014 effective words/s', 'datetime': '2023-02-07T14:57:24.768077', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:57:24.768 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:57:27,750][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145555-4d498094/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:57:27.750402', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:57:27,751][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:57:27,771][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145555-4d498094/files/../tmp/embedding_model.pt
2023-02-07 14:57:27.772 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:57:29.272 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:57:29.835 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:57:31.391 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8203833960393407, 'test_mae': 1.020432081850577, 'test_r2': 0.13884981314653355}
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.71
wandb: percentage 0.19697
wandb:   test_mae 1.02043
wandb:   test_mse 1.82038
wandb:    test_r2 0.13885
wandb: 
wandb: üöÄ View run cerulean-sweep-98 at: https://wandb.ai/xiaoqiz/mof2vec/runs/4d498094
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_145555-4d498094/logs
wandb: Agent Starting Run: c0z9avh9 with config:
wandb: 	data.data.wl_step: 1
wandb: 	data.nn.batch_size: 987
wandb: 	model.gensim.alpha: 0.0957257830565661
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 91
wandb: 	model.gensim.min_count: 5
wandb: 	model.gensim.sample: 0.6960467648489821
wandb: 	model.gensim.vector_size: 193
wandb: 	model.gensim.window: 6
wandb: 	model.sklearn.learning_rate: 0.007880682924009454
wandb: 	model.sklearn.max_depth: 73
wandb: 	model.sklearn.min_child_weight: 0.010364426500201003
wandb: 	model.sklearn.n_estimators: 543
wandb: 	model.sklearn.num_leaves: 401
wandb: 	model.sklearn.reg_alpha: 0.3679445103543349
wandb: 	model.sklearn.reg_lambda: 0.2146217387369473
wandb: 	model.sklearn.subsample: 0.27054562523303016
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145740-c0z9avh9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-99
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/c0z9avh9
2023-02-07 14:57:48.838 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 1 for sweep.
2023-02-07 14:57:48.839 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 987 for sweep.
2023-02-07 14:57:48.839 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.0957257830565661 for sweep.
2023-02-07 14:57:48.840 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:57:48.840 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 91 for sweep.
2023-02-07 14:57:48.840 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 5 for sweep.
2023-02-07 14:57:48.840 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.6960467648489821 for sweep.
2023-02-07 14:57:48.840 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 193 for sweep.
2023-02-07 14:57:48.841 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 6 for sweep.
2023-02-07 14:57:48.841 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.007880682924009454 for sweep.
2023-02-07 14:57:48.841 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 73 for sweep.
2023-02-07 14:57:48.841 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.010364426500201003 for sweep.
2023-02-07 14:57:48.842 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 543 for sweep.
2023-02-07 14:57:48.842 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 401 for sweep.
2023-02-07 14:57:48.842 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.3679445103543349 for sweep.
2023-02-07 14:57:48.842 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.2146217387369473 for sweep.
2023-02-07 14:57:48.843 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.27054562523303016 for sweep.
2023-02-07 14:57:48.843 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:57:48.847 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 1}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145740-c0z9avh9/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 987, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 193, 'window': 6, 'min_count': 5, 'dm': 0, 'sample': 0.6960467648489821, 'workers': 4, 'alpha': 0.0957257830565661, 'epochs': 91}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 543, 'max_depth': 73, 'num_leaves': 401, 'reg_alpha': 0.3679445103543349, 'reg_lambda': 0.2146217387369473, 'subsample': 0.27054562523303016, 'min_child_weight': 0.010364426500201003, 'n_jobs': 4, 'learning_rate': 0.007880682924009454}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 38/3257 [00:00<00:08, 378.48it/s]  2%|‚ñè         | 76/3257 [00:00<00:08, 378.25it/s]  4%|‚ñé         | 114/3257 [00:00<00:08, 376.81it/s]  5%|‚ñç         | 156/3257 [00:00<00:07, 392.28it/s]  6%|‚ñå         | 196/3257 [00:00<00:11, 266.21it/s]  7%|‚ñã         | 239/3257 [00:00<00:09, 307.55it/s]  9%|‚ñä         | 283/3257 [00:00<00:08, 342.59it/s] 10%|‚ñâ         | 325/3257 [00:00<00:08, 362.84it/s] 11%|‚ñà         | 365/3257 [00:01<00:07, 368.06it/s] 12%|‚ñà‚ñè        | 404/3257 [00:01<00:07, 370.51it/s] 14%|‚ñà‚ñé        | 443/3257 [00:01<00:07, 356.69it/s] 15%|‚ñà‚ñç        | 483/3257 [00:01<00:07, 365.94it/s] 16%|‚ñà‚ñå        | 525/3257 [00:01<00:07, 379.86it/s] 17%|‚ñà‚ñã        | 564/3257 [00:01<00:07, 370.63it/s] 18%|‚ñà‚ñä        | 602/3257 [00:01<00:07, 372.59it/s] 20%|‚ñà‚ñâ        | 642/3257 [00:01<00:06, 379.25it/s] 21%|‚ñà‚ñà        | 681/3257 [00:01<00:06, 374.77it/s] 22%|‚ñà‚ñà‚ñè       | 720/3257 [00:01<00:06, 374.29it/s] 23%|‚ñà‚ñà‚ñé       | 758/3257 [00:02<00:06, 373.77it/s] 24%|‚ñà‚ñà‚ñç       | 797/3257 [00:02<00:06, 377.01it/s] 26%|‚ñà‚ñà‚ñå       | 835/3257 [00:02<00:06, 375.83it/s] 27%|‚ñà‚ñà‚ñã       | 873/3257 [00:02<00:06, 368.97it/s] 28%|‚ñà‚ñà‚ñä       | 912/3257 [00:02<00:06, 372.51it/s] 29%|‚ñà‚ñà‚ñâ       | 950/3257 [00:02<00:06, 374.10it/s] 30%|‚ñà‚ñà‚ñà       | 988/3257 [00:02<00:06, 373.89it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1027/3257 [00:02<00:06, 369.13it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1064/3257 [00:02<00:05, 366.72it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 1101/3257 [00:03<00:05, 366.71it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1139/3257 [00:03<00:05, 366.05it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1177/3257 [00:03<00:05, 369.32it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1214/3257 [00:03<00:05, 350.58it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1253/3257 [00:03<00:05, 361.71it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1290/3257 [00:03<00:05, 349.72it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1326/3257 [00:03<00:07, 261.92it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1360/3257 [00:03<00:06, 279.52it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1393/3257 [00:03<00:06, 290.83it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1435/3257 [00:04<00:05, 323.49it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1478/3257 [00:04<00:05, 350.93it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1519/3257 [00:04<00:04, 367.07it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1557/3257 [00:04<00:04, 355.83it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1594/3257 [00:04<00:04, 356.36it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1633/3257 [00:04<00:04, 364.88it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1670/3257 [00:04<00:04, 349.97it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1706/3257 [00:04<00:04, 347.77it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1742/3257 [00:04<00:04, 338.02it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1779/3257 [00:05<00:04, 346.12it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1815/3257 [00:05<00:04, 348.69it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1851/3257 [00:05<00:03, 351.58it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1890/3257 [00:05<00:03, 361.81it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1927/3257 [00:05<00:03, 363.63it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1974/3257 [00:05<00:03, 392.74it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2014/3257 [00:05<00:03, 388.76it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2053/3257 [00:05<00:03, 386.69it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2093/3257 [00:05<00:02, 388.91it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2132/3257 [00:05<00:02, 377.46it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2173/3257 [00:06<00:02, 384.36it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2212/3257 [00:06<00:02, 380.79it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2251/3257 [00:06<00:02, 383.22it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2291/3257 [00:06<00:02, 384.32it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2333/3257 [00:06<00:02, 394.17it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2373/3257 [00:06<00:02, 388.20it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2412/3257 [00:06<00:02, 384.20it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2451/3257 [00:06<00:02, 367.79it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2493/3257 [00:06<00:01, 382.50it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2538/3257 [00:07<00:01, 400.16it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2579/3257 [00:07<00:01, 379.54it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2624/3257 [00:07<00:01, 397.97it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2665/3257 [00:07<00:01, 389.49it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2705/3257 [00:07<00:01, 380.49it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2746/3257 [00:07<00:01, 386.50it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2785/3257 [00:07<00:01, 266.16it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2824/3257 [00:07<00:01, 288.88it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2868/3257 [00:08<00:01, 324.40it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2905/3257 [00:08<00:01, 335.84it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2944/3257 [00:08<00:00, 347.48it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2981/3257 [00:08<00:00, 352.17it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3021/3257 [00:08<00:00, 365.15it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3067/3257 [00:08<00:00, 389.66it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3110/3257 [00:08<00:00, 398.78it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3151/3257 [00:08<00:00, 391.89it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3191/3257 [00:08<00:00, 391.39it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3231/3257 [00:08<00:00, 391.57it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:09<00:00, 361.43it/s]
2023-02-07 14:57:58.033 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:57:58,034][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d193,n5,mc5,s0.696047,t4>', 'datetime': '2023-02-07T14:57:58.034061', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:57:58,034][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:57:58,034][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:57:58,188][gensim.models.doc2vec][INFO] - collected 924 word types and 3257 unique tags from a corpus of 3257 examples and 1455748 words
[2023-02-07 14:57:58,188][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:57:58,190][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 599 unique words (64.83% of original 924, drops 325)', 'datetime': '2023-02-07T14:57:58.190666', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:57:58,191][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1454829 word corpus (99.94% of original 1455748, drops 919)', 'datetime': '2023-02-07T14:57:58.191899', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:57:58,194][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 924 items
[2023-02-07 14:57:58,194][gensim.models.word2vec][INFO] - sample=0.696047 downsamples 0 most-common words
[2023-02-07 14:57:58,194][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1454829 word corpus (100.0%% of prior 1454829)', 'datetime': '2023-02-07T14:57:58.194494', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:57:58,198][gensim.models.word2vec][INFO] - estimated required memory for 599 words and 193 dimensions: 4390160 bytes
[2023-02-07 14:57:58,198][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:57:58,201][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 599 vocabulary and 193 features, using sg=1 hs=0 sample=0.6960467648489821 negative=5 window=6 shrink_windows=True', 'datetime': '2023-02-07T14:57:58.201474', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:57:58,610][gensim.models.word2vec][INFO] - EPOCH 0: training on 1455748 raw words (1458086 effective words) took 0.4s, 3582779 effective words/s
[2023-02-07 14:57:58,981][gensim.models.word2vec][INFO] - EPOCH 1: training on 1455748 raw words (1458086 effective words) took 0.4s, 3945087 effective words/s
[2023-02-07 14:57:59,352][gensim.models.word2vec][INFO] - EPOCH 2: training on 1455748 raw words (1458086 effective words) took 0.4s, 3950155 effective words/s
[2023-02-07 14:57:59,715][gensim.models.word2vec][INFO] - EPOCH 3: training on 1455748 raw words (1458086 effective words) took 0.4s, 4023954 effective words/s
[2023-02-07 14:58:00,088][gensim.models.word2vec][INFO] - EPOCH 4: training on 1455748 raw words (1458086 effective words) took 0.4s, 3931993 effective words/s
[2023-02-07 14:58:00,459][gensim.models.word2vec][INFO] - EPOCH 5: training on 1455748 raw words (1458086 effective words) took 0.4s, 3942111 effective words/s
[2023-02-07 14:58:00,831][gensim.models.word2vec][INFO] - EPOCH 6: training on 1455748 raw words (1458086 effective words) took 0.4s, 3937729 effective words/s
[2023-02-07 14:58:01,204][gensim.models.word2vec][INFO] - EPOCH 7: training on 1455748 raw words (1458086 effective words) took 0.4s, 3920749 effective words/s
[2023-02-07 14:58:01,575][gensim.models.word2vec][INFO] - EPOCH 8: training on 1455748 raw words (1458086 effective words) took 0.4s, 3949568 effective words/s
[2023-02-07 14:58:01,944][gensim.models.word2vec][INFO] - EPOCH 9: training on 1455748 raw words (1458086 effective words) took 0.4s, 3972282 effective words/s
[2023-02-07 14:58:02,314][gensim.models.word2vec][INFO] - EPOCH 10: training on 1455748 raw words (1458086 effective words) took 0.4s, 3965461 effective words/s
[2023-02-07 14:58:02,689][gensim.models.word2vec][INFO] - EPOCH 11: training on 1455748 raw words (1458086 effective words) took 0.4s, 3907319 effective words/s
[2023-02-07 14:58:03,055][gensim.models.word2vec][INFO] - EPOCH 12: training on 1455748 raw words (1458086 effective words) took 0.4s, 3997251 effective words/s
[2023-02-07 14:58:03,430][gensim.models.word2vec][INFO] - EPOCH 13: training on 1455748 raw words (1458086 effective words) took 0.4s, 3905905 effective words/s
[2023-02-07 14:58:03,798][gensim.models.word2vec][INFO] - EPOCH 14: training on 1455748 raw words (1458086 effective words) took 0.4s, 3974317 effective words/s
[2023-02-07 14:58:04,168][gensim.models.word2vec][INFO] - EPOCH 15: training on 1455748 raw words (1458086 effective words) took 0.4s, 3963761 effective words/s
[2023-02-07 14:58:04,536][gensim.models.word2vec][INFO] - EPOCH 16: training on 1455748 raw words (1458086 effective words) took 0.4s, 3984533 effective words/s
[2023-02-07 14:58:04,911][gensim.models.word2vec][INFO] - EPOCH 17: training on 1455748 raw words (1458086 effective words) took 0.4s, 3904970 effective words/s
[2023-02-07 14:58:05,287][gensim.models.word2vec][INFO] - EPOCH 18: training on 1455748 raw words (1458086 effective words) took 0.4s, 3901696 effective words/s
[2023-02-07 14:58:05,663][gensim.models.word2vec][INFO] - EPOCH 19: training on 1455748 raw words (1458086 effective words) took 0.4s, 3890399 effective words/s
[2023-02-07 14:58:06,038][gensim.models.word2vec][INFO] - EPOCH 20: training on 1455748 raw words (1458086 effective words) took 0.4s, 3902787 effective words/s
[2023-02-07 14:58:06,419][gensim.models.word2vec][INFO] - EPOCH 21: training on 1455748 raw words (1458086 effective words) took 0.4s, 3845488 effective words/s
[2023-02-07 14:58:06,801][gensim.models.word2vec][INFO] - EPOCH 22: training on 1455748 raw words (1458086 effective words) took 0.4s, 3824803 effective words/s
[2023-02-07 14:58:07,182][gensim.models.word2vec][INFO] - EPOCH 23: training on 1455748 raw words (1458086 effective words) took 0.4s, 3839829 effective words/s
[2023-02-07 14:58:07,570][gensim.models.word2vec][INFO] - EPOCH 24: training on 1455748 raw words (1458086 effective words) took 0.4s, 3778241 effective words/s
[2023-02-07 14:58:07,948][gensim.models.word2vec][INFO] - EPOCH 25: training on 1455748 raw words (1458086 effective words) took 0.4s, 3869975 effective words/s
[2023-02-07 14:58:08,337][gensim.models.word2vec][INFO] - EPOCH 26: training on 1455748 raw words (1458086 effective words) took 0.4s, 3766040 effective words/s
[2023-02-07 14:58:08,704][gensim.models.word2vec][INFO] - EPOCH 27: training on 1455748 raw words (1458086 effective words) took 0.4s, 3986656 effective words/s
[2023-02-07 14:58:09,074][gensim.models.word2vec][INFO] - EPOCH 28: training on 1455748 raw words (1458086 effective words) took 0.4s, 3952382 effective words/s
[2023-02-07 14:58:09,431][gensim.models.word2vec][INFO] - EPOCH 29: training on 1455748 raw words (1458086 effective words) took 0.4s, 4101856 effective words/s
[2023-02-07 14:58:09,784][gensim.models.word2vec][INFO] - EPOCH 30: training on 1455748 raw words (1458086 effective words) took 0.4s, 4141599 effective words/s
[2023-02-07 14:58:10,141][gensim.models.word2vec][INFO] - EPOCH 31: training on 1455748 raw words (1458086 effective words) took 0.4s, 4105594 effective words/s
[2023-02-07 14:58:10,492][gensim.models.word2vec][INFO] - EPOCH 32: training on 1455748 raw words (1458086 effective words) took 0.3s, 4175505 effective words/s
[2023-02-07 14:58:10,835][gensim.models.word2vec][INFO] - EPOCH 33: training on 1455748 raw words (1458086 effective words) took 0.3s, 4257293 effective words/s
[2023-02-07 14:58:11,178][gensim.models.word2vec][INFO] - EPOCH 34: training on 1455748 raw words (1458086 effective words) took 0.3s, 4269778 effective words/s
[2023-02-07 14:58:11,528][gensim.models.word2vec][INFO] - EPOCH 35: training on 1455748 raw words (1458086 effective words) took 0.3s, 4186853 effective words/s
[2023-02-07 14:58:11,880][gensim.models.word2vec][INFO] - EPOCH 36: training on 1455748 raw words (1458086 effective words) took 0.4s, 4148789 effective words/s
[2023-02-07 14:58:12,230][gensim.models.word2vec][INFO] - EPOCH 37: training on 1455748 raw words (1458086 effective words) took 0.3s, 4183914 effective words/s
[2023-02-07 14:58:12,576][gensim.models.word2vec][INFO] - EPOCH 38: training on 1455748 raw words (1458086 effective words) took 0.3s, 4230346 effective words/s
[2023-02-07 14:58:12,921][gensim.models.word2vec][INFO] - EPOCH 39: training on 1455748 raw words (1458086 effective words) took 0.3s, 4241643 effective words/s
[2023-02-07 14:58:13,271][gensim.models.word2vec][INFO] - EPOCH 40: training on 1455748 raw words (1458086 effective words) took 0.3s, 4185945 effective words/s
[2023-02-07 14:58:13,622][gensim.models.word2vec][INFO] - EPOCH 41: training on 1455748 raw words (1458086 effective words) took 0.3s, 4176376 effective words/s
[2023-02-07 14:58:13,966][gensim.models.word2vec][INFO] - EPOCH 42: training on 1455748 raw words (1458086 effective words) took 0.3s, 4245924 effective words/s
[2023-02-07 14:58:14,319][gensim.models.word2vec][INFO] - EPOCH 43: training on 1455748 raw words (1458086 effective words) took 0.4s, 4147212 effective words/s
[2023-02-07 14:58:14,669][gensim.models.word2vec][INFO] - EPOCH 44: training on 1455748 raw words (1458086 effective words) took 0.3s, 4173805 effective words/s
[2023-02-07 14:58:15,020][gensim.models.word2vec][INFO] - EPOCH 45: training on 1455748 raw words (1458086 effective words) took 0.3s, 4166460 effective words/s
[2023-02-07 14:58:15,371][gensim.models.word2vec][INFO] - EPOCH 46: training on 1455748 raw words (1458086 effective words) took 0.3s, 4173463 effective words/s
[2023-02-07 14:58:15,723][gensim.models.word2vec][INFO] - EPOCH 47: training on 1455748 raw words (1458086 effective words) took 0.4s, 4148292 effective words/s
[2023-02-07 14:58:16,099][gensim.models.word2vec][INFO] - EPOCH 48: training on 1455748 raw words (1458086 effective words) took 0.4s, 3893956 effective words/s
[2023-02-07 14:58:16,518][gensim.models.word2vec][INFO] - EPOCH 49: training on 1455748 raw words (1458086 effective words) took 0.4s, 3495594 effective words/s
[2023-02-07 14:58:16,940][gensim.models.word2vec][INFO] - EPOCH 50: training on 1455748 raw words (1458086 effective words) took 0.4s, 3466780 effective words/s
[2023-02-07 14:58:17,358][gensim.models.word2vec][INFO] - EPOCH 51: training on 1455748 raw words (1458086 effective words) took 0.4s, 3498988 effective words/s
[2023-02-07 14:58:17,773][gensim.models.word2vec][INFO] - EPOCH 52: training on 1455748 raw words (1458086 effective words) took 0.4s, 3524525 effective words/s
[2023-02-07 14:58:18,186][gensim.models.word2vec][INFO] - EPOCH 53: training on 1455748 raw words (1458086 effective words) took 0.4s, 3545989 effective words/s
[2023-02-07 14:58:18,598][gensim.models.word2vec][INFO] - EPOCH 54: training on 1455748 raw words (1458086 effective words) took 0.4s, 3547742 effective words/s
[2023-02-07 14:58:19,012][gensim.models.word2vec][INFO] - EPOCH 55: training on 1455748 raw words (1458086 effective words) took 0.4s, 3539786 effective words/s
[2023-02-07 14:58:19,445][gensim.models.word2vec][INFO] - EPOCH 56: training on 1455748 raw words (1458086 effective words) took 0.4s, 3388361 effective words/s
[2023-02-07 14:58:19,862][gensim.models.word2vec][INFO] - EPOCH 57: training on 1455748 raw words (1458086 effective words) took 0.4s, 3506603 effective words/s
[2023-02-07 14:58:20,303][gensim.models.word2vec][INFO] - EPOCH 58: training on 1455748 raw words (1458086 effective words) took 0.4s, 3318679 effective words/s
[2023-02-07 14:58:20,718][gensim.models.word2vec][INFO] - EPOCH 59: training on 1455748 raw words (1458086 effective words) took 0.4s, 3525426 effective words/s
[2023-02-07 14:58:21,159][gensim.models.word2vec][INFO] - EPOCH 60: training on 1455748 raw words (1458086 effective words) took 0.4s, 3324873 effective words/s
[2023-02-07 14:58:21,581][gensim.models.word2vec][INFO] - EPOCH 61: training on 1455748 raw words (1458086 effective words) took 0.4s, 3467855 effective words/s
[2023-02-07 14:58:22,029][gensim.models.word2vec][INFO] - EPOCH 62: training on 1455748 raw words (1458086 effective words) took 0.4s, 3261710 effective words/s
[2023-02-07 14:58:22,448][gensim.models.word2vec][INFO] - EPOCH 63: training on 1455748 raw words (1458086 effective words) took 0.4s, 3492623 effective words/s
[2023-02-07 14:58:22,877][gensim.models.word2vec][INFO] - EPOCH 64: training on 1455748 raw words (1458086 effective words) took 0.4s, 3415143 effective words/s
[2023-02-07 14:58:23,297][gensim.models.word2vec][INFO] - EPOCH 65: training on 1455748 raw words (1458086 effective words) took 0.4s, 3483257 effective words/s
[2023-02-07 14:58:23,730][gensim.models.word2vec][INFO] - EPOCH 66: training on 1455748 raw words (1458086 effective words) took 0.4s, 3386628 effective words/s
[2023-02-07 14:58:24,150][gensim.models.word2vec][INFO] - EPOCH 67: training on 1455748 raw words (1458086 effective words) took 0.4s, 3480872 effective words/s
[2023-02-07 14:58:24,590][gensim.models.word2vec][INFO] - EPOCH 68: training on 1455748 raw words (1458086 effective words) took 0.4s, 3324046 effective words/s
[2023-02-07 14:58:25,011][gensim.models.word2vec][INFO] - EPOCH 69: training on 1455748 raw words (1458086 effective words) took 0.4s, 3475159 effective words/s
[2023-02-07 14:58:25,437][gensim.models.word2vec][INFO] - EPOCH 70: training on 1455748 raw words (1458086 effective words) took 0.4s, 3434179 effective words/s
[2023-02-07 14:58:25,859][gensim.models.word2vec][INFO] - EPOCH 71: training on 1455748 raw words (1458086 effective words) took 0.4s, 3469246 effective words/s
[2023-02-07 14:58:26,292][gensim.models.word2vec][INFO] - EPOCH 72: training on 1455748 raw words (1458086 effective words) took 0.4s, 3382310 effective words/s
[2023-02-07 14:58:26,717][gensim.models.word2vec][INFO] - EPOCH 73: training on 1455748 raw words (1458086 effective words) took 0.4s, 3434909 effective words/s
[2023-02-07 14:58:27,159][gensim.models.word2vec][INFO] - EPOCH 74: training on 1455748 raw words (1458086 effective words) took 0.4s, 3315843 effective words/s
[2023-02-07 14:58:27,580][gensim.models.word2vec][INFO] - EPOCH 75: training on 1455748 raw words (1458086 effective words) took 0.4s, 3471213 effective words/s
[2023-02-07 14:58:28,034][gensim.models.word2vec][INFO] - EPOCH 76: training on 1455748 raw words (1458086 effective words) took 0.5s, 3224467 effective words/s
[2023-02-07 14:58:28,467][gensim.models.word2vec][INFO] - EPOCH 77: training on 1455748 raw words (1458086 effective words) took 0.4s, 3376252 effective words/s
[2023-02-07 14:58:28,900][gensim.models.word2vec][INFO] - EPOCH 78: training on 1455748 raw words (1458086 effective words) took 0.4s, 3379995 effective words/s
[2023-02-07 14:58:29,328][gensim.models.word2vec][INFO] - EPOCH 79: training on 1455748 raw words (1458086 effective words) took 0.4s, 3414610 effective words/s
[2023-02-07 14:58:29,762][gensim.models.word2vec][INFO] - EPOCH 80: training on 1455748 raw words (1458086 effective words) took 0.4s, 3369054 effective words/s
[2023-02-07 14:58:30,183][gensim.models.word2vec][INFO] - EPOCH 81: training on 1455748 raw words (1458086 effective words) took 0.4s, 3482675 effective words/s
[2023-02-07 14:58:30,599][gensim.models.word2vec][INFO] - EPOCH 82: training on 1455748 raw words (1458086 effective words) took 0.4s, 3519087 effective words/s
[2023-02-07 14:58:30,991][gensim.models.word2vec][INFO] - EPOCH 83: training on 1455748 raw words (1458086 effective words) took 0.4s, 3734389 effective words/s
[2023-02-07 14:58:31,379][gensim.models.word2vec][INFO] - EPOCH 84: training on 1455748 raw words (1458086 effective words) took 0.4s, 3762358 effective words/s
[2023-02-07 14:58:31,768][gensim.models.word2vec][INFO] - EPOCH 85: training on 1455748 raw words (1458086 effective words) took 0.4s, 3761203 effective words/s
[2023-02-07 14:58:32,181][gensim.models.word2vec][INFO] - EPOCH 86: training on 1455748 raw words (1458086 effective words) took 0.4s, 3543108 effective words/s
[2023-02-07 14:58:32,595][gensim.models.word2vec][INFO] - EPOCH 87: training on 1455748 raw words (1458086 effective words) took 0.4s, 3530976 effective words/s
[2023-02-07 14:58:32,987][gensim.models.word2vec][INFO] - EPOCH 88: training on 1455748 raw words (1458086 effective words) took 0.4s, 3732882 effective words/s
[2023-02-07 14:58:33,382][gensim.models.word2vec][INFO] - EPOCH 89: training on 1455748 raw words (1458086 effective words) took 0.4s, 3701510 effective words/s
[2023-02-07 14:58:33,773][gensim.models.word2vec][INFO] - EPOCH 90: training on 1455748 raw words (1458086 effective words) took 0.4s, 3739318 effective words/s
[2023-02-07 14:58:33,774][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 132473068 raw words (132685826 effective words) took 35.6s, 3729977 effective words/s', 'datetime': '2023-02-07T14:58:33.774559', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:58:33.774 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:58:35,603][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145740-c0z9avh9/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:58:35.603529', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:58:35,604][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:58:35,610][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145740-c0z9avh9/files/../tmp/embedding_model.pt
2023-02-07 14:58:35.610 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:58:36.994 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:58:37.529 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:58:38.843 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 1.8189271184541034, 'test_mae': 1.0518793871546268, 'test_r2': 0.1395387195150305}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.39
wandb: percentage 0.35173
wandb:   test_mae 1.05188
wandb:   test_mse 1.81893
wandb:    test_r2 0.13954
wandb: 
wandb: üöÄ View run good-sweep-99 at: https://wandb.ai/xiaoqiz/mof2vec/runs/c0z9avh9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_145740-c0z9avh9/logs
wandb: Agent Starting Run: flst14lj with config:
wandb: 	data.data.wl_step: 2
wandb: 	data.nn.batch_size: 858
wandb: 	model.gensim.alpha: 0.07884504377853216
wandb: 	model.gensim.dm: 0
wandb: 	model.gensim.epochs: 66
wandb: 	model.gensim.min_count: 3
wandb: 	model.gensim.sample: 0.7798487589248175
wandb: 	model.gensim.vector_size: 200
wandb: 	model.gensim.window: 13
wandb: 	model.sklearn.learning_rate: 0.13625430579839806
wandb: 	model.sklearn.max_depth: 52
wandb: 	model.sklearn.min_child_weight: 0.004715538168892881
wandb: 	model.sklearn.n_estimators: 880
wandb: 	model.sklearn.num_leaves: 329
wandb: 	model.sklearn.reg_alpha: 0.15822975411850668
wandb: 	model.sklearn.reg_lambda: 0.9492108562060948
wandb: 	model.sklearn.subsample: 0.27380592606219
wandb: WARNING Ignored wandb.init() arg project when running a sweep.
wandb: WARNING Ignored wandb.init() arg entity when running a sweep.
wandb: Tracking run with wandb version 0.13.9
wandb: Run data is saved locally in /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145856-flst14lj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-100
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaoqiz/mof2vec
wandb: üßπ View sweep at https://wandb.ai/xiaoqiz/mof2vec/sweeps/m4fg9fjd
wandb: üöÄ View run at https://wandb.ai/xiaoqiz/mof2vec/runs/flst14lj
2023-02-07 14:59:04.652 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.data.wl_step with 2 for sweep.
2023-02-07 14:59:04.652 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding data.nn.batch_size with 858 for sweep.
2023-02-07 14:59:04.653 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.alpha with 0.07884504377853216 for sweep.
2023-02-07 14:59:04.653 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.dm with 0 for sweep.
2023-02-07 14:59:04.653 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.epochs with 66 for sweep.
2023-02-07 14:59:04.653 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.min_count with 3 for sweep.
2023-02-07 14:59:04.654 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.sample with 0.7798487589248175 for sweep.
2023-02-07 14:59:04.654 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.vector_size with 200 for sweep.
2023-02-07 14:59:04.654 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.gensim.window with 13 for sweep.
2023-02-07 14:59:04.654 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.learning_rate with 0.13625430579839806 for sweep.
2023-02-07 14:59:04.654 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.max_depth with 52 for sweep.
2023-02-07 14:59:04.655 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.min_child_weight with 0.004715538168892881 for sweep.
2023-02-07 14:59:04.655 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.n_estimators with 880 for sweep.
2023-02-07 14:59:04.655 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.num_leaves with 329 for sweep.
2023-02-07 14:59:04.655 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_alpha with 0.15822975411850668 for sweep.
2023-02-07 14:59:04.656 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.reg_lambda with 0.9492108562060948 for sweep.
2023-02-07 14:59:04.656 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:22 - Overriding model.sklearn.subsample with 0.27380592606219 for sweep.
2023-02-07 14:59:04.656 | DEBUG    | mofgraph2vec.trainer.sklearn_workflow:train:25 - Completed overriding config for sweep.
2023-02-07 14:59:04.660 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:32 - {'logger': {'project': 'mof2vec', 'entity': 'xiaoqiz', 'mode': 'online', 'tags': 'co2 uptake, rsm, xgb'}, 'seed': 1234, 'load_model': False, 'model_checkpoint': None, 'sweep': {'count': 100, 'id': None, 'config': {'method': 'bayes', 'metric': {'name': 'test_mse', 'goal': 'minimize'}, 'early_terminate': {'type': 'hyperband', 'min_iter': 5}, 'parameters': {'data.data.wl_step': {'min': 1, 'max': 8, 'distribution': 'int_uniform'}, 'model.gensim.vector_size': {'min': 8, 'max': 256, 'distribution': 'int_uniform'}, 'model.gensim.window': {'min': 1, 'max': 20, 'distribution': 'int_uniform'}, 'model.gensim.min_count': {'min': 1, 'max': 10, 'distribution': 'int_uniform'}, 'model.gensim.dm': {'values': [0, 1]}, 'model.gensim.sample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.gensim.alpha': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}, 'model.gensim.epochs': {'min': 20, 'max': 100, 'distribution': 'int_uniform'}, 'data.nn.batch_size': {'min': 32, 'max': 1024, 'distribution': 'int_uniform'}, 'model.sklearn.n_estimators': {'min': 10, 'max': 5000, 'distribution': 'int_uniform'}, 'model.sklearn.max_depth': {'min': 5, 'max': 100, 'distribution': 'int_uniform'}, 'model.sklearn.num_leaves': {'min': 5, 'max': 500, 'distribution': 'int_uniform'}, 'model.sklearn.reg_alpha': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.reg_lambda': {'min': -6, 'max': 0, 'distribution': 'log_uniform'}, 'model.sklearn.subsample': {'min': 0.2, 'max': 1.0, 'distribution': 'uniform'}, 'model.sklearn.min_child_weight': {'min': 0.001, 'max': 0.1, 'distribution': 'uniform'}, 'model.sklearn.learning_rate': {'min': -8, 'max': 0, 'distribution': 'log_uniform'}}}}, 'data': {'data': {'_target_': 'mofgraph2vec.graph.mof2doc.MOF2doc', 'cif_path': ['../../data/cifs/rsm/'], 'subsample': None, 'wl_step': 2}, 'nn': {'task': ['pure_uptake_CO2_298.00_15000'], 'MOF_id': 'cif.label', 'label_path': '../../data/data.csv', 'embedding_path': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145856-flst14lj/files/../tmp/embedding.csv', 'train_frac': 0.8, 'valid_frac': 0.1, 'test_frac': 0.1, 'batch_size': 858, 'num_workers': None}}, 'model': {'gensim': {'vector_size': 200, 'window': 13, 'min_count': 3, 'dm': 0, 'sample': 0.7798487589248175, 'workers': 4, 'alpha': 0.07884504377853216, 'epochs': 66}, 'cv': False, 'sklearn': {'_target_': 'mofgraph2vec.model.gbt.Regressor', 'n_estimators': 880, 'max_depth': 52, 'num_leaves': 329, 'reg_alpha': 0.15822975411850668, 'reg_lambda': 0.9492108562060948, 'subsample': 0.27380592606219, 'min_child_weight': 0.004715538168892881, 'n_jobs': 4, 'learning_rate': 0.13625430579839806}}, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'auto_lr_find': True, 'max_epochs': 500}}
  0%|          | 0/3257 [00:00<?, ?it/s]  1%|          | 30/3257 [00:00<00:10, 299.74it/s]  2%|‚ñè         | 63/3257 [00:00<00:10, 317.50it/s]  3%|‚ñé         | 96/3257 [00:00<00:09, 318.54it/s]  4%|‚ñç         | 128/3257 [00:00<00:09, 317.70it/s]  5%|‚ñå         | 163/3257 [00:00<00:09, 322.16it/s]  6%|‚ñå         | 199/3257 [00:00<00:09, 332.43it/s]  7%|‚ñã         | 237/3257 [00:00<00:08, 342.29it/s]  8%|‚ñä         | 273/3257 [00:00<00:08, 347.21it/s] 10%|‚ñâ         | 312/3257 [00:00<00:08, 354.66it/s] 11%|‚ñà         | 348/3257 [00:01<00:08, 353.80it/s] 12%|‚ñà‚ñè        | 384/3257 [00:01<00:08, 351.48it/s] 13%|‚ñà‚ñé        | 420/3257 [00:01<00:08, 345.57it/s] 14%|‚ñà‚ñç        | 455/3257 [00:01<00:08, 326.90it/s] 15%|‚ñà‚ñå        | 491/3257 [00:01<00:08, 334.47it/s] 16%|‚ñà‚ñå        | 527/3257 [00:01<00:08, 339.13it/s] 17%|‚ñà‚ñã        | 562/3257 [00:01<00:08, 335.74it/s] 18%|‚ñà‚ñä        | 596/3257 [00:01<00:07, 334.29it/s] 19%|‚ñà‚ñâ        | 632/3257 [00:01<00:07, 340.76it/s] 20%|‚ñà‚ñà        | 667/3257 [00:01<00:07, 327.09it/s] 21%|‚ñà‚ñà‚ñè       | 700/3257 [00:02<00:07, 322.24it/s] 23%|‚ñà‚ñà‚ñé       | 735/3257 [00:02<00:07, 330.10it/s] 24%|‚ñà‚ñà‚ñé       | 769/3257 [00:02<00:07, 332.78it/s] 25%|‚ñà‚ñà‚ñç       | 803/3257 [00:02<00:10, 232.92it/s] 26%|‚ñà‚ñà‚ñå       | 834/3257 [00:02<00:09, 248.92it/s] 27%|‚ñà‚ñà‚ñã       | 864/3257 [00:02<00:09, 260.06it/s] 28%|‚ñà‚ñà‚ñä       | 896/3257 [00:02<00:08, 275.25it/s] 29%|‚ñà‚ñà‚ñä       | 931/3257 [00:02<00:07, 293.96it/s] 30%|‚ñà‚ñà‚ñâ       | 967/3257 [00:03<00:07, 309.27it/s] 31%|‚ñà‚ñà‚ñà       | 1000/3257 [00:03<00:07, 310.61it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 1032/3257 [00:03<00:07, 306.85it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1064/3257 [00:03<00:07, 308.20it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 1096/3257 [00:03<00:06, 310.62it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 1128/3257 [00:03<00:06, 311.82it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 1163/3257 [00:03<00:06, 320.91it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 1196/3257 [00:03<00:06, 301.85it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 1231/3257 [00:03<00:06, 314.23it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 1263/3257 [00:04<00:06, 315.09it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 1295/3257 [00:04<00:06, 304.17it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 1329/3257 [00:04<00:06, 312.57it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1363/3257 [00:04<00:05, 320.01it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1396/3257 [00:04<00:05, 316.94it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1434/3257 [00:04<00:05, 332.99it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1472/3257 [00:04<00:05, 343.85it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1510/3257 [00:04<00:04, 352.22it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1546/3257 [00:04<00:05, 324.18it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1580/3257 [00:04<00:05, 327.50it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1617/3257 [00:05<00:04, 337.46it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1652/3257 [00:05<00:04, 328.68it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1686/3257 [00:05<00:04, 321.65it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1719/3257 [00:05<00:04, 323.48it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1752/3257 [00:05<00:04, 314.87it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1789/3257 [00:05<00:04, 329.41it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1823/3257 [00:05<00:04, 330.12it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1857/3257 [00:05<00:04, 329.03it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1891/3257 [00:05<00:04, 331.88it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1926/3257 [00:06<00:03, 334.58it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1970/3257 [00:06<00:03, 364.99it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2007/3257 [00:06<00:03, 352.06it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2043/3257 [00:06<00:03, 347.40it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2078/3257 [00:06<00:05, 233.32it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2111/3257 [00:06<00:04, 253.78it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2141/3257 [00:06<00:04, 262.47it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2177/3257 [00:06<00:03, 283.91it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2209/3257 [00:07<00:03, 290.37it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2242/3257 [00:07<00:03, 300.11it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2274/3257 [00:07<00:03, 300.55it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2310/3257 [00:07<00:02, 316.34it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2355/3257 [00:07<00:02, 352.94it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2393/3257 [00:07<00:02, 359.20it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2430/3257 [00:07<00:02, 347.06it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2466/3257 [00:07<00:02, 344.87it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 2505/3257 [00:07<00:02, 356.21it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2541/3257 [00:07<00:02, 354.70it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2577/3257 [00:08<00:01, 343.37it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2615/3257 [00:08<00:01, 350.74it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2651/3257 [00:08<00:01, 347.83it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2687/3257 [00:08<00:01, 349.42it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2723/3257 [00:08<00:01, 321.11it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2762/3257 [00:08<00:01, 338.24it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2799/3257 [00:08<00:01, 345.51it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2834/3257 [00:08<00:01, 332.26it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2876/3257 [00:08<00:01, 356.21it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2912/3257 [00:09<00:00, 347.30it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2948/3257 [00:09<00:00, 338.79it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2983/3257 [00:09<00:00, 334.23it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3020/3257 [00:09<00:00, 344.18it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3061/3257 [00:09<00:00, 362.18it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3098/3257 [00:09<00:00, 363.63it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3135/3257 [00:09<00:00, 364.62it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3172/3257 [00:09<00:00, 353.30it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3208/3257 [00:09<00:00, 347.49it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 3246/3257 [00:09<00:00, 354.86it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:10<00:00, 324.93it/s]
2023-02-07 14:59:14.984 | INFO     | mofgraph2vec.trainer.unsupervised:train:22 - Instantiate model. 
[2023-02-07 14:59:14,985][gensim.utils][INFO] - Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d200,n5,mc3,s0.779849,t4>', 'datetime': '2023-02-07T14:59:14.985890', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}
[2023-02-07 14:59:14,986][gensim.models.doc2vec][INFO] - collecting all words and their counts
[2023-02-07 14:59:14,986][gensim.models.doc2vec][INFO] - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags
[2023-02-07 14:59:15,183][gensim.models.doc2vec][INFO] - collected 2819 word types and 3257 unique tags from a corpus of 3257 examples and 2183622 words
[2023-02-07 14:59:15,183][gensim.models.word2vec][INFO] - Creating a fresh vocabulary
[2023-02-07 14:59:15,189][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 retains 2159 unique words (76.59% of original 2819, drops 660)', 'datetime': '2023-02-07T14:59:15.189122', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:59:15,189][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 2182579 word corpus (99.95% of original 2183622, drops 1043)', 'datetime': '2023-02-07T14:59:15.189378', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:59:15,196][gensim.models.word2vec][INFO] - deleting the raw counts dictionary of 2819 items
[2023-02-07 14:59:15,196][gensim.models.word2vec][INFO] - sample=0.779849 downsamples 0 most-common words
[2023-02-07 14:59:15,196][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2182579 word corpus (100.0%% of prior 2182579)', 'datetime': '2023-02-07T14:59:15.196832', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}
[2023-02-07 14:59:15,209][gensim.models.word2vec][INFO] - estimated required memory for 2159 words and 200 dimensions: 7790900 bytes
[2023-02-07 14:59:15,209][gensim.models.word2vec][INFO] - resetting layer weights
[2023-02-07 14:59:15,215][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 2159 vocabulary and 200 features, using sg=1 hs=0 sample=0.7798487589248175 negative=5 window=13 shrink_windows=True', 'datetime': '2023-02-07T14:59:15.215872', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
[2023-02-07 14:59:15,832][gensim.models.word2vec][INFO] - EPOCH 0: training on 2183622 raw words (2185836 effective words) took 0.6s, 3556084 effective words/s
[2023-02-07 14:59:16,404][gensim.models.word2vec][INFO] - EPOCH 1: training on 2183622 raw words (2185836 effective words) took 0.6s, 3836486 effective words/s
[2023-02-07 14:59:16,982][gensim.models.word2vec][INFO] - EPOCH 2: training on 2183622 raw words (2185836 effective words) took 0.6s, 3784056 effective words/s
[2023-02-07 14:59:17,557][gensim.models.word2vec][INFO] - EPOCH 3: training on 2183622 raw words (2185836 effective words) took 0.6s, 3815365 effective words/s
[2023-02-07 14:59:18,137][gensim.models.word2vec][INFO] - EPOCH 4: training on 2183622 raw words (2185836 effective words) took 0.6s, 3775830 effective words/s
[2023-02-07 14:59:18,714][gensim.models.word2vec][INFO] - EPOCH 5: training on 2183622 raw words (2185836 effective words) took 0.6s, 3798885 effective words/s
[2023-02-07 14:59:19,290][gensim.models.word2vec][INFO] - EPOCH 6: training on 2183622 raw words (2185836 effective words) took 0.6s, 3808821 effective words/s
[2023-02-07 14:59:19,867][gensim.models.word2vec][INFO] - EPOCH 7: training on 2183622 raw words (2185836 effective words) took 0.6s, 3794586 effective words/s
[2023-02-07 14:59:20,441][gensim.models.word2vec][INFO] - EPOCH 8: training on 2183622 raw words (2185836 effective words) took 0.6s, 3817550 effective words/s
[2023-02-07 14:59:21,000][gensim.models.word2vec][INFO] - EPOCH 9: training on 2183622 raw words (2185836 effective words) took 0.6s, 3927645 effective words/s
[2023-02-07 14:59:21,563][gensim.models.word2vec][INFO] - EPOCH 10: training on 2183622 raw words (2185836 effective words) took 0.6s, 3889073 effective words/s
[2023-02-07 14:59:22,122][gensim.models.word2vec][INFO] - EPOCH 11: training on 2183622 raw words (2185836 effective words) took 0.6s, 3919508 effective words/s
[2023-02-07 14:59:22,683][gensim.models.word2vec][INFO] - EPOCH 12: training on 2183622 raw words (2185836 effective words) took 0.6s, 3902484 effective words/s
[2023-02-07 14:59:23,247][gensim.models.word2vec][INFO] - EPOCH 13: training on 2183622 raw words (2185836 effective words) took 0.6s, 3889195 effective words/s
[2023-02-07 14:59:23,814][gensim.models.word2vec][INFO] - EPOCH 14: training on 2183622 raw words (2185836 effective words) took 0.6s, 3860686 effective words/s
[2023-02-07 14:59:24,380][gensim.models.word2vec][INFO] - EPOCH 15: training on 2183622 raw words (2185836 effective words) took 0.6s, 3871611 effective words/s
[2023-02-07 14:59:24,950][gensim.models.word2vec][INFO] - EPOCH 16: training on 2183622 raw words (2185836 effective words) took 0.6s, 3848508 effective words/s
[2023-02-07 14:59:25,512][gensim.models.word2vec][INFO] - EPOCH 17: training on 2183622 raw words (2185836 effective words) took 0.6s, 3893486 effective words/s
[2023-02-07 14:59:26,080][gensim.models.word2vec][INFO] - EPOCH 18: training on 2183622 raw words (2185836 effective words) took 0.6s, 3859557 effective words/s
[2023-02-07 14:59:26,648][gensim.models.word2vec][INFO] - EPOCH 19: training on 2183622 raw words (2185836 effective words) took 0.6s, 3854790 effective words/s
[2023-02-07 14:59:27,212][gensim.models.word2vec][INFO] - EPOCH 20: training on 2183622 raw words (2185836 effective words) took 0.6s, 3891379 effective words/s
[2023-02-07 14:59:27,786][gensim.models.word2vec][INFO] - EPOCH 21: training on 2183622 raw words (2185836 effective words) took 0.6s, 3813480 effective words/s
[2023-02-07 14:59:28,353][gensim.models.word2vec][INFO] - EPOCH 22: training on 2183622 raw words (2185836 effective words) took 0.6s, 3860744 effective words/s
[2023-02-07 14:59:28,913][gensim.models.word2vec][INFO] - EPOCH 23: training on 2183622 raw words (2185836 effective words) took 0.6s, 3919329 effective words/s
[2023-02-07 14:59:29,470][gensim.models.word2vec][INFO] - EPOCH 24: training on 2183622 raw words (2185836 effective words) took 0.6s, 3936725 effective words/s
[2023-02-07 14:59:30,035][gensim.models.word2vec][INFO] - EPOCH 25: training on 2183622 raw words (2185836 effective words) took 0.6s, 3874255 effective words/s
[2023-02-07 14:59:30,609][gensim.models.word2vec][INFO] - EPOCH 26: training on 2183622 raw words (2185836 effective words) took 0.6s, 3825960 effective words/s
[2023-02-07 14:59:31,177][gensim.models.word2vec][INFO] - EPOCH 27: training on 2183622 raw words (2185836 effective words) took 0.6s, 3855887 effective words/s
[2023-02-07 14:59:31,735][gensim.models.word2vec][INFO] - EPOCH 28: training on 2183622 raw words (2185836 effective words) took 0.6s, 3929858 effective words/s
[2023-02-07 14:59:32,297][gensim.models.word2vec][INFO] - EPOCH 29: training on 2183622 raw words (2185836 effective words) took 0.6s, 3897341 effective words/s
[2023-02-07 14:59:32,853][gensim.models.word2vec][INFO] - EPOCH 30: training on 2183622 raw words (2185836 effective words) took 0.6s, 3941223 effective words/s
[2023-02-07 14:59:33,409][gensim.models.word2vec][INFO] - EPOCH 31: training on 2183622 raw words (2185836 effective words) took 0.6s, 3946694 effective words/s
[2023-02-07 14:59:33,974][gensim.models.word2vec][INFO] - EPOCH 32: training on 2183622 raw words (2185836 effective words) took 0.6s, 3878089 effective words/s
[2023-02-07 14:59:34,531][gensim.models.word2vec][INFO] - EPOCH 33: training on 2183622 raw words (2185836 effective words) took 0.6s, 3929029 effective words/s
[2023-02-07 14:59:35,095][gensim.models.word2vec][INFO] - EPOCH 34: training on 2183622 raw words (2185836 effective words) took 0.6s, 3887605 effective words/s
[2023-02-07 14:59:35,659][gensim.models.word2vec][INFO] - EPOCH 35: training on 2183622 raw words (2185836 effective words) took 0.6s, 3883951 effective words/s
[2023-02-07 14:59:36,217][gensim.models.word2vec][INFO] - EPOCH 36: training on 2183622 raw words (2185836 effective words) took 0.6s, 3923040 effective words/s
[2023-02-07 14:59:36,778][gensim.models.word2vec][INFO] - EPOCH 37: training on 2183622 raw words (2185836 effective words) took 0.6s, 3911259 effective words/s
[2023-02-07 14:59:37,337][gensim.models.word2vec][INFO] - EPOCH 38: training on 2183622 raw words (2185836 effective words) took 0.6s, 3918455 effective words/s
[2023-02-07 14:59:37,891][gensim.models.word2vec][INFO] - EPOCH 39: training on 2183622 raw words (2185836 effective words) took 0.6s, 3954339 effective words/s
[2023-02-07 14:59:38,447][gensim.models.word2vec][INFO] - EPOCH 40: training on 2183622 raw words (2185836 effective words) took 0.6s, 3943206 effective words/s
[2023-02-07 14:59:39,020][gensim.models.word2vec][INFO] - EPOCH 41: training on 2183622 raw words (2185836 effective words) took 0.6s, 3823184 effective words/s
[2023-02-07 14:59:39,583][gensim.models.word2vec][INFO] - EPOCH 42: training on 2183622 raw words (2185836 effective words) took 0.6s, 3888782 effective words/s
[2023-02-07 14:59:40,168][gensim.models.word2vec][INFO] - EPOCH 43: training on 2183622 raw words (2185836 effective words) took 0.6s, 3748901 effective words/s
[2023-02-07 14:59:40,744][gensim.models.word2vec][INFO] - EPOCH 44: training on 2183622 raw words (2185836 effective words) took 0.6s, 3797304 effective words/s
[2023-02-07 14:59:41,307][gensim.models.word2vec][INFO] - EPOCH 45: training on 2183622 raw words (2185836 effective words) took 0.6s, 3898255 effective words/s
[2023-02-07 14:59:41,894][gensim.models.word2vec][INFO] - EPOCH 46: training on 2183622 raw words (2185836 effective words) took 0.6s, 3730522 effective words/s
[2023-02-07 14:59:42,467][gensim.models.word2vec][INFO] - EPOCH 47: training on 2183622 raw words (2185836 effective words) took 0.6s, 3822892 effective words/s
[2023-02-07 14:59:43,038][gensim.models.word2vec][INFO] - EPOCH 48: training on 2183622 raw words (2185836 effective words) took 0.6s, 3838479 effective words/s
[2023-02-07 14:59:43,606][gensim.models.word2vec][INFO] - EPOCH 49: training on 2183622 raw words (2185836 effective words) took 0.6s, 3854727 effective words/s
[2023-02-07 14:59:44,163][gensim.models.word2vec][INFO] - EPOCH 50: training on 2183622 raw words (2185836 effective words) took 0.6s, 3934955 effective words/s
[2023-02-07 14:59:44,731][gensim.models.word2vec][INFO] - EPOCH 51: training on 2183622 raw words (2185836 effective words) took 0.6s, 3867666 effective words/s
[2023-02-07 14:59:45,307][gensim.models.word2vec][INFO] - EPOCH 52: training on 2183622 raw words (2185836 effective words) took 0.6s, 3801200 effective words/s
[2023-02-07 14:59:45,885][gensim.models.word2vec][INFO] - EPOCH 53: training on 2183622 raw words (2185836 effective words) took 0.6s, 3789803 effective words/s
[2023-02-07 14:59:46,467][gensim.models.word2vec][INFO] - EPOCH 54: training on 2183622 raw words (2185836 effective words) took 0.6s, 3766196 effective words/s
[2023-02-07 14:59:47,051][gensim.models.word2vec][INFO] - EPOCH 55: training on 2183622 raw words (2185836 effective words) took 0.6s, 3751086 effective words/s
[2023-02-07 14:59:47,637][gensim.models.word2vec][INFO] - EPOCH 56: training on 2183622 raw words (2185836 effective words) took 0.6s, 3745249 effective words/s
[2023-02-07 14:59:48,223][gensim.models.word2vec][INFO] - EPOCH 57: training on 2183622 raw words (2185836 effective words) took 0.6s, 3740031 effective words/s
[2023-02-07 14:59:48,805][gensim.models.word2vec][INFO] - EPOCH 58: training on 2183622 raw words (2185836 effective words) took 0.6s, 3766179 effective words/s
[2023-02-07 14:59:49,407][gensim.models.word2vec][INFO] - EPOCH 59: training on 2183622 raw words (2185836 effective words) took 0.6s, 3640063 effective words/s
[2023-02-07 14:59:50,022][gensim.models.word2vec][INFO] - EPOCH 60: training on 2183622 raw words (2185836 effective words) took 0.6s, 3563415 effective words/s
[2023-02-07 14:59:50,628][gensim.models.word2vec][INFO] - EPOCH 61: training on 2183622 raw words (2185836 effective words) took 0.6s, 3613386 effective words/s
[2023-02-07 14:59:51,232][gensim.models.word2vec][INFO] - EPOCH 62: training on 2183622 raw words (2185836 effective words) took 0.6s, 3623448 effective words/s
[2023-02-07 14:59:51,841][gensim.models.word2vec][INFO] - EPOCH 63: training on 2183622 raw words (2185836 effective words) took 0.6s, 3596200 effective words/s
[2023-02-07 14:59:52,457][gensim.models.word2vec][INFO] - EPOCH 64: training on 2183622 raw words (2185836 effective words) took 0.6s, 3556875 effective words/s
[2023-02-07 14:59:53,083][gensim.models.word2vec][INFO] - EPOCH 65: training on 2183622 raw words (2185836 effective words) took 0.6s, 3498442 effective words/s
[2023-02-07 14:59:53,084][gensim.utils][INFO] - Doc2Vec lifecycle event {'msg': 'training on 144119052 raw words (144265176 effective words) took 37.9s, 3809713 effective words/s', 'datetime': '2023-02-07T14:59:53.084238', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}
2023-02-07 14:59:53.084 | INFO     | mofgraph2vec.trainer.unsupervised:train:34 - Evaluating the model performance. 
[2023-02-07 14:59:55,672][gensim.utils][INFO] - Doc2Vec lifecycle event {'fname_or_handle': '/scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145856-flst14lj/files/../tmp/embedding_model.pt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-07T14:59:55.672630', 'gensim': '4.3.0', 'python': '3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}
[2023-02-07 14:59:55,673][gensim.utils][INFO] - not storing attribute cum_table
[2023-02-07 14:59:55,691][gensim.utils][INFO] - saved /scratch/xiazhang/mofgraph2vec/experiments/workflow/wandb/run-20230207_145856-flst14lj/files/../tmp/embedding_model.pt
2023-02-07 14:59:55.691 | INFO     | mofgraph2vec.trainer.unsupervised:train:38 - Saving embedded vectors. 
2023-02-07 14:59:57.073 | INFO     | mofgraph2vec.data.datamodule:__init__:57 - Train: 2637 Valid: 294 Test: 326
2023-02-07 14:59:57.616 | INFO     | mofgraph2vec.trainer.sklearn_supervised:train:26 - Start fitting xgbt model. 
2023-02-07 14:59:58.928 | INFO     | mofgraph2vec.trainer.sklearn_workflow:train:35 - Model performance: {'test_mse': 2.195141618304269, 'test_mae': 1.0898775917369348, 'test_r2': -0.038433232738456224}
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:   accuracy ‚ñÅ
wandb: percentage ‚ñÅ
wandb:   test_mae ‚ñÅ
wandb:   test_mse ‚ñÅ
wandb:    test_r2 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   accuracy 0.93
wandb: percentage 0.23413
wandb:   test_mae 1.08988
wandb:   test_mse 2.19514
wandb:    test_r2 -0.03843
wandb: 
wandb: üöÄ View run cool-sweep-100 at: https://wandb.ai/xiaoqiz/mof2vec/runs/flst14lj
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230207_145856-flst14lj/logs
